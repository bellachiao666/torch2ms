--- pytorch+++ mindspore@@ -1,10 +1,13 @@-import torch.nn as nn
-import torch
-import torch.nn.functional as F
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# import torch.nn as nn
 from models.pointnet2_utils import PointNetSetAbstraction,PointNetFeaturePropagation
 
 
-class get_model(nn.Module):
+class get_model(msnn.Cell):
     def __init__(self, num_classes, normal_channel=False):
         super(get_model, self).__init__()
         if normal_channel:
@@ -23,7 +26,7 @@         self.drop1 = nn.Dropout(0.5)
         self.conv2 = nn.Conv1d(128, num_classes, 1)
 
-    def forward(self, xyz, cls_label):
+    def construct(self, xyz, cls_label):
         # Set Abstraction layers
         B,C,N = xyz.shape
         if self.normal_channel:
@@ -39,21 +42,21 @@         l2_points = self.fp3(l2_xyz, l3_xyz, l2_points, l3_points)
         l1_points = self.fp2(l1_xyz, l2_xyz, l1_points, l2_points)
         cls_label_one_hot = cls_label.view(B,16,1).repeat(1,1,N)
-        l0_points = self.fp1(l0_xyz, l1_xyz, torch.cat([cls_label_one_hot,l0_xyz,l0_points],1), l1_points)
+        l0_points = self.fp1(l0_xyz, l1_xyz, mint.cat([cls_label_one_hot,l0_xyz,l0_points],1), l1_points)
         # FC layers
-        feat =  F.relu(self.bn1(self.conv1(l0_points)))
+        feat =  nn.functional.relu(self.bn1(self.conv1(l0_points)))
         x = self.drop1(feat)
         x = self.conv2(x)
-        x = F.log_softmax(x, dim=1)
+        x = mint.special.log_softmax(x, dim=1)
         x = x.permute(0, 2, 1)
         return x, l3_points
 
 
-class get_loss(nn.Module):
+class get_loss(msnn.Cell):
     def __init__(self):
         super(get_loss, self).__init__()
 
-    def forward(self, pred, target, trans_feat):
-        total_loss = F.nll_loss(pred, target)
+    def construct(self, pred, target, trans_feat):
+        total_loss = nn.functional.nll_loss(pred, target)
 
         return total_loss