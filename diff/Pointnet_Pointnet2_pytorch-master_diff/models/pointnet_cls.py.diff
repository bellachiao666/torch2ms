--- pytorch+++ mindspore@@ -1,9 +1,12 @@-import torch.nn as nn
-import torch.utils.data
-import torch.nn.functional as F
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# import torch.nn as nn
 from pointnet_utils import PointNetEncoder, feature_transform_reguliarzer
 
-class get_model(nn.Module):
+class get_model(msnn.Cell):
     def __init__(self, k=40, normal_channel=True):
         super(get_model, self).__init__()
         if normal_channel:
@@ -14,26 +17,26 @@         self.fc1 = nn.Linear(1024, 512)
         self.fc2 = nn.Linear(512, 256)
         self.fc3 = nn.Linear(256, k)
-        self.dropout = nn.Dropout(p=0.4)
+        self.dropout = nn.Dropout(p = 0.4)
         self.bn1 = nn.BatchNorm1d(512)
         self.bn2 = nn.BatchNorm1d(256)
         self.relu = nn.ReLU()
 
-    def forward(self, x):
+    def construct(self, x):
         x, trans, trans_feat = self.feat(x)
-        x = F.relu(self.bn1(self.fc1(x)))
-        x = F.relu(self.bn2(self.dropout(self.fc2(x))))
+        x = nn.functional.relu(self.bn1(self.fc1(x)))
+        x = nn.functional.relu(self.bn2(self.dropout(self.fc2(x))))
         x = self.fc3(x)
-        x = F.log_softmax(x, dim=1)
+        x = mint.special.log_softmax(x, dim=1)
         return x, trans_feat
 
-class get_loss(torch.nn.Module):
+class get_loss(msnn.Cell):
     def __init__(self, mat_diff_loss_scale=0.001):
         super(get_loss, self).__init__()
         self.mat_diff_loss_scale = mat_diff_loss_scale
 
-    def forward(self, pred, target, trans_feat):
-        loss = F.nll_loss(pred, target)
+    def construct(self, pred, target, trans_feat):
+        loss = nn.functional.nll_loss(pred, target)
         mat_diff_loss = feature_transform_reguliarzer(trans_feat)
 
         total_loss = loss + mat_diff_loss * self.mat_diff_loss_scale
