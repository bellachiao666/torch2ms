--- pytorch+++ mindspore@@ -1,10 +1,10 @@ import torch
-import torch.nn.functional as F
 from torch.nn.utils.rnn import pad_sequence
 from torch import nn, einsum
 
 from einops import rearrange, repeat
 from einops.layers.torch import Rearrange
+from mindspore.mint import nn, ops
 
 # helpers
 
@@ -17,10 +17,10 @@ # adaptive token sampling functions and classes
 
 def log(t, eps = 1e-6):
-    return torch.log(t + eps)
+    return ops.log(input = t + eps)  # 'torch.log':没有对应的mindspore参数 'out';
 
 def sample_gumbel(shape, device, dtype, eps = 1e-6):
-    u = torch.empty(shape, device = device, dtype = dtype).uniform_(0, 1)
+    u = ops.empty(size = shape, dtype = dtype, device = device).uniform_(0, 1)  # 'torch.empty':没有对应的mindspore参数 'out';; 'torch.empty':没有对应的mindspore参数 'layout';; 'torch.empty':没有对应的mindspore参数 'requires_grad';; 'torch.empty':没有对应的mindspore参数 'memory_format';
     return -log(-log(u, eps), eps)
 
 def batched_index_select(values, indices, dim = 1):
@@ -39,7 +39,7 @@     dim += value_expand_len
     return values.gather(dim, indices)
 
-class AdaptiveTokenSampling(nn.Module):
+class AdaptiveTokenSampling(nn.Cell):
     def __init__(self, output_num_tokens, eps = 1e-6):
         super().__init__()
         self.eps = eps
@@ -58,7 +58,7 @@ 
         # weigh the attention scores by the norm of the values, sum across all heads
 
-        cls_attn = einsum('b h n, b h n -> b n', cls_attn, value_norms)
+        cls_attn = ops.einsum(equation = 'b h n, b h n -> b n', operands = cls_attn)
 
         # normalize to 1
 
@@ -85,7 +85,7 @@ 
         # calculate unique using torch.unique and then pad the sequence from the right
 
-        unique_sampled_token_ids_list = [torch.unique(t, sorted = True) for t in torch.unbind(sampled_token_ids)]
+        unique_sampled_token_ids_list = [ops.unique(input = t, sorted = True) for t in ops.unbind(input = sampled_token_ids)]
         unique_sampled_token_ids = pad_sequence(unique_sampled_token_ids_list, batch_first = True)
 
         # calculate the new mask, based on the padding
@@ -94,11 +94,11 @@ 
         # CLS token never gets masked out (gets a value of True)
 
-        new_mask = F.pad(new_mask, (1, 0), value = True)
+        new_mask = nn.functional.pad(input = new_mask, pad = (1, 0), value = True)
 
         # prepend a 0 token id to keep the CLS attention scores
 
-        unique_sampled_token_ids = F.pad(unique_sampled_token_ids, (1, 0), value = 0)
+        unique_sampled_token_ids = nn.functional.pad(input = unique_sampled_token_ids, pad = (1, 0), value = 0)
         expanded_unique_sampled_token_ids = repeat(unique_sampled_token_ids, 'b n -> b h n', h = heads)
 
         # gather the new attention scores
@@ -110,40 +110,40 @@ 
 # classes
 
-class FeedForward(nn.Module):
+class FeedForward(nn.Cell):
     def __init__(self, dim, hidden_dim, dropout = 0.):
         super().__init__()
         self.net = nn.Sequential(
-            nn.LayerNorm(dim),
-            nn.Linear(dim, hidden_dim),
+            nn.LayerNorm(normalized_shape = dim),
+            nn.Linear(in_features = dim, out_features = hidden_dim),
             nn.GELU(),
-            nn.Dropout(dropout),
-            nn.Linear(hidden_dim, dim),
-            nn.Dropout(dropout)
-        )
+            nn.Dropout(p = dropout),
+            nn.Linear(in_features = hidden_dim, out_features = dim),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
     def forward(self, x):
         return self.net(x)
 
-class Attention(nn.Module):
+class Attention(nn.Cell):
     def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., output_num_tokens = None):
         super().__init__()
         inner_dim = dim_head *  heads
         self.heads = heads
         self.scale = dim_head ** -0.5
 
-        self.norm = nn.LayerNorm(dim)
+        self.norm = nn.LayerNorm(normalized_shape = dim)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
         self.attend = nn.Softmax(dim = -1)
-        self.dropout = nn.Dropout(dropout)
-
-        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)
+        self.dropout = nn.Dropout(p = dropout)
+
+        self.to_qkv = nn.Linear(in_features = dim, out_features = inner_dim * 3, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         self.output_num_tokens = output_num_tokens
         self.ats = AdaptiveTokenSampling(output_num_tokens) if exists(output_num_tokens) else None
 
         self.to_out = nn.Sequential(
-            nn.Linear(inner_dim, dim),
-            nn.Dropout(dropout)
-        )
+            nn.Linear(in_features = inner_dim, out_features = dim),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, x, *, mask):
         num_tokens = x.shape[1]
@@ -152,7 +152,7 @@         qkv = self.to_qkv(x).chunk(3, dim = -1)
         q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = ops.matmul(input = q, other = k.transpose(-1, -2)) * self.scale  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         if exists(mask):
             dots_mask = rearrange(mask, 'b i -> b 1 i 1') * rearrange(mask, 'b j -> b 1 1 j')
@@ -169,12 +169,12 @@         if exists(self.output_num_tokens) and (num_tokens - 1) > self.output_num_tokens:
             attn, mask, sampled_token_ids = self.ats(attn, v, mask = mask)
 
-        out = torch.matmul(attn, v)
+        out = ops.matmul(input = attn, other = v)  # 'torch.matmul':没有对应的mindspore参数 'out';
         out = rearrange(out, 'b h n d -> b n (h d)')
 
         return self.to_out(out), mask, sampled_token_ids
 
-class Transformer(nn.Module):
+class Transformer(nn.Cell):
     def __init__(self, dim, depth, max_tokens_per_depth, heads, dim_head, mlp_dim, dropout = 0.):
         super().__init__()
         assert len(max_tokens_per_depth) == depth, 'max_tokens_per_depth must be a tuple of length that is equal to the depth of the transformer'
@@ -193,9 +193,9 @@ 
         # use mask to keep track of the paddings when sampling tokens
         # as the duplicates (when sampling) are just removed, as mentioned in the paper
-        mask = torch.ones((b, n), device = device, dtype = torch.bool)
-
-        token_ids = torch.arange(n, device = device)
+        mask = ops.ones(size = (b, n), dtype = torch.bool)  # 'torch.ones':没有对应的mindspore参数 'out';; 'torch.ones':没有对应的mindspore参数 'layout';; 'torch.ones':没有对应的mindspore参数 'device';; 'torch.ones':没有对应的mindspore参数 'requires_grad';
+
+        token_ids = ops.arange(start = n)  # 'torch.arange':没有对应的mindspore参数 'out';; 'torch.arange':没有对应的mindspore参数 'layout';; 'torch.arange':没有对应的mindspore参数 'device';; 'torch.arange':没有对应的mindspore参数 'requires_grad';
         token_ids = repeat(token_ids, 'n -> b n', b = b)
 
         for attn, ff in self.layers:
@@ -212,7 +212,7 @@ 
         return x, token_ids
 
-class ViT(nn.Module):
+class ViT(nn.Cell):
     def __init__(self, *, image_size, patch_size, num_classes, dim, depth, max_tokens_per_depth, heads, mlp_dim, channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):
         super().__init__()
         image_height, image_width = pair(image_size)
@@ -225,28 +225,28 @@ 
         self.to_patch_embedding = nn.Sequential(
             Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
-            nn.LayerNorm(patch_dim),
-            nn.Linear(patch_dim, dim),
-            nn.LayerNorm(dim)
-        )
-
-        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
-        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
-        self.dropout = nn.Dropout(emb_dropout)
+            nn.LayerNorm(normalized_shape = patch_dim),
+            nn.Linear(in_features = patch_dim, out_features = dim),
+            nn.LayerNorm(normalized_shape = dim)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.pos_embedding = nn.Parameter(ops.randn(size = 1, generator = num_patches + 1))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+        self.cls_token = nn.Parameter(ops.randn(size = 1, generator = 1))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+        self.dropout = nn.Dropout(p = emb_dropout)
 
         self.transformer = Transformer(dim, depth, max_tokens_per_depth, heads, dim_head, mlp_dim, dropout)
 
         self.mlp_head = nn.Sequential(
-            nn.LayerNorm(dim),
-            nn.Linear(dim, num_classes)
-        )
+            nn.LayerNorm(normalized_shape = dim),
+            nn.Linear(in_features = dim, out_features = num_classes)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, img, return_sampled_token_ids = False):
         x = self.to_patch_embedding(img)
         b, n, _ = x.shape
 
         cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)
-        x = torch.cat((cls_tokens, x), dim=1)
+        x = ops.cat(tensors = (cls_tokens, x), dim = 1)  # 'torch.cat':没有对应的mindspore参数 'out';
         x += self.pos_embedding[:, :(n + 1)]
         x = self.dropout(x)
 
