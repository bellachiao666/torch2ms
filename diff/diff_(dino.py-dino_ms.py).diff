--- pytorch+++ mindspore@@ -4,9 +4,9 @@ 
 import torch
 from torch import nn
-import torch.nn.functional as F
 
 from torchvision import transforms as T
+from mindspore.mint import nn, ops
 
 # helper functions
 
@@ -50,11 +50,11 @@     teacher_logits = teacher_logits.detach()
     student_probs = (student_logits / student_temp).softmax(dim = -1)
     teacher_probs = ((teacher_logits - centers) / teacher_temp).softmax(dim = -1)
-    return - (teacher_probs * torch.log(student_probs + eps)).sum(dim = -1).mean()
+    return - (teacher_probs * ops.log(input = student_probs + eps)).sum(dim = -1).mean()  # 'torch.log':没有对应的mindspore参数 'out';
 
 # augmentation utils
 
-class RandomApply(nn.Module):
+class RandomApply(nn.Cell):
     def __init__(self, fn, p):
         super().__init__()
         self.fn = fn
@@ -84,12 +84,12 @@ 
 # MLP class for projector and predictor
 
-class L2Norm(nn.Module):
+class L2Norm(nn.Cell):
     def forward(self, x, eps = 1e-6):
         norm = x.norm(dim = 1, keepdim = True).clamp(min = eps)
         return x / norm
 
-class MLP(nn.Module):
+class MLP(nn.Cell):
     def __init__(self, dim, dim_out, num_layers, hidden_size = 256):
         super().__init__()
 
@@ -100,15 +100,15 @@             is_last = ind == (len(dims) - 1)
 
             layers.extend([
-                nn.Linear(layer_dim_in, layer_dim_out),
+                nn.Linear(in_features = layer_dim_in, out_features = layer_dim_out),
                 nn.GELU() if not is_last else nn.Identity()
-            ])
-
-        self.net = nn.Sequential(
+            ])  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.net = nn.SequentialCell(
             *layers,
             L2Norm(),
-            nn.Linear(hidden_size, dim_out)
-        )
+            nn.Linear(in_features = hidden_size, out_features = dim_out)
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return self.net(x)
@@ -117,7 +117,7 @@ # will manage the interception of the hidden layer output
 # and pipe it into the projecter and predictor nets
 
-class NetWrapper(nn.Module):
+class NetWrapper(nn.Cell):
     def __init__(self, net, output_dim, projection_hidden_size, projection_num_layers, layer = -2):
         super().__init__()
         self.net = net
@@ -181,7 +181,7 @@ 
 # main class
 
-class Dino(nn.Module):
+class Dino(nn.Cell):
     def __init__(
         self,
         net,
@@ -204,21 +204,21 @@ 
         # default BYOL augmentation
 
-        DEFAULT_AUG = torch.nn.Sequential(
+        DEFAULT_AUG = nn.SequentialCell(
             RandomApply(
-                T.ColorJitter(0.8, 0.8, 0.8, 0.2),
+                mindspore.dataset.vision.RandomColorAdjust(brightness = 0, contrast = 0, saturation = 0, hue = 0),
                 p = 0.3
             ),
             T.RandomGrayscale(p=0.2),
             T.RandomHorizontalFlip(),
             RandomApply(
-                T.GaussianBlur((3, 3), (1.0, 2.0)),
+                mindspore.dataset.vision.GaussianBlur(kernel_size = (3, 3), sigma = (1.0, 2.0)),
                 p = 0.2
             ),
             T.Normalize(
                 mean=torch.tensor([0.485, 0.456, 0.406]),
                 std=torch.tensor([0.229, 0.224, 0.225])),
-        )
+        )  # 默认值不一致: brightness (PyTorch=0, MindSpore=(1, 1)); 默认值不一致: contrast (PyTorch=0, MindSpore=(1, 1)); 默认值不一致: saturation (PyTorch=0, MindSpore=(1, 1)); 默认值不一致: hue (PyTorch=0, MindSpore=(0, 0))
 
         self.augment1 = default(augment_fn, DEFAULT_AUG)
         self.augment2 = default(augment_fn2, DEFAULT_AUG)
@@ -233,8 +233,8 @@         self.teacher_encoder = None
         self.teacher_ema_updater = EMA(moving_average_decay)
 
-        self.register_buffer('teacher_centers', torch.zeros(1, num_classes_K))
-        self.register_buffer('last_teacher_centers',  torch.zeros(1, num_classes_K))
+        self.register_buffer('teacher_centers', ops.zeros(size = 1))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
+        self.register_buffer('last_teacher_centers',  ops.zeros(size = 1))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
 
         self.teacher_centering_ema_updater = EMA(center_moving_average_decay)
 
@@ -246,7 +246,7 @@         self.to(device)
 
         # send a mock image tensor to instantiate singleton parameters
-        self.forward(torch.randn(2, 3, image_size, image_size, device=device))
+        self.forward(ops.randn(size = 2, generator = 3, dtype = image_size))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
     @singleton('teacher_encoder')
     def _get_teacher_encoder(self):
@@ -296,7 +296,7 @@             centers = self.teacher_centers
         )
 
-        teacher_logits_avg = torch.cat((teacher_proj_one, teacher_proj_two)).mean(dim = 0)
+        teacher_logits_avg = ops.cat(tensors = (teacher_proj_one, teacher_proj_two)).mean(dim = 0)  # 'torch.cat':没有对应的mindspore参数 'out';
         self.last_teacher_centers.copy_(teacher_logits_avg)
 
         loss = (loss_fn_(teacher_proj_one, student_proj_two) + loss_fn_(teacher_proj_two, student_proj_one)) / 2
