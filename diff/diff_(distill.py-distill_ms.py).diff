--- pytorch+++ mindspore@@ -1,11 +1,11 @@ import torch
 from torch import nn
-from torch.nn import Module
 import torch.nn.functional as F
 
 from vit_pytorch.vit import ViT
 from vit_pytorch.t2t import T2TViT
 from vit_pytorch.efficient import ViT as EfficientViT
+from mindspore.mint import nn, ops
 
 from einops import rearrange, repeat
 
@@ -26,12 +26,12 @@         b, n, _ = x.shape
 
         cls_tokens = repeat(self.cls_token, '1 n d -> b n d', b = b)
-        x = torch.cat((cls_tokens, x), dim = 1)
+        x = ops.cat(tensors = (cls_tokens, x), dim = 1)  # 'torch.cat':没有对应的mindspore参数 'out';
         x += self.pos_embedding[:, :(n + 1)]
 
         if distilling:
             distill_tokens = repeat(distill_token, '1 n d -> b n d', b = b)
-            x = torch.cat((x, distill_tokens), dim = 1)
+            x = ops.cat(tensors = (x, distill_tokens), dim = 1)  # 'torch.cat':没有对应的mindspore参数 'out';
 
         x = self._attend(x)
 
@@ -102,7 +102,7 @@ 
 # knowledge distillation wrapper
 
-class DistillWrapper(Module):
+class DistillWrapper(nn.Cell):
     def __init__(
         self,
         *,
@@ -125,12 +125,12 @@         self.alpha = alpha
         self.hard = hard
 
-        self.distillation_token = nn.Parameter(torch.randn(1, 1, dim))
+        self.distillation_token = mindspore.Parameter(ops.randn(size = 1, generator = 1))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
-        self.distill_mlp = nn.Sequential(
-            nn.LayerNorm(dim) if mlp_layernorm else nn.Identity(),
-            nn.Linear(dim, num_classes)
-        )
+        self.distill_mlp = nn.SequentialCell(
+            nn.LayerNorm(normalized_shape = dim) if mlp_layernorm else nn.Identity(),
+            nn.Linear(in_features = dim, out_features = num_classes)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, img, labels, temperature = None, alpha = None, **kwargs):
 
@@ -147,9 +147,9 @@ 
         if not self.hard:
             distill_loss = F.kl_div(
-                F.log_softmax(distill_logits / T, dim = -1),
-                F.softmax(teacher_logits / T, dim = -1).detach(),
-            reduction = 'batchmean')
+                ops.special.log_softmax(input = distill_logits / T, dim = -1),
+                nn.functional.softmax(input = teacher_logits / T, dim = -1).detach(),
+            reduction = 'batchmean')  # 'torch.nn.functional.softmax':没有对应的mindspore参数 '_stacklevel';
             distill_loss *= T ** 2
 
         else:
