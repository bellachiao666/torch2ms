--- pytorch+++ mindspore@@ -1,11 +1,9 @@ from math import ceil
-
-import torch
 from torch import nn, einsum
-import torch.nn.functional as F
 
 from einops import rearrange, repeat
 from einops.layers.torch import Rearrange
+from mindspore.mint import nn, ops
 
 # helpers
 
@@ -24,20 +22,20 @@ 
 # classes
 
-class FeedForward(nn.Module):
+class FeedForward(nn.Cell):
     def __init__(self, dim, mult, dropout = 0.):
         super().__init__()
         self.net = nn.Sequential(
-            nn.Conv2d(dim, dim * mult, 1),
+            nn.Conv2d(in_channels = dim, out_channels = dim * mult, kernel_size = 1),
             nn.Hardswish(),
-            nn.Dropout(dropout),
-            nn.Conv2d(dim * mult, dim, 1),
-            nn.Dropout(dropout)
-        )
+            nn.Dropout(p = dropout),
+            nn.Conv2d(in_channels = dim * mult, out_channels = dim, kernel_size = 1),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';; 'torch.nn.Hardswish':没有对应的mindspore参数 'inplace';
     def forward(self, x):
         return self.net(x)
 
-class Attention(nn.Module):
+class Attention(nn.Cell):
     def __init__(self, dim, fmap_size, heads = 8, dim_key = 32, dim_value = 64, dropout = 0., dim_out = None, downsample = False):
         super().__init__()
         inner_dim_key = dim_key *  heads
@@ -47,32 +45,32 @@         self.heads = heads
         self.scale = dim_key ** -0.5
 
-        self.to_q = nn.Sequential(nn.Conv2d(dim, inner_dim_key, 1, stride = (2 if downsample else 1), bias = False), nn.BatchNorm2d(inner_dim_key))
-        self.to_k = nn.Sequential(nn.Conv2d(dim, inner_dim_key, 1, bias = False), nn.BatchNorm2d(inner_dim_key))
-        self.to_v = nn.Sequential(nn.Conv2d(dim, inner_dim_value, 1, bias = False), nn.BatchNorm2d(inner_dim_value))
+        self.to_q = nn.Sequential(nn.Conv2d(in_channels = dim, out_channels = inner_dim_key, kernel_size = 1, stride = (2 if downsample else 1), bias = False), nn.BatchNorm2d(num_features = inner_dim_key))  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';; 'torch.nn.BatchNorm2d':没有对应的mindspore参数 'device';
+        self.to_k = nn.Sequential(nn.Conv2d(in_channels = dim, out_channels = inner_dim_key, kernel_size = 1, bias = False), nn.BatchNorm2d(num_features = inner_dim_key))  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';; 'torch.nn.BatchNorm2d':没有对应的mindspore参数 'device';
+        self.to_v = nn.Sequential(nn.Conv2d(in_channels = dim, out_channels = inner_dim_value, kernel_size = 1, bias = False), nn.BatchNorm2d(num_features = inner_dim_value))  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';; 'torch.nn.BatchNorm2d':没有对应的mindspore参数 'device';
 
         self.attend = nn.Softmax(dim = -1)
-        self.dropout = nn.Dropout(dropout)
+        self.dropout = nn.Dropout(p = dropout)
 
-        out_batch_norm = nn.BatchNorm2d(dim_out)
+        out_batch_norm = nn.BatchNorm2d(num_features = dim_out)  # 'torch.nn.BatchNorm2d':没有对应的mindspore参数 'device';
         nn.init.zeros_(out_batch_norm.weight)
 
         self.to_out = nn.Sequential(
             nn.GELU(),
-            nn.Conv2d(inner_dim_value, dim_out, 1),
+            nn.Conv2d(in_channels = inner_dim_value, out_channels = dim_out, kernel_size = 1),
             out_batch_norm,
-            nn.Dropout(dropout)
-        )
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         # positional bias
 
-        self.pos_bias = nn.Embedding(fmap_size * fmap_size, heads)
+        self.pos_bias = nn.Embedding(num_embeddings = fmap_size * fmap_size, embedding_dim = heads)  # 'torch.nn.Embedding':没有对应的mindspore参数 'device';
 
-        q_range = torch.arange(0, fmap_size, step = (2 if downsample else 1))
-        k_range = torch.arange(fmap_size)
+        q_range = ops.arange(start = 0, end = fmap_size, step = (2 if downsample else 1))  # 'torch.arange':没有对应的mindspore参数 'out';; 'torch.arange':没有对应的mindspore参数 'layout';; 'torch.arange':没有对应的mindspore参数 'device';; 'torch.arange':没有对应的mindspore参数 'requires_grad';
+        k_range = ops.arange(start = fmap_size)  # 'torch.arange':没有对应的mindspore参数 'out';; 'torch.arange':没有对应的mindspore参数 'layout';; 'torch.arange':没有对应的mindspore参数 'device';; 'torch.arange':没有对应的mindspore参数 'requires_grad';
 
-        q_pos = torch.stack(torch.meshgrid(q_range, q_range, indexing = 'ij'), dim = -1)
-        k_pos = torch.stack(torch.meshgrid(k_range, k_range, indexing = 'ij'), dim = -1)
+        q_pos = ops.stack(tensors = ops.meshgrid(tensors = q_range, indexing = 'ij'), dim = -1)  # 'torch.stack':没有对应的mindspore参数 'out';
+        k_pos = ops.stack(tensors = ops.meshgrid(tensors = k_range, indexing = 'ij'), dim = -1)  # 'torch.stack':没有对应的mindspore参数 'out';
 
         q_pos, k_pos = map(lambda t: rearrange(t, 'i j c -> (i j) c'), (q_pos, k_pos))
         rel_pos = (q_pos[:, None, ...] - k_pos[None, :, ...]).abs()
@@ -96,18 +94,18 @@         qkv = (q, self.to_k(x), self.to_v(x))
         q, k, v = map(lambda t: rearrange(t, 'b (h d) ... -> b h (...) d', h = h), qkv)
 
-        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale
+        dots = ops.einsum(equation = 'b h i d, b h j d -> b h i j', operands = q) * self.scale
 
         dots = self.apply_pos_bias(dots)
 
         attn = self.attend(dots)
         attn = self.dropout(attn)
 
-        out = einsum('b h i j, b h j d -> b h i d', attn, v)
+        out = ops.einsum(equation = 'b h i j, b h j d -> b h i d', operands = attn)
         out = rearrange(out, 'b h (x y) d -> b (h d) x y', h = h, y = y)
         return self.to_out(out)
 
-class Transformer(nn.Module):
+class Transformer(nn.Cell):
     def __init__(self, dim, fmap_size, depth, heads, dim_key, dim_value, mlp_mult = 2, dropout = 0., dim_out = None, downsample = False):
         super().__init__()
         dim_out = default(dim_out, dim)
@@ -126,7 +124,7 @@             x = ff(x) + x
         return x
 
-class LeViT(nn.Module):
+class LeViT(nn.Cell):
     def __init__(
         self,
         *,
@@ -151,11 +149,11 @@         assert all(map(lambda t: len(t) == stages, (dims, depths, layer_heads))), 'dimensions, depths, and heads must be a tuple that is less than the designated number of stages'
 
         self.conv_embedding = nn.Sequential(
-            nn.Conv2d(3, 32, 3, stride = 2, padding = 1),
-            nn.Conv2d(32, 64, 3, stride = 2, padding = 1),
-            nn.Conv2d(64, 128, 3, stride = 2, padding = 1),
-            nn.Conv2d(128, dims[0], 3, stride = 2, padding = 1)
-        )
+            nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride = 2, padding = 1),
+            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 2, padding = 1),
+            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, stride = 2, padding = 1),
+            nn.Conv2d(in_channels = 128, out_channels = dims[0], kernel_size = 3, stride = 2, padding = 1)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         fmap_size = image_size // (2 ** 4)
         layers = []
@@ -172,12 +170,12 @@         self.backbone = nn.Sequential(*layers)
 
         self.pool = nn.Sequential(
-            nn.AdaptiveAvgPool2d(1),
+            nn.AdaptiveAvgPool2d(output_size = 1),
             Rearrange('... () () -> ...')
         )
 
-        self.distill_head = nn.Linear(dim, num_distill_classes) if exists(num_distill_classes) else always(None)
-        self.mlp_head = nn.Linear(dim, num_classes)
+        self.distill_head = nn.Linear(in_features = dim, out_features = num_distill_classes) if exists(num_distill_classes) else always(None)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+        self.mlp_head = nn.Linear(in_features = dim, out_features = num_classes)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, img):
         x = self.conv_embedding(img)
