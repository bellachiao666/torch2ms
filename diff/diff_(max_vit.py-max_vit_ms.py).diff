--- pytorch+++ mindspore@@ -5,6 +5,7 @@ 
 from einops import rearrange, repeat
 from einops.layers.torch import Rearrange, Reduce
+from mindspore.mint import nn, ops
 
 # helpers
 
@@ -19,7 +20,7 @@ 
 # helper classes
 
-class Residual(nn.Module):
+class Residual(nn.Cell):
     def __init__(self, dim, fn):
         super().__init__()
         self.fn = fn
@@ -27,42 +28,42 @@     def forward(self, x):
         return self.fn(x) + x
 
-class FeedForward(nn.Module):
+class FeedForward(nn.Cell):
     def __init__(self, dim, mult = 4, dropout = 0.):
         super().__init__()
         inner_dim = int(dim * mult)
-        self.net = nn.Sequential(
-            nn.LayerNorm(dim),
-            nn.Linear(dim, inner_dim),
+        self.net = nn.SequentialCell(
+            nn.LayerNorm(normalized_shape = dim),
+            nn.Linear(in_features = dim, out_features = inner_dim),
             nn.GELU(),
-            nn.Dropout(dropout),
-            nn.Linear(inner_dim, dim),
-            nn.Dropout(dropout)
-        )
+            nn.Dropout(p = dropout),
+            nn.Linear(in_features = inner_dim, out_features = dim),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
     def forward(self, x):
         return self.net(x)
 
 # MBConv
 
-class SqueezeExcitation(nn.Module):
+class SqueezeExcitation(nn.Cell):
     def __init__(self, dim, shrinkage_rate = 0.25):
         super().__init__()
         hidden_dim = int(dim * shrinkage_rate)
 
-        self.gate = nn.Sequential(
+        self.gate = nn.SequentialCell(
             Reduce('b c h w -> b c', 'mean'),
-            nn.Linear(dim, hidden_dim, bias = False),
+            nn.Linear(in_features = dim, out_features = hidden_dim, bias = False),
             nn.SiLU(),
-            nn.Linear(hidden_dim, dim, bias = False),
+            nn.Linear(in_features = hidden_dim, out_features = dim, bias = False),
             nn.Sigmoid(),
             Rearrange('b c -> b c 1 1')
-        )
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return x * self.gate(x)
 
 
-class MBConvResidual(nn.Module):
+class MBConvResidual(nn.Cell):
     def __init__(self, fn, dropout = 0.):
         super().__init__()
         self.fn = fn
@@ -73,7 +74,7 @@         out = self.dropsample(out)
         return out + x
 
-class Dropsample(nn.Module):
+class Dropsample(nn.Cell):
     def __init__(self, prob = 0):
         super().__init__()
         self.prob = prob
@@ -99,17 +100,17 @@     hidden_dim = int(expansion_rate * dim_out)
     stride = 2 if downsample else 1
 
-    net = nn.Sequential(
-        nn.Conv2d(dim_in, hidden_dim, 1),
-        nn.BatchNorm2d(hidden_dim),
+    net = nn.SequentialCell(
+        nn.Conv2d(in_channels = dim_in, out_channels = hidden_dim, kernel_size = 1),
+        nn.BatchNorm2d(num_features = hidden_dim),
         nn.GELU(),
-        nn.Conv2d(hidden_dim, hidden_dim, 3, stride = stride, padding = 1, groups = hidden_dim),
-        nn.BatchNorm2d(hidden_dim),
+        nn.Conv2d(in_channels = hidden_dim, out_channels = hidden_dim, kernel_size = 3, stride = stride, padding = 1, groups = hidden_dim),
+        nn.BatchNorm2d(num_features = hidden_dim),
         nn.GELU(),
         SqueezeExcitation(hidden_dim, shrinkage_rate = shrinkage_rate),
-        nn.Conv2d(hidden_dim, dim_out, 1),
-        nn.BatchNorm2d(dim_out)
-    )
+        nn.Conv2d(in_channels = hidden_dim, out_channels = dim_out, kernel_size = 1),
+        nn.BatchNorm2d(num_features = dim_out)
+    )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';; 'torch.nn.BatchNorm2d':没有对应的mindspore参数 'device';
 
     if dim_in == dim_out and not downsample:
         net = MBConvResidual(net, dropout = dropout)
@@ -118,7 +119,7 @@ 
 # attention related classes
 
-class Attention(nn.Module):
+class Attention(nn.Cell):
     def __init__(
         self,
         dim,
@@ -132,25 +133,25 @@         self.heads = dim // dim_head
         self.scale = dim_head ** -0.5
 
-        self.norm = nn.LayerNorm(dim)
-        self.to_qkv = nn.Linear(dim, dim * 3, bias = False)
-
-        self.attend = nn.Sequential(
+        self.norm = nn.LayerNorm(normalized_shape = dim)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
+        self.to_qkv = nn.Linear(in_features = dim, out_features = dim * 3, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.attend = nn.SequentialCell(
             nn.Softmax(dim = -1),
-            nn.Dropout(dropout)
+            nn.Dropout(p = dropout)
         )
 
-        self.to_out = nn.Sequential(
-            nn.Linear(dim, dim, bias = False),
-            nn.Dropout(dropout)
-        )
+        self.to_out = nn.SequentialCell(
+            nn.Linear(in_features = dim, out_features = dim, bias = False),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         # relative positional bias
 
-        self.rel_pos_bias = nn.Embedding((2 * window_size - 1) ** 2, self.heads)
-
-        pos = torch.arange(window_size)
-        grid = torch.stack(torch.meshgrid(pos, pos, indexing = 'ij'))
+        self.rel_pos_bias = nn.Embedding(num_embeddings = (2 * window_size - 1) ** 2, embedding_dim = self.heads)  # 'torch.nn.Embedding':没有对应的mindspore参数 'device';
+
+        pos = ops.arange(start = window_size)  # 'torch.arange':没有对应的mindspore参数 'out';; 'torch.arange':没有对应的mindspore参数 'layout';; 'torch.arange':没有对应的mindspore参数 'device';; 'torch.arange':没有对应的mindspore参数 'requires_grad';
+        grid = ops.stack(tensors = ops.meshgrid(tensors = pos, indexing = 'ij'))  # 'torch.stack':没有对应的mindspore参数 'out';
         grid = rearrange(grid, 'c i j -> (i j) c')
         rel_pos = rearrange(grid, 'i ... -> i 1 ...') - rearrange(grid, 'j ... -> 1 j ...')
         rel_pos += window_size - 1
@@ -181,7 +182,7 @@ 
         # sim
 
-        sim = einsum('b h i d, b h j d -> b h i j', q, k)
+        sim = ops.einsum(equation = 'b h i d, b h j d -> b h i j', operands = q)
 
         # add positional bias
 
@@ -194,7 +195,7 @@ 
         # aggregate
 
-        out = einsum('b h i j, b h j d -> b h i d', attn, v)
+        out = ops.einsum(equation = 'b h i j, b h j d -> b h i d', operands = attn)
 
         # merge heads
 
@@ -205,7 +206,7 @@         out = self.to_out(out)
         return rearrange(out, '(b x y) ... -> b x y ...', x = height, y = width)
 
-class MaxViT(nn.Module):
+class MaxViT(nn.Cell):
     def __init__(
         self,
         *,
@@ -227,10 +228,10 @@ 
         dim_conv_stem = default(dim_conv_stem, dim)
 
-        self.conv_stem = nn.Sequential(
-            nn.Conv2d(channels, dim_conv_stem, 3, stride = 2, padding = 1),
-            nn.Conv2d(dim_conv_stem, dim_conv_stem, 3, padding = 1)
-        )
+        self.conv_stem = nn.SequentialCell(
+            nn.Conv2d(in_channels = channels, out_channels = dim_conv_stem, kernel_size = 3, stride = 2, padding = 1),
+            nn.Conv2d(in_channels = dim_conv_stem, out_channels = dim_conv_stem, kernel_size = 3, padding = 1)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         # variables
 
@@ -240,7 +241,7 @@         dims = (dim_conv_stem, *dims)
         dim_pairs = tuple(zip(dims[:-1], dims[1:]))
 
-        self.layers = nn.ModuleList([])
+        self.layers = nn.CellList([])
 
         # shorthand for window size for efficient block - grid like attention
 
@@ -253,7 +254,7 @@                 is_first = stage_ind == 0
                 stage_dim_in = layer_dim_in if is_first else layer_dim
 
-                block = nn.Sequential(
+                block = nn.SequentialCell(
                     MBConv(
                         stage_dim_in,
                         layer_dim,
@@ -276,11 +277,11 @@ 
         # mlp head out
 
-        self.mlp_head = nn.Sequential(
+        self.mlp_head = nn.SequentialCell(
             Reduce('b d h w -> b d', 'mean'),
-            nn.LayerNorm(dims[-1]),
-            nn.Linear(dims[-1], num_classes)
-        )
+            nn.LayerNorm(normalized_shape = dims[-1]),
+            nn.Linear(in_features = dims[-1], out_features = num_classes)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         x = self.conv_stem(x)
