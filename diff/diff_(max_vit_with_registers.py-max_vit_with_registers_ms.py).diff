--- pytorch+++ mindspore@@ -2,11 +2,10 @@ 
 import torch
 from torch import nn, einsum
-import torch.nn.functional as F
-from torch.nn import Module, ModuleList, Sequential
 
 from einops import rearrange, repeat, reduce, pack, unpack
 from einops.layers.torch import Rearrange, Reduce
+from mindspore.mint import nn, ops
 
 # helpers
 
@@ -29,35 +28,35 @@ 
 def FeedForward(dim, mult = 4, dropout = 0.):
     inner_dim = int(dim * mult)
-    return Sequential(
-        nn.LayerNorm(dim),
-        nn.Linear(dim, inner_dim),
+    return nn.SequentialCell(
+        nn.LayerNorm(normalized_shape = dim),
+        nn.Linear(in_features = dim, out_features = inner_dim),
         nn.GELU(),
-        nn.Dropout(dropout),
-        nn.Linear(inner_dim, dim),
-        nn.Dropout(dropout)
-    )
+        nn.Dropout(p = dropout),
+        nn.Linear(in_features = inner_dim, out_features = dim),
+        nn.Dropout(p = dropout)
+    )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
 # MBConv
 
-class SqueezeExcitation(Module):
+class SqueezeExcitation(nn.Cell):
     def __init__(self, dim, shrinkage_rate = 0.25):
         super().__init__()
         hidden_dim = int(dim * shrinkage_rate)
 
-        self.gate = Sequential(
+        self.gate = nn.SequentialCell(
             Reduce('b c h w -> b c', 'mean'),
-            nn.Linear(dim, hidden_dim, bias = False),
+            nn.Linear(in_features = dim, out_features = hidden_dim, bias = False),
             nn.SiLU(),
-            nn.Linear(hidden_dim, dim, bias = False),
+            nn.Linear(in_features = hidden_dim, out_features = dim, bias = False),
             nn.Sigmoid(),
             Rearrange('b c -> b c 1 1')
-        )
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return x * self.gate(x)
 
-class MBConvResidual(Module):
+class MBConvResidual(nn.Cell):
     def __init__(self, fn, dropout = 0.):
         super().__init__()
         self.fn = fn
@@ -68,7 +67,7 @@         out = self.dropsample(out)
         return out + x
 
-class Dropsample(Module):
+class Dropsample(nn.Cell):
     def __init__(self, prob = 0):
         super().__init__()
         self.prob = prob
@@ -94,17 +93,17 @@     hidden_dim = int(expansion_rate * dim_out)
     stride = 2 if downsample else 1
 
-    net = Sequential(
-        nn.Conv2d(dim_in, hidden_dim, 1),
-        nn.BatchNorm2d(hidden_dim),
+    net = nn.SequentialCell(
+        nn.Conv2d(in_channels = dim_in, out_channels = hidden_dim, kernel_size = 1),
+        nn.BatchNorm2d(num_features = hidden_dim),
         nn.GELU(),
-        nn.Conv2d(hidden_dim, hidden_dim, 3, stride = stride, padding = 1, groups = hidden_dim),
-        nn.BatchNorm2d(hidden_dim),
+        nn.Conv2d(in_channels = hidden_dim, out_channels = hidden_dim, kernel_size = 3, stride = stride, padding = 1, groups = hidden_dim),
+        nn.BatchNorm2d(num_features = hidden_dim),
         nn.GELU(),
         SqueezeExcitation(hidden_dim, shrinkage_rate = shrinkage_rate),
-        nn.Conv2d(hidden_dim, dim_out, 1),
-        nn.BatchNorm2d(dim_out)
-    )
+        nn.Conv2d(in_channels = hidden_dim, out_channels = dim_out, kernel_size = 1),
+        nn.BatchNorm2d(num_features = dim_out)
+    )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';; 'torch.nn.BatchNorm2d':没有对应的mindspore参数 'device';
 
     if dim_in == dim_out and not downsample:
         net = MBConvResidual(net, dropout = dropout)
@@ -113,7 +112,7 @@ 
 # attention related classes
 
-class Attention(Module):
+class Attention(nn.Cell):
     def __init__(
         self,
         dim,
@@ -129,33 +128,33 @@         self.heads = dim // dim_head
         self.scale = dim_head ** -0.5
 
-        self.norm = nn.LayerNorm(dim)
-        self.to_qkv = nn.Linear(dim, dim * 3, bias = False)
-
-        self.attend = nn.Sequential(
+        self.norm = nn.LayerNorm(normalized_shape = dim)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
+        self.to_qkv = nn.Linear(in_features = dim, out_features = dim * 3, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.attend = nn.SequentialCell(
             nn.Softmax(dim = -1),
-            nn.Dropout(dropout)
+            nn.Dropout(p = dropout)
         )
 
-        self.to_out = nn.Sequential(
-            nn.Linear(dim, dim, bias = False),
-            nn.Dropout(dropout)
-        )
+        self.to_out = nn.SequentialCell(
+            nn.Linear(in_features = dim, out_features = dim, bias = False),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         # relative positional bias
 
         num_rel_pos_bias = (2 * window_size - 1) ** 2
 
-        self.rel_pos_bias = nn.Embedding(num_rel_pos_bias + 1, self.heads)
-
-        pos = torch.arange(window_size)
-        grid = torch.stack(torch.meshgrid(pos, pos, indexing = 'ij'))
+        self.rel_pos_bias = nn.Embedding(num_embeddings = num_rel_pos_bias + 1, embedding_dim = self.heads)  # 'torch.nn.Embedding':没有对应的mindspore参数 'device';
+
+        pos = ops.arange(start = window_size)  # 'torch.arange':没有对应的mindspore参数 'out';; 'torch.arange':没有对应的mindspore参数 'layout';; 'torch.arange':没有对应的mindspore参数 'device';; 'torch.arange':没有对应的mindspore参数 'requires_grad';
+        grid = ops.stack(tensors = ops.meshgrid(tensors = pos, indexing = 'ij'))  # 'torch.stack':没有对应的mindspore参数 'out';
         grid = rearrange(grid, 'c i j -> (i j) c')
         rel_pos = rearrange(grid, 'i ... -> i 1 ...') - rearrange(grid, 'j ... -> 1 j ...')
         rel_pos += window_size - 1
         rel_pos_indices = (rel_pos * torch.tensor([2 * window_size - 1, 1])).sum(dim = -1)
 
-        rel_pos_indices = F.pad(rel_pos_indices, (num_registers, 0, num_registers, 0), value = num_rel_pos_bias)
+        rel_pos_indices = nn.functional.pad(input = rel_pos_indices, pad = (num_registers, 0, num_registers, 0), value = num_rel_pos_bias)
         self.register_buffer('rel_pos_indices', rel_pos_indices, persistent = False)
 
     def forward(self, x):
@@ -177,7 +176,7 @@ 
         # sim
 
-        sim = einsum('b h i d, b h j d -> b h i j', q, k)
+        sim = ops.einsum(equation = 'b h i d, b h j d -> b h i j', operands = q)
 
         # add positional bias
 
@@ -190,14 +189,14 @@ 
         # aggregate
 
-        out = einsum('b h i j, b h j d -> b h i d', attn, v)
+        out = ops.einsum(equation = 'b h i j, b h j d -> b h i d', operands = attn)
 
         # combine heads out
 
         out = rearrange(out, 'b h n d -> b n (h d)')
         return self.to_out(out)
 
-class MaxViT(Module):
+class MaxViT(nn.Cell):
     def __init__(
         self,
         *,
@@ -221,10 +220,10 @@ 
         dim_conv_stem = default(dim_conv_stem, dim)
 
-        self.conv_stem = Sequential(
-            nn.Conv2d(channels, dim_conv_stem, 3, stride = 2, padding = 1),
-            nn.Conv2d(dim_conv_stem, dim_conv_stem, 3, padding = 1)
-        )
+        self.conv_stem = nn.SequentialCell(
+            nn.Conv2d(in_channels = channels, out_channels = dim_conv_stem, kernel_size = 3, stride = 2, padding = 1),
+            nn.Conv2d(in_channels = dim_conv_stem, out_channels = dim_conv_stem, kernel_size = 3, padding = 1)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         # variables
 
@@ -234,7 +233,7 @@         dims = (dim_conv_stem, *dims)
         dim_pairs = tuple(zip(dims[:-1], dims[1:]))
 
-        self.layers = nn.ModuleList([])
+        self.layers = nn.CellList([])
 
         # window size
 
@@ -263,23 +262,23 @@                 grid_attn = Attention(dim = layer_dim, dim_head = dim_head, dropout = dropout, window_size = window_size, num_registers = num_register_tokens)
                 grid_ff = FeedForward(dim = layer_dim, dropout = dropout)
 
-                register_tokens = nn.Parameter(torch.randn(num_register_tokens, layer_dim))
-
-                self.layers.append(ModuleList([
+                register_tokens = mindspore.Parameter(ops.randn(size = num_register_tokens, generator = layer_dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+
+                self.layers.append(nn.CellList([
                     conv,
-                    ModuleList([block_attn, block_ff]),
-                    ModuleList([grid_attn, grid_ff])
+                    nn.CellList([block_attn, block_ff]),
+                    nn.CellList([grid_attn, grid_ff])
                 ]))
 
                 self.register_tokens.append(register_tokens)
 
         # mlp head out
 
-        self.mlp_head = nn.Sequential(
+        self.mlp_head = nn.SequentialCell(
             Reduce('b d h w -> b d', 'mean'),
-            nn.LayerNorm(dims[-1]),
-            nn.Linear(dims[-1], num_classes)
-        )
+            nn.LayerNorm(normalized_shape = dims[-1]),
+            nn.Linear(in_features = dims[-1], out_features = num_classes)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         b, w = x.shape[0], self.window_size
