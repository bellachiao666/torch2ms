--- pytorch+++ mindspore@@ -3,6 +3,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
+from mindspore.mint import nn, ops
 
 from einops import rearrange, repeat, reduce
 
@@ -13,16 +14,16 @@ 
 def prob_mask_like(t, prob):
     batch, seq_length, _ = t.shape
-    return torch.zeros((batch, seq_length)).float().uniform_(0, 1) < prob
+    return ops.zeros(size = (batch, seq_length)).float().uniform_(0, 1) < prob  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
 
 def get_mask_subset_with_prob(patched_input, prob):
     batch, seq_len, _, device = *patched_input.shape, patched_input.device
     max_masked = math.ceil(prob * seq_len)
 
-    rand = torch.rand((batch, seq_len), device=device)
+    rand = ops.rand(size = (batch, seq_len))  # 'torch.rand':没有对应的mindspore参数 'out';; 'torch.rand':没有对应的mindspore参数 'layout';; 'torch.rand':没有对应的mindspore参数 'device';; 'torch.rand':没有对应的mindspore参数 'requires_grad';; 'torch.rand':没有对应的mindspore参数 'pin_memory';
     _, sampled_indices = rand.topk(max_masked, dim=-1)
 
-    new_mask = torch.zeros((batch, seq_len), device=device)
+    new_mask = ops.zeros(size = (batch, seq_len))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
     new_mask.scatter_(1, sampled_indices, 1)
     return new_mask.bool()
 
@@ -30,7 +31,7 @@ # mpp loss
 
 
-class MPPLoss(nn.Module):
+class MPPLoss(nn.Cell):
     def __init__(
         self,
         patch_size,
@@ -61,13 +62,13 @@         target = target.clamp(max = mpv) # clamp just in case
         avg_target = reduce(target, 'b c (h p1) (w p2) -> b (h w) c', 'mean', p1 = p, p2 = p).contiguous()
 
-        channel_bins = torch.arange(bin_size, mpv, bin_size, device = device)
+        channel_bins = ops.arange(start = bin_size, end = mpv, step = bin_size)  # 'torch.arange':没有对应的mindspore参数 'out';; 'torch.arange':没有对应的mindspore参数 'layout';; 'torch.arange':没有对应的mindspore参数 'device';; 'torch.arange':没有对应的mindspore参数 'requires_grad';
         discretized_target = torch.bucketize(avg_target, channel_bins)
 
-        bin_mask = (2 ** bits) ** torch.arange(0, c, device = device).long()
+        bin_mask = (2 ** bits) ** ops.arange(start = 0, end = c).long()  # 'torch.arange':没有对应的mindspore参数 'out';; 'torch.arange':没有对应的mindspore参数 'layout';; 'torch.arange':没有对应的mindspore参数 'device';; 'torch.arange':没有对应的mindspore参数 'requires_grad';
         bin_mask = rearrange(bin_mask, 'c -> () () c')
 
-        target_label = torch.sum(bin_mask * discretized_target, dim = -1)
+        target_label = ops.sum(input = bin_mask * discretized_target)
 
         loss = F.cross_entropy(predicted_patches[mask], target_label[mask])
         return loss
@@ -76,7 +77,7 @@ # main class
 
 
-class MPP(nn.Module):
+class MPP(nn.Cell):
     def __init__(
         self,
         transformer,
@@ -97,10 +98,10 @@                             max_pixel_val, mean, std)
 
         # extract patching function
-        self.patch_to_emb = nn.Sequential(transformer.to_patch_embedding[1:])
+        self.patch_to_emb = nn.SequentialCell(transformer.to_patch_embedding[1:])
 
         # output transformation
-        self.to_bits = nn.Linear(dim, 2**(output_channel_bits * channels))
+        self.to_bits = nn.Linear(in_features = dim, out_features = 2**(output_channel_bits * channels))  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         # vit related dimensions
         self.patch_size = patch_size
@@ -111,7 +112,7 @@         self.random_patch_prob = random_patch_prob
 
         # token ids
-        self.mask_token = nn.Parameter(torch.randn(1, 1, channels * patch_size ** 2))
+        self.mask_token = mindspore.Parameter(ops.randn(size = 1, generator = 1))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
     def forward(self, input, **kwargs):
         transformer = self.transformer
@@ -138,13 +139,10 @@                                                random_patch_sampling_prob).to(mask.device)
 
             bool_random_patch_prob = mask * (random_patch_prob == True)
-            random_patches = torch.randint(0,
-                                           input.shape[1],
-                                           (input.shape[0], input.shape[1]),
-                                           device=input.device)
+            random_patches = ops.randint(low = 0, high = input.shape[1], size = (input.shape[0], input.shape[1]))  # 'torch.randint':没有对应的mindspore参数 'out';; 'torch.randint':没有对应的mindspore参数 'layout';; 'torch.randint':没有对应的mindspore参数 'device';; 'torch.randint':没有对应的mindspore参数 'requires_grad';
             randomized_input = masked_input[
-                torch.arange(masked_input.shape[0]).unsqueeze(-1),
-                random_patches]
+                ops.arange(start = masked_input.shape[0]).unsqueeze(-1),
+                random_patches]  # 'torch.arange':没有对应的mindspore参数 'out';; 'torch.arange':没有对应的mindspore参数 'layout';; 'torch.arange':没有对应的mindspore参数 'device';; 'torch.arange':没有对应的mindspore参数 'requires_grad';
             masked_input[bool_random_patch_prob] = randomized_input[
                 bool_random_patch_prob]
 
@@ -159,7 +157,7 @@         # add cls token to input sequence
         b, n, _ = masked_input.shape
         cls_tokens = repeat(transformer.cls_token, '() n d -> b n d', b=b)
-        masked_input = torch.cat((cls_tokens, masked_input), dim=1)
+        masked_input = ops.cat(tensors = (cls_tokens, masked_input), dim = 1)  # 'torch.cat':没有对应的mindspore参数 'out';
 
         # add positional embeddings to input
         masked_input += transformer.pos_embedding[:, :(n + 1)]
