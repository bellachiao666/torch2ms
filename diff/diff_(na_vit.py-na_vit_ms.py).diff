--- pytorch+++ mindspore@@ -1,3 +1,4 @@+from mindspore.mint import nn, ops
 from __future__ import annotations
 
 from functools import partial, lru_cache
@@ -29,9 +30,9 @@ 
 @lru_cache(maxsize=128)
 def posemb_grid(ph, pw, device):
-    h_idx = torch.arange(ph, device=device).repeat_interleave(pw)
-    w_idx = torch.arange(pw, device=device).repeat(ph)
-    return torch.stack([h_idx, w_idx], dim=-1)
+    h_idx = ops.arange(start = ph).repeat_interleave(pw)  # 'torch.arange':没有对应的mindspore参数 'out';; 'torch.arange':没有对应的mindspore参数 'layout';; 'torch.arange':没有对应的mindspore参数 'device';; 'torch.arange':没有对应的mindspore参数 'requires_grad';
+    w_idx = ops.arange(start = pw).repeat(ph)  # 'torch.arange':没有对应的mindspore参数 'out';; 'torch.arange':没有对应的mindspore参数 'layout';; 'torch.arange':没有对应的mindspore参数 'device';; 'torch.arange':没有对应的mindspore参数 'requires_grad';
+    return ops.stack(tensors = [h_idx, w_idx], dim = -1)  # 'torch.stack':没有对应的mindspore参数 'out';
 
 # auto grouping images
 
@@ -82,8 +83,8 @@ class LayerNorm(nn.Module):
     def __init__(self, dim):
         super().__init__()
-        self.gamma = nn.Parameter(torch.ones(dim))
-        self.register_buffer('beta', torch.zeros(dim))
+        self.gamma = nn.Parameter(ops.ones(size = dim))  # 'torch.ones':没有对应的mindspore参数 'out';; 'torch.ones':没有对应的mindspore参数 'layout';; 'torch.ones':没有对应的mindspore参数 'device';; 'torch.ones':没有对应的mindspore参数 'requires_grad';
+        self.register_buffer('beta', ops.zeros(size = dim))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
 
     def forward(self, x):
         return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)
@@ -94,10 +95,10 @@     def __init__(self, heads, dim):
         super().__init__()
         self.scale = dim ** 0.5
-        self.gamma = nn.Parameter(torch.ones(heads, 1, dim))
+        self.gamma = nn.Parameter(ops.ones(size = heads, dtype = dim))  # 'torch.ones':没有对应的mindspore参数 'out';; 'torch.ones':没有对应的mindspore参数 'layout';; 'torch.ones':没有对应的mindspore参数 'device';; 'torch.ones':没有对应的mindspore参数 'requires_grad';
 
     def forward(self, x):
-        normed = F.normalize(x, dim = -1)
+        normed = nn.functional.normalize(input = x, dim = -1)  # 'torch.nn.functional.normalize':没有对应的mindspore参数 'out';
         return normed * self.scale * self.gamma
 
 # feedforward
@@ -105,12 +106,12 @@ def FeedForward(dim, hidden_dim, dropout = 0.):
     return nn.Sequential(
         LayerNorm(dim),
-        nn.Linear(dim, hidden_dim),
+        nn.Linear(in_features = dim, out_features = hidden_dim),
         nn.GELU(),
-        nn.Dropout(dropout),
-        nn.Linear(hidden_dim, dim),
-        nn.Dropout(dropout)
-    )
+        nn.Dropout(p = dropout),
+        nn.Linear(in_features = hidden_dim, out_features = dim),
+        nn.Dropout(p = dropout)
+    )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
 class Attention(nn.Module):
     def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
@@ -124,13 +125,13 @@ 
         self.dropout_p = dropout
 
-        self.to_q = nn.Linear(dim, inner_dim, bias = False)
-        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)
+        self.to_q = nn.Linear(in_features = dim, out_features = inner_dim, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+        self.to_kv = nn.Linear(in_features = dim, out_features = inner_dim * 2, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         self.to_out = nn.Sequential(
-            nn.Linear(inner_dim, dim, bias = False),
-            nn.Dropout(dropout)
-        )
+            nn.Linear(in_features = inner_dim, out_features = dim, bias = False),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(
         self,
@@ -223,20 +224,20 @@ 
         self.to_patch_embedding = nn.Sequential(
             LayerNorm(patch_dim),
-            nn.Linear(patch_dim, dim),
+            nn.Linear(in_features = patch_dim, out_features = dim),
             LayerNorm(dim),
-        )
-
-        self.pos_embed_height = nn.Parameter(torch.randn(patch_height_dim, dim))
-        self.pos_embed_width = nn.Parameter(torch.randn(patch_width_dim, dim))
-
-        self.dropout = nn.Dropout(emb_dropout)
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.pos_embed_height = nn.Parameter(ops.randn(size = patch_height_dim, generator = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+        self.pos_embed_width = nn.Parameter(ops.randn(size = patch_width_dim, generator = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+
+        self.dropout = nn.Dropout(p = emb_dropout)
 
         self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)
 
         # final attention pooling queries
 
-        self.attn_pool_queries = nn.Parameter(torch.randn(dim))
+        self.attn_pool_queries = nn.Parameter(ops.randn(size = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
         self.attn_pool = Attention(dim = dim, dim_head = dim_head, heads = heads)
 
         # output to logits
@@ -245,8 +246,8 @@ 
         self.mlp_head = nn.Sequential(
             LayerNorm(dim),
-            nn.Linear(dim, num_classes, bias = False)
-        )
+            nn.Linear(in_features = dim, out_features = num_classes, bias = False)
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     @property
     def device(self):
@@ -309,20 +310,18 @@                     token_dropout = self.calc_token_dropout(*image_dims)
                     seq_len = seq.shape[0]
                     num_keep = max(1, int(seq_len * (1 - token_dropout)))
-                    keep_indices = torch.randn((seq_len,), device=device).topk(num_keep, dim=-1).indices
+                    keep_indices = ops.randn(size = (seq_len,)).topk(num_keep, dim=-1).indices  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
                     sequences[i] = seq[keep_indices]
                     positions[i] = pos[keep_indices]
 
             # build image_ids efficiently using repeat_interleave
             patch_counts = [seq.shape[0] for seq in sequences]
-            image_ids = torch.repeat_interleave(
-                arange(len(images)),
-                torch.tensor(patch_counts, device=device)
-            )
+            image_ids = ops.repeat_interleave(
+                input = arange(len(images)), repeats = torch.tensor(patch_counts, device=device))
 
             batched_image_ids.append(image_ids)
-            batched_sequences.append(torch.cat(sequences, dim=0))
-            batched_positions.append(torch.cat(positions, dim=0))
+            batched_sequences.append(ops.cat(tensors = sequences, dim = 0))  # 'torch.cat':没有对应的mindspore参数 'out';
+            batched_positions.append(ops.cat(tensors = positions, dim = 0))  # 'torch.cat':没有对应的mindspore参数 'out';
 
         # derive key padding mask
 
