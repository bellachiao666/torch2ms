--- pytorch+++ mindspore@@ -1,3 +1,4 @@+from mindspore.mint import nn, ops
 from __future__ import annotations
 
 from typing import List
@@ -32,36 +33,36 @@ 
 def FeedForward(dim, hidden_dim, dropout = 0.):
     return nn.Sequential(
-        nn.LayerNorm(dim, bias = False),
-        nn.Linear(dim, hidden_dim),
+        nn.LayerNorm(normalized_shape = dim, bias = False),
+        nn.Linear(in_features = dim, out_features = hidden_dim),
         nn.GELU(),
-        nn.Dropout(dropout),
-        nn.Linear(hidden_dim, dim),
-        nn.Dropout(dropout)
-    )
+        nn.Dropout(p = dropout),
+        nn.Linear(in_features = hidden_dim, out_features = dim),
+        nn.Dropout(p = dropout)
+    )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
 class Attention(Module):
     def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., qk_norm = True):
         super().__init__()
-        self.norm = nn.LayerNorm(dim, bias = False)
+        self.norm = nn.LayerNorm(normalized_shape = dim, bias = False)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
 
         dim_inner = heads * dim_head
         self.heads = heads
         self.dim_head = dim_head
 
-        self.to_queries = nn.Linear(dim, dim_inner, bias = False)
-        self.to_keys = nn.Linear(dim, dim_inner, bias = False)
-        self.to_values = nn.Linear(dim, dim_inner, bias = False)
+        self.to_queries = nn.Linear(in_features = dim, out_features = dim_inner, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+        self.to_keys = nn.Linear(in_features = dim, out_features = dim_inner, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+        self.to_values = nn.Linear(in_features = dim, out_features = dim_inner, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         # in the paper, they employ qk rmsnorm, a way to stabilize attention
         # will use layernorm in place of rmsnorm, which has been shown to work in certain papers. requires l2norm on non-ragged dimension to be supported in nested tensors
 
-        self.query_norm = nn.LayerNorm(dim_head, bias = False) if qk_norm else nn.Identity()
-        self.key_norm = nn.LayerNorm(dim_head, bias = False) if qk_norm else nn.Identity()
+        self.query_norm = nn.LayerNorm(normalized_shape = dim_head, bias = False) if qk_norm else nn.Identity()  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
+        self.key_norm = nn.LayerNorm(normalized_shape = dim_head, bias = False) if qk_norm else nn.Identity()  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
 
         self.dropout = dropout
 
-        self.to_out = nn.Linear(dim_inner, dim, bias = False)
+        self.to_out = nn.Linear(in_features = dim_inner, out_features = dim, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(
         self, 
@@ -121,7 +122,7 @@                 FeedForward(dim, mlp_dim, dropout = dropout)
             ]))
 
-        self.norm = nn.LayerNorm(dim, bias = False)
+        self.norm = nn.LayerNorm(normalized_shape = dim, bias = False)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
 
     def forward(self, x):
 
@@ -175,21 +176,21 @@         self.to_patches = Rearrange('c (h p1) (w p2) -> h w (c p1 p2)', p1 = patch_size, p2 = patch_size)
 
         self.to_patch_embedding = nn.Sequential(
-            nn.LayerNorm(patch_dim),
-            nn.Linear(patch_dim, dim),
-            nn.LayerNorm(dim),
-        )
-
-        self.pos_embed_height = nn.Parameter(torch.randn(patch_height_dim, dim))
-        self.pos_embed_width = nn.Parameter(torch.randn(patch_width_dim, dim))
-
-        self.dropout = nn.Dropout(emb_dropout)
+            nn.LayerNorm(normalized_shape = patch_dim),
+            nn.Linear(in_features = patch_dim, out_features = dim),
+            nn.LayerNorm(normalized_shape = dim),
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.pos_embed_height = nn.Parameter(ops.randn(size = patch_height_dim, generator = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+        self.pos_embed_width = nn.Parameter(ops.randn(size = patch_width_dim, generator = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+
+        self.dropout = nn.Dropout(p = emb_dropout)
 
         self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout, qk_rmsnorm)
 
         # final attention pooling queries
 
-        self.attn_pool_queries = nn.Parameter(torch.randn(dim))
+        self.attn_pool_queries = nn.Parameter(ops.randn(size = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
         self.attn_pool = Attention(dim = dim, dim_head = dim_head, heads = heads)
 
         # output to logits
@@ -197,9 +198,9 @@         self.to_latent = nn.Identity()
 
         self.mlp_head = nn.Sequential(
-            nn.LayerNorm(dim, bias = False),
-            nn.Linear(dim, num_classes, bias = False)
-        )
+            nn.LayerNorm(normalized_shape = dim, bias = False),
+            nn.Linear(in_features = dim, out_features = num_classes, bias = False)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     @property
     def device(self):
@@ -222,7 +223,7 @@ 
         for patches in all_patches:
             patch_height, patch_width = patches.shape[:2]
-            hw_indices = torch.stack(torch.meshgrid((arange(patch_height), arange(patch_width)), indexing = 'ij'), dim = -1)
+            hw_indices = ops.stack(tensors = ops.meshgrid(tensors = (arange(patch_height), arange(patch_width)), indexing = 'ij'), dim = -1)  # 'torch.stack':没有对应的mindspore参数 'out';
             hw_indices = rearrange(hw_indices, 'h w c -> (h w) c')
             positions.append(hw_indices)
 
@@ -242,7 +243,7 @@             kept_positions = []
 
             for one_image_tokens, one_image_positions, seq_len, num_keep in zip(tokens, positions, seq_lens, keep_seq_lens):
-                keep_indices = torch.randn((seq_len,), device = device).topk(num_keep, dim = -1).indices
+                keep_indices = ops.randn(size = (seq_len,)).topk(num_keep, dim = -1).indices  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
                 one_image_kept_tokens = one_image_tokens[keep_indices]
                 one_image_kept_positions = one_image_positions[keep_indices]
@@ -254,14 +255,14 @@ 
         # add all height and width factorized positions
 
-        height_indices, width_indices = torch.cat(positions).unbind(dim = -1)
+        height_indices, width_indices = ops.cat(tensors = positions).unbind(dim = -1)  # 'torch.cat':没有对应的mindspore参数 'out';
         height_embed, width_embed = self.pos_embed_height[height_indices], self.pos_embed_width[width_indices]
 
         pos_embed = height_embed + width_embed
 
         # use nested tensor for transformers and save on padding computation
 
-        tokens = torch.cat(tokens)
+        tokens = ops.cat(tensors = tokens)  # 'torch.cat':没有对应的mindspore参数 'out';
 
         # linear projection to patch embeddings
 
@@ -292,7 +293,7 @@ 
         # back to unjagged
 
-        logits = torch.stack(pooled.unbind())
+        logits = ops.stack(tensors = pooled.unbind())  # 'torch.stack':没有对应的mindspore参数 'out';
 
         logits = rearrange(logits, 'b 1 d -> b d')
 
@@ -320,10 +321,10 @@     # 5 images of different resolutions - List[Tensor]
 
     images = [
-        torch.randn(3, 256, 256), torch.randn(3, 128, 128),
-        torch.randn(3, 128, 256), torch.randn(3, 256, 128),
-        torch.randn(3, 64, 256)
-    ]
+        ops.randn(size = 3, generator = 256), ops.randn(size = 3, generator = 128),
+        ops.randn(size = 3, generator = 128), ops.randn(size = 3, generator = 256),
+        ops.randn(size = 3, generator = 64)
+    ]  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
     assert v(images).shape == (5, 1000)
 
