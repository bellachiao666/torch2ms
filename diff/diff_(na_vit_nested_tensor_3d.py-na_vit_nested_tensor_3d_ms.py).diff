--- pytorch+++ mindspore@@ -1,3 +1,4 @@+from mindspore.mint import nn, ops
 from __future__ import annotations
 
 from typing import List
@@ -32,36 +33,36 @@ 
 def FeedForward(dim, hidden_dim, dropout = 0.):
     return nn.Sequential(
-        nn.LayerNorm(dim, bias = False),
-        nn.Linear(dim, hidden_dim),
+        nn.LayerNorm(normalized_shape = dim, bias = False),
+        nn.Linear(in_features = dim, out_features = hidden_dim),
         nn.GELU(),
-        nn.Dropout(dropout),
-        nn.Linear(hidden_dim, dim),
-        nn.Dropout(dropout)
-    )
+        nn.Dropout(p = dropout),
+        nn.Linear(in_features = hidden_dim, out_features = dim),
+        nn.Dropout(p = dropout)
+    )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
 class Attention(Module):
     def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., qk_norm = True):
         super().__init__()
-        self.norm = nn.LayerNorm(dim, bias = False)
+        self.norm = nn.LayerNorm(normalized_shape = dim, bias = False)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
 
         dim_inner = heads * dim_head
         self.heads = heads
         self.dim_head = dim_head
 
-        self.to_queries = nn.Linear(dim, dim_inner, bias = False)
-        self.to_keys = nn.Linear(dim, dim_inner, bias = False)
-        self.to_values = nn.Linear(dim, dim_inner, bias = False)
+        self.to_queries = nn.Linear(in_features = dim, out_features = dim_inner, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+        self.to_keys = nn.Linear(in_features = dim, out_features = dim_inner, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+        self.to_values = nn.Linear(in_features = dim, out_features = dim_inner, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         # in the paper, they employ qk rmsnorm, a way to stabilize attention
         # will use layernorm in place of rmsnorm, which has been shown to work in certain papers. requires l2norm on non-ragged dimension to be supported in nested tensors
 
-        self.query_norm = nn.LayerNorm(dim_head, bias = False) if qk_norm else nn.Identity()
-        self.key_norm = nn.LayerNorm(dim_head, bias = False) if qk_norm else nn.Identity()
+        self.query_norm = nn.LayerNorm(normalized_shape = dim_head, bias = False) if qk_norm else nn.Identity()  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
+        self.key_norm = nn.LayerNorm(normalized_shape = dim_head, bias = False) if qk_norm else nn.Identity()  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
 
         self.dropout = dropout
 
-        self.to_out = nn.Linear(dim_inner, dim, bias = False)
+        self.to_out = nn.Linear(in_features = dim_inner, out_features = dim, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(
         self, 
@@ -122,7 +123,7 @@                 FeedForward(dim, mlp_dim, dropout = dropout)
             ]))
 
-        self.norm = nn.LayerNorm(dim, bias = False)
+        self.norm = nn.LayerNorm(normalized_shape = dim, bias = False)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
 
     def forward(self, x):
 
@@ -179,31 +180,31 @@         self.to_patches = Rearrange('c (f pf) (h p1) (w p2) -> f h w (c pf p1 p2)', p1 = patch_size, p2 = patch_size, pf = frame_patch_size)
 
         self.to_patch_embedding = nn.Sequential(
-            nn.LayerNorm(patch_dim),
-            nn.Linear(patch_dim, dim),
-            nn.LayerNorm(dim),
-        )
-
-        self.pos_embed_frame = nn.Parameter(torch.zeros(patch_frame_dim, dim))
-        self.pos_embed_height = nn.Parameter(torch.zeros(patch_height_dim, dim))
-        self.pos_embed_width = nn.Parameter(torch.zeros(patch_width_dim, dim))
+            nn.LayerNorm(normalized_shape = patch_dim),
+            nn.Linear(in_features = patch_dim, out_features = dim),
+            nn.LayerNorm(normalized_shape = dim),
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.pos_embed_frame = nn.Parameter(ops.zeros(size = patch_frame_dim))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
+        self.pos_embed_height = nn.Parameter(ops.zeros(size = patch_height_dim))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
+        self.pos_embed_width = nn.Parameter(ops.zeros(size = patch_width_dim))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
 
         # register tokens
 
-        self.register_tokens = nn.Parameter(torch.zeros(num_registers, dim))
+        self.register_tokens = nn.Parameter(ops.zeros(size = num_registers))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
 
         nn.init.normal_(self.pos_embed_frame, std = 0.02)
         nn.init.normal_(self.pos_embed_height, std = 0.02)
         nn.init.normal_(self.pos_embed_width, std = 0.02)
         nn.init.normal_(self.register_tokens, std = 0.02)
 
-        self.dropout = nn.Dropout(emb_dropout)
+        self.dropout = nn.Dropout(p = emb_dropout)
 
         self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout, qk_rmsnorm)
 
         # final attention pooling queries
 
-        self.attn_pool_queries = nn.Parameter(torch.randn(dim))
+        self.attn_pool_queries = nn.Parameter(ops.randn(size = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
         self.attn_pool = Attention(dim = dim, dim_head = dim_head, heads = heads)
 
         # output to logits
@@ -211,9 +212,9 @@         self.to_latent = nn.Identity()
 
         self.mlp_head = nn.Sequential(
-            nn.LayerNorm(dim, bias = False),
-            nn.Linear(dim, num_classes, bias = False)
-        )
+            nn.LayerNorm(normalized_shape = dim, bias = False),
+            nn.Linear(in_features = dim, out_features = num_classes, bias = False)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     @property
     def device(self):
@@ -236,7 +237,7 @@ 
         for patches in all_patches:
             patch_frame, patch_height, patch_width = patches.shape[:3]
-            fhw_indices = torch.stack(torch.meshgrid((arange(patch_frame), arange(patch_height), arange(patch_width)), indexing = 'ij'), dim = -1)
+            fhw_indices = ops.stack(tensors = ops.meshgrid(tensors = (arange(patch_frame), arange(patch_height), arange(patch_width)), indexing = 'ij'), dim = -1)  # 'torch.stack':没有对应的mindspore参数 'out';
             fhw_indices = rearrange(fhw_indices, 'f h w c -> (f h w) c')
 
             positions.append(fhw_indices)
@@ -257,7 +258,7 @@             kept_positions = []
 
             for one_image_tokens, one_image_positions, seq_len, num_keep in zip(tokens, positions, seq_lens, keep_seq_lens):
-                keep_indices = torch.randn((seq_len,), device = device).topk(num_keep, dim = -1).indices
+                keep_indices = ops.randn(size = (seq_len,)).topk(num_keep, dim = -1).indices  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
                 one_image_kept_tokens = one_image_tokens[keep_indices]
                 one_image_kept_positions = one_image_positions[keep_indices]
@@ -270,12 +271,12 @@         # add all height and width factorized positions
 
 
-        frame_indices, height_indices, width_indices = torch.cat(positions).unbind(dim = -1)
+        frame_indices, height_indices, width_indices = ops.cat(tensors = positions).unbind(dim = -1)  # 'torch.cat':没有对应的mindspore参数 'out';
         frame_embed, height_embed, width_embed = self.pos_embed_frame[frame_indices], self.pos_embed_height[height_indices], self.pos_embed_width[width_indices]
 
         pos_embed = frame_embed + height_embed + width_embed
 
-        tokens = torch.cat(tokens)
+        tokens = ops.cat(tensors = tokens)  # 'torch.cat':没有对应的mindspore参数 'out';
 
         # linear projection to patch embeddings
 
@@ -289,7 +290,7 @@ 
         tokens = tokens.split(seq_lens.tolist())
 
-        tokens = [torch.cat((self.register_tokens, one_tokens)) for one_tokens in tokens]
+        tokens = [ops.cat(tensors = (self.register_tokens, one_tokens)) for one_tokens in tokens]  # 'torch.cat':没有对应的mindspore参数 'out';
 
         # use nested tensor for transformers and save on padding computation
 
@@ -314,7 +315,7 @@ 
         # back to unjagged
 
-        logits = torch.stack(pooled.unbind())
+        logits = ops.stack(tensors = pooled.unbind())  # 'torch.stack':没有对应的mindspore参数 'out';
 
         logits = rearrange(logits, 'b 1 d -> b d')
 
@@ -346,10 +347,10 @@     # 5 volumetric data (videos or CT scans) of different resolutions - List[Tensor]
 
     volumes = [
-        torch.randn(3, 2, 256, 256), torch.randn(3, 8, 128, 128),
-        torch.randn(3, 4, 128, 256), torch.randn(3, 2, 256, 128),
-        torch.randn(3, 4, 64, 256)
-    ]
+        ops.randn(size = 3, generator = 2, dtype = 256), ops.randn(size = 3, generator = 8, dtype = 128),
+        ops.randn(size = 3, generator = 4, dtype = 256), ops.randn(size = 3, generator = 2, dtype = 128),
+        ops.randn(size = 3, generator = 4, dtype = 256)
+    ]  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
     assert v(volumes).shape == (5, 1000)
 
