--- pytorch+++ mindspore@@ -1,9 +1,9 @@ from functools import partial
-import torch
 from torch import nn
 
 from einops import rearrange, repeat
 from einops.layers.torch import Rearrange, Reduce
+from mindspore.mint import nn, ops
 
 # helpers
 
@@ -21,54 +21,54 @@ 
 # helper classes
 
-class ChanLayerNorm(nn.Module):
+class ChanLayerNorm(nn.Cell):
     def __init__(self, dim, eps = 1e-5):
         super().__init__()
         self.eps = eps
-        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))
-        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))
-
-    def forward(self, x):
-        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)
-        mean = torch.mean(x, dim = 1, keepdim = True)
+        self.g = mindspore.Parameter(ops.ones(size = 1, dtype = 1))  # 'torch.ones':没有对应的mindspore参数 'out';; 'torch.ones':没有对应的mindspore参数 'layout';; 'torch.ones':没有对应的mindspore参数 'device';; 'torch.ones':没有对应的mindspore参数 'requires_grad';
+        self.b = mindspore.Parameter(ops.zeros(size = 1, dtype = 1))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
+
+    def forward(self, x):
+        var = ops.var(input = x, dim = 1, keepdim = True)  # 'torch.var':没有对应的mindspore参数 'out';
+        mean = ops.mean(input = x, dim = 1, keepdim = True)
         return (x - mean) / (var + self.eps).sqrt() * self.g + self.b
 
-class Downsample(nn.Module):
+class Downsample(nn.Cell):
     def __init__(self, dim_in, dim_out):
         super().__init__()
-        self.conv = nn.Conv2d(dim_in, dim_out, 3, stride = 2, padding = 1)
+        self.conv = nn.Conv2d(in_channels = dim_in, out_channels = dim_out, kernel_size = 3, stride = 2, padding = 1)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return self.conv(x)
 
-class PEG(nn.Module):
+class PEG(nn.Cell):
     def __init__(self, dim, kernel_size = 3):
         super().__init__()
-        self.proj = nn.Conv2d(dim, dim, kernel_size = kernel_size, padding = kernel_size // 2, groups = dim, stride = 1)
+        self.proj = nn.Conv2d(in_channels = dim, out_channels = dim, kernel_size = kernel_size, stride = 1, padding = kernel_size // 2, groups = dim)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return self.proj(x) + x
 
 # feedforward
 
-class FeedForward(nn.Module):
+class FeedForward(nn.Cell):
     def __init__(self, dim, expansion_factor = 4, dropout = 0.):
         super().__init__()
         inner_dim = dim * expansion_factor
-        self.net = nn.Sequential(
+        self.net = nn.SequentialCell(
             ChanLayerNorm(dim),
-            nn.Conv2d(dim, inner_dim, 1),
+            nn.Conv2d(in_channels = dim, out_channels = inner_dim, kernel_size = 1),
             nn.GELU(),
-            nn.Dropout(dropout),
-            nn.Conv2d(inner_dim, dim, 1),
-            nn.Dropout(dropout)
-        )
+            nn.Dropout(p = dropout),
+            nn.Conv2d(in_channels = inner_dim, out_channels = dim, kernel_size = 1),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
     def forward(self, x):
         return self.net(x)
 
 # attention
 
-class ScalableSelfAttention(nn.Module):
+class ScalableSelfAttention(nn.Cell):
     def __init__(
         self,
         dim,
@@ -82,17 +82,17 @@         self.heads = heads
         self.scale = dim_key ** -0.5
         self.attend = nn.Softmax(dim = -1)
-        self.dropout = nn.Dropout(dropout)
+        self.dropout = nn.Dropout(p = dropout)
 
         self.norm = ChanLayerNorm(dim)
-        self.to_q = nn.Conv2d(dim, dim_key * heads, 1, bias = False)
-        self.to_k = nn.Conv2d(dim, dim_key * heads, reduction_factor, stride = reduction_factor, bias = False)
-        self.to_v = nn.Conv2d(dim, dim_value * heads, reduction_factor, stride = reduction_factor, bias = False)
-
-        self.to_out = nn.Sequential(
-            nn.Conv2d(dim_value * heads, dim, 1),
-            nn.Dropout(dropout)
-        )
+        self.to_q = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_k = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = reduction_factor, stride = reduction_factor, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_v = nn.Conv2d(in_channels = dim, out_channels = dim_value * heads, kernel_size = reduction_factor, stride = reduction_factor, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+
+        self.to_out = nn.SequentialCell(
+            nn.Conv2d(in_channels = dim_value * heads, out_channels = dim, kernel_size = 1),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         height, width, heads = *x.shape[-2:], self.heads
@@ -107,7 +107,7 @@ 
         # similarity
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = ops.matmul(input = q, other = k.transpose(-1, -2)) * self.scale  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # attention
 
@@ -116,14 +116,14 @@ 
         # aggregate values
 
-        out = torch.matmul(attn, v)
+        out = ops.matmul(input = attn, other = v)  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # merge back heads
 
         out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = height, y = width)
         return self.to_out(out)
 
-class InteractiveWindowedSelfAttention(nn.Module):
+class InteractiveWindowedSelfAttention(nn.Cell):
     def __init__(
         self,
         dim,
@@ -138,19 +138,19 @@         self.scale = dim_key ** -0.5
         self.window_size = window_size
         self.attend = nn.Softmax(dim = -1)
-        self.dropout = nn.Dropout(dropout)
+        self.dropout = nn.Dropout(p = dropout)
 
         self.norm = ChanLayerNorm(dim)
-        self.local_interactive_module = nn.Conv2d(dim_value * heads, dim_value * heads, 3, padding = 1)
-
-        self.to_q = nn.Conv2d(dim, dim_key * heads, 1, bias = False)
-        self.to_k = nn.Conv2d(dim, dim_key * heads, 1, bias = False)
-        self.to_v = nn.Conv2d(dim, dim_value * heads, 1, bias = False)
-
-        self.to_out = nn.Sequential(
-            nn.Conv2d(dim_value * heads, dim, 1),
-            nn.Dropout(dropout)
-        )
+        self.local_interactive_module = nn.Conv2d(in_channels = dim_value * heads, out_channels = dim_value * heads, kernel_size = 3, padding = 1)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+
+        self.to_q = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_k = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_v = nn.Conv2d(in_channels = dim, out_channels = dim_value * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+
+        self.to_out = nn.SequentialCell(
+            nn.Conv2d(in_channels = dim_value * heads, out_channels = dim, kernel_size = 1),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         height, width, heads, wsz = *x.shape[-2:], self.heads, self.window_size
@@ -172,7 +172,7 @@ 
         # similarity
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = ops.matmul(input = q, other = k.transpose(-1, -2)) * self.scale  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # attention
 
@@ -181,7 +181,7 @@ 
         # aggregate values
 
-        out = torch.matmul(attn, v)
+        out = ops.matmul(input = attn, other = v)  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # reshape the windows back to full feature map (and merge heads)
 
@@ -193,7 +193,7 @@ 
         return self.to_out(out)
 
-class Transformer(nn.Module):
+class Transformer(nn.Cell):
     def __init__(
         self,
         dim,
@@ -210,11 +210,11 @@         norm_output = True
     ):
         super().__init__()
-        self.layers = nn.ModuleList([])
+        self.layers = nn.CellList([])
         for ind in range(depth):
             is_first = ind == 0
 
-            self.layers.append(nn.ModuleList([
+            self.layers.append(nn.CellList([
                 ScalableSelfAttention(dim, heads = heads, dim_key = ssa_dim_key, dim_value = ssa_dim_value, reduction_factor = ssa_reduction_factor, dropout = dropout),
                 FeedForward(dim, expansion_factor = ff_expansion_factor, dropout = dropout),
                 PEG(dim) if is_first else None,
@@ -237,7 +237,7 @@ 
         return self.norm(x)
 
-class ScalableViT(nn.Module):
+class ScalableViT(nn.Cell):
     def __init__(
         self,
         *,
@@ -256,7 +256,7 @@         dropout = 0.
     ):
         super().__init__()
-        self.to_patches = nn.Conv2d(channels, dim, 7, stride = 4, padding = 3)
+        self.to_patches = nn.Conv2d(in_channels = channels, out_channels = dim, kernel_size = 7, stride = 4, padding = 3)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         assert isinstance(depth, tuple), 'depth needs to be tuple if integers indicating number of transformer blocks at that stage'
 
@@ -276,21 +276,21 @@         hyperparams_per_stage = list(map(partial(cast_tuple, length = num_stages), hyperparams_per_stage))
         assert all(tuple(map(lambda arr: len(arr) == num_stages, hyperparams_per_stage)))
 
-        self.layers = nn.ModuleList([])
+        self.layers = nn.CellList([])
 
         for ind, (layer_dim, layer_depth, layer_heads, layer_ssa_dim_key, layer_ssa_dim_value, layer_ssa_reduction_factor, layer_iwsa_dim_key, layer_iwsa_dim_value, layer_window_size) in enumerate(zip(dims, depth, *hyperparams_per_stage)):
             is_last = ind == (num_stages - 1)
 
-            self.layers.append(nn.ModuleList([
+            self.layers.append(nn.CellList([
                 Transformer(dim = layer_dim, depth = layer_depth, heads = layer_heads, ff_expansion_factor = ff_expansion_factor, dropout = dropout, ssa_dim_key = layer_ssa_dim_key, ssa_dim_value = layer_ssa_dim_value, ssa_reduction_factor = layer_ssa_reduction_factor, iwsa_dim_key = layer_iwsa_dim_key, iwsa_dim_value = layer_iwsa_dim_value, iwsa_window_size = layer_window_size, norm_output = not is_last),
                 Downsample(layer_dim, layer_dim * 2) if not is_last else None
             ]))
 
-        self.mlp_head = nn.Sequential(
+        self.mlp_head = nn.SequentialCell(
             Reduce('b d h w -> b d', 'mean'),
-            nn.LayerNorm(dims[-1]),
-            nn.Linear(dims[-1], num_classes)
-        )
+            nn.LayerNorm(normalized_shape = dims[-1]),
+            nn.Linear(in_features = dims[-1], out_features = num_classes)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, img):
         x = self.to_patches(img)
