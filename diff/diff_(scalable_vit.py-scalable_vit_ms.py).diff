--- pytorch+++ mindspore@@ -1,3 +1,4 @@+from mindspore.mint import nn, ops
 from functools import partial
 import torch
 from torch import nn
@@ -25,18 +26,18 @@     def __init__(self, dim, eps = 1e-5):
         super().__init__()
         self.eps = eps
-        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))
-        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))
-
-    def forward(self, x):
-        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)
-        mean = torch.mean(x, dim = 1, keepdim = True)
+        self.g = nn.Parameter(ops.ones(size = 1, dtype = 1))  # 'torch.ones':没有对应的mindspore参数 'out';; 'torch.ones':没有对应的mindspore参数 'layout';; 'torch.ones':没有对应的mindspore参数 'device';; 'torch.ones':没有对应的mindspore参数 'requires_grad';
+        self.b = nn.Parameter(ops.zeros(size = 1, dtype = 1))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
+
+    def forward(self, x):
+        var = ops.var(input = x, dim = 1, keepdim = True)  # 'torch.var':没有对应的mindspore参数 'out';
+        mean = ops.mean(input = x, dim = 1, keepdim = True)
         return (x - mean) / (var + self.eps).sqrt() * self.g + self.b
 
 class Downsample(nn.Module):
     def __init__(self, dim_in, dim_out):
         super().__init__()
-        self.conv = nn.Conv2d(dim_in, dim_out, 3, stride = 2, padding = 1)
+        self.conv = nn.Conv2d(in_channels = dim_in, out_channels = dim_out, kernel_size = 3, stride = 2, padding = 1)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return self.conv(x)
@@ -44,7 +45,7 @@ class PEG(nn.Module):
     def __init__(self, dim, kernel_size = 3):
         super().__init__()
-        self.proj = nn.Conv2d(dim, dim, kernel_size = kernel_size, padding = kernel_size // 2, groups = dim, stride = 1)
+        self.proj = nn.Conv2d(in_channels = dim, out_channels = dim, kernel_size = kernel_size, stride = 1, padding = kernel_size // 2, groups = dim)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return self.proj(x) + x
@@ -57,12 +58,12 @@         inner_dim = dim * expansion_factor
         self.net = nn.Sequential(
             ChanLayerNorm(dim),
-            nn.Conv2d(dim, inner_dim, 1),
+            nn.Conv2d(in_channels = dim, out_channels = inner_dim, kernel_size = 1),
             nn.GELU(),
-            nn.Dropout(dropout),
-            nn.Conv2d(inner_dim, dim, 1),
-            nn.Dropout(dropout)
-        )
+            nn.Dropout(p = dropout),
+            nn.Conv2d(in_channels = inner_dim, out_channels = dim, kernel_size = 1),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
     def forward(self, x):
         return self.net(x)
 
@@ -82,17 +83,17 @@         self.heads = heads
         self.scale = dim_key ** -0.5
         self.attend = nn.Softmax(dim = -1)
-        self.dropout = nn.Dropout(dropout)
+        self.dropout = nn.Dropout(p = dropout)
 
         self.norm = ChanLayerNorm(dim)
-        self.to_q = nn.Conv2d(dim, dim_key * heads, 1, bias = False)
-        self.to_k = nn.Conv2d(dim, dim_key * heads, reduction_factor, stride = reduction_factor, bias = False)
-        self.to_v = nn.Conv2d(dim, dim_value * heads, reduction_factor, stride = reduction_factor, bias = False)
+        self.to_q = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_k = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = reduction_factor, stride = reduction_factor, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_v = nn.Conv2d(in_channels = dim, out_channels = dim_value * heads, kernel_size = reduction_factor, stride = reduction_factor, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         self.to_out = nn.Sequential(
-            nn.Conv2d(dim_value * heads, dim, 1),
-            nn.Dropout(dropout)
-        )
+            nn.Conv2d(in_channels = dim_value * heads, out_channels = dim, kernel_size = 1),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         height, width, heads = *x.shape[-2:], self.heads
@@ -107,7 +108,7 @@ 
         # similarity
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = ops.matmul(input = q, other = k.transpose(-1, -2)) * self.scale  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # attention
 
@@ -116,7 +117,7 @@ 
         # aggregate values
 
-        out = torch.matmul(attn, v)
+        out = ops.matmul(input = attn, other = v)  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # merge back heads
 
@@ -138,19 +139,19 @@         self.scale = dim_key ** -0.5
         self.window_size = window_size
         self.attend = nn.Softmax(dim = -1)
-        self.dropout = nn.Dropout(dropout)
+        self.dropout = nn.Dropout(p = dropout)
 
         self.norm = ChanLayerNorm(dim)
-        self.local_interactive_module = nn.Conv2d(dim_value * heads, dim_value * heads, 3, padding = 1)
-
-        self.to_q = nn.Conv2d(dim, dim_key * heads, 1, bias = False)
-        self.to_k = nn.Conv2d(dim, dim_key * heads, 1, bias = False)
-        self.to_v = nn.Conv2d(dim, dim_value * heads, 1, bias = False)
+        self.local_interactive_module = nn.Conv2d(in_channels = dim_value * heads, out_channels = dim_value * heads, kernel_size = 3, padding = 1)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+
+        self.to_q = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_k = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_v = nn.Conv2d(in_channels = dim, out_channels = dim_value * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         self.to_out = nn.Sequential(
-            nn.Conv2d(dim_value * heads, dim, 1),
-            nn.Dropout(dropout)
-        )
+            nn.Conv2d(in_channels = dim_value * heads, out_channels = dim, kernel_size = 1),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         height, width, heads, wsz = *x.shape[-2:], self.heads, self.window_size
@@ -172,7 +173,7 @@ 
         # similarity
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = ops.matmul(input = q, other = k.transpose(-1, -2)) * self.scale  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # attention
 
@@ -181,7 +182,7 @@ 
         # aggregate values
 
-        out = torch.matmul(attn, v)
+        out = ops.matmul(input = attn, other = v)  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # reshape the windows back to full feature map (and merge heads)
 
@@ -256,7 +257,7 @@         dropout = 0.
     ):
         super().__init__()
-        self.to_patches = nn.Conv2d(channels, dim, 7, stride = 4, padding = 3)
+        self.to_patches = nn.Conv2d(in_channels = channels, out_channels = dim, kernel_size = 7, stride = 4, padding = 3)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         assert isinstance(depth, tuple), 'depth needs to be tuple if integers indicating number of transformer blocks at that stage'
 
@@ -288,9 +289,9 @@ 
         self.mlp_head = nn.Sequential(
             Reduce('b d h w -> b d', 'mean'),
-            nn.LayerNorm(dims[-1]),
-            nn.Linear(dims[-1], num_classes)
-        )
+            nn.LayerNorm(normalized_shape = dims[-1]),
+            nn.Linear(in_features = dims[-1], out_features = num_classes)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, img):
         x = self.to_patches(img)
