--- pytorch+++ mindspore@@ -1,9 +1,9 @@ from functools import partial
-import torch
 from torch import nn
 
 from einops import rearrange, repeat
 from einops.layers.torch import Rearrange, Reduce
+from mindspore.mint import nn, ops
 
 # helpers
 
@@ -21,54 +21,54 @@ 
 # helper classes
 
-class ChanLayerNorm(nn.Module):
+class ChanLayerNorm(nn.Cell):
     def __init__(self, dim, eps = 1e-5):
         super().__init__()
         self.eps = eps
-        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))
-        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))
-
-    def forward(self, x):
-        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)
-        mean = torch.mean(x, dim = 1, keepdim = True)
+        self.g = nn.Parameter(ops.ones(size = 1, dtype = 1))  # 'torch.ones':没有对应的mindspore参数 'out';; 'torch.ones':没有对应的mindspore参数 'layout';; 'torch.ones':没有对应的mindspore参数 'device';; 'torch.ones':没有对应的mindspore参数 'requires_grad';
+        self.b = nn.Parameter(ops.zeros(size = 1, dtype = 1))  # 'torch.zeros':没有对应的mindspore参数 'out';; 'torch.zeros':没有对应的mindspore参数 'layout';; 'torch.zeros':没有对应的mindspore参数 'device';; 'torch.zeros':没有对应的mindspore参数 'requires_grad';
+
+    def forward(self, x):
+        var = ops.var(input = x, dim = 1, keepdim = True)  # 'torch.var':没有对应的mindspore参数 'out';
+        mean = ops.mean(input = x, dim = 1, keepdim = True)
         return (x - mean) / (var + self.eps).sqrt() * self.g + self.b
 
-class Downsample(nn.Module):
+class Downsample(nn.Cell):
     def __init__(self, dim_in, dim_out):
         super().__init__()
-        self.conv = nn.Conv2d(dim_in, dim_out, 3, stride = 2, padding = 1)
+        self.conv = nn.Conv2d(in_channels = dim_in, out_channels = dim_out, kernel_size = 3, stride = 2, padding = 1)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return self.conv(x)
 
-class PEG(nn.Module):
+class PEG(nn.Cell):
     def __init__(self, dim, kernel_size = 3):
         super().__init__()
-        self.proj = nn.Conv2d(dim, dim, kernel_size = kernel_size, padding = kernel_size // 2, groups = dim, stride = 1)
+        self.proj = nn.Conv2d(in_channels = dim, out_channels = dim, kernel_size = kernel_size, stride = 1, padding = kernel_size // 2, groups = dim)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return self.proj(x) + x
 
 # feedforward
 
-class FeedForward(nn.Module):
+class FeedForward(nn.Cell):
     def __init__(self, dim, expansion_factor = 4, dropout = 0.):
         super().__init__()
         inner_dim = dim * expansion_factor
         self.net = nn.Sequential(
             ChanLayerNorm(dim),
-            nn.Conv2d(dim, inner_dim, 1),
+            nn.Conv2d(in_channels = dim, out_channels = inner_dim, kernel_size = 1),
             nn.GELU(),
-            nn.Dropout(dropout),
-            nn.Conv2d(inner_dim, dim, 1),
-            nn.Dropout(dropout)
-        )
+            nn.Dropout(p = dropout),
+            nn.Conv2d(in_channels = inner_dim, out_channels = dim, kernel_size = 1),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
     def forward(self, x):
         return self.net(x)
 
 # attention
 
-class ScalableSelfAttention(nn.Module):
+class ScalableSelfAttention(nn.Cell):
     def __init__(
         self,
         dim,
@@ -82,17 +82,17 @@         self.heads = heads
         self.scale = dim_key ** -0.5
         self.attend = nn.Softmax(dim = -1)
-        self.dropout = nn.Dropout(dropout)
+        self.dropout = nn.Dropout(p = dropout)
 
         self.norm = ChanLayerNorm(dim)
-        self.to_q = nn.Conv2d(dim, dim_key * heads, 1, bias = False)
-        self.to_k = nn.Conv2d(dim, dim_key * heads, reduction_factor, stride = reduction_factor, bias = False)
-        self.to_v = nn.Conv2d(dim, dim_value * heads, reduction_factor, stride = reduction_factor, bias = False)
+        self.to_q = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_k = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = reduction_factor, stride = reduction_factor, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_v = nn.Conv2d(in_channels = dim, out_channels = dim_value * heads, kernel_size = reduction_factor, stride = reduction_factor, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         self.to_out = nn.Sequential(
-            nn.Conv2d(dim_value * heads, dim, 1),
-            nn.Dropout(dropout)
-        )
+            nn.Conv2d(in_channels = dim_value * heads, out_channels = dim, kernel_size = 1),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         height, width, heads = *x.shape[-2:], self.heads
@@ -107,7 +107,7 @@ 
         # similarity
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = ops.matmul(input = q, other = k.transpose(-1, -2)) * self.scale  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # attention
 
@@ -116,14 +116,14 @@ 
         # aggregate values
 
-        out = torch.matmul(attn, v)
+        out = ops.matmul(input = attn, other = v)  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # merge back heads
 
         out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = height, y = width)
         return self.to_out(out)
 
-class InteractiveWindowedSelfAttention(nn.Module):
+class InteractiveWindowedSelfAttention(nn.Cell):
     def __init__(
         self,
         dim,
@@ -138,19 +138,19 @@         self.scale = dim_key ** -0.5
         self.window_size = window_size
         self.attend = nn.Softmax(dim = -1)
-        self.dropout = nn.Dropout(dropout)
+        self.dropout = nn.Dropout(p = dropout)
 
         self.norm = ChanLayerNorm(dim)
-        self.local_interactive_module = nn.Conv2d(dim_value * heads, dim_value * heads, 3, padding = 1)
-
-        self.to_q = nn.Conv2d(dim, dim_key * heads, 1, bias = False)
-        self.to_k = nn.Conv2d(dim, dim_key * heads, 1, bias = False)
-        self.to_v = nn.Conv2d(dim, dim_value * heads, 1, bias = False)
+        self.local_interactive_module = nn.Conv2d(in_channels = dim_value * heads, out_channels = dim_value * heads, kernel_size = 3, padding = 1)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+
+        self.to_q = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_k = nn.Conv2d(in_channels = dim, out_channels = dim_key * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
+        self.to_v = nn.Conv2d(in_channels = dim, out_channels = dim_value * heads, kernel_size = 1, bias = False)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         self.to_out = nn.Sequential(
-            nn.Conv2d(dim_value * heads, dim, 1),
-            nn.Dropout(dropout)
-        )
+            nn.Conv2d(in_channels = dim_value * heads, out_channels = dim, kernel_size = 1),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         height, width, heads, wsz = *x.shape[-2:], self.heads, self.window_size
@@ -172,7 +172,7 @@ 
         # similarity
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = ops.matmul(input = q, other = k.transpose(-1, -2)) * self.scale  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # attention
 
@@ -181,7 +181,7 @@ 
         # aggregate values
 
-        out = torch.matmul(attn, v)
+        out = ops.matmul(input = attn, other = v)  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         # reshape the windows back to full feature map (and merge heads)
 
@@ -193,7 +193,7 @@ 
         return self.to_out(out)
 
-class Transformer(nn.Module):
+class Transformer(nn.Cell):
     def __init__(
         self,
         dim,
@@ -237,7 +237,7 @@ 
         return self.norm(x)
 
-class ScalableViT(nn.Module):
+class ScalableViT(nn.Cell):
     def __init__(
         self,
         *,
@@ -256,7 +256,7 @@         dropout = 0.
     ):
         super().__init__()
-        self.to_patches = nn.Conv2d(channels, dim, 7, stride = 4, padding = 3)
+        self.to_patches = nn.Conv2d(in_channels = channels, out_channels = dim, kernel_size = 7, stride = 4, padding = 3)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';
 
         assert isinstance(depth, tuple), 'depth needs to be tuple if integers indicating number of transformer blocks at that stage'
 
@@ -288,9 +288,9 @@ 
         self.mlp_head = nn.Sequential(
             Reduce('b d h w -> b d', 'mean'),
-            nn.LayerNorm(dims[-1]),
-            nn.Linear(dims[-1], num_classes)
-        )
+            nn.LayerNorm(normalized_shape = dims[-1]),
+            nn.Linear(in_features = dims[-1], out_features = num_classes)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, img):
         x = self.to_patches(img)
