--- pytorch+++ mindspore@@ -1,3 +1,4 @@+from mindspore.mint import nn, ops
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
@@ -17,50 +18,34 @@ 
         # 基础卷积
         self.stem = nn.Sequential(
-            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1, bias=False),
-            nn.BatchNorm2d(32),
-            nn.ReLU(inplace=True),
-        )
+            nn.Conv2d(in_channels = in_channels, out_channels = 32, kernel_size = 3, padding = 1, bias = False),
+            nn.BatchNorm2d(num_features = 32),
+            nn.ReLU(),
+        )  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device';; 'torch.nn.BatchNorm2d':没有对应的mindspore参数 'device';; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace';
 
         # 测试 mint.nn 前缀，你的转换器需要把这些都转换掉
         # 包含关键字缺省、参数顺序、字段不一致等情况
         self.block = nn.Sequential(
             nn.Conv2d(
-                32,
-                64,
-                kernel_size=3,
-                stride=1,
-                padding=1,
-                bias=nn.Linear(1, 1)  # 参数中嵌套 API
-            ),
+                in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1, bias = nn.Linear(in_features = 1, out_features = 1)),
 
             nn.Sequential(
-                nn.ReLU(inplace=True),
+                nn.ReLU(),
                 nn.Conv2d(
-                    in_channels=64,
-                    out_channels=64,
-                    kernel_size=3,
-                    padding=1,
-                    bias=False
-                ),
+                    in_channels = 64, out_channels = 64, kernel_size = 3, padding = 1, bias = False),
             ),
 
             # 使用 wrapper 前缀调用
             myconv(
-                64,
-                128,
-                kernel_size=3,
-                padding=1,
-                bias=False
-            ),
-        )
+                in_channels = 64, out_channels = 128, kernel_size = 3, padding = 1, bias = False),
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';; 'torch.nn.Conv2d':没有对应的mindspore参数 'device';; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace';
 
         # 全连接部分
         self.classifier = nn.Sequential(
-            nn.Linear(128 * 7 * 7, num_classes),
-            nn.ReLU(inplace=True),
-            nn.Linear(num_classes, num_classes)
-        )
+            nn.Linear(in_features = 128 * 7 * 7, out_features = num_classes),
+            nn.ReLU(),
+            nn.Linear(in_features = num_classes, out_features = num_classes)
+        )  # 'torch.nn.Linear':没有对应的mindspore参数 'device';; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace';
 
 
     def forward(self, x):
@@ -70,10 +55,8 @@         x = self.block(x)
 
         # 测试多行函数调用
-        x = F.adaptive_avg_pool2d(
-            x,
-            (7, 7)
-        )
+        x = nn.functional.adaptive_avg_pool2d(
+            input = x, output_size = (7, 7))
 
         # 测试 view/reshape 不应被转换器破坏
         x = x.view(x.size(0), -1)
@@ -82,6 +65,6 @@         x = self.classifier(x)
 
         # 测试 F.xxx 不应误伤
-        x = F.softmax(x, dim=1)
+        x = nn.functional.softmax(input = x, dim = 1)  # 'torch.nn.functional.softmax':没有对应的mindspore参数 '_stacklevel';
 
         return x
