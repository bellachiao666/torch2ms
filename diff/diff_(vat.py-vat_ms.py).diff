--- pytorch+++ mindspore@@ -2,12 +2,12 @@ from contextlib import nullcontext
 
 import torch
-import torch.nn.functional as F
 from torch import nn, cat, stack, tensor
-from torch.nn import Module, ModuleList
+from torch.nn import ModuleList
 
 from einops import rearrange, repeat, pack, unpack
 from einops.layers.torch import Rearrange
+from mindspore.mint import nn, ops
 
 # helpers
 
@@ -22,13 +22,13 @@ 
 # classes
 
-class FiLM(Module):
+class FiLM(nn.Cell):
     def __init__(
         self,
         dim,
     ):
         super().__init__()
-        proj = nn.Linear(dim, dim * 2)
+        proj = nn.Linear(in_features = dim, out_features = dim * 2)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         self.to_gamma_beta = nn.Sequential(
             proj,
@@ -43,7 +43,7 @@ 
         return tokens * gamma + beta
 
-class FeedForward(Module):
+class FeedForward(nn.Cell):
     def __init__(
         self,
         dim,
@@ -52,18 +52,18 @@     ):
         super().__init__()
         self.net = nn.Sequential(
-            nn.LayerNorm(dim),
-            nn.Linear(dim, hidden_dim),
+            nn.LayerNorm(normalized_shape = dim),
+            nn.Linear(in_features = dim, out_features = hidden_dim),
             nn.GELU(),
-            nn.Dropout(dropout),
-            nn.Linear(hidden_dim, dim),
-            nn.Dropout(dropout)
-        )
+            nn.Dropout(p = dropout),
+            nn.Linear(in_features = hidden_dim, out_features = dim),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return self.net(x)
 
-class Attention(Module):
+class Attention(nn.Cell):
     def __init__(
         self,
         dim,
@@ -81,21 +81,21 @@         self.heads = heads
         self.scale = dim_head ** -0.5
 
-        self.norm = nn.LayerNorm(dim)
+        self.norm = nn.LayerNorm(normalized_shape = dim)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
 
         self.cross_attend = cross_attend
-        self.context_norm = nn.LayerNorm(dim_context) if cross_attend else None
+        self.context_norm = nn.LayerNorm(normalized_shape = dim_context) if cross_attend else None  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
 
         self.attend = nn.Softmax(dim = -1)
-        self.dropout = nn.Dropout(dropout)
-
-        self.to_q = nn.Linear(dim, inner_dim, bias = False)
-        self.to_kv = nn.Linear(dim_context, inner_dim * 2, bias = False)
+        self.dropout = nn.Dropout(p = dropout)
+
+        self.to_q = nn.Linear(in_features = dim, out_features = inner_dim, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+        self.to_kv = nn.Linear(in_features = dim_context, out_features = inner_dim * 2, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         self.to_out = nn.Sequential(
-            nn.Linear(inner_dim, dim),
-            nn.Dropout(dropout)
-        ) if project_out else nn.Identity()
+            nn.Linear(in_features = inner_dim, out_features = dim),
+            nn.Dropout(p = dropout)
+        ) if project_out else nn.Identity()  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, x, context = None):
 
@@ -116,16 +116,16 @@         qkv = (self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1))
         q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = ops.matmul(input = q, other = k.transpose(-1, -2)) * self.scale  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         attn = self.attend(dots)
         attn = self.dropout(attn)
 
-        out = torch.matmul(attn, v)
+        out = ops.matmul(input = attn, other = v)  # 'torch.matmul':没有对应的mindspore参数 'out';
         out = rearrange(out, 'b h n d -> b n (h d)')
         return self.to_out(out)
 
-class Transformer(Module):
+class Transformer(nn.Cell):
     def __init__(
         self,
         dim,
@@ -136,7 +136,7 @@         dropout = 0.
     ):
         super().__init__()
-        self.norm = nn.LayerNorm(dim)
+        self.norm = nn.LayerNorm(normalized_shape = dim)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
         self.layers = ModuleList([])
 
         for _ in range(depth):
@@ -166,7 +166,7 @@ 
         return x, hiddens
 
-class ViT(Module):
+class ViT(nn.Cell):
     def __init__(
         self,
         *,
@@ -199,23 +199,23 @@ 
         self.to_patch_embedding = nn.Sequential(
             Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
-            nn.LayerNorm(patch_dim),
-            nn.Linear(patch_dim, dim),
-            nn.LayerNorm(dim),
-        )
-
-        self.pos_embedding = nn.Parameter(torch.randn(num_patches, dim))
-        self.cls_token = nn.Parameter(torch.randn(dim))
-        self.dropout = nn.Dropout(emb_dropout)
+            nn.LayerNorm(normalized_shape = patch_dim),
+            nn.Linear(in_features = patch_dim, out_features = dim),
+            nn.LayerNorm(normalized_shape = dim),
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.pos_embedding = nn.Parameter(ops.randn(size = num_patches, generator = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+        self.cls_token = nn.Parameter(ops.randn(size = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+        self.dropout = nn.Dropout(p = emb_dropout)
 
         self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)
 
         self.pool = pool
         self.to_latent = nn.Identity()
 
-        self.mlp_head = nn.Linear(dim, num_classes)
-
-        self.register_tokens = nn.Parameter(torch.randn(num_register_tokens, dim) * 1e-2)
+        self.mlp_head = nn.Linear(in_features = dim, out_features = num_classes)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.register_tokens = nn.Parameter(ops.randn(size = num_register_tokens, generator = dim) * 1e-2)  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
     def forward(self, img, return_hiddens = False):
         x = self.to_patch_embedding(img)
@@ -235,7 +235,7 @@         # return the representation trajectory
 
         if return_hiddens:
-            return x, stack(hiddens)
+            return x, ops.stack(tensors = hiddens)  # 'torch.stack':没有对应的mindspore参数 'out';
 
         register_tokens, cls_tokens, x = unpack(x, packed_shape, 'b * d')
 
@@ -249,7 +249,7 @@ # https://openreview.net/forum?id=TalHOvvLZu
 # simple way to get SOTA on Libero dataset (beating fine-tuned pi-zero)
 
-class VAT(Module):
+class VAT(nn.Cell):
     def __init__(
         self,
         vit: ViT | dict,
@@ -295,26 +295,26 @@ 
         self.is_video = is_video
         self.time_seq_len = time_seq_len
-        self.time_pos_emb = nn.Parameter(torch.randn(time_seq_len, vit_dim) * 1e-2) if is_video else None
+        self.time_pos_emb = nn.Parameter(ops.randn(size = time_seq_len, generator = vit_dim) * 1e-2) if is_video else None  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
         # maybe view embeddings
 
-        self.view_emb = nn.Parameter(torch.randn(num_views, vit_dim) * 1e-2) if exists(num_views) and num_views > 1 else None
+        self.view_emb = nn.Parameter(ops.randn(size = num_views, generator = vit_dim) * 1e-2) if exists(num_views) and num_views > 1 else None  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
         # handle maybe task conditioning
 
         self.has_tasks = exists(num_tasks)
 
         if self.has_tasks:
-            self.task_emb = nn.Parameter(torch.randn(num_tasks, dim) * 1e-2)
+            self.task_emb = nn.Parameter(ops.randn(size = num_tasks, generator = dim) * 1e-2)  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
         # register tokens from Darcet et al.
 
-        self.register_tokens = nn.Parameter(torch.randn(num_register_tokens, dim) * 1e-2)
+        self.register_tokens = nn.Parameter(ops.randn(size = num_register_tokens, generator = dim) * 1e-2)  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
         # to action tokens
 
-        self.action_pos_emb = nn.Parameter(torch.randn(action_chunk_len, dim) * 1e-2)
+        self.action_pos_emb = nn.Parameter(ops.randn(size = action_chunk_len, generator = dim) * 1e-2)  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
         self.layers = ModuleList([])
 
@@ -329,15 +329,15 @@                 FeedForward(dim = dim, hidden_dim = mlp_dim, dropout = dropout)
             ]))
 
-        self.final_norm = nn.LayerNorm(dim)
-        self.to_pred_action = nn.Linear(dim, dim_action, bias = False)
+        self.final_norm = nn.LayerNorm(normalized_shape = dim)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
+        self.to_pred_action = nn.Linear(in_features = dim, out_features = dim_action, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         # handle the extra token
 
         self.accept_extra_token = exists(dim_extra_token)
 
         if exists(dim_extra_token):
-            self.to_extra_token = nn.Linear(dim_extra_token, dim)
+            self.to_extra_token = nn.Linear(in_features = dim_extra_token, out_features = dim)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(
         self,
@@ -380,7 +380,7 @@         with vit_forward_context():
             embed, hiddens = self.vit(images, return_hiddens = True)
 
-        hiddens = cat((hiddens, embed[None, ...]))
+        hiddens = ops.cat(tensors = (hiddens, embed[None, ...]))  # 'torch.cat':没有对应的mindspore参数 'out';
 
         # extract the hiddens needed for the action cross attention
 
@@ -471,13 +471,13 @@             if not return_hiddens:
                 return pred_action
 
-            return pred_action, stack(hiddens)
+            return pred_action, ops.stack(tensors = hiddens)  # 'torch.stack':没有对应的mindspore参数 'out';
 
         assert pred_action.shape[1] == actions.shape[1]
 
         # they found l1 loss suffices
 
-        return F.l1_loss(pred_action, actions)
+        return nn.functional.l1_loss(input = pred_action, target = actions)  # 'torch.nn.functional.l1_loss':没有对应的mindspore参数 'size_average';; 'torch.nn.functional.l1_loss':没有对应的mindspore参数 'reduce';
 
 # quick test
 
@@ -512,11 +512,11 @@         )
     )
 
-    images = torch.randn(2, 2, 3, 4, 256, 256) # (2 views with 4 frames)
-    tasks = torch.randint(0, 4, (2,))
-    extra = torch.randn(2, 33)                 # extra internal state
-
-    actions = torch.randn(2, 7, 20)         # actions for learning
+    images = ops.randn(size = 2, generator = 2, dtype = 4)  # (2 views with 4 frames); 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+    tasks = ops.randint(low = 0, high = 4, size = (2,))  # 'torch.randint':没有对应的mindspore参数 'out';; 'torch.randint':没有对应的mindspore参数 'layout';; 'torch.randint':没有对应的mindspore参数 'device';; 'torch.randint':没有对应的mindspore参数 'requires_grad';
+    extra = ops.randn(size = 2, generator = 33)  # extra internal state; 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+
+    actions = ops.randn(size = 2, generator = 7)  # actions for learning; 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
     loss = vat(images, actions = actions, tasks = tasks, extra = extra, freeze_vit = True)
     loss.backward()
