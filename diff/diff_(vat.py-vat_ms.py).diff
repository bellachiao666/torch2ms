--- pytorch+++ mindspore@@ -2,12 +2,11 @@ from contextlib import nullcontext
 
 import torch
-import torch.nn.functional as F
 from torch import nn, cat, stack, tensor
-from torch.nn import Module, ModuleList
 
 from einops import rearrange, repeat, pack, unpack
 from einops.layers.torch import Rearrange
+from mindspore.mint import nn, ops
 
 # helpers
 
@@ -22,15 +21,15 @@ 
 # classes
 
-class FiLM(Module):
+class FiLM(nn.Cell):
     def __init__(
         self,
         dim,
     ):
         super().__init__()
-        proj = nn.Linear(dim, dim * 2)
-
-        self.to_gamma_beta = nn.Sequential(
+        proj = nn.Linear(in_features = dim, out_features = dim * 2)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.to_gamma_beta = nn.SequentialCell(
             proj,
             Rearrange('b (two d) -> two b 1 d', two = 2)
         )
@@ -43,7 +42,7 @@ 
         return tokens * gamma + beta
 
-class FeedForward(Module):
+class FeedForward(nn.Cell):
     def __init__(
         self,
         dim,
@@ -51,19 +50,19 @@         dropout = 0.
     ):
         super().__init__()
-        self.net = nn.Sequential(
-            nn.LayerNorm(dim),
-            nn.Linear(dim, hidden_dim),
+        self.net = nn.SequentialCell(
+            nn.LayerNorm(normalized_shape = dim),
+            nn.Linear(in_features = dim, out_features = hidden_dim),
             nn.GELU(),
-            nn.Dropout(dropout),
-            nn.Linear(hidden_dim, dim),
-            nn.Dropout(dropout)
-        )
+            nn.Dropout(p = dropout),
+            nn.Linear(in_features = hidden_dim, out_features = dim),
+            nn.Dropout(p = dropout)
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, x):
         return self.net(x)
 
-class Attention(Module):
+class Attention(nn.Cell):
     def __init__(
         self,
         dim,
@@ -81,21 +80,21 @@         self.heads = heads
         self.scale = dim_head ** -0.5
 
-        self.norm = nn.LayerNorm(dim)
+        self.norm = nn.LayerNorm(normalized_shape = dim)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
 
         self.cross_attend = cross_attend
-        self.context_norm = nn.LayerNorm(dim_context) if cross_attend else None
+        self.context_norm = nn.LayerNorm(normalized_shape = dim_context) if cross_attend else None  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
 
         self.attend = nn.Softmax(dim = -1)
-        self.dropout = nn.Dropout(dropout)
-
-        self.to_q = nn.Linear(dim, inner_dim, bias = False)
-        self.to_kv = nn.Linear(dim_context, inner_dim * 2, bias = False)
-
-        self.to_out = nn.Sequential(
-            nn.Linear(inner_dim, dim),
-            nn.Dropout(dropout)
-        ) if project_out else nn.Identity()
+        self.dropout = nn.Dropout(p = dropout)
+
+        self.to_q = nn.Linear(in_features = dim, out_features = inner_dim, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+        self.to_kv = nn.Linear(in_features = dim_context, out_features = inner_dim * 2, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.to_out = nn.SequentialCell(
+            nn.Linear(in_features = inner_dim, out_features = dim),
+            nn.Dropout(p = dropout)
+        ) if project_out else nn.Identity()  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(self, x, context = None):
 
@@ -116,16 +115,16 @@         qkv = (self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1))
         q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = ops.matmul(input = q, other = k.transpose(-1, -2)) * self.scale  # 'torch.matmul':没有对应的mindspore参数 'out';
 
         attn = self.attend(dots)
         attn = self.dropout(attn)
 
-        out = torch.matmul(attn, v)
+        out = ops.matmul(input = attn, other = v)  # 'torch.matmul':没有对应的mindspore参数 'out';
         out = rearrange(out, 'b h n d -> b n (h d)')
         return self.to_out(out)
 
-class Transformer(Module):
+class Transformer(nn.Cell):
     def __init__(
         self,
         dim,
@@ -136,11 +135,11 @@         dropout = 0.
     ):
         super().__init__()
-        self.norm = nn.LayerNorm(dim)
-        self.layers = ModuleList([])
+        self.norm = nn.LayerNorm(normalized_shape = dim)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
+        self.layers = nn.CellList([])
 
         for _ in range(depth):
-            self.layers.append(ModuleList([
+            self.layers.append(nn.CellList([
                 Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),
                 FeedForward(dim, mlp_dim, dropout = dropout)
             ]))
@@ -166,7 +165,7 @@ 
         return x, hiddens
 
-class ViT(Module):
+class ViT(nn.Cell):
     def __init__(
         self,
         *,
@@ -197,25 +196,25 @@         patch_dim = channels * patch_height * patch_width
         assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
 
-        self.to_patch_embedding = nn.Sequential(
+        self.to_patch_embedding = nn.SequentialCell(
             Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
-            nn.LayerNorm(patch_dim),
-            nn.Linear(patch_dim, dim),
-            nn.LayerNorm(dim),
-        )
-
-        self.pos_embedding = nn.Parameter(torch.randn(num_patches, dim))
-        self.cls_token = nn.Parameter(torch.randn(dim))
-        self.dropout = nn.Dropout(emb_dropout)
+            nn.LayerNorm(normalized_shape = patch_dim),
+            nn.Linear(in_features = patch_dim, out_features = dim),
+            nn.LayerNorm(normalized_shape = dim),
+        )  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';; 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.pos_embedding = mindspore.Parameter(ops.randn(size = num_patches, generator = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+        self.cls_token = mindspore.Parameter(ops.randn(size = dim))  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+        self.dropout = nn.Dropout(p = emb_dropout)
 
         self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)
 
         self.pool = pool
         self.to_latent = nn.Identity()
 
-        self.mlp_head = nn.Linear(dim, num_classes)
-
-        self.register_tokens = nn.Parameter(torch.randn(num_register_tokens, dim) * 1e-2)
+        self.mlp_head = nn.Linear(in_features = dim, out_features = num_classes)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
+
+        self.register_tokens = mindspore.Parameter(ops.randn(size = num_register_tokens, generator = dim) * 1e-2)  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
     def forward(self, img, return_hiddens = False):
         x = self.to_patch_embedding(img)
@@ -235,7 +234,7 @@         # return the representation trajectory
 
         if return_hiddens:
-            return x, stack(hiddens)
+            return x, ops.stack(tensors = hiddens)  # 'torch.stack':没有对应的mindspore参数 'out';
 
         register_tokens, cls_tokens, x = unpack(x, packed_shape, 'b * d')
 
@@ -249,7 +248,7 @@ # https://openreview.net/forum?id=TalHOvvLZu
 # simple way to get SOTA on Libero dataset (beating fine-tuned pi-zero)
 
-class VAT(Module):
+class VAT(nn.Cell):
     def __init__(
         self,
         vit: ViT | dict,
@@ -295,49 +294,49 @@ 
         self.is_video = is_video
         self.time_seq_len = time_seq_len
-        self.time_pos_emb = nn.Parameter(torch.randn(time_seq_len, vit_dim) * 1e-2) if is_video else None
+        self.time_pos_emb = mindspore.Parameter(ops.randn(size = time_seq_len, generator = vit_dim) * 1e-2) if is_video else None  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
         # maybe view embeddings
 
-        self.view_emb = nn.Parameter(torch.randn(num_views, vit_dim) * 1e-2) if exists(num_views) and num_views > 1 else None
+        self.view_emb = mindspore.Parameter(ops.randn(size = num_views, generator = vit_dim) * 1e-2) if exists(num_views) and num_views > 1 else None  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
         # handle maybe task conditioning
 
         self.has_tasks = exists(num_tasks)
 
         if self.has_tasks:
-            self.task_emb = nn.Parameter(torch.randn(num_tasks, dim) * 1e-2)
+            self.task_emb = mindspore.Parameter(ops.randn(size = num_tasks, generator = dim) * 1e-2)  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
         # register tokens from Darcet et al.
 
-        self.register_tokens = nn.Parameter(torch.randn(num_register_tokens, dim) * 1e-2)
+        self.register_tokens = mindspore.Parameter(ops.randn(size = num_register_tokens, generator = dim) * 1e-2)  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
         # to action tokens
 
-        self.action_pos_emb = nn.Parameter(torch.randn(action_chunk_len, dim) * 1e-2)
-
-        self.layers = ModuleList([])
+        self.action_pos_emb = mindspore.Parameter(ops.randn(size = action_chunk_len, generator = dim) * 1e-2)  # 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+
+        self.layers = nn.CellList([])
 
         for _ in range(depth):
             maybe_film = FiLM(dim = dim) if self.has_tasks else None
             maybe_self_attn = Attention(dim = dim, heads = self_attn_heads, dim_head = self_attn_dim_head, dropout = dropout) if add_self_attn else None
 
-            self.layers.append(ModuleList([
+            self.layers.append(nn.CellList([
                 maybe_film,
                 maybe_self_attn,
                 Attention(dim = dim, dim_context = vit_dim, heads = heads, dim_head = dim_head, dropout = dropout, cross_attend = True),
                 FeedForward(dim = dim, hidden_dim = mlp_dim, dropout = dropout)
             ]))
 
-        self.final_norm = nn.LayerNorm(dim)
-        self.to_pred_action = nn.Linear(dim, dim_action, bias = False)
+        self.final_norm = nn.LayerNorm(normalized_shape = dim)  # 'torch.nn.LayerNorm':没有对应的mindspore参数 'device';
+        self.to_pred_action = nn.Linear(in_features = dim, out_features = dim_action, bias = False)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
         # handle the extra token
 
         self.accept_extra_token = exists(dim_extra_token)
 
         if exists(dim_extra_token):
-            self.to_extra_token = nn.Linear(dim_extra_token, dim)
+            self.to_extra_token = nn.Linear(in_features = dim_extra_token, out_features = dim)  # 'torch.nn.Linear':没有对应的mindspore参数 'device';
 
     def forward(
         self,
@@ -380,7 +379,7 @@         with vit_forward_context():
             embed, hiddens = self.vit(images, return_hiddens = True)
 
-        hiddens = cat((hiddens, embed[None, ...]))
+        hiddens = ops.cat(tensors = (hiddens, embed[None, ...]))  # 'torch.cat':没有对应的mindspore参数 'out';
 
         # extract the hiddens needed for the action cross attention
 
@@ -471,13 +470,13 @@             if not return_hiddens:
                 return pred_action
 
-            return pred_action, stack(hiddens)
+            return pred_action, ops.stack(tensors = hiddens)  # 'torch.stack':没有对应的mindspore参数 'out';
 
         assert pred_action.shape[1] == actions.shape[1]
 
         # they found l1 loss suffices
 
-        return F.l1_loss(pred_action, actions)
+        return nn.functional.l1_loss(input = pred_action, target = actions)  # 'torch.nn.functional.l1_loss':没有对应的mindspore参数 'size_average';; 'torch.nn.functional.l1_loss':没有对应的mindspore参数 'reduce';
 
 # quick test
 
@@ -512,11 +511,11 @@         )
     )
 
-    images = torch.randn(2, 2, 3, 4, 256, 256) # (2 views with 4 frames)
-    tasks = torch.randint(0, 4, (2,))
-    extra = torch.randn(2, 33)                 # extra internal state
-
-    actions = torch.randn(2, 7, 20)         # actions for learning
+    images = ops.randn(size = 2, generator = 2, dtype = 4)  # (2 views with 4 frames); 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+    tasks = ops.randint(low = 0, high = 4, size = (2,))  # 'torch.randint':没有对应的mindspore参数 'out';; 'torch.randint':没有对应的mindspore参数 'layout';; 'torch.randint':没有对应的mindspore参数 'device';; 'torch.randint':没有对应的mindspore参数 'requires_grad';
+    extra = ops.randn(size = 2, generator = 33)  # extra internal state; 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
+
+    actions = ops.randn(size = 2, generator = 7)  # actions for learning; 'torch.randn':没有对应的mindspore参数 'out';; 'torch.randn':没有对应的mindspore参数 'layout';; 'torch.randn':没有对应的mindspore参数 'device';; 'torch.randn':没有对应的mindspore参数 'requires_grad';; 'torch.randn':没有对应的mindspore参数 'pin_memory';
 
     loss = vat(images, actions = actions, tasks = tasks, extra = extra, freeze_vit = True)
     loss.backward()
