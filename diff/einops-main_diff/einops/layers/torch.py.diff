--- pytorch+++ mindspore@@ -1,8 +1,11 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from typing import Dict, Optional, cast
 
-import torch
-
-from einops._torch_specific import apply_for_scriptable_torch
+# from einops._torch_specific import apply_for_scriptable_torch
 
 from . import RearrangeMixin, ReduceMixin
 from ._einmix import _EinmixMixin
@@ -10,8 +13,8 @@ __author__ = "Alex Rogozhnikov"
 
 
-class Rearrange(RearrangeMixin, torch.nn.Module):
-    def forward(self, input):
+class Rearrange(RearrangeMixin, msnn.Cell):
+    def construct(self, input):
         recipe = self._multirecipe[input.ndim]
         return apply_for_scriptable_torch(recipe, input, reduction_type="rearrange", axes_dims=self._axes_lengths)
 
@@ -20,8 +23,8 @@         pass
 
 
-class Reduce(ReduceMixin, torch.nn.Module):
-    def forward(self, input):
+class Reduce(ReduceMixin, msnn.Cell):
+    def construct(self, input):
         recipe = self._multirecipe[input.ndim]
         return apply_for_scriptable_torch(recipe, input, reduction_type=self.reduction, axes_dims=self._axes_lengths)
 
@@ -30,14 +33,14 @@         pass
 
 
-class EinMix(_EinmixMixin, torch.nn.Module):
+class EinMix(_EinmixMixin, msnn.Cell):
     def _create_parameters(self, weight_shape, weight_bound, bias_shape, bias_bound):
-        self.weight = torch.nn.Parameter(
-            torch.zeros(weight_shape).uniform_(-weight_bound, weight_bound), requires_grad=True
+        self.weight = ms.Parameter(
+            mint.zeros(weight_shape).uniform_(-weight_bound, weight_bound), requires_grad=True
         )
         if bias_shape is not None:
-            self.bias = torch.nn.Parameter(
-                torch.zeros(bias_shape).uniform_(-bias_bound, bias_bound), requires_grad=True
+            self.bias = ms.Parameter(
+                mint.zeros(bias_shape).uniform_(-bias_bound, bias_bound), requires_grad=True
             )
         else:
             self.bias = None
@@ -51,16 +54,16 @@     ):
         self.pre_rearrange = None
         if pre_reshape_pattern is not None:
-            self.pre_rearrange = Rearrange(pre_reshape_pattern, **cast(dict, pre_reshape_lengths))
+            self.pre_rearrange = Rearrange(pre_reshape_pattern, **cast(dict, pre_reshape_lengths))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.post_rearrange = None
         if post_reshape_pattern is not None:
-            self.post_rearrange = Rearrange(post_reshape_pattern, **cast(dict, post_reshape_lengths))
+            self.post_rearrange = Rearrange(post_reshape_pattern, **cast(dict, post_reshape_lengths))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
-    def forward(self, input):
+    def construct(self, input):
         if self.pre_rearrange is not None:
             input = self.pre_rearrange(input)
-        result = torch.einsum(self.einsum_pattern, input, self.weight)
+        result = mint.einsum(self.einsum_pattern, input, self.weight)
         if self.bias is not None:
             result += self.bias
         if self.post_rearrange is not None:
