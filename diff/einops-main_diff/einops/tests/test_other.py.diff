--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from doctest import testmod
 
 import numpy as np
@@ -77,7 +82,7 @@             assert np.array_equal(result1, result2)
 
             # testing we can't optimize this formula again
-            combination3 = _optimize_transformation(*combination2)
+            combination3 = _optimize_transformation(*combination2)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             for a, b in zip(combination2, combination3):
                 assert np.array_equal(a, b)
 
@@ -207,7 +212,7 @@         print(out_shape)
         result_placeholder = rearrange(
             input_symbol, "a b (c1 c2) (d1 d2) -> (a b d1) c1 (c2 d2)", **parse_shape(input_symbol, "a b c1 _"), d2=2
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         result = backend.eval_symbol(result_placeholder, [(input_symbol, np.zeros([10, 20, 30, 40]))])
         print(result.shape)
         assert result.shape == (10 * 20 * 20, 30, 1 * 2)
@@ -261,19 +266,18 @@     """
     if not is_backend_tested("torch"):
         pytest.skip()
-    import torch
-    from torch import nn
+    # import torch
 
     from einops import einsum, pack, reduce, repeat, unpack
-    from einops._torch_specific import allow_ops_in_compiled_graph
+    # from einops._torch_specific import allow_ops_in_compiled_graph
 
     allow_ops_in_compiled_graph()
 
-    class TorchModuleWithOperations(nn.Module):
+    class TorchModuleWithOperations(msnn.Cell):
         def __init__(self) -> None:
             super().__init__()
 
-        def forward(self, x_abc, suffix=""):
+        def construct(self, x_abc, suffix=""):
             a, b, c = x_abc.shape
 
             def suf(pattern):
@@ -292,14 +296,14 @@             return x1 + addition
 
     original = TorchModuleWithOperations()
-    compiled = torch.compile(original, fullgraph=True)
+    compiled = torch.compile(original, fullgraph=True)  # 'torch.compile' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     for size in [10, 20, 40]:
-        x = torch.rand([size, size + 1, size + 2])
+        x = mint.rand([size, size + 1, size + 2])
         for suffix in ["", "suf1", "other_suffix"]:
             result1 = compiled(x, suffix)
             result2 = original(x.double(), suffix).float()
 
-            torch.testing.assert_close(result1, result2, atol=1e-5, rtol=1e-5)
+            torch.testing.assert_close(result1, result2, atol=1e-5, rtol=1e-5)  # 'torch.testing.assert_close' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def test_torch_compile_for_layers():
@@ -310,21 +314,21 @@     if not is_backend_tested("torch"):
         pytest.skip()
 
-    import torch
-    from torch import nn
-
-    from einops.layers.torch import EinMix, Rearrange, Reduce
-
-    original = nn.Sequential(
+    # import torch
+
+    # from einops.layers.torch import EinMix, Rearrange, Reduce
+
+    original = msnn.SequentialCell(
+        [
         Rearrange("b (t c) -> b t c", c=16),
         EinMix("b t c -> qkv b t cout", weight_shape="qkv c cout", bias_shape="qkv cout", qkv=3, c=16, cout=8),
-        Reduce("qkv b t cout -> b t qkv", "min", cout=8),
-    )
-
-    compiled = torch.compile(original, fullgraph=True)
+        Reduce("qkv b t cout -> b t qkv", "min", cout=8)
+    ])
+
+    compiled = torch.compile(original, fullgraph=True)  # 'torch.compile' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     for size in [16, 32, 64]:
-        x = torch.rand([size, size])
+        x = mint.rand([size, size])
         result1 = original(x)
         result2 = compiled(x)
-        assert torch.allclose(result1, result2)
+        assert mint.allclose(result1, result2)
