--- pytorch+++ mindspore@@ -1,33 +1,27 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import os
 import argparse
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
+# import torch
+# import torch.optim as optim
 from transformers import AutoTokenizer, GPT2TokenizerFast
 from transformers import T5Tokenizer, T5ForConditionalGeneration
 import functools
-from torch.optim.lr_scheduler import StepLR
-import torch.nn.functional as F
-import torch.distributed as dist
-import torch.multiprocessing as mp
-from torch.nn.parallel import DistributedDataParallel as DDP
-from torch.utils.data.distributed import DistributedSampler
+# from torch.optim.lr_scheduler import StepLR
+# from torch.utils.data.distributed import DistributedSampler
 from transformers.models.t5.modeling_t5 import T5Block
 from nlp import load_dataset
 
-from torch.distributed.fsdp import (
+# from torch.distributed.fsdp import (
     FullyShardedDataParallel as FSDP,
-    CPUOffload,
-    MixedPrecision,
-    BackwardPrefetch,
-    ShardingStrategy,
-    FullStateDictConfig,
     StateDictType,
 )
 
 from functools import partial
-from torch.utils.data import DataLoader
+# from torch.utils.data import DataLoader
 from pathlib import Path
 from summarization_dataset import *
 import policies
@@ -92,8 +86,8 @@     train_dataset = wikihow(tokenizer, 'train', 1500, 512, 150, False)
     val_dataset = wikihow(tokenizer, 'validation', 300, 512, 150, False)
 
-    sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True)
-    sampler2 = DistributedSampler(val_dataset, rank=rank, num_replicas=world_size)
+    sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True)  # 'torch.utils.data.distributed.DistributedSampler' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    sampler2 = DistributedSampler(val_dataset, rank=rank, num_replicas=world_size)  # 'torch.utils.data.distributed.DistributedSampler' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     setup()
 
@@ -106,10 +100,10 @@     train_kwargs.update(cuda_kwargs)
     test_kwargs.update(cuda_kwargs)
 
-    train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)
-    val_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)
-
-    torch.cuda.set_device(local_rank)
+    train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)  # 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    val_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)  # 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    torch.cuda.set_device(local_rank)  # 'torch.cuda.set_device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     # Set up FSDP parameters
     mixed_precision_policy, t5_auto_wrap_policy = get_policies(train_config, rank)
@@ -120,16 +114,16 @@         mixed_precision=mixed_precision_policy,
         sharding_strategy=fsdp_config.sharding_strategy,
         device_id=torch.cuda.current_device(),
-        limit_all_gathers=fsdp_config.limit_all_gathers)
+        limit_all_gathers=fsdp_config.limit_all_gathers)  # 'torch.cuda.current_device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.fsdp.FullyShardedDataParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     # Enabling this causes https://github.com/pytorch/examples/issues/1210
     if fsdp_config.fsdp_activation_checkpointing:
         policies.apply_fsdp_checkpointing(model)
 
     # Set up optimizer and scheduler
-    optimizer = optim.AdamW(model.parameters(), lr=train_config.lr)
-
-    scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)
+    optimizer = mint.optim.AdamW(model.parameters(), lr=train_config.lr)
+
+    scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)  # 'torch.optim.lr_scheduler.StepLR' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     best_val_loss = float("inf")
     curr_val_loss = float("inf")
     file_save_name = "T5-model-"
@@ -165,10 +159,10 @@             if args.track_memory:
                 mem_alloc_tracker.append(
                     format_metrics_to_gb(torch.cuda.memory_allocated())
-                )
+                )  # 'torch.cuda.memory_allocated' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 mem_reserved_tracker.append(
                     format_metrics_to_gb(torch.cuda.memory_reserved())
-                )
+                )  # 'torch.cuda.memory_reserved' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         if train_config.save_model and curr_val_loss < best_val_loss:
 
@@ -191,7 +185,7 @@             if rank==0:
                 print(f"-->>>> New Val Loss Record: {best_val_loss}")
 
-    dist.barrier()
+    mint.distributed.barrier()
     cleanup()
 
 
@@ -212,6 +206,6 @@                         help='running the validation')
     args = parser.parse_args()
 
-    torch.manual_seed(args.seed)
+    torch.manual_seed(args.seed)  # 'torch.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     fsdp_main(args)
