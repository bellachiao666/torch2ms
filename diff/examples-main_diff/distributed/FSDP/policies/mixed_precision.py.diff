--- pytorch+++ mindspore@@ -1,6 +1,10 @@-import torch
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 
-from torch.distributed.fsdp import (
+# from torch.distributed.fsdp import (
     # FullyShardedDataParallel as FSDP,
     # CPUOffload,
     MixedPrecision,
@@ -10,29 +14,29 @@ 
 # requires grad scaler in main loop
 fpSixteen = MixedPrecision(
-    param_dtype=torch.float16,
+    param_dtype=ms.float16,
     # Gradient communication precision.
-    reduce_dtype=torch.float16,
+    reduce_dtype=ms.float16,
     # Buffer precision.
-    buffer_dtype=torch.float16,
-)
+    buffer_dtype=ms.float16,
+)  # 'torch.distributed.fsdp.MixedPrecision' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 bfSixteen = MixedPrecision(
-    param_dtype=torch.bfloat16,
+    param_dtype=ms.bfloat16,
     # Gradient communication precision.
-    reduce_dtype=torch.bfloat16,
+    reduce_dtype=ms.bfloat16,
     # Buffer precision.
-    buffer_dtype=torch.bfloat16,
-)
+    buffer_dtype=ms.bfloat16,
+)  # 'torch.distributed.fsdp.MixedPrecision' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 bfSixteen_working = MixedPrecision(
-    param_dtype=torch.float32,
-    reduce_dtype=torch.bfloat16,
-    buffer_dtype=torch.bfloat16,
-)
+    param_dtype=ms.float32,
+    reduce_dtype=ms.bfloat16,
+    buffer_dtype=ms.bfloat16,
+)  # 'torch.distributed.fsdp.MixedPrecision' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 fp32_policy = MixedPrecision(
-    param_dtype=torch.float32,
-    reduce_dtype=torch.float32,
-    buffer_dtype=torch.float32,
-)
+    param_dtype=ms.float32,
+    reduce_dtype=ms.float32,
+    buffer_dtype=ms.float32,
+)  # 'torch.distributed.fsdp.MixedPrecision' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
