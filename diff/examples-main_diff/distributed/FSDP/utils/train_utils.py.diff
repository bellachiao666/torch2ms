--- pytorch+++ mindspore@@ -1,6 +1,11 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import os
-import torch
-import torch.distributed as dist
+# import torch
+# import torch.distributed as dist
 from datetime import datetime
 import tqdm
 from transformers import AutoTokenizer, GPT2TokenizerFast
@@ -10,11 +15,11 @@ 
 def setup():
     # initialize the process group
-    dist.init_process_group("nccl")
+    mint.distributed.init_process_group("nccl")
 
 
 def cleanup():
-    dist.destroy_process_group()
+    dist.destroy_process_group()  # 'torch.distributed.destroy_process_group' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 def get_date_of_run():
     """create date and time for file save uniqueness
@@ -35,7 +40,7 @@ def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):
     model.train()
     local_rank = int(os.environ['LOCAL_RANK'])
-    fsdp_loss = torch.zeros(2).to(local_rank)
+    fsdp_loss = mint.zeros(2).to(local_rank)
 
     if sampler:
         sampler.set_epoch(epoch)
@@ -56,7 +61,7 @@         if rank==0:
             inner_pbar.update(1)
 
-    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)
+    mint.distributed.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)
     train_accuracy = fsdp_loss[0] / fsdp_loss[1]
 
 
@@ -72,11 +77,12 @@     model.eval()
     correct = 0
     local_rank = int(os.environ['LOCAL_RANK'])
-    fsdp_loss = torch.zeros(2).to(local_rank)
+    fsdp_loss = mint.zeros(2).to(local_rank)
     if rank == 0:
         inner_pbar = tqdm.tqdm(
             range(len(val_loader)), colour="green", desc="Validation Epoch"
         )
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     with torch.no_grad():
         for batch in val_loader:
             for key in batch.keys():
@@ -88,7 +94,7 @@             if rank==0:
                 inner_pbar.update(1)
 
-    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)
+    mint.distributed.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)
     val_loss = fsdp_loss[0] / fsdp_loss[1]
     if rank == 0:
         inner_pbar.close()
