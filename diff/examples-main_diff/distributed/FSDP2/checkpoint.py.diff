--- pytorch+++ mindspore@@ -1,9 +1,13 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import os
 import time
 
-import torch
-import torch.nn as nn
-from torch.distributed.checkpoint.state_dict import (
+# import torch
+# from torch.distributed.checkpoint.state_dict import (
     _init_optim_state,
     get_model_state_dict,
     get_optimizer_state_dict,
@@ -11,8 +15,8 @@     set_optimizer_state_dict,
     StateDictOptions,
 )
-from torch.distributed.fsdp import FSDPModule
-from torch.distributed.tensor import distribute_tensor, DTensor
+# from torch.distributed.fsdp import FSDPModule
+from ms.Tensor import distribute_tensor, DTensor
 
 
 MODEL_CHECKPOINT = "model_state_dict.pt"
@@ -47,6 +51,7 @@     def is_empty(self):
         return self.last_training_time is None
 
+    # 'torch.distributed.fsdp.FSDPModule' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def load_model(self, model: FSDPModule):
         last_model_checkpoint = (
             f"{self.folder}/{'dcp_api' if self.dcp_api else 'dtensor_api'}"
@@ -54,7 +59,7 @@         )
         full_sd = torch.load(
             last_model_checkpoint, mmap=True, weights_only=True, map_location="cpu"
-        )
+        )  # 'torch.load' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.dcp_api:
             set_model_state_dict(
                 model=model,
@@ -63,7 +68,7 @@                     full_state_dict=True,
                     broadcast_from_rank0=True,
                 ),
-            )
+            )  # 'torch.distributed.checkpoint.state_dict.StateDictOptions' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.checkpoint.state_dict.set_model_state_dict' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             return
         meta_sharded_sd = model.state_dict()
         sharded_sd = {}
@@ -73,11 +78,13 @@                 full_tensor,
                 sharded_meta_param.device_mesh,
                 sharded_meta_param.placements,
-            )
-            sharded_sd[param_name] = nn.Parameter(sharded_tensor)
+            )  # 'torch.distributed.tensor.distribute_tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            sharded_sd[param_name] = ms.Parameter(sharded_tensor)
         # choose `assign=True` since we cannot call `copy_` on meta tensor
         model.load_state_dict(sharded_sd, strict=False, assign=True)
 
+    # 'torch.distributed.fsdp.FSDPModule' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def load_optim(self, model: FSDPModule, opt: torch.optim.Optimizer):
         last_optim_checkpoint = (
             f"{self.folder}/{'dcp_api' if self.dcp_api else 'dtensor_api'}"
@@ -85,7 +92,7 @@         )
         full_sd = torch.load(
             last_optim_checkpoint, mmap=True, weights_only=True, map_location="cpu"
-        )
+        )  # 'torch.load' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.dcp_api:
             set_optimizer_state_dict(
                 model=model,
@@ -95,9 +102,9 @@                     full_state_dict=True,
                     broadcast_from_rank0=True,
                 ),
-            )
+            )  # 'torch.distributed.checkpoint.state_dict.StateDictOptions' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.checkpoint.state_dict.set_optimizer_state_dict' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             return
-        _init_optim_state(opt)
+        _init_optim_state(opt)  # 'torch.distributed.checkpoint.state_dict._init_optim_state' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         param_groups = opt.state_dict()["param_groups"]
         state = opt.state_dict()["state"]
 
@@ -122,7 +129,7 @@                             full_tensor,
                             sharded_tensor.device_mesh,
                             sharded_tensor.placements,
-                        )
+                        )  # 'torch.distributed.tensor.distribute_tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                     else:
                         # step is plain tensor
                         param_state[attr] = full_tensor
@@ -133,6 +140,7 @@             }
         )
 
+    # 'torch.distributed.fsdp.FSDPModule' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def _get_full_model_state_dict(self, model: FSDPModule):
         if self.dcp_api:
             return get_model_state_dict(
@@ -141,18 +149,20 @@                     full_state_dict=True,
                     cpu_offload=True,
                 ),
-            )
+            )  # 'torch.distributed.checkpoint.state_dict.StateDictOptions' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.checkpoint.state_dict.get_model_state_dict' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         sharded_sd = model.state_dict()
         cpu_state_dict = {}
         for param_name, sharded_param in sharded_sd.items():
             full_param = sharded_param.full_tensor()
-            if torch.distributed.get_rank() == 0:
+            if mint.distributed.get_rank() == 0:
                 cpu_state_dict[param_name] = full_param.cpu()
             else:
                 del full_param
         return cpu_state_dict
 
+    # 'torch.distributed.fsdp.FSDPModule' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def _get_full_optimizer_state_dict(
         self,
         model: FSDPModule,
@@ -166,8 +176,8 @@                     full_state_dict=True,
                     cpu_offload=True,
                 ),
-            )
-        is_rank_zero = torch.distributed.get_rank() == 0
+            )  # 'torch.distributed.checkpoint.state_dict.StateDictOptions' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.checkpoint.state_dict.get_optimizer_state_dict' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        is_rank_zero = mint.distributed.get_rank() == 0
         sharded_sd = opt.state_dict()
         sharded_state = sharded_sd["state"]
         full_state = {}
@@ -196,14 +206,16 @@         else:
             return {}
 
+    # 'torch.distributed.fsdp.FSDPModule' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def save(self, model: FSDPModule, optim: torch.optim.Optimizer):
         model_state_dict = self._get_full_model_state_dict(model)
         optim_state_dict = self._get_full_optimizer_state_dict(model, optim)
-        if torch.distributed.get_rank() == 0:
+        if mint.distributed.get_rank() == 0:
             new_training_time = int(time.time() * 1000)
             new_checkpoint_folder = f"{self.folder}/{'dcp_api' if self.dcp_api else 'dtensor_api'}/{new_training_time}"
             new_model_checkpoint = f"{new_checkpoint_folder}/{MODEL_CHECKPOINT}"
             new_optim_checkpoint = f"{new_checkpoint_folder}/{OPTIM_CHECKPOINT}"
             os.makedirs(new_checkpoint_folder, exist_ok=True)
-            torch.save(model_state_dict, new_model_checkpoint)
-            torch.save(optim_state_dict, new_optim_checkpoint)
+            torch.save(model_state_dict, new_model_checkpoint)  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            torch.save(optim_state_dict, new_optim_checkpoint)  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
