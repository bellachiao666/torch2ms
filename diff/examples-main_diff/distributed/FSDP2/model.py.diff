--- pytorch+++ mindspore@@ -1,8 +1,11 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from dataclasses import dataclass
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 
 @dataclass
@@ -15,7 +18,7 @@     dropout_p: float = 0.1
 
 
-class Attention(nn.Module):
+class Attention(msnn.Cell):
     def __init__(self, args: ModelArgs):
         super().__init__()
         assert args.dim % args.n_heads == 0
@@ -24,12 +27,12 @@         self.dropout_p = args.dropout_p
         self.resid_dropout = nn.Dropout(args.dropout_p)
 
-        self.wq = nn.Linear(args.dim, args.dim, bias=False)
-        self.wk = nn.Linear(args.dim, args.dim, bias=False)
-        self.wv = nn.Linear(args.dim, args.dim, bias=False)
-        self.wo = nn.Linear(args.dim, args.dim, bias=False)
+        self.wq = nn.Linear(args.dim, args.dim, bias = False)
+        self.wk = nn.Linear(args.dim, args.dim, bias = False)
+        self.wv = nn.Linear(args.dim, args.dim, bias = False)
+        self.wo = nn.Linear(args.dim, args.dim, bias = False)
 
-    def forward(self, x):
+    def construct(self, x):
         bsz, seq_len, _ = x.size()
         queries, keys, values = self.wq(x), self.wk(x), self.wv(x)
         queries = queries.view(bsz, seq_len, self.n_heads, self.head_dim)
@@ -46,7 +49,7 @@             values,
             None,
             self.dropout_p if self.training else 0,
-        )
+        )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         output = output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)
         return self.resid_dropout(self.wo(output))
 
@@ -57,7 +60,7 @@         self.wo.reset_parameters()
 
 
-class FeedForward(nn.Module):
+class FeedForward(msnn.Cell):
     def __init__(self, dim, hidden_dim, dropout_p):
         super().__init__()
         self.w1 = nn.Linear(dim, hidden_dim)
@@ -65,7 +68,7 @@         self.w2 = nn.Linear(hidden_dim, dim)
         self.resid_dropout = nn.Dropout(dropout_p)
 
-    def forward(self, x):
+    def construct(self, x):
         return self.resid_dropout(self.w2(self.gelu(self.w1(x))))
 
     def reset_parameters(self):
@@ -73,7 +76,7 @@         self.w2.reset_parameters()
 
 
-class TransformerBlock(nn.Module):
+class TransformerBlock(msnn.Cell):
     def __init__(self, args: ModelArgs):
         super().__init__()
         self.attention_norm = nn.LayerNorm(args.dim)
@@ -83,7 +86,7 @@             args.dim, hidden_dim=4 * args.dim, dropout_p=args.dropout_p
         )
 
-    def forward(self, x):
+    def construct(self, x):
         h = x + self.attention(self.attention_norm(x))
         out = h + self.feed_forward(self.ffn_norm(h))
         return out
@@ -97,7 +100,7 @@ 
 # A toy transformer model, partly inspired by the nanoGPT model:
 # https://github.com/karpathy/nanoGPT.
-class Transformer(nn.Module):
+class Transformer(msnn.Cell):
     def __init__(self, args: ModelArgs):
         super().__init__()
         assert args.vocab_size is not None
@@ -107,17 +110,17 @@         self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)
         self.pos_embeddings = nn.Embedding(args.max_seq_len, args.dim)
         self.dropout = nn.Dropout(args.dropout_p)
-        self.layers = nn.ModuleList()
+        self.layers = msnn.CellList()
         for _ in range(args.n_layers):
             self.layers.append(TransformerBlock(args))
         self.norm = nn.LayerNorm(args.dim)
-        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)
+        self.output = nn.Linear(args.dim, args.vocab_size, bias = False)
 
-    def forward(self, tokens):
+    def construct(self, tokens):
         _bsz, seq_len = tokens.size()
         assert seq_len <= self.max_seq_len
         h = self.tok_embeddings(tokens)
-        pos = torch.arange(0, seq_len, device=tokens.device)
+        pos = mint.arange(0, seq_len, device=tokens.device)
         p = self.pos_embeddings(pos)  # positional embeddings of shape (seq_len, dim)
         h = h + p
         h = self.dropout(h)
