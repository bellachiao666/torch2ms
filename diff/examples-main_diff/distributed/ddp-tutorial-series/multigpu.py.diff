--- pytorch+++ mindspore@@ -1,12 +1,17 @@-import torch
-import torch.nn.functional as F
-from torch.utils.data import Dataset, DataLoader
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# import torch
+# import torch.nn.functional as F
+# from torch.utils.data import Dataset, DataLoader
 from datautils import MyTrainDataset
 
-import torch.multiprocessing as mp
-from torch.utils.data.distributed import DistributedSampler
-from torch.nn.parallel import DistributedDataParallel as DDP
-from torch.distributed import init_process_group, destroy_process_group
+# import torch.multiprocessing as mp
+# from torch.utils.data.distributed import DistributedSampler
+# from torch.nn.parallel import DistributedDataParallel as DDP
+# from torch.distributed import init_process_group, destroy_process_group
 import os
 
 
@@ -18,13 +23,15 @@     """
     os.environ["MASTER_ADDR"] = "localhost"
     os.environ["MASTER_PORT"] = "12355"
-    torch.cuda.set_device(rank)
-    init_process_group(backend="nccl", rank=rank, world_size=world_size)
+    torch.cuda.set_device(rank)  # 'torch.cuda.set_device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    mint.distributed.init_process_group(backend = "nccl", world_size = world_size, rank = rank)
 
 class Trainer:
+    # 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
         self,
-        model: torch.nn.Module,
+        model: msnn.Cell,
         train_data: DataLoader,
         optimizer: torch.optim.Optimizer,
         gpu_id: int,
@@ -35,12 +42,12 @@         self.train_data = train_data
         self.optimizer = optimizer
         self.save_every = save_every
-        self.model = DDP(model, device_ids=[gpu_id])
+        self.model = DDP(model, device_ids=[gpu_id])  # 'torch.nn.parallel.DistributedDataParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def _run_batch(self, source, targets):
         self.optimizer.zero_grad()
         output = self.model(source)
-        loss = F.cross_entropy(output, targets)
+        loss = F.cross_entropy(output, targets)  # 'torch.nn.functional.cross_entropy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         loss.backward()
         self.optimizer.step()
 
@@ -56,7 +63,7 @@     def _save_checkpoint(self, epoch):
         ckp = self.model.module.state_dict()
         PATH = "checkpoint.pt"
-        torch.save(ckp, PATH)
+        torch.save(ckp, PATH)  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         print(f"Epoch {epoch} | Training checkpoint saved at {PATH}")
 
     def train(self, max_epochs: int):
@@ -68,11 +75,12 @@ 
 def load_train_objs():
     train_set = MyTrainDataset(2048)  # load your dataset
-    model = torch.nn.Linear(20, 1)  # load your model
-    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
+    model = nn.Linear(20, 1)  # load your model
+    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)  # 'torch.optim.SGD' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     return train_set, model, optimizer
 
 
+# 'torch.utils.data.Dataset' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def prepare_dataloader(dataset: Dataset, batch_size: int):
     return DataLoader(
         dataset,
@@ -80,7 +88,7 @@         pin_memory=True,
         shuffle=False,
         sampler=DistributedSampler(dataset)
-    )
+    )  # 'torch.utils.data.distributed.DistributedSampler' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def main(rank: int, world_size: int, save_every: int, total_epochs: int, batch_size: int):
@@ -89,7 +97,7 @@     train_data = prepare_dataloader(dataset, batch_size)
     trainer = Trainer(model, train_data, optimizer, rank, save_every)
     trainer.train(total_epochs)
-    destroy_process_group()
+    destroy_process_group()  # 'torch.distributed.destroy_process_group' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 if __name__ == "__main__":
@@ -100,5 +108,5 @@     parser.add_argument('--batch_size', default=32, type=int, help='Input batch size on each device (default: 32)')
     args = parser.parse_args()
 
-    world_size = torch.cuda.device_count()
-    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)
+    world_size = torch.cuda.device_count()  # 'torch.cuda.device_count' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    mp.spawn(main, args=(world_size, args.save_every, args.total_epochs, args.batch_size), nprocs=world_size)  # 'torch.multiprocessing.spawn' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
