--- pytorch+++ mindspore@@ -1,23 +1,28 @@-import torch
-import torch.nn.functional as F
-from torch.utils.data import Dataset, DataLoader
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# import torch
+# import torch.nn.functional as F
+# from torch.utils.data import Dataset, DataLoader
 from datautils import MyTrainDataset
-
-import torch.multiprocessing as mp
-from torch.utils.data.distributed import DistributedSampler
-from torch.nn.parallel import DistributedDataParallel as DDP
-from torch.distributed import init_process_group, destroy_process_group
+# from torch.utils.data.distributed import DistributedSampler
+# from torch.nn.parallel import DistributedDataParallel as DDP
+# from torch.distributed import init_process_group, destroy_process_group
 import os
 
 
 def ddp_setup():
-    torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))
-    init_process_group(backend="nccl")
+    torch.cuda.set_device(int(os.environ["LOCAL_RANK"]))  # 'torch.cuda.set_device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    mint.distributed.init_process_group(backend = "nccl")
 
 class Trainer:
+    # 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
         self,
-        model: torch.nn.Module,
+        model: msnn.Cell,
         train_data: DataLoader,
         optimizer: torch.optim.Optimizer,
         save_every: int,
@@ -35,11 +40,11 @@             print("Loading snapshot")
             self._load_snapshot(snapshot_path)
 
-        self.model = DDP(self.model, device_ids=[self.local_rank])
+        self.model = DDP(self.model, device_ids=[self.local_rank])  # 'torch.nn.parallel.DistributedDataParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def _load_snapshot(self, snapshot_path):
         loc = f"cuda:{self.local_rank}"
-        snapshot = torch.load(snapshot_path, map_location=loc)
+        snapshot = torch.load(snapshot_path, map_location=loc)  # 'torch.load' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         self.model.load_state_dict(snapshot["MODEL_STATE"])
         self.epochs_run = snapshot["EPOCHS_RUN"]
         print(f"Resuming training from snapshot at Epoch {self.epochs_run}")
@@ -47,7 +52,7 @@     def _run_batch(self, source, targets):
         self.optimizer.zero_grad()
         output = self.model(source)
-        loss = F.cross_entropy(output, targets)
+        loss = F.cross_entropy(output, targets)  # 'torch.nn.functional.cross_entropy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         loss.backward()
         self.optimizer.step()
 
@@ -65,7 +70,7 @@             "MODEL_STATE": self.model.module.state_dict(),
             "EPOCHS_RUN": epoch,
         }
-        torch.save(snapshot, self.snapshot_path)
+        torch.save(snapshot, self.snapshot_path)  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         print(f"Epoch {epoch} | Training snapshot saved at {self.snapshot_path}")
 
     def train(self, max_epochs: int):
@@ -77,11 +82,12 @@ 
 def load_train_objs():
     train_set = MyTrainDataset(2048)  # load your dataset
-    model = torch.nn.Linear(20, 1)  # load your model
-    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
+    model = nn.Linear(20, 1)  # load your model
+    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)  # 'torch.optim.SGD' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     return train_set, model, optimizer
 
 
+# 'torch.utils.data.Dataset' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def prepare_dataloader(dataset: Dataset, batch_size: int):
     return DataLoader(
         dataset,
@@ -89,7 +95,7 @@         pin_memory=True,
         shuffle=False,
         sampler=DistributedSampler(dataset)
-    )
+    )  # 'torch.utils.data.distributed.DistributedSampler' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def main(save_every: int, total_epochs: int, batch_size: int, snapshot_path: str = "snapshot.pt"):
@@ -98,7 +104,7 @@     train_data = prepare_dataloader(dataset, batch_size)
     trainer = Trainer(model, train_data, optimizer, save_every, snapshot_path)
     trainer.train(total_epochs)
-    destroy_process_group()
+    destroy_process_group()  # 'torch.distributed.destroy_process_group' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 if __name__ == "__main__":
