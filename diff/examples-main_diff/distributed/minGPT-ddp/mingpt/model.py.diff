--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 Full definition of a GPT Language Model, all of it in this single file.
 Adapted from https://github.com/karpathy/minGPT/blob/master/mingpt/model.py
@@ -6,9 +11,9 @@ from dataclasses import dataclass
 import math
 
-import torch
-import torch.nn as nn
-from torch.nn import functional as F
+# import torch
+# import torch.nn as nn
+# from torch.nn import functional as F
 
 
 @dataclass
@@ -31,17 +36,17 @@     learning_rate: float = 3e-4
     weight_decay: float = 0.1
 
-class MultiheadAttentionLayer(nn.Module):
+class MultiheadAttentionLayer(msnn.Cell):
     """
     A multi-head masked self-attention layer with a projection at the end.
     """
 
-    def __init__(self, config, device="cpu", dtype=torch.float32):
+    def __init__(self, config, device="cpu", dtype=ms.float32):
         super().__init__()
         assert config.n_embd % config.n_head == 0
         self.resid_drop = nn.Dropout(config.resid_pdrop)
-        self.c_proj = nn.Linear(config.n_embd, config.n_embd, device=device, dtype=dtype)
-        self.register_buffer("mask", torch.tril(torch.ones(config.block_size, config.block_size))
+        self.c_proj = nn.Linear(config.n_embd, config.n_embd, dtype = dtype)  # 'torch.nn.Linear':没有对应的mindspore参数 'device' (position 3);
+        self.register_buffer("mask", mint.tril(mint.ones(config.block_size, config.block_size))
                              .view(1, 1, config.block_size, config.block_size))
         self.attn = torch.nn.MultiheadAttention(
             embed_dim=config.n_embd,
@@ -50,45 +55,46 @@             batch_first=True,
             device=device,
             dtype=dtype
-        )
-
-    def forward(self, x):
+        )  # 'torch.nn.MultiheadAttention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x):
         _, seq_size, _ = x.size()
         y = self.attn(x, x, x, attn_mask=self.mask[0, 0, :seq_size, :seq_size])[0]
         y = self.resid_drop(self.c_proj(y))
         return y
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     """ an unassuming Transformer block """
     def __init__(self, config: GPTConfig):
         super().__init__()
         self.ln1 = nn.LayerNorm(config.n_embd)
         self.ln2 = nn.LayerNorm(config.n_embd)
         self.attn = MultiheadAttentionLayer(config)
-        self.mlp = nn.Sequential(
+        self.mlp = msnn.SequentialCell(
+            [
             nn.Linear(config.n_embd, 4 * config.n_embd),
             nn.GELU(),
             nn.Linear(4 * config.n_embd, config.n_embd),
-            nn.Dropout(config.resid_pdrop),
-        )
-
-    def forward(self, x):
+            nn.Dropout(config.resid_pdrop)
+        ])
+
+    def construct(self, x):
         x = x + self.attn(self.ln1(x))
         x = x + self.mlp(self.ln2(x))
         return x
 
-class EmbeddingStem(nn.Module):
-    def __init__(self, config: GPTConfig, device="cpu", dtype=torch.float32):
-        super().__init__()
-        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd, device=device, dtype=dtype)
-        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd, device=device, dtype=dtype))
+class EmbeddingStem(msnn.Cell):
+    def __init__(self, config: GPTConfig, device="cpu", dtype=ms.float32):
+        super().__init__()
+        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd, dtype = dtype)  # 'torch.nn.Embedding':没有对应的mindspore参数 'device' (position 9);
+        self.pos_emb = ms.Parameter(mint.zeros(1, config.block_size, config.n_embd, device=device, dtype=dtype))
         self.drop = nn.Dropout(config.embd_pdrop)
         self.block_size = config.block_size
 
     def reset_parameters(self):
         self.tok_emb.reset_parameters()
 
-    def forward(self, idx):
+    def construct(self, idx):
         b, t = idx.size()
         assert t <= self.block_size, f"Cannot forward sequence of length {t}, block size is only {self.block_size}"
 
@@ -96,7 +102,7 @@         position_embeddings = self.pos_emb[:, :t, :]  # each position maps to a (learnable) position vector
         return self.drop(token_embeddings + position_embeddings)
         
-class GPT(nn.Module):
+class GPT(msnn.Cell):
     """ GPT Language Model """
 
     def __init__(self, config: GPTConfig):
@@ -107,10 +113,10 @@         # input embedding stem
         self.emb_stem = EmbeddingStem(config)
         # transformer
-        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])
+        self.blocks = msnn.SequentialCell(*[Block(config) for _ in range(config.n_layer)])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         # decoder head
         self.ln_f = nn.LayerNorm(config.n_embd)
-        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
+        self.head = nn.Linear(config.n_embd, config.vocab_size, bias = False)
 
         # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper
         self.apply(self._init_weights)
@@ -156,7 +162,7 @@             module.bias.data.zero_()
             module.weight.data.fill_(1.0)
 
-    def forward(self, idx, targets=None):
+    def construct(self, idx, targets=None):
         x = self.emb_stem(idx)
         x = self.blocks(x)
         x = self.ln_f(x)
@@ -165,10 +171,12 @@         # if we are given some desired targets also calculate the loss
         loss = None
         if targets is not None:
-            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)
+            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)  # 'torch.nn.functional.cross_entropy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         return logits, loss
 
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 装饰器 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.no_grad()
     def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):
         """
@@ -185,22 +193,22 @@             logits = logits[:, -1, :] / temperature
             # optionally crop the logits to only the top k options
             if top_k is not None:
-                v, _ = torch.topk(logits, top_k)
+                v, _ = mint.topk(logits, top_k)
                 logits[logits < v[:, [-1]]] = -float('Inf')
             # apply softmax to convert logits to (normalized) probabilities
-            probs = F.softmax(logits, dim=-1)
+            probs = nn.functional.softmax(logits, dim = -1)
             # either sample from the distribution or take the most likely element
             if do_sample:
-                idx_next = torch.multinomial(probs, num_samples=1)
+                idx_next = mint.multinomial(probs, num_samples=1)
             else:
-                _, idx_next = torch.topk(probs, k=1, dim=-1)
+                _, idx_next = mint.topk(probs, k=1, dim=-1)
             # append sampled index to the running sequence and continue
-            idx = torch.cat((idx, idx_next), dim=1)
+            idx = mint.cat((idx, idx_next), dim=1)
 
         return idx
 
 
-def create_optimizer(model: torch.nn.Module, opt_config: OptimizerConfig):
+def create_optimizer(model: msnn.Cell, opt_config: OptimizerConfig):
     """
     This long function is unfortunately doing something very simple and is being very defensive:
     We are separating out all parameters of the model into two buckets: those that will experience
@@ -211,8 +219,8 @@     # separate out all parameters to those that will and won't experience regularizing weight decay
     decay = set()
     no_decay = set()
-    whitelist_weight_modules = (torch.nn.Linear, )
-    blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)
+    whitelist_weight_modules = (nn.Linear, )
+    blacklist_weight_modules = (nn.LayerNorm, nn.Embedding)
     for mn, m in model.named_modules():
         for pn, p in m.named_parameters():
             fpn = '%s.%s' % (mn, pn) if mn else pn # full param name
@@ -248,5 +256,5 @@         {"params": [param_dict[pn] for pn in sorted(list(decay))], "weight_decay": opt_config.weight_decay},
         {"params": [param_dict[pn] for pn in sorted(list(no_decay))], "weight_decay": 0.0},
     ]
-    optimizer = torch.optim.AdamW(optim_groups, lr=opt_config.learning_rate, betas=(0.9, 0.95))
+    optimizer = mint.optim.AdamW(optim_groups, lr=opt_config.learning_rate, betas=(0.9, 0.95))
     return optimizer