--- pytorch+++ mindspore@@ -1,16 +1,21 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import os
 import threading
 from datetime import datetime
 import warnings
 
-import torch
-import torch.distributed as dist
-import torch.distributed.rpc as rpc
-import torch.multiprocessing as mp
-import torch.nn as nn
-from torch import optim
+# import torch
+# import torch.distributed as dist
+# import torch.distributed.rpc as rpc
+# import torch.multiprocessing as mp
+# import torch.nn as nn
+# from torch import optim
 
-import torchvision
+# import torchvision
 
 # Suppress deprecated ProcessGroup warning
 warnings.filterwarnings("ignore", message="You are using a Backend.*ProcessGroup")
@@ -31,18 +36,19 @@ class BatchUpdateParameterServer(object):
 
     def __init__(self, batch_update_size=batch_update_size):
-        self.model = torchvision.models.resnet50(num_classes=num_classes)
+        self.model = torchvision.models.resnet50(num_classes=num_classes)  # 'torchvision.models.resnet50' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         self.lock = threading.Lock()
-        self.future_model = torch.futures.Future()
+        self.future_model = torch.futures.Future()  # 'torch.futures.Future' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         self.batch_update_size = batch_update_size
         self.curr_update_size = 0
-        self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)
+        self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)  # 'torch.optim.SGD' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         for p in self.model.parameters():
-            p.grad = torch.zeros_like(p)
+            p.grad = mint.zeros_like(p)
 
     def get_model(self):
         return self.model
 
+    # 装饰器 'torch.distributed.rpc.functions.async_execution' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @staticmethod
     @rpc.functions.async_execution
     def update_and_fetch_model(ps_rref, grads):
@@ -62,7 +68,7 @@                 self.optimizer.zero_grad(set_to_none=False)
                 fut.set_result(self.model)
                 timed_log("PS updated model")
-                self.future_model = torch.futures.Future()
+                self.future_model = torch.futures.Future()  # 'torch.futures.Future' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         return fut
 
@@ -74,17 +80,17 @@         self.loss_fn = nn.MSELoss()
         self.one_hot_indices = torch.LongTensor(batch_size) \
                                     .random_(0, num_classes) \
-                                    .view(batch_size, 1)
+                                    .view(batch_size, 1)  # 'torch.LongTensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.LongTensor.random_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.LongTensor.random_.view' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def get_next_batch(self):
         for _ in range(num_batches):
-            inputs = torch.randn(batch_size, 3, image_w, image_h)
-            labels = torch.zeros(batch_size, num_classes) \
+            inputs = mint.randn(batch_size, 3, image_w, image_h)
+            labels = mint.zeros(batch_size, num_classes) \
                         .scatter_(1, self.one_hot_indices, 1)
             yield inputs.cuda(), labels.cuda()
 
     def train(self):
-        name = rpc.get_worker_info().name
+        name = rpc.get_worker_info().name  # 'torch.distributed.rpc.get_worker_info' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         m = self.ps_rref.rpc_sync().get_model().cuda()
         for inputs, labels in self.get_next_batch():
             timed_log(f"{name} processing one batch")
@@ -94,7 +100,7 @@                 self.ps_rref.owner(),
                 BatchUpdateParameterServer.update_and_fetch_model,
                 args=(self.ps_rref, [p.grad for p in m.cpu().parameters()]),
-            ).cuda()
+            ).cuda()  # 'torch.distributed.rpc.rpc_sync' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.rpc.rpc_sync.cuda' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             timed_log(f"{name} got updated model")
 
 
@@ -105,14 +111,14 @@ 
 def run_ps(trainers):
     timed_log("Start training")
-    ps_rref = rpc.RRef(BatchUpdateParameterServer())
+    ps_rref = rpc.RRef(BatchUpdateParameterServer())  # 'torch.distributed.rpc.RRef' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     futs = []
     for trainer in trainers:
         futs.append(
             rpc.rpc_async(trainer, run_trainer, args=(ps_rref,))
-        )
+        )  # 'torch.distributed.rpc.rpc_async' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-    torch.futures.wait_all(futs)
+    torch.futures.wait_all(futs)  # 'torch.futures.wait_all' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     timed_log("Finish training")
 
 
@@ -121,7 +127,7 @@     os.environ['MASTER_PORT'] = '29500'
     
     # Initialize the process group first
-    dist.init_process_group(
+    mint.distributed.init_process_group(
         backend="gloo",
         rank=rank,
         world_size=world_size
@@ -130,14 +136,14 @@     options=rpc.TensorPipeRpcBackendOptions(
         num_worker_threads=16,
         rpc_timeout=60
-     )
+     )  # 'torch.distributed.rpc.TensorPipeRpcBackendOptions' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if rank != 0:
         rpc.init_rpc(
             f"trainer{rank}",
             rank=rank,
             world_size=world_size,
             rpc_backend_options=options
-        )
+        )  # 'torch.distributed.rpc.init_rpc' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         # trainer passively waiting for ps to kick off training iterations
     else:
         rpc.init_rpc(
@@ -145,14 +151,14 @@             rank=rank,
             world_size=world_size,
             rpc_backend_options=options
-        )
+        )  # 'torch.distributed.rpc.init_rpc' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         run_ps([f"trainer{r}" for r in range(1, world_size)])
 
     # block until all rpcs finish
-    rpc.shutdown()
-    dist.destroy_process_group()
+    rpc.shutdown()  # 'torch.distributed.rpc.shutdown' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    dist.destroy_process_group()  # 'torch.distributed.destroy_process_group' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 if __name__=="__main__":
     world_size = batch_update_size + 1
-    mp.spawn(run, args=(world_size, ), nprocs=world_size, join=True)
+    mp.spawn(run, args=(world_size, ), nprocs=world_size, join=True)  # 'torch.multiprocessing.spawn' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
