--- pytorch+++ mindspore@@ -1,18 +1,23 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import os
 import threading
 import time
 import warnings
 from functools import wraps
 
-import torch
-import torch.nn as nn
-import torch.distributed.autograd as dist_autograd
-import torch.distributed.rpc as rpc
-import torch.multiprocessing as mp
-import torch.optim as optim
-from torch.distributed.rpc import RRef
-
-from torchvision.models.resnet import Bottleneck
+# import torch
+# import torch.nn as nn
+# import torch.distributed.autograd as dist_autograd
+# import torch.distributed.rpc as rpc
+# import torch.multiprocessing as mp
+# import torch.optim as optim
+# from torch.distributed.rpc import RRef
+
+# from torchvision.models.resnet import Bottleneck
 
 # Suppress warnings that can't be fixed from user code
 warnings.filterwarnings("ignore", 
@@ -38,9 +43,9 @@ 
 def conv1x1(in_planes, out_planes, stride=1):
     """1x1 convolution"""
-    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)
-
-class ResNetBase(nn.Module):
+    return nn.Conv2d(in_planes, out_planes, kernel_size = 1, stride = stride, bias = False)
+
+class ResNetBase(msnn.Cell):
     def __init__(self, block, inplanes, num_classes=1000,
                  groups=1, width_per_group=64, norm_layer=None):
         super(ResNetBase, self).__init__()
@@ -58,10 +63,11 @@         downsample = None
         previous_dilation = self.dilation
         if stride != 1 or self.inplanes != planes * self._block.expansion:
-            downsample = nn.Sequential(
+            downsample = msnn.SequentialCell(
+                [
                 conv1x1(self.inplanes, planes * self._block.expansion, stride),
-                norm_layer(planes * self._block.expansion),
-            )
+                norm_layer(planes * self._block.expansion)
+            ])
 
         layers = []
         layers.append(self._block(self.inplanes, planes, stride, downsample, self.groups,
@@ -72,14 +78,14 @@                                       base_width=self.base_width, dilation=self.dilation,
                                       norm_layer=norm_layer))
 
-        return nn.Sequential(*layers)
+        return msnn.SequentialCell(*layers)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def parameter_rrefs(self):
         r"""
         Create one RRef for each parameter in the given local module, and return a
         list of RRefs.
         """
-        return [RRef(p) for p in self.parameters()]
+        return [RRef(p) for p in self.parameters()]  # 'torch.distributed.rpc.RRef' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 class ResNetShard1(ResNetBase):
@@ -88,24 +94,25 @@     """
     def __init__(self, device, *args, **kwargs):
         super(ResNetShard1, self).__init__(
-            Bottleneck, 64, num_classes=num_classes, *args, **kwargs)
+            Bottleneck, 64, num_classes=num_classes, *args, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.device = device
-        self.seq = nn.Sequential(
-            nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False),
+        self.seq = msnn.SequentialCell(
+            [
+            nn.Conv2d(3, self.inplanes, kernel_size = 7, stride = 2, padding = 3, bias = False),
             self._norm_layer(self.inplanes),
-            nn.ReLU(inplace=True),
-            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1),
             self._make_layer(64, 3),
             self._make_layer(128, 4, stride=2)
-        ).to(self.device)
+        ]).to(self.device)  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.BatchNorm2d):
-                nn.init.ones_(m.weight)
-                nn.init.zeros_(m.bias)
+                nn.init.ones_(m.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                nn.init.zeros_(m.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def forward(self, x_rref):
         x = x_rref.to_here().to(self.device)
@@ -120,25 +127,26 @@     """
     def __init__(self, device, *args, **kwargs):
         super(ResNetShard2, self).__init__(
-            Bottleneck, 512, num_classes=num_classes, *args, **kwargs)
+            Bottleneck, 512, num_classes=num_classes, *args, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.device = device
-        self.seq = nn.Sequential(
+        self.seq = msnn.SequentialCell(
+            [
             self._make_layer(256, 6, stride=2),
             self._make_layer(512, 3, stride=2),
-            nn.AdaptiveAvgPool2d((1, 1)),
-        ).to(self.device)
-
-        self.fc =  nn.Linear(512 * self._block.expansion, num_classes).to(self.device)
+            nn.AdaptiveAvgPool2d((1, 1))
+        ]).to(self.device)
+
+        self.fc =  nn.Linear(512 * self._block.expansion, num_classes).to(self.device)  # 'torch.nn.Linear.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def forward(self, x_rref):
         x = x_rref.to_here().to(self.device)
         with self._lock:
-            out = self.fc(torch.flatten(self.seq(x), 1))
+            out = self.fc(mint.flatten(self.seq(x), 1))
         return out.cpu()
 
 
-class DistResNet50(nn.Module):
+class DistResNet50(msnn.Cell):
     """
     Assemble two parts as an nn.Module and define pipelining logic
     """
@@ -153,7 +161,7 @@             ResNetShard1,
             args = ("cuda:0",) + args,
             kwargs = kwargs
-        )
+        )  # 'torch.distributed.rpc.remote' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # Put the second part of the ResNet50 on workers[1]
         self.p2_rref = rpc.remote(
@@ -161,20 +169,20 @@             ResNetShard2,
             args = ("cuda:1",) + args,
             kwargs = kwargs
-        )
-
-    def forward(self, xs):
+        )  # 'torch.distributed.rpc.remote' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, xs):
         # Split the input batch xs into micro-batches, and collect async RPC
         # futures into a list
         out_futures = []
         for x in iter(xs.split(self.split_size, dim=0)):
-            x_rref = RRef(x)
+            x_rref = RRef(x)  # 'torch.distributed.rpc.RRef' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             y_rref = self.p1_rref.remote().forward(x_rref)
             z_fut = self.p2_rref.rpc_async().forward(y_rref)
             out_futures.append(z_fut)
 
         # collect and cat all output tensors into one tensor.
-        return torch.cat(torch.futures.wait_all(out_futures))
+        return mint.cat(torch.futures.wait_all(out_futures))  # 'torch.futures.wait_all' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def parameter_rrefs(self):
         remote_params = []
@@ -196,8 +204,8 @@ def create_optimizer_for_remote_params(worker_name, param_rrefs, lr=0.05):
     """Create torch.compiled optimizers on  each worker"""
     params = [p.to_here() for p in param_rrefs]
-    opt = optim.SGD(params, lr=lr)
-    opt.step = torch.compile(opt.step)
+    opt = optim.SGD(params, lr=lr)  # 'torch.optim.SGD' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    opt.step = torch.compile(opt.step)  # 'torch.compile' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     return opt
 
 
@@ -216,30 +224,31 @@         "worker1",
         create_optimizer_for_remote_params,
         args=("worker1", p1_param_rrefs)
-    )
+    )  # 'torch.distributed.rpc.remote' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     opt2_rref = rpc.remote(
         "worker2",
         create_optimizer_for_remote_params,
         args=("worker2", p2_param_rrefs)
-    )
+    )  # 'torch.distributed.rpc.remote' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     one_hot_indices = torch.LongTensor(batch_size) \
                            .random_(0, num_classes) \
-                           .view(batch_size, 1)
+                           .view(batch_size, 1)  # 'torch.LongTensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.LongTensor.random_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.LongTensor.random_.view' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     for i in range(num_batches):
         print(f"Processing batch {i}")
         # generate random inputs and labels
-        inputs = torch.randn(batch_size, 3, image_w, image_h)
-        labels = torch.zeros(batch_size, num_classes) \
+        inputs = mint.randn(batch_size, 3, image_w, image_h)
+        labels = mint.zeros(batch_size, num_classes) \
                       .scatter_(1, one_hot_indices, 1)
 
         # The distributed autograd context is the dedicated scope for the
         # distributed backward pass to store gradients, which can later be
         # retrieved using the context_id by the distributed optimizer.
+        # 'torch.distributed.autograd.context' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         with dist_autograd.context() as context_id:
             outputs = model(inputs)
-            dist_autograd.backward(context_id, [loss_fn(outputs, labels)])
+            dist_autograd.backward(context_id, [loss_fn(outputs, labels)])  # 'torch.distributed.autograd.backward' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             
             opt1_rref.rpc_sync().step()
             opt2_rref.rpc_sync().step()
@@ -253,7 +262,7 @@     os.environ['MASTER_PORT'] = '29500'
 
     # Higher timeout is added to accommodate for kernel compilation time in case of ROCm.
-    options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=256, rpc_timeout=300)
+    options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=256, rpc_timeout=300)  # 'torch.distributed.rpc.TensorPipeRpcBackendOptions' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     if rank == 0:
         rpc.init_rpc(
@@ -261,7 +270,7 @@             rank=rank,
             world_size=world_size,
             rpc_backend_options=options
-        )
+        )  # 'torch.distributed.rpc.init_rpc' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         run_master(num_split)
     else:
         rpc.init_rpc(
@@ -269,11 +278,11 @@             rank=rank,
             world_size=world_size,
             rpc_backend_options=options
-        )
+        )  # 'torch.distributed.rpc.init_rpc' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         pass
 
     # block until all rpcs finish
-    rpc.shutdown()
+    rpc.shutdown()  # 'torch.distributed.rpc.shutdown' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 if __name__=="__main__":
@@ -283,6 +292,6 @@     world_size = 3
     for num_split in [1, 2, 4, 8]:
         tik = time.time()
-        mp.spawn(run_worker, args=(world_size, num_split), nprocs=world_size, join=True)
+        mp.spawn(run_worker, args=(world_size, num_split), nprocs=world_size, join=True)  # 'torch.multiprocessing.spawn' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         tok = time.time()
         print(f"number of splits = {num_split}, execution time = {tok - tik}")
