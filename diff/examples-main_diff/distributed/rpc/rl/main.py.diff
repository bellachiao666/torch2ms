--- pytorch+++ mindspore@@ -1,17 +1,21 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import argparse
 import gymnasium as gym
 import numpy as np
 import os
 from itertools import count
 
-import torch
-import torch.distributed.rpc as rpc
-import torch.multiprocessing as mp
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-from torch.distributed.rpc import RRef, rpc_sync, rpc_async, remote
-from torch.distributions import Categorical
+# import torch
+# import torch.distributed.rpc as rpc
+# import torch.multiprocessing as mp
+# import torch.nn as nn
+# import torch.optim as optim
+# from torch.distributed.rpc import RRef, rpc_sync, rpc_async, remote
+# from torch.distributions import Categorical
 
 TOTAL_EPISODE_STEP = 5000
 AGENT_NAME = "agent"
@@ -28,14 +32,14 @@                     help='interval between training status logs (default: 10)')
 args = parser.parse_args()
 
-torch.manual_seed(args.seed)
+torch.manual_seed(args.seed)  # 'torch.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def _call_method(method, rref, *args, **kwargs):
     r"""
     a helper function to call a method on the given RRef
     """
-    return method(rref.local_value(), *args, **kwargs)
+    return method(rref.local_value(), *args, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _remote_method(method, rref, *args, **kwargs):
@@ -44,10 +48,10 @@     result using RPC
     """
     args = [method, rref] + list(args)
-    return rpc_sync(rref.owner(), _call_method, args=args, kwargs=kwargs)
-
-
-class Policy(nn.Module):
+    return rpc_sync(rref.owner(), _call_method, args=args, kwargs=kwargs)  # 'torch.distributed.rpc.rpc_sync' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+
+class Policy(msnn.Cell):
     r"""
     Borrowing the ``Policy`` class from the Reinforcement Learning example.
     Copying the code to make these two examples independent.
@@ -56,18 +60,18 @@     def __init__(self):
         super(Policy, self).__init__()
         self.affine1 = nn.Linear(4, 128)
-        self.dropout = nn.Dropout(p=0.6)
+        self.dropout = nn.Dropout(p = 0.6)
         self.affine2 = nn.Linear(128, 2)
 
         self.saved_log_probs = []
         self.rewards = []
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.affine1(x)
         x = self.dropout(x)
-        x = F.relu(x)
+        x = nn.functional.relu(x)
         action_scores = self.affine2(x)
-        return F.softmax(action_scores, dim=1)
+        return nn.functional.softmax(action_scores, dim = 1)
 
 class Observer:
     r"""
@@ -83,7 +87,7 @@     other applications with much more expensive environment.
     """
     def __init__(self):
-        self.id = rpc.get_worker_info().id
+        self.id = rpc.get_worker_info().id  # 'torch.distributed.rpc.get_worker_info' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         self.env = gym.make('CartPole-v1')
         self.env.reset(seed=args.seed)
 
@@ -112,17 +116,17 @@ class Agent:
     def __init__(self, world_size):
         self.ob_rrefs = []
-        self.agent_rref = RRef(self)
+        self.agent_rref = RRef(self)  # 'torch.distributed.rpc.RRef' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         self.rewards = {}
         self.saved_log_probs = {}
         self.policy = Policy()
-        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)
+        self.optimizer = mint.optim.Adam(self.policy.parameters(), lr=1e-2)
         self.eps = np.finfo(np.float32).eps.item()
         self.running_reward = 0
         self.reward_threshold = gym.make('CartPole-v1').spec.reward_threshold
         for ob_rank in range(1, world_size):
-            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))
-            self.ob_rrefs.append(remote(ob_info, Observer))
+            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))  # 'torch.distributed.rpc.get_worker_info' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            self.ob_rrefs.append(remote(ob_info, Observer))  # 'torch.distributed.rpc.remote' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             self.rewards[ob_info.id] = []
             self.saved_log_probs[ob_info.id] = []
 
@@ -136,9 +140,9 @@         NB: no need to enforce thread-safety here as GIL will serialize
         executions.
         """
-        state = torch.from_numpy(state).float().unsqueeze(0)
+        state = ms.float32().unsqueeze(0)  # 'torch.from_numpy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         probs = self.policy(state)
-        m = Categorical(probs)
+        m = Categorical(probs)  # 'torch.distributions.Categorical' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         action = m.sample()
         self.saved_log_probs[ob_id].append(m.log_prob(action))
         return action.item()
@@ -162,7 +166,7 @@                     _call_method,
                     args=(Observer.run_episode, ob_rref, self.agent_rref, n_steps)
                 )
-            )
+            )  # 'torch.distributed.rpc.rpc_async' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # wait until all obervers have finished this episode
         for fut in futs:
@@ -196,12 +200,12 @@         for r in rewards[::-1]:
             R = r + args.gamma * R
             returns.insert(0, R)
-        returns = torch.tensor(returns)
+        returns = ms.Tensor(returns)
         returns = (returns - returns.mean()) / (returns.std() + self.eps)
         for log_prob, R in zip(probs, returns):
             policy_loss.append(-log_prob * R)
         self.optimizer.zero_grad()
-        policy_loss = torch.cat(policy_loss).sum()
+        policy_loss = mint.cat(policy_loss).sum()
         policy_loss.backward()
         self.optimizer.step()
         return min_reward
@@ -216,7 +220,7 @@     os.environ['MASTER_PORT'] = '29500'
     if rank == 0:
         # rank0 is the agent
-        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)
+        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)  # 'torch.distributed.rpc.init_rpc' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         agent = Agent(world_size)
         for i_episode in count(1):
@@ -233,9 +237,9 @@                 break
     else:
         # other ranks are the observer
-        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)
+        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)  # 'torch.distributed.rpc.init_rpc' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         # observers passively waiting for instructions from agents
-    rpc.shutdown()
+    rpc.shutdown()  # 'torch.distributed.rpc.shutdown' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def main():
@@ -244,7 +248,7 @@         args=(args.world_size, ),
         nprocs=args.world_size,
         join=True
-    )
+    )  # 'torch.multiprocessing.spawn' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 if __name__ == '__main__':
     main()
