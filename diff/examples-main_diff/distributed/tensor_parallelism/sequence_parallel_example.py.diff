--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 This is the script to test Sequence Parallel(SP) on a toy model in a
 Megetron-LM SPMD style. We show an E2E working flow from forward,
@@ -19,13 +24,13 @@ 
 import os
 import sys
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
-import torch.distributed as dist
-from torch.distributed._tensor import Shard
+# import torch.distributed as dist
+# from torch.distributed._tensor import Shard
 
-from torch.distributed.tensor.parallel import (
+from ms.Tensor.parallel import (
     parallelize_module,
     ColwiseParallel,
     RowwiseParallel,
@@ -42,9 +47,9 @@     sys.exit()
 # ---------------------------
 
-from torch.distributed._tensor.device_mesh import init_device_mesh
+# from torch.distributed._tensor.device_mesh import init_device_mesh
 
-class ToyModel(nn.Module):
+class ToyModel(msnn.Cell):
     """MLP based model"""
 
     def __init__(self):
@@ -53,7 +58,7 @@         self.relu = nn.ReLU()
         self.out_proj = nn.Linear(32, 5)
 
-    def forward(self, x):
+    def construct(self, x):
         return self.out_proj(self.relu(self.in_proj(x)))
 
 
@@ -63,11 +68,11 @@ """
 logger = get_logger()
 
-device_type = torch.accelerator.current_accelerator().type
+device_type = torch.accelerator.current_accelerator().type  # 'torch.accelerator.current_accelerator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 # create a device mesh based on the given world_size.
 device_mesh = init_device_mesh(
     device_type=device_type, mesh_shape=(int(os.environ["WORLD_SIZE"]),)
-)
+)  # 'torch.distributed._tensor.device_mesh.init_device_mesh' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 _rank = device_mesh.get_rank()
 
@@ -86,12 +91,12 @@         "in_proj": ColwiseParallel(input_layouts=Shard(0)),
         "out_proj": RowwiseParallel(output_layouts=Shard(0)),
     },
-)
+)  # 'torch.distributed._tensor.Shard' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.tensor.parallel.ColwiseParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.tensor.parallel.RowwiseParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.tensor.parallel.parallelize_module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 # Create a optimizer for the parallelized module.
 lr = 0.25
-optimizer = torch.optim.AdamW(sp_model.parameters(), lr=lr, foreach=True)
+optimizer = mint.optim.AdamW(sp_model.parameters(), lr=lr, foreach=True)
 
 
 # Perform a num of iterations of forward/backward
@@ -101,7 +106,7 @@ 
 for i in range(num_iters):
     # For SP, input can be different across all ranks.
-    inp = torch.rand(20, 10, device=device_type)
+    inp = mint.rand(20, 10, device=device_type)
     output = sp_model(inp)
     output.sum().backward()
     optimizer.step()
@@ -109,5 +114,6 @@ 
 rank_log(_rank, logger, "Sequence Parallel training completed!")
 
+# 'torch.distributed.is_initialized' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 if dist.is_initialized():
-    dist.destroy_process_group()
+    dist.destroy_process_group()  # 'torch.distributed.destroy_process_group' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
