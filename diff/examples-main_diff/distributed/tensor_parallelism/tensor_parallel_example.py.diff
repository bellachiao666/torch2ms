--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 This is the script to test Tensor Parallel(TP) on a toy model in a
 Megetron-LM SPMD style. We show an E2E working flow from forward,
@@ -34,10 +39,10 @@ 
 import os
 import sys
-import torch
-import torch.nn as nn
-import torch.distributed as dist
-from torch.distributed.tensor.parallel import (
+# import torch
+# import torch.nn as nn
+# import torch.distributed as dist
+from ms.Tensor.parallel import (
     parallelize_module,
     ColwiseParallel,
     RowwiseParallel,
@@ -52,9 +57,9 @@     sys.exit()
 # ---------------------------
 
-from torch.distributed._tensor.device_mesh import init_device_mesh
+# from torch.distributed._tensor.device_mesh import init_device_mesh
 
-class ToyModel(nn.Module):
+class ToyModel(msnn.Cell):
     """MLP based model"""
 
     def __init__(self):
@@ -63,7 +68,7 @@         self.relu = nn.ReLU()
         self.out_proj = nn.Linear(32, 5)
 
-    def forward(self, x):
+    def construct(self, x):
         return self.out_proj(self.relu(self.in_proj(x)))
 
 
@@ -75,8 +80,8 @@ 
 # create a device mesh based on the given world_size.
 _world_size = int(os.environ["WORLD_SIZE"])
-device_type = torch.accelerator.current_accelerator().type
-device_mesh = init_device_mesh(device_type=device_type, mesh_shape=(_world_size,))
+device_type = torch.accelerator.current_accelerator().type  # 'torch.accelerator.current_accelerator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+device_mesh = init_device_mesh(device_type=device_type, mesh_shape=(_world_size,))  # 'torch.distributed._tensor.device_mesh.init_device_mesh' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 _rank = device_mesh.get_rank()
 
 
@@ -99,11 +104,11 @@         "in_proj": ColwiseParallel(),
         "out_proj": RowwiseParallel(),
     },
-)
+)  # 'torch.distributed.tensor.parallel.ColwiseParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.tensor.parallel.RowwiseParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributed.tensor.parallel.parallelize_module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 # Create an optimizer for the parallelized module.
 lr = 0.25
-optimizer = torch.optim.AdamW(tp_model.parameters(), lr=lr, foreach=True)
+optimizer = mint.optim.AdamW(tp_model.parameters(), lr=lr, foreach=True)
 
 
 # Perform a num of iterations of forward/backward
@@ -114,8 +119,8 @@ for i in range(num_iters):
     # For TP, input needs to be same across all TP ranks.
     # Setting the random seed is to mimic the behavior of dataloader.
-    torch.manual_seed(i)
-    inp = torch.rand(20, 10, device=device_type)
+    torch.manual_seed(i)  # 'torch.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    inp = mint.rand(20, 10, device=device_type)
     output = tp_model(inp)
     output.sum().backward()
     optimizer.step()
@@ -123,5 +128,6 @@ 
 rank_log(_rank, logger, "Tensor Parallel training completed!")
 
+# 'torch.distributed.is_initialized' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 if dist.is_initialized():
-    dist.destroy_process_group()
+    dist.destroy_process_group()  # 'torch.distributed.destroy_process_group' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
