--- pytorch+++ mindspore@@ -1,6 +1,11 @@-import torch
-from torch.fx import Proxy, symbolic_trace
-from torch.fx.node import map_arg
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# import torch
+# from torch.fx import Proxy, symbolic_trace
+# from torch.fx.node import map_arg
 
 
 '''
@@ -28,23 +33,23 @@ 
 
 # Sample module
-class M(torch.nn.Module):
+class M(msnn.Cell):
     def __init__(self):
         super().__init__()
-        self.relu = torch.nn.ReLU()
+        self.relu = nn.ReLU()
 
-    def forward(self, x):
+    def construct(self, x):
         return self.relu(x) + 1.0
 
 # Symbolically trace an instance of `M`. After tracing, `self.relu` is
 # represented as a `call_module` Node. The full operation in the
 # generated `forward` function's code will appear as `self.relu(x)`
-m = symbolic_trace(M())
+m = symbolic_trace(M())  # 'torch.fx.symbolic_trace' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 # Insert nodes from the ReLU graph in place of the original call to
 # `self.relu`
 # create a graph-appending tracer pointing to the original graph
-tracer = torch.fx.proxy.GraphAppendingTracer(m.graph)
+tracer = torch.fx.proxy.GraphAppendingTracer(m.graph)  # 'torch.fx.proxy.GraphAppendingTracer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 for node in m.graph.nodes:
     # Find `call_module` Node in `m` that corresponds to `self.relu`.
     # This is the Node we want to swap out for an inlined version of the
@@ -53,8 +58,8 @@         with m.graph.inserting_before(node):
             # Create a Proxy from each Node in the current Node's
             # args/kwargs
-            proxy_args = map_arg(node.args, lambda n: Proxy(n, tracer))
-            proxy_kwargs = map_arg(node.kwargs, lambda n: Proxy(n, tracer))
+            proxy_args = map_arg(node.args, lambda n: Proxy(n, tracer))  # 'torch.fx.Proxy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.fx.node.map_arg' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            proxy_kwargs = map_arg(node.kwargs, lambda n: Proxy(n, tracer))  # 'torch.fx.Proxy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.fx.node.map_arg' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             # Call `m.relu` with the newly-created Proxy arguments.
             # `m.relu` is the generic version of the function; by
             # calling it with Proxies created from Nodes in `m`, we're
@@ -62,7 +67,7 @@             # The result of this call is another Proxy, which we can
             # hook into our existing Graph to complete the function
             # inlining.
-            proxy_output = m.relu(*proxy_args, **proxy_kwargs)
+            proxy_output = m.relu(*proxy_args, **proxy_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             # Replace the relu `call_module` node with the inlined
             # version of the function
             node.replace_all_uses_with(proxy_output.node)
