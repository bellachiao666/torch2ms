--- pytorch+++ mindspore@@ -1,5 +1,9 @@-import torch
-import torch.fx
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# import torch
 """
 In this example we are going do define a library of
 "composite" operations. Composite operations are those
@@ -19,7 +23,7 @@ """
 
 
-def sigmoid_lowp(x : torch.Tensor):
+def sigmoid_lowp(x : ms.Tensor):
     x = x.float()
     x = x.sigmoid()
     return x.half()
@@ -29,27 +33,27 @@ # through. Later, we will see how we can:
 # a. Inline the implementation of such a function and
 # b. Define a tracer that automatically traces through such a function
-torch.fx.wrap(sigmoid_lowp)
+torch.fx.wrap(sigmoid_lowp)  # 'torch.fx.wrap' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-def add_lowp(a : torch.Tensor, b : torch.Tensor):
+def add_lowp(a : ms.Tensor, b : ms.Tensor):
     a, b = a.float(), b.float()
     c = a + b
     return c.half()
 
-torch.fx.wrap(add_lowp)
+torch.fx.wrap(add_lowp)  # 'torch.fx.wrap' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 # Let's see what happens when we symbolically trace through some code
 # that uses these functions
 
-class Foo(torch.nn.Module):
-    def forward(self, x, y):
+class Foo(msnn.Cell):
+    def construct(self, x, y):
         x = sigmoid_lowp(x)
         y = sigmoid_lowp(y)
         return add_lowp(x, y)
 
 
-traced = torch.fx.symbolic_trace(Foo())
+traced = torch.fx.symbolic_trace(Foo())  # 'torch.fx.symbolic_trace' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 print(traced.code)
 """
 def forward(self, x, y):
@@ -67,24 +71,25 @@ # Now let's define a function that allows for inlining these calls
 # during graph manipulation.
 
+# 'torch.fx.Node' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def inline_lowp_func(n : torch.fx.Node):
     # If we find a call to a function in our "lowp" module, inline it
     if n.op == 'call_function' and n.target.__module__ == inline_lowp_func.__module__:
         # We want to insert the operations comprising the implementation of the
         # function before the function itself. Then, we can swap the output value
         # of the function call with the output value for its implementation nodes
-        tracer = torch.fx.proxy.GraphAppendingTracer(n.graph)
+        tracer = torch.fx.proxy.GraphAppendingTracer(n.graph)  # 'torch.fx.proxy.GraphAppendingTracer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         with n.graph.inserting_before(n):
             # We can inline code by using `fx.Proxy` instances.
             # map_arg traverses all aggregate types and applies the given function
             # to Node instances in the data structure. In this case, we are applying
             # the fx.Proxy constructor.
-            proxy_args = torch.fx.node.map_arg(n.args, lambda x: torch.fx.Proxy(x, tracer))
-            proxy_kwargs = torch.fx.node.map_arg(n.kwargs, lambda x: torch.fx.Proxy(x, tracer))
+            proxy_args = torch.fx.node.map_arg(n.args, lambda x: torch.fx.Proxy(x, tracer))  # 'torch.fx.Proxy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.fx.node.map_arg' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            proxy_kwargs = torch.fx.node.map_arg(n.kwargs, lambda x: torch.fx.Proxy(x, tracer))  # 'torch.fx.Proxy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.fx.node.map_arg' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             # Call the function itself with proxy arguments. This will emit
             # nodes in the graph corresponding to the operations in the im-
             # plementation of the function
-            output_proxy = n.target(*proxy_args, **proxy_kwargs)
+            output_proxy = n.target(*proxy_args, **proxy_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             # Now replace the original node's uses with the output node of
             # the implementation.
             node.replace_all_uses_with(output_proxy.node)
@@ -121,24 +126,25 @@ # New instance of our module
 f = Foo()
 
+# 'torch.fx.Tracer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class InliningTracer(torch.fx.Tracer):
     FNS_TO_INLINE = [add_lowp]
 
     def create_node(self, kind, target, args, kwargs, name=None, type_expr=None):
         if kind == 'call_function' and target in self.FNS_TO_INLINE:
-            tracer = torch.fx.proxy.GraphAppendingTracer(self.graph)
+            tracer = torch.fx.proxy.GraphAppendingTracer(self.graph)  # 'torch.fx.proxy.GraphAppendingTracer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             # Trace through the implementation of the function rather than
             # create a node
-            proxy_args = torch.fx.node.map_arg(args, lambda x: torch.fx.Proxy(x, tracer))
-            proxy_kwargs = torch.fx.node.map_arg(kwargs, lambda x: torch.fx.Proxy(x, tracer))
-            return target(*proxy_args, **proxy_kwargs).node
+            proxy_args = torch.fx.node.map_arg(args, lambda x: torch.fx.Proxy(x, tracer))  # 'torch.fx.Proxy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.fx.node.map_arg' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            proxy_kwargs = torch.fx.node.map_arg(kwargs, lambda x: torch.fx.Proxy(x, tracer))  # 'torch.fx.Proxy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.fx.node.map_arg' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            return target(*proxy_args, **proxy_kwargs).node  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             return super().create_node(kind, target, args, kwargs, name, type_expr)
 
 
 tracer = InliningTracer()
 graph = tracer.trace(f)
-module = torch.fx.GraphModule(f, graph)
+module = torch.fx.GraphModule(f, graph)  # 'torch.fx.GraphModule' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 print(module.code)
 """
 def forward(self, x, y):
