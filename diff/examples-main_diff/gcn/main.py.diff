--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import os
 import time
 import requests
@@ -5,13 +10,12 @@ import numpy as np
 import argparse
 
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.optim import Adam
-
-
-class GraphConv(nn.Module):
+# import torch
+# from torch import nn
+# from torch.optim import Adam
+
+
+class GraphConv(msnn.Cell):
     """
         Graph Convolutional Layer described in "Semi-Supervised Classification with Graph Convolutional Networks".
 
@@ -34,16 +38,16 @@         super(GraphConv, self).__init__()
 
         # Initialize the weight matrix W (in this case called `kernel`)
-        self.kernel = nn.Parameter(torch.Tensor(input_dim, output_dim))
-        nn.init.xavier_normal_(self.kernel) # Initialize the weights using Xavier initialization
+        self.kernel = ms.Parameter(ms.Tensor(input_dim, output_dim))
+        nn.init.xavier_normal_(self.kernel)  # Initialize the weights using Xavier initialization; 'torch.nn.init.xavier_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # Initialize the bias (if use_bias is True)
         self.bias = None
         if use_bias:
-            self.bias = nn.Parameter(torch.Tensor(output_dim))
-            nn.init.zeros_(self.bias) # Initialize the bias to zeros
-
-    def forward(self, input_tensor, adj_mat):
+            self.bias = ms.Parameter(ms.Tensor(output_dim))
+            nn.init.zeros_(self.bias)  # Initialize the bias to zeros; 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, input_tensor, adj_mat):
         """
         Performs a graph convolution operation.
 
@@ -55,8 +59,8 @@             torch.Tensor: Output tensor after the graph convolution operation.
         """
 
-        support = torch.mm(input_tensor, self.kernel) # Matrix multiplication between input and weight matrix
-        output = torch.spmm(adj_mat, support) # Sparse matrix multiplication between adjacency matrix and support
+        support = mint.mm(input_tensor, self.kernel) # Matrix multiplication between input and weight matrix
+        output = torch.spmm(adj_mat, support)  # Sparse matrix multiplication between adjacency matrix and support; 'torch.spmm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         # Add the bias (if bias is not None)
         if self.bias is not None:
             output = output + self.bias
@@ -64,7 +68,7 @@         return output
 
 
-class GCN(nn.Module):
+class GCN(msnn.Cell):
     """
     Graph Convolutional Network (GCN) as described in the paper `"Semi-Supervised Classification with Graph 
     Convolutional Networks" <https://arxiv.org/pdf/1609.02907.pdf>`.
@@ -86,7 +90,7 @@         # Define the dropout layer
         self.dropout = nn.Dropout(dropout_p)
 
-    def forward(self, input_tensor, adj_mat):
+    def construct(self, input_tensor, adj_mat):
         """
         Performs forward pass of the Graph Convolutional Network (GCN).
 
@@ -102,14 +106,14 @@ 
         # Perform the first graph convolutional layer
         x = self.gc1(input_tensor, adj_mat)
-        x = F.relu(x) # Apply ReLU activation function
+        x = nn.functional.relu(x) # Apply ReLU activation function
         x = self.dropout(x) # Apply dropout regularization
 
         # Perform the second graph convolutional layer
         x = self.gc2(x, adj_mat)
 
         # Apply log-softmax activation function for classification
-        return F.log_softmax(x, dim=1)
+        return mint.special.log_softmax(x, dim=1)
 
 
 def load_cora(path='./cora', device='cpu'):
@@ -131,16 +135,16 @@     cites_tensor = np.genfromtxt(cites_path, dtype=np.int32)
 
     # Process features
-    features = torch.FloatTensor(content_tensor[:, 1:-1].astype(np.int32)) # Extract feature values
-    scale_vector = torch.sum(features, dim=1) # Compute sum of features for each node
+    features = torch.FloatTensor(content_tensor[:, 1:-1].astype(np.int32))  # Extract feature values; 'torch.FloatTensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    scale_vector = mint.sum(features, dim=1) # Compute sum of features for each node
     scale_vector = 1 / scale_vector # Compute reciprocal of the sums
     scale_vector[scale_vector == float('inf')] = 0 # Handle division by zero cases
-    scale_vector = torch.diag(scale_vector).to_sparse() # Convert the scale vector to a sparse diagonal matrix
+    scale_vector = torch.diag(scale_vector).to_sparse()  # Convert the scale vector to a sparse diagonal matrix; 'torch.diag' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.diag.to_sparse' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     features = scale_vector @ features # Scale the features using the scale vector
 
     # Process labels
     classes, labels = np.unique(content_tensor[:, -1], return_inverse=True) # Extract unique classes and map labels to indices
-    labels = torch.LongTensor(labels) # Convert labels to a tensor
+    labels = torch.LongTensor(labels)  # Convert labels to a tensor; 'torch.LongTensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     # Process adjacency matrix
     idx = content_tensor[:, 0].astype(np.int32) # Extract node indices
@@ -153,13 +157,13 @@ 
     V = len(idx) # Number of nodes
     E = edges.shape[0] # Number of edges
-    adj_mat = torch.sparse_coo_tensor(edges.T, torch.ones(E), (V, V), dtype=torch.int64) # Create the initial adjacency matrix as a sparse tensor
-    adj_mat = torch.eye(V) + adj_mat # Add self-loops to the adjacency matrix
-
-    degree_mat = torch.sum(adj_mat, dim=1) # Compute the sum of each row in the adjacency matrix (degree matrix)
-    degree_mat = torch.sqrt(1 / degree_mat) # Compute the reciprocal square root of the degrees
+    adj_mat = torch.sparse_coo_tensor(edges.T, mint.ones(E), (V, V), dtype=ms.int64)  # Create the initial adjacency matrix as a sparse tensor; 'torch.sparse_coo_tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    adj_mat = mint.eye(V) + adj_mat # Add self-loops to the adjacency matrix
+
+    degree_mat = mint.sum(adj_mat, dim=1) # Compute the sum of each row in the adjacency matrix (degree matrix)
+    degree_mat = mint.sqrt(1 / degree_mat) # Compute the reciprocal square root of the degrees
     degree_mat[degree_mat == float('inf')] = 0 # Handle division by zero cases
-    degree_mat = torch.diag(degree_mat).to_sparse() # Convert the degree matrix to a sparse diagonal matrix
+    degree_mat = torch.diag(degree_mat).to_sparse()  # Convert the degree matrix to a sparse diagonal matrix; 'torch.diag' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.diag.to_sparse' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     adj_mat = degree_mat @ adj_mat @ degree_mat # Apply the renormalization trick
 
@@ -171,7 +175,7 @@     optimizer.zero_grad()
 
     # Forward pass
-    output = model(*input) 
+    output = model(*input)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     loss = criterion(output[mask_train], target[mask_train]) # Compute the loss using the training mask
 
     loss.backward()
@@ -188,8 +192,9 @@ 
 def test(model, criterion, input, target, mask):
     model.eval()
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     with torch.no_grad():
-        output = model(*input)
+        output = model(*input)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         output, target = output[mask], target[mask]
 
         loss = criterion(output, target)
@@ -221,14 +226,14 @@                         help='random seed (default: 42)')
     args = parser.parse_args()
 
-    use_accel = not args.no_accel and torch.accelerator.is_available()
-
-    torch.manual_seed(args.seed)
+    use_accel = not args.no_accel and torch.accelerator.is_available()  # 'torch.accelerator.is_available' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    torch.manual_seed(args.seed)  # 'torch.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     if use_accel:
-        device = torch.accelerator.current_accelerator()
+        device = torch.accelerator.current_accelerator()  # 'torch.accelerator.current_accelerator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     else:
-        device = torch.device('cpu')
+        device = torch.device('cpu')  # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     print(f'Using {device} device')
 
@@ -240,11 +245,11 @@ 
     print('Loading dataset...')
     features, labels, adj_mat = load_cora(device=device)
-    idx = torch.randperm(len(labels)).to(device)
+    idx = mint.randperm(len(labels)).to(device)
     idx_test, idx_val, idx_train = idx[:1000], idx[1000:1500], idx[1500:]
 
     gcn = GCN(features.shape[1], args.hidden_dim, labels.max().item() + 1, args.include_bias, args.dropout_p).to(device)
-    optimizer = Adam(gcn.parameters(), lr=args.lr, weight_decay=args.l2)
+    optimizer = mint.optim.Adam(gcn.parameters(), lr = args.lr, weight_decay = args.l2)
     criterion = nn.NLLLoss()
 
     for epoch in range(args.epochs):
