--- pytorch+++ mindspore@@ -1,9 +1,13 @@-import torch
-from torch.nn.utils.rnn import pad_sequence
-from torch.utils.data import DataLoader
-from torchtext.data.utils import get_tokenizer
-from torchtext.vocab import build_vocab_from_iterator
-from torchtext.datasets import Multi30k, multi30k
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# from torch.nn.utils.rnn import pad_sequence
+# from torch.utils.data import DataLoader
+# from torchtext.data.utils import get_tokenizer
+# from torchtext.vocab import build_vocab_from_iterator
+# from torchtext.datasets import Multi30k, multi30k
 
 # Turns an iterable into a generator
 def _yield_tokens(iterable_data, tokenizer, src):
@@ -33,12 +37,12 @@     }
 
     # Get training examples from torchtext (the multi30k dataset)
-    train_iterator = Multi30k(split="train", language_pair=(src_lang, tgt_lang))
-    valid_iterator = Multi30k(split="valid", language_pair=(src_lang, tgt_lang))
+    train_iterator = Multi30k(split="train", language_pair=(src_lang, tgt_lang))  # 'torchtext.datasets.Multi30k' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    valid_iterator = Multi30k(split="valid", language_pair=(src_lang, tgt_lang))  # 'torchtext.datasets.Multi30k' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     # Grab a tokenizer for these languages
-    src_tokenizer = get_tokenizer("spacy", src_lang)
-    tgt_tokenizer = get_tokenizer("spacy", tgt_lang)
+    src_tokenizer = get_tokenizer("spacy", src_lang)  # 'torchtext.data.utils.get_tokenizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    tgt_tokenizer = get_tokenizer("spacy", tgt_lang)  # 'torchtext.data.utils.get_tokenizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     # Build a vocabulary object for these languages
     src_vocab = build_vocab_from_iterator(
@@ -46,14 +50,14 @@         min_freq=1,
         specials=list(special_symbols.keys()),
         special_first=True
-    )
+    )  # 'torchtext.vocab.build_vocab_from_iterator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     tgt_vocab = build_vocab_from_iterator(
         _yield_tokens(train_iterator, tgt_tokenizer, False),
         min_freq=1,
         specials=list(special_symbols.keys()),
         special_first=True
-    )
+    )  # 'torchtext.vocab.build_vocab_from_iterator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     src_vocab.set_default_index(special_symbols["<unk>"])
     tgt_vocab.set_default_index(special_symbols["<unk>"])
@@ -68,10 +72,10 @@ 
     # Function to add BOS/EOS and create tensor for input sequence indices
     def _tensor_transform(token_ids):
-        return torch.cat(
-            (torch.tensor([special_symbols["<bos>"]]),
-             torch.tensor(token_ids),
-             torch.tensor([special_symbols["<eos>"]]))
+        return mint.cat(
+            (ms.Tensor([special_symbols["<bos>"]]),
+             ms.Tensor(token_ids),
+             ms.Tensor([special_symbols["<eos>"]]))
         )
 
     src_lang_transform = _seq_transform(src_tokenizer, src_vocab, _tensor_transform)
@@ -85,18 +89,18 @@             src_batch.append(src_lang_transform(src_sample.rstrip("\n")))
             tgt_batch.append(tgt_lang_transform(tgt_sample.rstrip("\n")))
 
-        src_batch = pad_sequence(src_batch, padding_value=special_symbols["<pad>"])
-        tgt_batch = pad_sequence(tgt_batch, padding_value=special_symbols["<pad>"])
+        src_batch = pad_sequence(src_batch, padding_value=special_symbols["<pad>"])  # 'torch.nn.utils.rnn.pad_sequence' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        tgt_batch = pad_sequence(tgt_batch, padding_value=special_symbols["<pad>"])  # 'torch.nn.utils.rnn.pad_sequence' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return src_batch, tgt_batch
 
     # Create the dataloader
-    train_dataloader = DataLoader(train_iterator, batch_size=opts.batch, collate_fn=_collate_fn)
-    valid_dataloader = DataLoader(valid_iterator, batch_size=opts.batch, collate_fn=_collate_fn)
+    train_dataloader = DataLoader(train_iterator, batch_size=opts.batch, collate_fn=_collate_fn)  # 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    valid_dataloader = DataLoader(valid_iterator, batch_size=opts.batch, collate_fn=_collate_fn)  # 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     return train_dataloader, valid_dataloader, src_vocab, tgt_vocab, src_lang_transform, tgt_lang_transform, special_symbols
 
 def generate_square_subsequent_mask(size, device):
-    mask = (torch.triu(torch.ones((size, size), device=device)) == 1).transpose(0, 1)
+    mask = (mint.triu(mint.ones((size, size), device=device)) == 1).transpose(0, 1)
     mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
     return mask
 
@@ -109,7 +113,7 @@ 
     # Generate the mask
     tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)
-    src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)
+    src_mask = mint.zeros((src_seq_len, src_seq_len),device=device).type(ms.bool_)
 
     # Overlay the mask over the original input
     src_padding_mask = (src == pad_idx).transpose(0, 1)
