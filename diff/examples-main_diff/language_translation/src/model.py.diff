--- pytorch+++ mindspore@@ -1,10 +1,12 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import math
+# from torch import nn
 
-import torch
-from torch.nn import functional as F
-from torch import nn
-
-class PositionalEncoding(nn.Module):
+class PositionalEncoding(msnn.Cell):
     def __init__(
         self,
         emb_size,
@@ -12,20 +14,20 @@         maxlen=5000
     ):
         super(PositionalEncoding, self).__init__()
-        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)
-        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
-        pos_embedding = torch.zeros((maxlen, emb_size))
-        pos_embedding[:, 0::2] = torch.sin(pos * den)
-        pos_embedding[:, 1::2] = torch.cos(pos * den)
+        den = mint.exp(- mint.arange(0, emb_size, 2)* math.log(10000) / emb_size)
+        pos = mint.arange(0, maxlen).reshape(maxlen, 1)
+        pos_embedding = mint.zeros((maxlen, emb_size))
+        pos_embedding[:, 0::2] = mint.sin(pos * den)
+        pos_embedding[:, 1::2] = mint.cos(pos * den)
         pos_embedding = pos_embedding.unsqueeze(-2)
 
         self.dropout = nn.Dropout(dropout)
         self.register_buffer('pos_embedding', pos_embedding)
 
-    def forward(self, token_embedding):
+    def construct(self, token_embedding):
         return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])
 
-class Translator(nn.Module):
+class Translator(msnn.Cell):
     def __init__(
             self,
             num_encoder_layers,
@@ -52,7 +54,7 @@             num_decoder_layers=num_decoder_layers,
             dim_feedforward=dim_feedforward,
             dropout=dropout
-        )
+        )  # 'torch.nn.Transformer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         self.ff = nn.Linear(embed_size, tgt_vocab_size)
 
@@ -61,9 +63,9 @@     def _init_weights(self):
         for p in self.parameters():
             if p.dim() > 1:
-                nn.init.xavier_uniform_(p)
+                nn.init.xavier_uniform_(p)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):
+    def construct(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):
 
         src_emb = self.pos_enc(self.src_embedding(src))
         tgt_emb = self.pos_enc(self.tgt_embedding(trg))
