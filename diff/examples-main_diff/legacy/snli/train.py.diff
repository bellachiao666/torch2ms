--- pytorch+++ mindspore@@ -1,44 +1,50 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import os
 import time
 import glob
 
-import torch
-import torch.optim as O
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
-from torchtext.legacy import data
-from torchtext.legacy import datasets
+# from torchtext.legacy import data
+# from torchtext.legacy import datasets
 
 from model import SNLIClassifier
 from util import get_args, makedirs
 
 
 args = get_args()
+# 'torch.cuda.is_available' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 if torch.cuda.is_available():
-    torch.cuda.set_device(args.gpu)
-    device = torch.device('cuda:{}'.format(args.gpu))
+    torch.cuda.set_device(args.gpu)  # 'torch.cuda.set_device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    device = torch.device('cuda:{}'.format(args.gpu))  # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch.backends.mps.is_available' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 elif torch.backends.mps.is_available():
-    device = torch.device('mps')
+    device = torch.device('mps')  # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 else:
-    device = torch.device('cpu')
+    device = torch.device('cpu')  # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-inputs = data.Field(lower=args.lower, tokenize='spacy')
-answers = data.Field(sequential=False)
+inputs = data.Field(lower=args.lower, tokenize='spacy')  # 'torchtext.legacy.data.Field' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+answers = data.Field(sequential=False)  # 'torchtext.legacy.data.Field' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-train, dev, test = datasets.SNLI.splits(inputs, answers)
+train, dev, test = datasets.SNLI.splits(inputs, answers)  # 'torchtext.legacy.datasets.SNLI.splits' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 inputs.build_vocab(train, dev, test)
 if args.word_vectors:
     if os.path.isfile(args.vector_cache):
-        inputs.vocab.vectors = torch.load(args.vector_cache)
+        inputs.vocab.vectors = torch.load(args.vector_cache)  # 'torch.load' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     else:
         inputs.vocab.load_vectors(args.word_vectors)
         makedirs(os.path.dirname(args.vector_cache))
-        torch.save(inputs.vocab.vectors, args.vector_cache)
+        torch.save(inputs.vocab.vectors, args.vector_cache)  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 answers.build_vocab(train)
 
 train_iter, dev_iter, test_iter = data.BucketIterator.splits(
-            (train, dev, test), batch_size=args.batch_size, device=device)
+            (train, dev, test), batch_size=args.batch_size, device=device)  # 'torchtext.legacy.data.BucketIterator.splits' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 config = args
 config.n_embed = len(inputs.vocab)
@@ -50,7 +56,7 @@     config.n_cells *= 2
 
 if args.resume_snapshot:
-    model = torch.load(args.resume_snapshot, map_location=device)
+    model = torch.load(args.resume_snapshot, map_location=device)  # 'torch.load' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 else:
     model = SNLIClassifier(config)
     if args.word_vectors:
@@ -58,7 +64,7 @@         model.to(device)
 
 criterion = nn.CrossEntropyLoss()
-opt = O.Adam(model.parameters(), lr=args.lr)
+opt = mint.optim.Adam(model.parameters(), lr=args.lr)
 
 iterations = 0
 start = time.time()
@@ -83,7 +89,7 @@         answer = model(batch)
 
         # calculate accuracy of predictions in the current batch
-        n_correct += (torch.max(answer, 1)[1].view(batch.label.size()) == batch.label).sum().item()
+        n_correct += (mint.max(answer, 1)[1].view(batch.label.size()) == batch.label).sum().item()
         n_total += batch.batch_size
         train_acc = 100. * n_correct/n_total
 
@@ -97,7 +103,7 @@         if iterations % args.save_every == 0:
             snapshot_prefix = os.path.join(args.save_path, 'snapshot')
             snapshot_path = snapshot_prefix + '_acc_{:.4f}_loss_{:.6f}_iter_{}_model.pt'.format(train_acc, loss.item(), iterations)
-            torch.save(model, snapshot_path)
+            torch.save(model, snapshot_path)  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             for f in glob.glob(snapshot_prefix + '*'):
                 if f != snapshot_path:
                     os.remove(f)
@@ -110,10 +116,11 @@ 
             # calculate accuracy on validation set
             n_dev_correct, dev_loss = 0, 0
+            # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             with torch.no_grad():
                 for dev_batch_idx, dev_batch in enumerate(dev_iter):
                      answer = model(dev_batch)
-                     n_dev_correct += (torch.max(answer, 1)[1].view(dev_batch.label.size()) == dev_batch.label).sum().item()
+                     n_dev_correct += (mint.max(answer, 1)[1].view(dev_batch.label.size()) == dev_batch.label).sum().item()
                      dev_loss = criterion(answer, dev_batch.label)
             dev_acc = 100. * n_dev_correct / len(dev)
 
@@ -131,7 +138,7 @@                 snapshot_path = snapshot_prefix + '_devacc_{}_devloss_{}__iter_{}_model.pt'.format(dev_acc, dev_loss.item(), iterations)
 
                 # save model, delete previous 'best_snapshot' files
-                torch.save(model, snapshot_path)
+                torch.save(model, snapshot_path)  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 for f in glob.glob(snapshot_prefix + '*'):
                     if f != snapshot_path:
                         os.remove(f)
