--- pytorch+++ mindspore@@ -1,14 +1,19 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 # This code is based on the implementation of Mohammad Pezeshki available at
 # https://github.com/mohammadpz/pytorch_forward_forward and licensed under the MIT License.
 # Modifications/Improvements to the original code have been made by Vivek V Patel.
 
 import argparse
-import torch
-import torch.nn as nn
-from torchvision.datasets import MNIST
-from torchvision.transforms import Compose, ToTensor, Normalize, Lambda
-from torch.utils.data import DataLoader
-from torch.optim import Adam
+# import torch
+# import torch.nn as nn
+# from torchvision.datasets import MNIST
+# from torchvision.transforms import Compose, ToTensor, Normalize, Lambda
+# from torch.utils.data import DataLoader
+# from torch.optim import Adam
 
 
 def get_y_neg(y):
@@ -16,8 +21,8 @@     for idx, y_samp in enumerate(y):
         allowed_indices = list(range(10))
         allowed_indices.remove(y_samp.item())
-        y_neg[idx] = torch.tensor(allowed_indices)[
-            torch.randint(len(allowed_indices), size=(1,))
+        y_neg[idx] = ms.Tensor(allowed_indices)[
+            mint.randint(len(allowed_indices), size=(1,))
         ].item()
     return y_neg.to(device)
 
@@ -29,7 +34,7 @@     return x_
 
 
-class Net(torch.nn.Module):
+class Net(msnn.Cell):
     def __init__(self, dims):
 
         super().__init__()
@@ -46,7 +51,7 @@                 h = layer(h)
                 goodness = goodness + [h.pow(2).mean(1)]
             goodness_per_label += [sum(goodness).unsqueeze(1)]
-        goodness_per_label = torch.cat(goodness_per_label, 1)
+        goodness_per_label = mint.cat(goodness_per_label, 1)
         return goodness_per_label.argmax(1)
 
     def train(self, x_pos, x_neg):
@@ -59,22 +64,22 @@ class Layer(nn.Linear):
     def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):
         super().__init__(in_features, out_features, bias, device, dtype)
-        self.relu = torch.nn.ReLU()
-        self.opt = Adam(self.parameters(), lr=args.lr)
+        self.relu = nn.ReLU()
+        self.opt = mint.optim.Adam(self.parameters(), lr = args.lr)
         self.threshold = args.threshold
         self.num_epochs = args.epochs
 
     def forward(self, x):
         x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)
-        return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))
+        return self.relu(mint.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))
 
     def train(self, x_pos, x_neg):
         for i in range(self.num_epochs):
             g_pos = self.forward(x_pos).pow(2).mean(1)
             g_neg = self.forward(x_neg).pow(2).mean(1)
-            loss = torch.log1p(
-                torch.exp(
-                    torch.cat([-g_pos + self.threshold, g_neg - self.threshold])
+            loss = mint.special.log1p(
+                mint.exp(
+                    mint.cat([-g_pos + self.threshold, g_neg - self.threshold])
                 )
             ).mean()
             self.opt.zero_grad()
@@ -132,11 +137,11 @@         help="how many batches to wait before logging training status",
     )
     args = parser.parse_args()
-    use_accel = not args.no_accel and torch.accelerator.is_available()
+    use_accel = not args.no_accel and torch.accelerator.is_available()  # 'torch.accelerator.is_available' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if use_accel:
-        device = torch.accelerator.current_accelerator()
+        device = torch.accelerator.current_accelerator()  # 'torch.accelerator.current_accelerator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     else:
-        device = torch.device("cpu")
+        device = torch.device("cpu")  # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     train_kwargs = {"batch_size": args.train_size}
     test_kwargs = {"batch_size": args.test_size}
@@ -146,19 +151,18 @@         train_kwargs.update(accel_kwargs)
         test_kwargs.update(accel_kwargs)
 
-    transform = Compose(
+    transform = ms.dataset.transforms.Compose(
         [
             ToTensor(),
             Normalize((0.1307,), (0.3081,)),
-            Lambda(lambda x: torch.flatten(x)),
-        ]
-    )
+            Lambda(lambda x: mint.flatten(x)),
+        ])  # 'torchvision.transforms.ToTensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torchvision.transforms.Normalize' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torchvision.transforms.Lambda' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     train_loader = DataLoader(
         MNIST("./data/", train=True, download=True, transform=transform), **train_kwargs
-    )
+    )  # 'torchvision.datasets.MNIST' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
     test_loader = DataLoader(
         MNIST("./data/", train=False, download=True, transform=transform), **test_kwargs
-    )
+    )  # 'torchvision.datasets.MNIST' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
     net = Net([784, 500, 500])
     x, y = next(iter(train_loader))
     x, y = x.to(device), y.to(device)
@@ -170,5 +174,5 @@     x_te, y_te = next(iter(test_loader))
     x_te, y_te = x_te.to(device), y_te.to(device)
     if args.save_model:
-        torch.save(net.state_dict(), "mnist_ff.pt")
+        torch.save(net.state_dict(), "mnist_ff.pt")  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     print("test error:", 1.0 - net.predict(x_te).eq(y_te).float().mean().item())
