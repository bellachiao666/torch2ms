--- pytorch+++ mindspore@@ -1,26 +1,30 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from __future__ import print_function
 
 import argparse
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-from torch.optim.lr_scheduler import StepLR
-from torchvision import datasets, transforms
+# import torch
+# import torch.nn as nn
+# import torch.optim as optim
+# from torch.optim.lr_scheduler import StepLR
+# from torchvision import datasets, transforms
 
 
-class Net(nn.Module):
+class Net(msnn.Cell):
     def __init__(self):
         super(Net, self).__init__()
-        self.rnn = nn.LSTM(input_size=28, hidden_size=64, batch_first=True)
+        self.rnn = nn.LSTM(input_size=28, hidden_size=64, batch_first=True)  # 'torch.nn.LSTM' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         self.batchnorm = nn.BatchNorm1d(64)
         self.dropout1 = nn.Dropout2d(0.25)
         self.dropout2 = nn.Dropout2d(0.5)
         self.fc1 = nn.Linear(64, 32)
         self.fc2 = nn.Linear(32, 10)
 
-    def forward(self, input):
+    def construct(self, input):
         # Shape of input is (batch_size,1, 28, 28)
         # converting shape of input to (batch_size, 28, 28)
         # as required by RNN when batch_first is set True
@@ -33,10 +37,10 @@         output = self.batchnorm(output)
         output = self.dropout1(output)
         output = self.fc1(output)
-        output = F.relu(output)
+        output = nn.functional.relu(output)
         output = self.dropout2(output)
         output = self.fc2(output)
-        output = F.log_softmax(output, dim=1)
+        output = mint.special.log_softmax(output, dim=1)
         return output
 
 
@@ -46,7 +50,7 @@         data, target = data.to(device), target.to(device)
         optimizer.zero_grad()
         output = model(data)
-        loss = F.nll_loss(output, target)
+        loss = nn.functional.nll_loss(output, target)
         loss.backward()
         optimizer.step()
         if batch_idx % args.log_interval == 0:
@@ -61,11 +65,12 @@     model.eval()
     test_loss = 0
     correct = 0
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     with torch.no_grad():
         for data, target in test_loader:
             data, target = data.to(device), target.to(device)
             output = model(data)
-            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
+            test_loss += ms.Tensor.item()  # sum up batch loss
             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
             correct += pred.eq(target.view_as(pred)).sum().item()
             if args.dry_run:
@@ -104,38 +109,38 @@     args = parser.parse_args()
 
     if args.accel:
-        device = torch.accelerator.current_accelerator()
+        device = torch.accelerator.current_accelerator()  # 'torch.accelerator.current_accelerator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     else:
-        device = torch.device("cpu")
+        device = torch.device("cpu")  # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-    torch.manual_seed(args.seed)
+    torch.manual_seed(args.seed)  # 'torch.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     kwargs = {'num_workers': 1, 'pin_memory': True} if args.accel else {}
     train_loader = torch.utils.data.DataLoader(
         datasets.MNIST('../data', train=True, download=True,
-                       transform=transforms.Compose([
+                       transform=ms.dataset.transforms.Compose([
                            transforms.ToTensor(),
                            transforms.Normalize((0.1307,), (0.3081,))
                        ])),
-        batch_size=args.batch_size, shuffle=True, **kwargs)
+        batch_size=args.batch_size, shuffle=True, **kwargs)  # 'torchvision.transforms.ToTensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torchvision.transforms.Normalize' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torchvision.datasets.MNIST' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
     test_loader = torch.utils.data.DataLoader(
-        datasets.MNIST('../data', train=False, transform=transforms.Compose([
+        datasets.MNIST('../data', train=False, transform=ms.dataset.transforms.Compose([
             transforms.ToTensor(),
             transforms.Normalize((0.1307,), (0.3081,))
         ])),
-        batch_size=args.test_batch_size, shuffle=True, **kwargs)
+        batch_size=args.test_batch_size, shuffle=True, **kwargs)  # 'torchvision.transforms.ToTensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torchvision.transforms.Normalize' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torchvision.datasets.MNIST' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     model = Net().to(device)
-    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)
+    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)  # 'torch.optim.Adadelta' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
+    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)  # 'torch.optim.lr_scheduler.StepLR' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     for epoch in range(1, args.epochs + 1):
         train(args, model, device, train_loader, optimizer, epoch)
         test(args, model, device, test_loader)
         scheduler.step()
 
     if args.save_model:
-        torch.save(model.state_dict(), "mnist_rnn.pt")
+        torch.save(model.state_dict(), "mnist_rnn.pt")  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 if __name__ == '__main__':
