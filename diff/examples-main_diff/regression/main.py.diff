--- pytorch+++ mindspore@@ -1,19 +1,21 @@ #!/usr/bin/env python
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from __future__ import print_function
 from itertools import count
 
-import torch
-import torch.nn.functional as F
-
 POLY_DEGREE = 4
-W_target = torch.randn(POLY_DEGREE, 1) * 5
-b_target = torch.randn(1) * 5
+W_target = mint.randn(POLY_DEGREE, 1) * 5
+b_target = mint.randn(1) * 5
 
 
 def make_features(x):
     """Builds features i.e. a matrix with columns [x, x^2, x^3, x^4]."""
     x = x.unsqueeze(1)
-    return torch.cat([x ** i for i in range(1, POLY_DEGREE+1)], 1)
+    return mint.cat([x ** i for i in range(1, POLY_DEGREE+1)], 1)
 
 
 def f(x):
@@ -32,14 +34,14 @@ 
 def get_batch(batch_size=32):
     """Builds a batch i.e. (x, f(x)) pair."""
-    random = torch.randn(batch_size)
+    random = mint.randn(batch_size)
     x = make_features(random)
     y = f(x)
     return x, y
 
 
 # Define model
-fc = torch.nn.Linear(W_target.size(0), 1)
+fc = nn.Linear(W_target.size(0), 1)
 
 for batch_idx in count(1):
     # Get data
@@ -49,7 +51,7 @@     fc.zero_grad()
 
     # Forward pass
-    output = F.smooth_l1_loss(fc(batch_x), batch_y)
+    output = nn.functional.smooth_l1_loss(fc(batch_x), batch_y)
     loss = output.item()
 
     # Backward pass
