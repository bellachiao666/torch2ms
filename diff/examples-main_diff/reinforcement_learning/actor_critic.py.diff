--- pytorch+++ mindspore@@ -1,14 +1,18 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import argparse
 import gymnasium as gym
 import numpy as np
 from itertools import count
 from collections import namedtuple
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-from torch.distributions import Categorical
+# import torch
+# import torch.nn as nn
+# import torch.optim as optim
+# from torch.distributions import Categorical
 
 # Cart Pole
 
@@ -27,13 +31,13 @@ render_mode = "human" if args.render else None
 env = gym.make('CartPole-v1', render_mode=render_mode)
 env.reset(seed=args.seed)
-torch.manual_seed(args.seed)
+torch.manual_seed(args.seed)  # 'torch.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])
 
 
-class Policy(nn.Module):
+class Policy(msnn.Cell):
     """
     implements both actor and critic in one model
     """
@@ -51,15 +55,15 @@         self.saved_actions = []
         self.rewards = []
 
-    def forward(self, x):
+    def construct(self, x):
         """
         forward of both actor and critic
         """
-        x = F.relu(self.affine1(x))
+        x = nn.functional.relu(self.affine1(x))
 
         # actor: choses action to take from state s_t
         # by returning probability of each action
-        action_prob = F.softmax(self.action_head(x), dim=-1)
+        action_prob = nn.functional.softmax(self.action_head(x), dim = -1)
 
         # critic: evaluates being in the state s_t
         state_values = self.value_head(x)
@@ -71,16 +75,16 @@ 
 
 model = Policy()
-optimizer = optim.Adam(model.parameters(), lr=3e-2)
+optimizer = mint.optim.Adam(model.parameters(), lr=3e-2)
 eps = np.finfo(np.float32).eps.item()
 
 
 def select_action(state):
-    state = torch.from_numpy(state).float()
+    state = ms.float32()  # 'torch.from_numpy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     probs, state_value = model(state)
 
     # create a categorical distribution over the list of probabilities of actions
-    m = Categorical(probs)
+    m = Categorical(probs)  # 'torch.distributions.Categorical' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     # and sample an action using the distribution
     action = m.sample()
@@ -108,7 +112,7 @@         R = r + args.gamma * R
         returns.insert(0, R)
 
-    returns = torch.tensor(returns)
+    returns = ms.Tensor(returns)
     returns = (returns - returns.mean()) / (returns.std() + eps)
 
     for (log_prob, value), R in zip(saved_actions, returns):
@@ -118,13 +122,13 @@         policy_losses.append(-log_prob * advantage)
 
         # calculate critic (value) loss using L1 smooth loss
-        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))
+        value_losses.append(nn.functional.smooth_l1_loss(value, ms.Tensor([R])))
 
     # reset gradients
     optimizer.zero_grad()
 
     # sum up all the values of policy_losses and value_losses
-    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()
+    loss = mint.stack(policy_losses).sum() + mint.stack(value_losses).sum()
 
     # perform backprop
     loss.backward()
