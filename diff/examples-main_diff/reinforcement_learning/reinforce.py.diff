--- pytorch+++ mindspore@@ -1,13 +1,17 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import argparse
 import gymnasium as gym
 import numpy as np
 from itertools import count
 from collections import deque
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import torch.optim as optim
-from torch.distributions import Categorical
+# import torch
+# import torch.nn as nn
+# import torch.optim as optim
+# from torch.distributions import Categorical
 
 
 parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')
@@ -25,36 +29,36 @@ render_mode = "human" if args.render else None
 env = gym.make('CartPole-v1', render_mode=render_mode)
 env.reset(seed=args.seed)
-torch.manual_seed(args.seed)
+torch.manual_seed(args.seed)  # 'torch.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
-class Policy(nn.Module):
+class Policy(msnn.Cell):
     def __init__(self):
         super(Policy, self).__init__()
         self.affine1 = nn.Linear(4, 128)
-        self.dropout = nn.Dropout(p=0.6)
+        self.dropout = nn.Dropout(p = 0.6)
         self.affine2 = nn.Linear(128, 2)
 
         self.saved_log_probs = []
         self.rewards = []
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.affine1(x)
         x = self.dropout(x)
-        x = F.relu(x)
+        x = nn.functional.relu(x)
         action_scores = self.affine2(x)
-        return F.softmax(action_scores, dim=1)
+        return nn.functional.softmax(action_scores, dim = 1)
 
 
 policy = Policy()
-optimizer = optim.Adam(policy.parameters(), lr=1e-2)
+optimizer = mint.optim.Adam(policy.parameters(), lr=1e-2)
 eps = np.finfo(np.float32).eps.item()
 
 
 def select_action(state):
-    state = torch.from_numpy(state).float().unsqueeze(0)
+    state = ms.float32().unsqueeze(0)  # 'torch.from_numpy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     probs = policy(state)
-    m = Categorical(probs)
+    m = Categorical(probs)  # 'torch.distributions.Categorical' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     action = m.sample()
     policy.saved_log_probs.append(m.log_prob(action))
     return action.item()
@@ -67,12 +71,12 @@     for r in policy.rewards[::-1]:
         R = r + args.gamma * R
         returns.appendleft(R)
-    returns = torch.tensor(returns)
+    returns = ms.Tensor(returns)
     returns = (returns - returns.mean()) / (returns.std() + eps)
     for log_prob, R in zip(policy.saved_log_probs, returns):
         policy_loss.append(-log_prob * R)
     optimizer.zero_grad()
-    policy_loss = torch.cat(policy_loss).sum()
+    policy_loss = mint.cat(policy_loss).sum()
     policy_loss.backward()
     optimizer.step()
     del policy.rewards[:]
