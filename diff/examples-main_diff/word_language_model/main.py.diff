--- pytorch+++ mindspore@@ -1,11 +1,15 @@ # coding: utf-8
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import argparse
 import time
 import math
 import os
-import torch
-import torch.nn as nn
-import torch.onnx
+# import torch
+# import torch.nn as nn
 
 import data
 from model import PositionalEncoding, RNNModel, TransformerModel
@@ -54,13 +58,14 @@ args = parser.parse_args()
 
 # Set the random seed manually for reproducibility.
-torch.manual_seed(args.seed)
-
+torch.manual_seed(args.seed)  # 'torch.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+# 'torch.accelerator.is_available' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 if args.accel and torch.accelerator.is_available():
-    device = torch.accelerator.current_accelerator()
+    device = torch.accelerator.current_accelerator()  # 'torch.accelerator.current_accelerator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 else:
-    device = torch.device("cpu")
+    device = torch.device("cpu")  # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 print("Using device:", device)
 
@@ -108,7 +113,7 @@ 
 criterion = nn.NLLLoss()
 if args.use_optimizer:
-    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)
+    optimizer = mint.optim.AdamW(model.parameters(), lr=args.lr)
 
 ###############################################################################
 # Training code
@@ -117,7 +122,7 @@ def repackage_hidden(h):
     """Wraps hidden states in new Tensors, to detach them from their history."""
 
-    if isinstance(h, torch.Tensor):
+    if isinstance(h, ms.Tensor):
         return h.detach()
     else:
         return tuple(repackage_hidden(v) for v in h)
@@ -147,6 +152,7 @@     ntokens = len(corpus.dictionary)
     if args.model != 'Transformer':
         hidden = model.init_hidden(eval_batch_size)
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     with torch.no_grad():
         for i in range(0, data_source.size(0) - 1, args.bptt):
             data, targets = get_batch(data_source, i)
@@ -186,7 +192,7 @@         loss.backward()
 
         # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
-        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)
+        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)  # 'torch.nn.utils.clip_grad_norm_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if args.use_optimizer:
             optimizer.step()
         else:
@@ -211,9 +217,9 @@ def export_onnx(path, batch_size, seq_len):
     print('The model is also exported in ONNX format at {}.'.format(os.path.realpath(args.onnx_export)))
     model.eval()
-    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)
+    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)  # 'torch.LongTensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.LongTensor.zero_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.LongTensor.zero_.view' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.LongTensor.zero_.view.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     hidden = model.init_hidden(batch_size)
-    torch.onnx.export(model, (dummy_input, hidden), path)
+    ms.jit(model, (dummy_input, hidden), path)
 
 
 # Loop over epochs.
@@ -234,7 +240,7 @@         # Save the model if the validation loss is the best we've seen so far.
         if not best_val_loss or val_loss < best_val_loss:
             with open(args.save, 'wb') as f:
-                torch.save(model, f)
+                torch.save(model, f)  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             best_val_loss = val_loss
         else:
             # Anneal the learning rate if no improvement has been seen in the validation dataset.
@@ -249,29 +255,30 @@         safe_globals = [
             PositionalEncoding,
             TransformerModel,
-            torch.nn.functional.relu,
+            nn.functional.relu,
             torch.nn.modules.activation.MultiheadAttention,
-            torch.nn.modules.container.ModuleList,
-            torch.nn.modules.dropout.Dropout,
-            torch.nn.modules.linear.Linear,
-            torch.nn.modules.linear.NonDynamicallyQuantizableLinear,
-            torch.nn.modules.normalization.LayerNorm,
-            torch.nn.modules.sparse.Embedding,
+            msnn.CellList,
+            nn.Dropout,
+            nn.Linear,
+            nn.functional.linear.NonDynamicallyQuantizableLinear,
+            nn.LayerNorm,
+            nn.Embedding,
             torch.nn.modules.transformer.TransformerEncoder,
             torch.nn.modules.transformer.TransformerEncoderLayer,
         ]
     else:
         safe_globals = [
             RNNModel,
-            torch.nn.modules.dropout.Dropout,
-            torch.nn.modules.linear.Linear,
+            nn.Dropout,
+            nn.Linear,
             torch.nn.modules.rnn.GRU,
             torch.nn.modules.rnn.LSTM,
             torch.nn.modules.rnn.RNN,
-            torch.nn.modules.sparse.Embedding,
+            nn.Embedding,
         ]
+    # 'torch.serialization.safe_globals' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     with torch.serialization.safe_globals(safe_globals):
-        model = torch.load(f)
+        model = torch.load(f)  # 'torch.load' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     # after load the rnn params are not a continuous chunk of memory
     # this makes them a continuous chunk, and will speed up forward pass
     # Currently, only rnn model supports flatten_parameters function.
