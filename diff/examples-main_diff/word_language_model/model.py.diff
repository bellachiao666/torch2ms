--- pytorch+++ mindspore@@ -1,9 +1,12 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import math
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch.nn as nn
 
-class RNNModel(nn.Module):
+class RNNModel(msnn.Cell):
     """Container module with an encoder, a recurrent module, and a decoder."""
 
     def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):
@@ -19,7 +22,7 @@             except KeyError as e:
                 raise ValueError( """An invalid option for `--model` was supplied,
                                  options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']""") from e
-            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)
+            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)  # 'torch.nn.RNN' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         self.decoder = nn.Linear(nhid, ntoken)
 
         # Optionally tie weights as in:
@@ -41,17 +44,17 @@ 
     def init_weights(self):
         initrange = 0.1
-        nn.init.uniform_(self.encoder.weight, -initrange, initrange)
-        nn.init.zeros_(self.decoder.bias)
-        nn.init.uniform_(self.decoder.weight, -initrange, initrange)
+        nn.init.uniform_(self.encoder.weight, -initrange, initrange)  # 'torch.nn.init.uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.zeros_(self.decoder.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.uniform_(self.decoder.weight, -initrange, initrange)  # 'torch.nn.init.uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-    def forward(self, input, hidden):
+    def construct(self, input, hidden):
         emb = self.drop(self.encoder(input))
         output, hidden = self.rnn(emb, hidden)
         output = self.drop(output)
         decoded = self.decoder(output)
         decoded = decoded.view(-1, self.ntoken)
-        return F.log_softmax(decoded, dim=1), hidden
+        return mint.special.log_softmax(decoded, dim=1), hidden
 
     def init_hidden(self, bsz):
         weight = next(self.parameters())
@@ -62,7 +65,7 @@             return weight.new_zeros(self.nlayers, bsz, self.nhid)
 
 # Temporarily leave PositionalEncoding module here. Will be moved somewhere else.
-class PositionalEncoding(nn.Module):
+class PositionalEncoding(msnn.Cell):
     r"""Inject some information about the relative or absolute position of the tokens in the sequence.
         The positional encodings have the same dimension as the embeddings, so that the two can be summed.
         Here, we use sine and cosine functions of different frequencies.
@@ -80,17 +83,17 @@ 
     def __init__(self, d_model, dropout=0.1, max_len=5000):
         super(PositionalEncoding, self).__init__()
-        self.dropout = nn.Dropout(p=dropout)
+        self.dropout = nn.Dropout(p = dropout)
 
-        pe = torch.zeros(max_len, d_model)
-        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
-        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
-        pe[:, 0::2] = torch.sin(position * div_term)
-        pe[:, 1::2] = torch.cos(position * div_term)
+        pe = mint.zeros(max_len, d_model)
+        position = mint.arange(0, max_len, dtype=ms.float32).unsqueeze(1)
+        div_term = mint.exp(mint.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
+        pe[:, 0::2] = mint.sin(position * div_term)
+        pe[:, 1::2] = mint.cos(position * div_term)
         pe = pe.unsqueeze(0).transpose(0, 1)
         self.register_buffer('pe', pe)
 
-    def forward(self, x):
+    def construct(self, x):
         r"""Inputs of forward function
         Args:
             x: the sequence fed to the positional encoder model (required).
@@ -104,6 +107,7 @@         x = x + self.pe[:x.size(0), :]
         return self.dropout(x)
 
+# 'torch.nn.Transformer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class TransformerModel(nn.Transformer):
     """Container module with an encoder, a recurrent or transformer module, and a decoder."""
 
@@ -120,13 +124,13 @@         self.init_weights()
 
     def _generate_square_subsequent_mask(self, sz):
-        return torch.log(torch.tril(torch.ones(sz,sz)))
+        return mint.log(mint.tril(mint.ones(sz,sz)))
 
     def init_weights(self):
         initrange = 0.1
-        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)
-        nn.init.zeros_(self.decoder.bias)
-        nn.init.uniform_(self.decoder.weight, -initrange, initrange)
+        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)  # 'torch.nn.init.uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.zeros_(self.decoder.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.uniform_(self.decoder.weight, -initrange, initrange)  # 'torch.nn.init.uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def forward(self, src, has_mask=True):
         if has_mask:
@@ -141,4 +145,4 @@         src = self.pos_encoder(src)
         output = self.encoder(src, mask=self.src_mask)
         output = self.decoder(output)
-        return F.log_softmax(output, dim=-1)
+        return mint.special.log_softmax(output, dim=-1)
