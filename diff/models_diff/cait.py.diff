--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Class-Attention in Image Transformers (CaiT)
 
 Paper: 'Going deeper with Image Transformers' - https://arxiv.org/abs/2103.17239
@@ -11,8 +16,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Type, Any
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, use_fused_attn
@@ -24,7 +29,7 @@ __all__ = ['Cait', 'ClassAttn', 'LayerScaleBlockClassAttn', 'LayerScaleBlock', 'TalkingHeadAttn']
 
 
-class ClassAttn(nn.Module):
+class ClassAttn(msnn.Cell):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to do CA
     fused_attn: torch.jit.Final[bool]
@@ -53,7 +58,7 @@         self.proj = nn.Linear(dim, dim, **dd)
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x):
+    def construct(self, x):
         B, N, C = x.shape
         q = self.q(x[:, 0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
         k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
@@ -63,7 +68,7 @@             x_cls = torch.nn.functional.scaled_dot_product_attention(
                 q, k, v,
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
@@ -78,7 +83,7 @@         return x_cls
 
 
-class LayerScaleBlockClassAttn(nn.Module):
+class LayerScaleBlockClassAttn(msnn.Cell):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to add CA and LayerScale
     def __init__(
@@ -90,10 +95,10 @@             proj_drop: float = 0.,
             attn_drop: float = 0.,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
-            attn_block: Type[nn.Module] = ClassAttn,
-            mlp_block: Type[nn.Module] = Mlp,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
+            attn_block: Type[msnn.Cell] = ClassAttn,
+            mlp_block: Type[msnn.Cell] = Mlp,
             init_values: float = 1e-4,
             device=None,
             dtype=None,
@@ -109,7 +114,7 @@             proj_drop=proj_drop,
             **dd,
         )
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         self.norm2 = norm_layer(dim, **dd)
         mlp_hidden_dim = int(dim * mlp_ratio)
         self.mlp = mlp_block(
@@ -119,17 +124,17 @@             drop=proj_drop,
             **dd,
         )
-        self.gamma_1 = nn.Parameter(init_values * torch.ones(dim, **dd))
-        self.gamma_2 = nn.Parameter(init_values * torch.ones(dim, **dd))
-
-    def forward(self, x, x_cls):
-        u = torch.cat((x_cls, x), dim=1)
+        self.gamma_1 = ms.Parameter(init_values * mint.ones(dim, **dd))
+        self.gamma_2 = ms.Parameter(init_values * mint.ones(dim, **dd))
+
+    def construct(self, x, x_cls):
+        u = mint.cat((x_cls, x), dim=1)
         x_cls = x_cls + self.drop_path(self.gamma_1 * self.attn(self.norm1(u)))
         x_cls = x_cls + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x_cls)))
         return x_cls
 
 
-class TalkingHeadAttn(nn.Module):
+class TalkingHeadAttn(msnn.Cell):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to add Talking Heads Attention (https://arxiv.org/pdf/2003.02436v1.pdf)
     def __init__(
@@ -161,7 +166,7 @@ 
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x):
+    def construct(self, x):
         B, N, C = x.shape
         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
         q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]
@@ -181,7 +186,7 @@         return x
 
 
-class LayerScaleBlock(nn.Module):
+class LayerScaleBlock(msnn.Cell):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to add layerScale
     def __init__(
@@ -193,10 +198,10 @@             proj_drop: float = 0.,
             attn_drop: float = 0.,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
-            attn_block: Type[nn.Module] = TalkingHeadAttn,
-            mlp_block: Type[nn.Module] = Mlp,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
+            attn_block: Type[msnn.Cell] = TalkingHeadAttn,
+            mlp_block: Type[msnn.Cell] = Mlp,
             init_values: float = 1e-4,
             device=None,
             dtype=None,
@@ -212,7 +217,7 @@             proj_drop=proj_drop,
             **dd,
         )
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         self.norm2 = norm_layer(dim, **dd)
         mlp_hidden_dim = int(dim * mlp_ratio)
         self.mlp = mlp_block(
@@ -222,16 +227,16 @@             drop=proj_drop,
             **dd,
         )
-        self.gamma_1 = nn.Parameter(init_values * torch.ones(dim, **dd))
-        self.gamma_2 = nn.Parameter(init_values * torch.ones(dim, **dd))
-
-    def forward(self, x):
+        self.gamma_1 = ms.Parameter(init_values * mint.ones(dim, **dd))
+        self.gamma_2 = ms.Parameter(init_values * mint.ones(dim, **dd))
+
+    def construct(self, x):
         x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))
         x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
         return x
 
 
-class Cait(nn.Module):
+class Cait(msnn.Cell):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to adapt to our cait models
     def __init__(
@@ -251,16 +256,16 @@             proj_drop_rate: float = 0.,
             attn_drop_rate: float = 0.,
             drop_path_rate: float = 0.,
-            block_layers: Type[nn.Module] = LayerScaleBlock,
-            block_layers_token: Type[nn.Module] = LayerScaleBlockClassAttn,
-            patch_layer: Type[nn.Module] = PatchEmbed,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
-            attn_block: Type[nn.Module] = TalkingHeadAttn,
-            mlp_block: Type[nn.Module] = Mlp,
+            block_layers: Type[msnn.Cell] = LayerScaleBlock,
+            block_layers_token: Type[msnn.Cell] = LayerScaleBlockClassAttn,
+            patch_layer: Type[msnn.Cell] = PatchEmbed,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            attn_block: Type[msnn.Cell] = TalkingHeadAttn,
+            mlp_block: Type[msnn.Cell] = Mlp,
             init_values: float = 1e-4,
-            attn_block_token_only: Type[nn.Module] = ClassAttn,
-            mlp_block_token_only: Type[nn.Module] = Mlp,
+            attn_block_token_only: Type[msnn.Cell] = ClassAttn,
+            mlp_block_token_only: Type[msnn.Cell] = Mlp,
             depth_token_only: int = 2,
             mlp_ratio_token_only: float = 4.0,
             device=None,
@@ -285,12 +290,12 @@         num_patches = self.patch_embed.num_patches
         r = self.patch_embed.feat_ratio() if hasattr(self.patch_embed, 'feat_ratio') else patch_size
 
-        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim, **dd))
-        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim, **dd))
-        self.pos_drop = nn.Dropout(p=pos_drop_rate)
+        self.cls_token = ms.Parameter(mint.zeros(1, 1, embed_dim, **dd))
+        self.pos_embed = ms.Parameter(mint.zeros(1, num_patches, embed_dim, **dd))
+        self.pos_drop = nn.Dropout(p = pos_drop_rate)
 
         dpr = [drop_path_rate for i in range(depth)]
-        self.blocks = nn.Sequential(*[block_layers(
+        self.blocks = msnn.SequentialCell(*[block_layers(
             dim=embed_dim,
             num_heads=num_heads,
             mlp_ratio=mlp_ratio,
@@ -307,7 +312,7 @@         ) for i in range(depth)])
         self.feature_info = [dict(num_chs=embed_dim, reduction=r, module=f'blocks.{i}') for i in range(depth)]
 
-        self.blocks_token_only = nn.ModuleList([block_layers_token(
+        self.blocks_token_only = msnn.CellList([block_layers_token(
             dim=embed_dim,
             num_heads=num_heads,
             mlp_ratio=mlp_ratio_token_only,
@@ -323,7 +328,7 @@         self.norm = norm_layer(embed_dim, **dd)
 
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(embed_dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(embed_dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()
 
         trunc_normal_(self.pos_embed, std=.02)
         trunc_normal_(self.cls_token, std=.02)
@@ -333,10 +338,10 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif isinstance(m, nn.LayerNorm):
-            nn.init.constant_(m.bias, 0)
-            nn.init.constant_(m.weight, 1.0)
+            nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(m.weight, 1.0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def no_weight_decay(self):
@@ -364,7 +369,7 @@         return _matcher
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -372,17 +377,17 @@         if global_pool is not None:
             assert global_pool in ('', 'token', 'avg')
             self.global_pool = global_pool
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -430,7 +435,7 @@         cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
         for i, blk in enumerate(self.blocks_token_only):
             cls_tokens = blk(x, cls_tokens)
-        x = torch.cat((cls_tokens, x), dim=1)
+        x = mint.cat((cls_tokens, x), dim=1)
         x = self.norm(x)
 
         return x, intermediates
@@ -446,9 +451,9 @@         take_indices, max_index = feature_take_indices(len(self.blocks), indices)
         self.blocks = self.blocks[:max_index + 1]  # truncate blocks
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
-            self.blocks_token_only = nn.ModuleList()  # prune token blocks with head
+            self.blocks_token_only = msnn.CellList()  # prune token blocks with head
             self.reset_classifier(0, '')
         return take_indices
 
@@ -463,7 +468,7 @@         cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
         for i, blk in enumerate(self.blocks_token_only):
             cls_tokens = blk(x, cls_tokens)
-        x = torch.cat((cls_tokens, x), dim=1)
+        x = mint.cat((cls_tokens, x), dim=1)
         x = self.norm(x)
         return x
 
@@ -473,7 +478,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
