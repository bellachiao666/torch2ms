--- pytorch+++ mindspore@@ -1,10 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ ConvMixer
 
 """
 from typing import Optional, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectAdaptivePool2d
@@ -15,16 +20,16 @@ __all__ = ['ConvMixer']
 
 
-class Residual(nn.Module):
-    def __init__(self, fn: nn.Module):
+class Residual(msnn.Cell):
+    def __init__(self, fn: msnn.Cell):
         super().__init__()
         self.fn = fn
 
-    def forward(self, x):
+    def construct(self, x):
         return self.fn(x) + x
 
 
-class ConvMixer(nn.Module):
+class ConvMixer(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -35,7 +40,7 @@             num_classes: int = 1000,
             global_pool: str = 'avg',
             drop_rate: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
             **kwargs,
@@ -46,14 +51,14 @@         self.num_features = self.head_hidden_size = dim
         self.grad_checkpointing = False
 
-        self.stem = nn.Sequential(
+        self.stem = msnn.SequentialCell(
             nn.Conv2d(in_chans, dim, kernel_size=patch_size, stride=patch_size, **dd),
             act_layer(),
             nn.BatchNorm2d(dim, **dd)
         )
-        self.blocks = nn.Sequential(
-            *[nn.Sequential(
-                    Residual(nn.Sequential(
+        self.blocks = msnn.SequentialCell(
+            *[msnn.SequentialCell(
+                    Residual(msnn.SequentialCell(
                         nn.Conv2d(dim, dim, kernel_size, groups=dim, padding="same", **dd),
                         act_layer(),
                         nn.BatchNorm2d(dim, **dd)
@@ -65,7 +70,7 @@         )
         self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -77,14 +82,14 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
         self.num_classes = num_classes
         if global_pool is not None:
             self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
 
     def forward_features(self, x):
         x = self.stem(x)
@@ -99,7 +104,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
