--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ DeiT - Data-efficient Image Transformers
 
 DeiT model defs and weights from https://github.com/facebookresearch/deit, original copyright below
@@ -13,8 +18,8 @@ from functools import partial
 from typing import Optional, Type
 
-import torch
-from torch import nn as nn
+# import torch
+# from torch import nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import resample_abs_pos_embed
@@ -39,10 +44,10 @@         dd = {'device': kwargs.get('device', None), 'dtype': kwargs.get('dtype', None)}
 
         self.num_prefix_tokens = 2
-        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim, **dd))
-        self.pos_embed = nn.Parameter(
-            torch.zeros(1, self.patch_embed.num_patches + self.num_prefix_tokens, self.embed_dim, **dd))
-        self.head_dist = nn.Linear(self.embed_dim, self.num_classes, **dd) if self.num_classes > 0 else nn.Identity()
+        self.dist_token = ms.Parameter(mint.zeros(1, 1, self.embed_dim, **dd))
+        self.pos_embed = ms.Parameter(
+            mint.zeros(1, self.patch_embed.num_patches + self.num_prefix_tokens, self.embed_dim, **dd))
+        self.head_dist = nn.Linear(self.embed_dim, self.num_classes, **dd) if self.num_classes > 0 else msnn.Identity()
         self.distilled_training = False  # must set this True to train w/ distillation token
 
         self.init_weights(weight_init)
@@ -61,13 +66,13 @@         )
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head, self.head_dist
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
         self.num_classes = num_classes
-        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
-        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else msnn.Identity()
+        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else msnn.Identity()
 
     @torch.jit.ignore
     def set_distilled_training(self, enable=True):
@@ -90,7 +95,7 @@             # deit-3, updated JAX (big vision)
             # position embedding does not overlap with class token, add then concat
             x = x + pos_embed
-            x = torch.cat((
+            x = mint.cat((
                 self.cls_token.expand(x.shape[0], -1, -1),
                 self.dist_token.expand(x.shape[0], -1, -1),
                 x),
@@ -98,7 +103,7 @@         else:
             # original timm, JAX, and deit vit impl
             # pos_embed has entry for class token, concat then add
-            x = torch.cat((
+            x = mint.cat((
                 self.cls_token.expand(x.shape[0], -1, -1),
                 self.dist_token.expand(x.shape[0], -1, -1),
                 x),
@@ -106,7 +111,7 @@             x = x + pos_embed
         return self.pos_drop(x)
 
-    def forward_head(self, x, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x, pre_logits: bool = False) -> ms.Tensor:
         x, x_dist = x[:, 0], x[:, 1]
         if pre_logits:
             return (x + x_dist) / 2
