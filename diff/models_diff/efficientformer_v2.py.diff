--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ EfficientFormer-V2
 
 @article{
@@ -18,8 +23,8 @@ from functools import partial
 from typing import Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -66,7 +71,7 @@ }
 
 
-class ConvNorm(nn.Module):
+class ConvNorm(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -98,14 +103,14 @@         )
         self.bn = create_norm_layer(norm_layer, out_channels, **norm_kwargs, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv(x)
         x = self.bn(x)
         return x
 
 
-class Attention2d(torch.nn.Module):
-    attention_bias_cache: Dict[str, torch.Tensor]
+class Attention2d(msnn.Cell):
+    attention_bias_cache: Dict[str, ms.Tensor]
 
     def __init__(
             self,
@@ -114,7 +119,7 @@             num_heads: int = 8,
             attn_ratio: int = 4,
             resolution: Union[int, Tuple[int, int]] = 7,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             stride: Optional[int] = None,
             device=None,
             dtype=None,
@@ -129,7 +134,7 @@         if stride is not None:
             resolution = tuple([math.ceil(r / stride) for r in resolution])
             self.stride_conv = ConvNorm(dim, dim, kernel_size=3, stride=stride, groups=dim, **dd)
-            self.upsample = nn.Upsample(scale_factor=stride, mode='bilinear')
+            self.upsample = nn.Upsample(scale_factor = stride, mode = 'bilinear')
         else:
             self.stride_conv = None
             self.upsample = None
@@ -151,23 +156,24 @@         self.act = act_layer()
         self.proj = ConvNorm(self.dh, dim, 1, **dd)
 
-        pos = torch.stack(ndgrid(
-            torch.arange(self.resolution[0], device=device, dtype=torch.long),
-            torch.arange(self.resolution[1], device=device, dtype=torch.long),
+        pos = mint.stack(ndgrid(
+            mint.arange(self.resolution[0], device=device, dtype=ms.int64),
+            mint.arange(self.resolution[1], device=device, dtype=ms.int64),
         )).flatten(1)
         rel_pos = (pos[..., :, None] - pos[..., None, :]).abs()
         rel_pos = (rel_pos[0] * self.resolution[1]) + rel_pos[1]
-        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, self.N, **dd))
+        self.attention_biases = ms.Parameter(mint.zeros(num_heads, self.N, **dd))
         self.register_buffer('attention_bias_idxs', rel_pos, persistent=False)
         self.attention_bias_cache = {}  # per-device attention_biases cache (data-parallel compat)
 
-    @torch.no_grad()
     def train(self, mode=True):
         super().train(mode)
         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def get_attention_biases(self, device: torch.device) -> ms.Tensor:
         if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
@@ -176,7 +182,7 @@                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
-    def forward(self, x):
+    def construct(self, x):
         B, C, H, W = x.shape
         if self.stride_conv is not None:
             x = self.stride_conv(x)
@@ -203,7 +209,7 @@         return x
 
 
-class LocalGlobalQuery(torch.nn.Module):
+class LocalGlobalQuery(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
@@ -217,7 +223,7 @@         self.local = nn.Conv2d(in_dim, in_dim, kernel_size=3, stride=2, padding=1, groups=in_dim, **dd)
         self.proj = ConvNorm(in_dim, out_dim, 1, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         local_q = self.local(x)
         pool_q = self.pool(x)
         q = local_q + pool_q
@@ -225,8 +231,8 @@         return q
 
 
-class Attention2dDownsample(torch.nn.Module):
-    attention_bias_cache: Dict[str, torch.Tensor]
+class Attention2dDownsample(msnn.Cell):
+    attention_bias_cache: Dict[str, ms.Tensor]
 
     def __init__(
             self,
@@ -236,7 +242,7 @@             attn_ratio: int = 4,
             resolution: Union[int, Tuple[int, int]] = 7,
             out_dim: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
     ):
@@ -265,27 +271,28 @@         self.act = act_layer()
         self.proj = ConvNorm(self.dh, self.out_dim, 1, **dd)
 
-        self.attention_biases = nn.Parameter(torch.zeros(num_heads, self.N, **dd))
-        k_pos = torch.stack(ndgrid(
-            torch.arange(self.resolution[0], device=device, dtype=torch.long),
-            torch.arange(self.resolution[1], device=device, dtype=torch.long),
+        self.attention_biases = ms.Parameter(mint.zeros(num_heads, self.N, **dd))
+        k_pos = mint.stack(ndgrid(
+            mint.arange(self.resolution[0], device=device, dtype=ms.int64),
+            mint.arange(self.resolution[1], device=device, dtype=ms.int64),
         )).flatten(1)
-        q_pos = torch.stack(ndgrid(
-            torch.arange(0, self.resolution[0], step=2, device=device, dtype=torch.long),
-            torch.arange(0, self.resolution[1], step=2, device=device, dtype=torch.long),
+        q_pos = mint.stack(ndgrid(
+            mint.arange(0, self.resolution[0], step=2, device=device, dtype=ms.int64),
+            mint.arange(0, self.resolution[1], step=2, device=device, dtype=ms.int64),
         )).flatten(1)
         rel_pos = (q_pos[..., :, None] - k_pos[..., None, :]).abs()
         rel_pos = (rel_pos[0] * self.resolution[1]) + rel_pos[1]
         self.register_buffer('attention_bias_idxs', rel_pos, persistent=False)
         self.attention_bias_cache = {}  # per-device attention_biases cache (data-parallel compat)
 
-    @torch.no_grad()
     def train(self, mode=True):
         super().train(mode)
         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def get_attention_biases(self, device: torch.device) -> ms.Tensor:
         if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
@@ -294,7 +301,7 @@                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
-    def forward(self, x):
+    def construct(self, x):
         B, C, H, W = x.shape
 
         q = self.q(x).reshape(B, self.num_heads, -1, self.N2).permute(0, 1, 3, 2)
@@ -314,7 +321,7 @@         return x
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -324,8 +331,8 @@             padding: Union[int, Tuple[int, int]] = 1,
             resolution: Union[int, Tuple[int, int]] = 7,
             use_attn: bool = False,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Optional[Type[nn.Module]] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Optional[Type[msnn.Cell]] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -335,7 +342,7 @@         kernel_size = to_2tuple(kernel_size)
         stride = to_2tuple(stride)
         padding = to_2tuple(padding)
-        norm_layer = norm_layer or nn.Identity()
+        norm_layer = norm_layer or msnn.Identity()
         self.conv = ConvNorm(
             in_chs,
             out_chs,
@@ -357,14 +364,14 @@         else:
             self.attn = None
 
-    def forward(self, x):
+    def construct(self, x):
         out = self.conv(x)
         if self.attn is not None:
             return self.attn(x) + out
         return out
 
 
-class ConvMlpWithNorm(nn.Module):
+class ConvMlpWithNorm(msnn.Cell):
     """
     Implementation of MLP with 1*1 convolutions.
     Input: tensor with shape [B, C, H, W]
@@ -375,8 +382,8 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             drop: float = 0.,
             mid_conv: bool = False,
             device=None,
@@ -407,12 +414,12 @@                 **dd,
             )
         else:
-            self.mid = nn.Identity()
+            self.mid = msnn.Identity()
         self.drop1 = nn.Dropout(drop)
         self.fc2 = ConvNorm(hidden_features, out_features, 1, norm_layer=norm_layer, **dd)
         self.drop2 = nn.Dropout(drop)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.fc1(x)
         x = self.mid(x)
         x = self.drop1(x)
@@ -421,13 +428,13 @@         return x
 
 
-class EfficientFormerV2Block(nn.Module):
+class EfficientFormerV2Block(msnn.Cell):
     def __init__(
             self,
             dim: int,
             mlp_ratio: float = 4.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             proj_drop: float = 0.,
             drop_path: float = 0.,
             layer_scale_init_value: Optional[float] = 1e-5,
@@ -449,8 +456,8 @@                 **dd,
             )
             self.ls1 = LayerScale2d(
-                dim, layer_scale_init_value, **dd) if layer_scale_init_value is not None else nn.Identity()
-            self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+                dim, layer_scale_init_value, **dd) if layer_scale_init_value is not None else msnn.Identity()
+            self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         else:
             self.token_mixer = None
             self.ls1 = None
@@ -466,23 +473,23 @@             **dd,
         )
         self.ls2 = LayerScale2d(
-            dim, layer_scale_init_value, **dd) if layer_scale_init_value is not None else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+            dim, layer_scale_init_value, **dd) if layer_scale_init_value is not None else msnn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         if self.token_mixer is not None:
             x = x + self.drop_path1(self.ls1(self.token_mixer(x)))
         x = x + self.drop_path2(self.ls2(self.mlp(x)))
         return x
 
 
-class Stem4(nn.Sequential):
+class Stem4(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -512,7 +519,7 @@         )
 
 
-class EfficientFormerV2Stage(nn.Module):
+class EfficientFormerV2Stage(msnn.Cell):
 
     def __init__(
             self,
@@ -529,8 +536,8 @@             proj_drop: float = .0,
             drop_path: Union[float, List[float]] = 0.,
             layer_scale_init_value: Optional[float] = 1e-5,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -554,7 +561,7 @@             resolution = tuple([math.ceil(r / 2) for r in resolution])
         else:
             assert dim == dim_out
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         blocks = []
         for block_idx in range(depth):
@@ -573,9 +580,9 @@                 **dd,
             )
             blocks += [b]
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell(*blocks)
+
+    def construct(self, x):
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
@@ -584,7 +591,7 @@         return x
 
 
-class EfficientFormerV2(nn.Module):
+class EfficientFormerV2(msnn.Cell):
     def __init__(
             self,
             depths: Tuple[int, ...],
@@ -651,16 +658,16 @@             prev_dim = embed_dims[i]
             self.feature_info += [dict(num_chs=prev_dim, reduction=stride, module=f'stages.{i}')]
             stages.append(stage)
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)
 
         # Classifier head
         self.num_features = self.head_hidden_size = embed_dims[-1]
         self.norm = norm_layer(embed_dims[-1], **dd)
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(embed_dims[-1], num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(embed_dims[-1], num_classes, **dd) if num_classes > 0 else msnn.Identity()
         self.dist = distillation
         if self.dist:
-            self.head_dist = nn.Linear(embed_dims[-1], num_classes, **dd) if num_classes > 0 else nn.Identity()
+            self.head_dist = nn.Linear(embed_dims[-1], num_classes, **dd) if num_classes > 0 else msnn.Identity()
         else:
             self.head_dist = None
 
@@ -672,7 +679,7 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def no_weight_decay(self):
@@ -692,15 +699,15 @@             s.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head, self.head_dist
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
         self.num_classes = num_classes
         if global_pool is not None:
             self.global_pool = global_pool
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
+        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
 
     @torch.jit.ignore
     def set_distilled_training(self, enable=True):
@@ -708,13 +715,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -768,7 +775,7 @@         take_indices, max_index = feature_take_indices(len(self.stages), indices)
         self.stages = self.stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
@@ -793,7 +800,7 @@             # during standard train/finetune, inference average the classifier predictions
             return (x + x_dist) / 2
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
