--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ PP-HGNet (V1 & V2)
 
 Reference:
@@ -8,9 +13,8 @@ """
 from typing import Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectAdaptivePool2d, DropPath, calculate_drop_path_rates, create_conv2d
@@ -22,7 +26,7 @@ __all__ = ['HighPerfGpuNet']
 
 
-class LearnableAffineBlock(nn.Module):
+class LearnableAffineBlock(msnn.Cell):
     def __init__(
             self,
             scale_value: float = 1.0,
@@ -32,14 +36,14 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.scale = nn.Parameter(torch.tensor([scale_value], **dd), requires_grad=True)
-        self.bias = nn.Parameter(torch.tensor([bias_value], **dd), requires_grad=True)
-
-    def forward(self, x):
+        self.scale = ms.Parameter(ms.Tensor([scale_value], **dd), requires_grad=True)
+        self.bias = ms.Parameter(ms.Tensor([bias_value], **dd), requires_grad=True)
+
+    def construct(self, x):
         return self.scale * x + self.bias
 
 
-class ConvBNAct(nn.Module):
+class ConvBNAct(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -70,13 +74,13 @@         if self.use_act:
             self.act = nn.ReLU()
         else:
-            self.act = nn.Identity()
+            self.act = msnn.Identity()
         if self.use_act and self.use_lab:
             self.lab = LearnableAffineBlock(**dd)
         else:
-            self.lab = nn.Identity()
-
-    def forward(self, x):
+            self.lab = msnn.Identity()
+
+    def construct(self, x):
         x = self.conv(x)
         x = self.bn(x)
         x = self.act(x)
@@ -84,7 +88,7 @@         return x
 
 
-class LightConvBNAct(nn.Module):
+class LightConvBNAct(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -115,13 +119,13 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         x = self.conv2(x)
         return x
 
 
-class EseModule(nn.Module):
+class EseModule(msnn.Cell):
     def __init__(self, chs: int, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
@@ -135,20 +139,20 @@         )
         self.sigmoid = nn.Sigmoid()
 
-    def forward(self, x):
+    def construct(self, x):
         identity = x
         x = x.mean((2, 3), keepdim=True)
         x = self.conv(x)
         x = self.sigmoid(x)
-        return torch.mul(identity, x)
-
-
-class StemV1(nn.Module):
+        return mint.mul(identity, x)
+
+
+class StemV1(msnn.Cell):
     # for PP-HGNet
     def __init__(self, stem_chs: List[int], device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.stem = nn.Sequential(*[
+        self.stem = msnn.SequentialCell(*[
             ConvBNAct(
                 stem_chs[i],
                 stem_chs[i + 1],
@@ -157,15 +161,15 @@                 **dd) for i in range(
                 len(stem_chs) - 1)
         ])
-        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
-
-    def forward(self, x):
+        self.pool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
+
+    def construct(self, x):
         x = self.stem(x)
         x = self.pool(x)
         return x
 
 
-class StemV2(nn.Module):
+class StemV2(msnn.Cell):
     # for PP-HGNetv2
     def __init__(
             self,
@@ -218,22 +222,22 @@             use_lab=use_lab,
             **dd,
         )
-        self.pool = nn.MaxPool2d(kernel_size=2, stride=1, ceil_mode=True)
-
-    def forward(self, x):
+        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 1, ceil_mode = True)
+
+    def construct(self, x):
         x = self.stem1(x)
-        x = F.pad(x, (0, 1, 0, 1))
+        x = nn.functional.pad(x, (0, 1, 0, 1))
         x2 = self.stem2a(x)
-        x2 = F.pad(x2, (0, 1, 0, 1))
+        x2 = nn.functional.pad(x2, (0, 1, 0, 1))
         x2 = self.stem2b(x2)
         x1 = self.pool(x)
-        x = torch.cat([x1, x2], dim=1)
+        x = mint.cat([x1, x2], dim=1)
         x = self.stem3(x)
         x = self.stem4(x)
         return x
 
 
-class HighPerfGpuBlock(nn.Module):
+class HighPerfGpuBlock(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -253,7 +257,7 @@         super().__init__()
         self.residual = residual
 
-        self.layers = nn.ModuleList()
+        self.layers = msnn.CellList()
         for i in range(layer_num):
             if light_block:
                 self.layers.append(
@@ -296,7 +300,7 @@                 use_lab=use_lab,
                 **dd,
             )
-            self.aggregation = nn.Sequential(
+            self.aggregation = msnn.SequentialCell(
                 aggregation_squeeze_conv,
                 aggregation_excitation_conv,
             )
@@ -310,27 +314,27 @@                 **dd,
             )
             att = EseModule(out_chs, **dd)
-            self.aggregation = nn.Sequential(
+            self.aggregation = msnn.SequentialCell(
                 aggregation_conv,
                 att,
             )
 
-        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()
-
-    def forward(self, x):
+        self.drop_path = DropPath(drop_path) if drop_path else msnn.Identity()
+
+    def construct(self, x):
         identity = x
         output = [x]
         for layer in self.layers:
             x = layer(x)
             output.append(x)
-        x = torch.cat(output, dim=1)
+        x = mint.cat(output, dim=1)
         x = self.aggregation(x)
         if self.residual:
             x = self.drop_path(x) + identity
         return x
 
 
-class HighPerfGpuStage(nn.Module):
+class HighPerfGpuStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -363,7 +367,7 @@                 **dd,
             )
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         blocks_list = []
         for i in range(block_num):
@@ -382,10 +386,10 @@                     **dd,
                 )
             )
-        self.blocks = nn.Sequential(*blocks_list)
+        self.blocks = msnn.SequentialCell(*blocks_list)
         self.grad_checkpointing= False
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
@@ -394,7 +398,7 @@         return x
 
 
-class ClassifierHead(nn.Module):
+class ClassifierHead(msnn.Cell):
     def __init__(
             self,
             in_features: int,
@@ -428,15 +432,15 @@             act = nn.ReLU()
             if use_lab:
                 lab = LearnableAffineBlock(**dd)
-                self.last_conv = nn.Sequential(last_conv, act, lab)
+                self.last_conv = msnn.SequentialCell(last_conv, act, lab)
             else:
-                self.last_conv = nn.Sequential(last_conv, act)
+                self.last_conv = msnn.SequentialCell(last_conv, act)
         else:
-            self.last_conv = nn.Identity()
+            self.last_conv = msnn.Identity()
 
         self.dropout = nn.Dropout(drop_rate)
-        self.flatten = nn.Flatten(1) if pool_type else nn.Identity()  # don't flatten if pooling disabled
-        self.fc = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.flatten = mint.flatten(1) if pool_type else msnn.Identity()  # don't flatten if pooling disabled
+        self.fc = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else msnn.Identity()
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
@@ -444,11 +448,11 @@             if not pool_type:
                 assert num_classes == 0, 'Classifier head must be removed if pooling is disabled'
             self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)
-            self.flatten = nn.Flatten(1) if pool_type else nn.Identity()  # don't flatten if pooling disabled
-
-        self.fc = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
+            self.flatten = mint.flatten(1) if pool_type else msnn.Identity()  # don't flatten if pooling disabled
+
+        self.fc = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else msnn.Identity()
+
+    def construct(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         x = self.last_conv(x)
         x = self.dropout(x)
@@ -459,7 +463,7 @@         return x
 
 
-class HighPerfGpuNet(nn.Module):
+class HighPerfGpuNet(msnn.Cell):
 
     def __init__(
             self,
@@ -522,7 +526,7 @@             if downsample:
                 current_stride *= 2
             self.feature_info += [dict(num_chs=self.num_features, reduction=current_stride, module=f'stages.{i}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)
 
         self.head = ClassifierHead(
             self.num_features,
@@ -537,12 +541,12 @@ 
         for n, m in self.named_modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.BatchNorm2d):
-                nn.init.ones_(m.weight)
-                nn.init.zeros_(m.bias)
+                nn.init.ones_(m.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                nn.init.zeros_(m.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.Linear):
-                nn.init.zeros_(m.bias)
+                nn.init.zeros_(m.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -557,7 +561,7 @@             s.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None, device=None, dtype=None):
@@ -566,13 +570,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -627,7 +631,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
