--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ HRNet
 
 Copied from https://github.com/HRNet/HRNet-Image-Classification
@@ -11,8 +16,8 @@ import logging
 from typing import Dict, List, Type, Optional, Tuple
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
@@ -354,11 +359,11 @@ )
 
 
-class HighResolutionModule(nn.Module):
+class HighResolutionModule(msnn.Cell):
     def __init__(
             self,
             num_branches: int,
-            block_types: Type[nn.Module],
+            block_types: Type[msnn.Cell],
             num_blocks: Tuple[int, ...],
             num_in_chs: List[int],
             num_channels: Tuple[int, ...],
@@ -391,7 +396,7 @@             **dd,
         )
         self.fuse_layers = self._make_fuse_layers(**dd)
-        self.fuse_act = nn.ReLU(False)
+        self.fuse_act = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
     def _check_branches(self, num_branches, block_types, num_blocks, num_in_chs, num_channels):
         error_msg = ''
@@ -409,7 +414,7 @@         dd = {'device': device, 'dtype': dtype}
         downsample = None
         if stride != 1 or self.num_in_chs[branch_index] != num_channels[branch_index] * block_type.expansion:
-            downsample = nn.Sequential(
+            downsample = msnn.SequentialCell(
                 nn.Conv2d(
                     self.num_in_chs[branch_index],
                     num_channels[branch_index] * block_type.expansion,
@@ -426,7 +431,7 @@         for i in range(1, num_blocks[branch_index]):
             layers.append(block_type(self.num_in_chs[branch_index], num_channels[branch_index], **dd))
 
-        return nn.Sequential(*layers)
+        return msnn.SequentialCell(*layers)
 
     def _make_branches(self, num_branches, block_type, num_blocks, num_channels, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
@@ -434,12 +439,12 @@         for i in range(num_branches):
             branches.append(self._make_one_branch(i, block_type, num_blocks, num_channels, **dd))
 
-        return nn.ModuleList(branches)
+        return msnn.CellList(branches)
 
     def _make_fuse_layers(self, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         if self.num_branches == 1:
-            return nn.Identity()
+            return msnn.Identity()
 
         num_branches = self.num_branches
         num_in_chs = self.num_in_chs
@@ -448,37 +453,37 @@             fuse_layer = []
             for j in range(num_branches):
                 if j > i:
-                    fuse_layer.append(nn.Sequential(
+                    fuse_layer.append(msnn.SequentialCell(
                         nn.Conv2d(num_in_chs[j], num_in_chs[i], 1, 1, 0, bias=False, **dd),
                         nn.BatchNorm2d(num_in_chs[i], momentum=_BN_MOMENTUM, **dd),
-                        nn.Upsample(scale_factor=2 ** (j - i), mode='nearest')))
+                        nn.Upsample(scale_factor = 2 ** (j - i), mode = 'nearest')))
                 elif j == i:
-                    fuse_layer.append(nn.Identity())
+                    fuse_layer.append(msnn.Identity())
                 else:
                     conv3x3s = []
                     for k in range(i - j):
                         if k == i - j - 1:
                             num_out_chs_conv3x3 = num_in_chs[i]
-                            conv3x3s.append(nn.Sequential(
+                            conv3x3s.append(msnn.SequentialCell(
                                 nn.Conv2d(num_in_chs[j], num_out_chs_conv3x3, 3, 2, 1, bias=False, **dd),
                                 nn.BatchNorm2d(num_out_chs_conv3x3, momentum=_BN_MOMENTUM, **dd)
                             ))
                         else:
                             num_out_chs_conv3x3 = num_in_chs[j]
-                            conv3x3s.append(nn.Sequential(
+                            conv3x3s.append(msnn.SequentialCell(
                                 nn.Conv2d(num_in_chs[j], num_out_chs_conv3x3, 3, 2, 1, bias=False, **dd),
                                 nn.BatchNorm2d(num_out_chs_conv3x3, momentum=_BN_MOMENTUM, **dd),
-                                nn.ReLU(False)
-                            ))
-                    fuse_layer.append(nn.Sequential(*conv3x3s))
-            fuse_layers.append(nn.ModuleList(fuse_layer))
-
-        return nn.ModuleList(fuse_layers)
+                                nn.ReLU()
+                            ))  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+                    fuse_layer.append(msnn.SequentialCell(*conv3x3s))
+            fuse_layers.append(msnn.CellList(fuse_layer))
+
+        return msnn.CellList(fuse_layers)
 
     def get_num_in_chs(self):
         return self.num_in_chs
 
-    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:
+    def construct(self, x: List[ms.Tensor]) -> List[ms.Tensor]:
         if self.num_branches == 1:
             return [self.branches[0](x[0])]
 
@@ -497,7 +502,7 @@         return x_fuse
 
 
-class SequentialList(nn.Sequential):
+class SequentialList(msnn.SequentialCell):
 
     def __init__(self, *args):
         super().__init__(*args)
@@ -512,15 +517,15 @@         # type: (torch.Tensor) -> (List[torch.Tensor])
         pass
 
-    def forward(self, x) -> List[torch.Tensor]:
+    def forward(self, x) -> List[ms.Tensor]:
         for module in self:
             x = module(x)
         return x
 
 
 @torch.jit.interface
-class ModuleInterface(torch.nn.Module):
-    def forward(self, input: torch.Tensor) -> torch.Tensor: # `input` has a same name in Sequential forward
+class ModuleInterface(msnn.Cell):
+    def construct(self, input: ms.Tensor) -> ms.Tensor: # `input` has a same name in Sequential forward
         pass
 
 
@@ -530,7 +535,7 @@ }
 
 
-class HighResolutionNet(nn.Module):
+class HighResolutionNet(msnn.Cell):
 
     def __init__(
             self,
@@ -554,10 +559,10 @@         stem_width = cfg['stem_width']
         self.conv1 = nn.Conv2d(in_chans, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **dd)
         self.bn1 = nn.BatchNorm2d(stem_width, momentum=_BN_MOMENTUM, **dd)
-        self.act1 = nn.ReLU(inplace=True)
+        self.act1 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.conv2 = nn.Conv2d(stem_width, 64, kernel_size=3, stride=2, padding=1, bias=False, **dd)
         self.bn2 = nn.BatchNorm2d(64, momentum=_BN_MOMENTUM, **dd)
-        self.act2 = nn.ReLU(inplace=True)
+        self.act2 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         self.stage1_cfg = cfg['stage1']
         num_channels = self.stage1_cfg['num_channels'][0]
@@ -612,9 +617,9 @@             else:
                 self.num_features = self.head_hidden_size = 256
                 self.incre_modules = None
-            self.global_pool = nn.Identity()
-            self.head_drop = nn.Identity()
-            self.classifier = nn.Identity()
+            self.global_pool = msnn.Identity()
+            self.head_drop = msnn.Identity()
+            self.classifier = msnn.Identity()
 
         curr_stride = 2
         # module names aren't actually valid here, hook or FeatureNet based extraction would not work
@@ -636,7 +641,7 @@         incre_modules = []
         for i, channels in enumerate(pre_stage_channels):
             incre_modules.append(self._make_layer(head_block_type, channels, self.head_channels[i], 1, stride=1, **dd))
-        incre_modules = nn.ModuleList(incre_modules)
+        incre_modules = msnn.CellList(incre_modules)
         if incre_only:
             return incre_modules, None, None
 
@@ -645,7 +650,7 @@         for i in range(len(pre_stage_channels) - 1):
             in_channels = self.head_channels[i] * head_block_type.expansion
             out_channels = self.head_channels[i + 1] * head_block_type.expansion
-            downsamp_module = nn.Sequential(
+            downsamp_module = msnn.SequentialCell(
                 nn.Conv2d(
                     in_channels=in_channels,
                     out_channels=out_channels,
@@ -656,12 +661,12 @@                     **dd,
                 ),
                 nn.BatchNorm2d(out_channels, momentum=_BN_MOMENTUM, **dd),
-                nn.ReLU(inplace=True)
-            )
+                nn.ReLU()
+            )  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
             downsamp_modules.append(downsamp_module)
-        downsamp_modules = nn.ModuleList(downsamp_modules)
-
-        final_layer = nn.Sequential(
+        downsamp_modules = msnn.CellList(downsamp_modules)
+
+        final_layer = msnn.SequentialCell(
             nn.Conv2d(
                 in_channels=self.head_channels[3] * head_block_type.expansion,
                 out_channels=self.num_features,
@@ -672,8 +677,8 @@                 **dd,
             ),
             nn.BatchNorm2d(self.num_features, momentum=_BN_MOMENTUM, **dd),
-            nn.ReLU(inplace=True)
-        )
+            nn.ReLU()
+        )  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         return incre_modules, downsamp_modules, final_layer
 
@@ -686,30 +691,30 @@         for i in range(num_branches_cur):
             if i < num_branches_pre:
                 if num_channels_cur_layer[i] != num_channels_pre_layer[i]:
-                    transition_layers.append(nn.Sequential(
+                    transition_layers.append(msnn.SequentialCell(
                         nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False, **dd),
                         nn.BatchNorm2d(num_channels_cur_layer[i], momentum=_BN_MOMENTUM, **dd),
-                        nn.ReLU(inplace=True)))
+                        nn.ReLU()))  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
                 else:
-                    transition_layers.append(nn.Identity())
+                    transition_layers.append(msnn.Identity())
             else:
                 conv3x3s = []
                 for j in range(i + 1 - num_branches_pre):
                     _in_chs = num_channels_pre_layer[-1]
                     _out_chs = num_channels_cur_layer[i] if j == i - num_branches_pre else _in_chs
-                    conv3x3s.append(nn.Sequential(
+                    conv3x3s.append(msnn.SequentialCell(
                         nn.Conv2d(_in_chs, _out_chs, 3, 2, 1, bias=False, **dd),
                         nn.BatchNorm2d(_out_chs, momentum=_BN_MOMENTUM, **dd),
-                        nn.ReLU(inplace=True)))
-                transition_layers.append(nn.Sequential(*conv3x3s))
-
-        return nn.ModuleList(transition_layers)
+                        nn.ReLU()))  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+                transition_layers.append(msnn.SequentialCell(*conv3x3s))
+
+        return msnn.CellList(transition_layers)
 
     def _make_layer(self, block_type, inplanes, planes, block_types, stride=1, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         downsample = None
         if stride != 1 or inplanes != planes * block_type.expansion:
-            downsample = nn.Sequential(
+            downsample = msnn.SequentialCell(
                 nn.Conv2d(inplanes, planes * block_type.expansion, kernel_size=1, stride=stride, bias=False, **dd),
                 nn.BatchNorm2d(planes * block_type.expansion, momentum=_BN_MOMENTUM, **dd),
             )
@@ -719,7 +724,7 @@         for i in range(1, block_types):
             layers.append(block_type(inplanes, planes, **dd))
 
-        return nn.Sequential(*layers)
+        return msnn.SequentialCell(*layers)
 
     def _make_stage(self, layer_config, num_in_chs, multi_scale_output=True, device=None, dtype=None):
         num_modules = layer_config['num_modules']
@@ -753,10 +758,10 @@         for m in self.modules():
             if isinstance(m, nn.Conv2d):
                 nn.init.kaiming_normal_(
-                    m.weight, mode='fan_out', nonlinearity='relu')
+                    m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.BatchNorm2d):
-                nn.init.constant_(m.weight, 1)
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.weight, 1)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -775,7 +780,7 @@         assert not enable, "gradient checkpointing not supported"
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.classifier
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -783,16 +788,16 @@         self.global_pool, self.classifier = create_classifier(
             self.num_features, self.num_classes, pool_type=global_pool)
 
-    def stages(self, x) -> List[torch.Tensor]:
+    def stages(self, x) -> List[ms.Tensor]:
         x = self.layer1(x)
 
         xl = [t(x) for i, t in enumerate(self.transition1)]
         yl = self.stage2(xl)
 
-        xl = [t(yl[-1]) if not isinstance(t, nn.Identity) else yl[i] for i, t in enumerate(self.transition2)]
+        xl = [t(yl[-1]) if not isinstance(t, msnn.Identity) else yl[i] for i, t in enumerate(self.transition2)]
         yl = self.stage3(xl)
 
-        xl = [t(yl[-1]) if not isinstance(t, nn.Identity) else yl[i] for i, t in enumerate(self.transition3)]
+        xl = [t(yl[-1]) if not isinstance(t, msnn.Identity) else yl[i] for i, t in enumerate(self.transition3)]
         yl = self.stage4(xl)
         return yl
 
@@ -827,7 +832,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.classifier(x)
 
-    def forward(self, x):
+    def construct(self, x):
         y = self.forward_features(x)
         x = self.forward_head(y)
         return x
@@ -873,7 +878,7 @@     def forward_features(self, x):
         assert False, 'Not supported'
 
-    def forward(self, x) -> List[torch.Tensor]:
+    def forward(self, x) -> List[ms.Tensor]:
         out = []
         x = self.conv1(x)
         x = self.bn1(x)
