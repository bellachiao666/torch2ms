--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 InceptionNeXt paper: https://arxiv.org/abs/2303.16900
 Original implementation & weights from: https://github.com/sail-sg/inceptionnext
@@ -6,8 +11,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import trunc_normal_, DropPath, calculate_drop_path_rates, to_2tuple, get_padding, SelectAdaptivePool2d
@@ -19,7 +24,7 @@ __all__ = ['MetaNeXt']
 
 
-class InceptionDWConv2d(nn.Module):
+class InceptionDWConv2d(msnn.Cell):
     """ Inception depthwise convolution
     """
 
@@ -49,9 +54,9 @@             padding=(band_padding, 0), dilation=(dilation, 1), groups=gc, **dd)
         self.split_indexes = (in_chs - 3 * gc, gc, gc, gc)
 
-    def forward(self, x):
-        x_id, x_hw, x_w, x_h = torch.split(x, self.split_indexes, dim=1)
-        return torch.cat((
+    def construct(self, x):
+        x_id, x_hw, x_w, x_h = mint.split(x, self.split_indexes, dim=1)
+        return mint.cat((
             x_id,
             self.dwconv_hw(x_hw),
             self.dwconv_w(x_w),
@@ -60,7 +65,7 @@         )
 
 
-class ConvMlp(nn.Module):
+class ConvMlp(msnn.Cell):
     """ MLP using 1x1 convs that keeps spatial dims
     copied from timm: https://github.com/huggingface/pytorch-image-models/blob/v0.6.11/timm/models/layers/mlp.py
     """
@@ -70,8 +75,8 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             bias: bool = True,
             drop: float = 0.,
             device=None,
@@ -84,12 +89,12 @@         bias = to_2tuple(bias)
 
         self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=bias[0], **dd)
-        self.norm = norm_layer(hidden_features, **dd) if norm_layer else nn.Identity()
+        self.norm = norm_layer(hidden_features, **dd) if norm_layer else msnn.Identity()
         self.act = act_layer()
         self.drop = nn.Dropout(drop)
         self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=bias[1], **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.fc1(x)
         x = self.norm(x)
         x = self.act(x)
@@ -98,7 +103,7 @@         return x
 
 
-class MlpClassifierHead(nn.Module):
+class MlpClassifierHead(msnn.Cell):
     """ MLP classification head
     """
 
@@ -108,8 +113,8 @@             num_classes: int = 1000,
             pool_type: str = 'avg',
             mlp_ratio: float = 3,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
             drop: float = 0.,
             bias: bool = True,
             device=None,
@@ -135,9 +140,9 @@             assert pool_type, 'Cannot disable pooling'
             self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True)
 
-        self.fc2 = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
+        self.fc2 = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
+
+    def construct(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         x = self.fc1(x)
         x = self.act(x)
@@ -146,7 +151,7 @@         return x if pre_logits else self.fc2(x)
 
 
-class MetaNeXtBlock(nn.Module):
+class MetaNeXtBlock(msnn.Cell):
     """ MetaNeXtBlock Block
     Args:
         dim (int): Number of input channels.
@@ -158,11 +163,11 @@             self,
             dim: int,
             dilation: int = 1,
-            token_mixer: Type[nn.Module] = InceptionDWConv2d,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            mlp_layer: Type[nn.Module] = ConvMlp,
+            token_mixer: Type[msnn.Cell] = InceptionDWConv2d,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            mlp_layer: Type[msnn.Cell] = ConvMlp,
             mlp_ratio: float = 4,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             ls_init_value: float = 1e-6,
             drop_path: float = 0.,
             device=None,
@@ -173,10 +178,10 @@         self.token_mixer = token_mixer(dim, dilation=dilation, **dd)
         self.norm = norm_layer(dim, **dd)
         self.mlp = mlp_layer(dim, int(mlp_ratio * dim), act_layer=act_layer, **dd)
-        self.gamma = nn.Parameter(ls_init_value * torch.ones(dim, **dd)) if ls_init_value else None
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.gamma = ms.Parameter(ls_init_value * mint.ones(dim, **dd)) if ls_init_value else None
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x
         x = self.token_mixer(x)
         x = self.norm(x)
@@ -187,7 +192,7 @@         return x
 
 
-class MetaNeXtStage(nn.Module):
+class MetaNeXtStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -197,9 +202,9 @@             dilation: Tuple[int, int] = (1, 1),
             drop_path_rates: Optional[List[float]] = None,
             ls_init_value: float = 1.0,
-            token_mixer: Type[nn.Module] = InceptionDWConv2d,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            token_mixer: Type[msnn.Cell] = InceptionDWConv2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             mlp_ratio: float = 4,
             device=None,
             dtype=None,
@@ -208,7 +213,7 @@         super().__init__()
         self.grad_checkpointing = False
         if stride > 1 or dilation[0] != dilation[1]:
-            self.downsample = nn.Sequential(
+            self.downsample = msnn.SequentialCell(
                 norm_layer(in_chs, **dd),
                 nn.Conv2d(
                     in_chs,
@@ -220,7 +225,7 @@                 ),
             )
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         drop_path_rates = drop_path_rates or [0.] * depth
         stage_blocks = []
@@ -236,9 +241,9 @@                 mlp_ratio=mlp_ratio,
                 **dd,
             ))
-        self.blocks = nn.Sequential(*stage_blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell(*stage_blocks)
+
+    def construct(self, x):
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
@@ -247,7 +252,7 @@         return x
 
 
-class MetaNeXt(nn.Module):
+class MetaNeXt(msnn.Cell):
     r""" MetaNeXt
         A PyTorch impl of : `InceptionNeXt: When Inception Meets ConvNeXt` - https://arxiv.org/abs/2303.16900
 
@@ -273,9 +278,9 @@             output_stride: int = 32,
             depths: Tuple[int, ...] = (3, 3, 9, 3),
             dims: Tuple[int, ...] = (96, 192, 384, 768),
-            token_mixers: Union[Type[nn.Module], List[Type[nn.Module]]] = InceptionDWConv2d,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            act_layer: Type[nn.Module] = nn.GELU,
+            token_mixers: Union[Type[msnn.Cell], List[Type[msnn.Cell]]] = InceptionDWConv2d,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             mlp_ratios: Union[int, Tuple[int, ...]] = (4, 4, 4, 3),
             drop_rate: float = 0.,
             drop_path_rate: float = 0.,
@@ -295,7 +300,7 @@         self.drop_rate = drop_rate
         self.feature_info = []
 
-        self.stem = nn.Sequential(
+        self.stem = msnn.SequentialCell(
             nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4, **dd),
             norm_layer(dims[0], **dd)
         )
@@ -305,7 +310,7 @@         curr_stride = 4
         dilation = 1
         # feature resolution stages, each consisting of multiple residual blocks
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         for i in range(num_stage):
             stride = 2 if curr_stride == 2 or i > 0 else 1
             if curr_stride >= output_stride and stride > 1:
@@ -339,7 +344,7 @@         if isinstance(m, (nn.Conv2d, nn.Linear)):
             trunc_normal_(m.weight, std=.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -352,7 +357,7 @@         )
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc2
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -370,13 +375,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -432,7 +437,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
