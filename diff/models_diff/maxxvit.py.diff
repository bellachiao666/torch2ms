--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ MaxVit and CoAtNet Vision Transformer - CNN Hybrids in PyTorch
 
 This is a from-scratch implementation of both CoAtNet and MaxVit in PyTorch.
@@ -40,9 +45,9 @@ from functools import partial
 from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
 
-import torch
-from torch import nn
-from torch.jit import Final
+# import torch
+# from torch import nn
+# from torch.jit import Final
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -166,7 +171,7 @@     weight_init: str = 'vit_eff'
 
 
-class Attention2d(nn.Module):
+class Attention2d(msnn.Cell):
     """Multi-head attention for 2D NCHW tensors."""
     fused_attn: Final[bool]
 
@@ -212,7 +217,7 @@         self.proj = nn.Conv2d(dim_attn, dim_out, 1, bias=bias, **dd)
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x: torch.Tensor, shared_rel_pos: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, shared_rel_pos: Optional[ms.Tensor] = None) -> ms.Tensor:
         B, C, H, W = x.shape
 
         if self.head_first:
@@ -227,13 +232,7 @@             elif shared_rel_pos is not None:
                 attn_bias = shared_rel_pos
 
-            x = torch.nn.functional.scaled_dot_product_attention(
-                q.transpose(-1, -2).contiguous(),
-                k.transpose(-1, -2).contiguous(),
-                v.transpose(-1, -2).contiguous(),
-                attn_mask=attn_bias,
-                dropout_p=self.attn_drop.p if self.training else 0.,
-            ).transpose(-1, -2).reshape(B, -1, H, W)
+            x = mint.transpose(-1, -2).reshape(B, -1, H, W)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q.transpose(-2, -1) @ k
@@ -250,7 +249,7 @@         return x
 
 
-class AttentionCl(nn.Module):
+class AttentionCl(msnn.Cell):
     """Channels-last multi-head attention (B, ..., C)."""
     fused_attn: Final[bool]
 
@@ -297,7 +296,7 @@         self.proj = nn.Linear(dim_attn, dim_out, bias=bias, **dd)
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x: torch.Tensor, shared_rel_pos: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, shared_rel_pos: Optional[ms.Tensor] = None) -> ms.Tensor:
         B = x.shape[0]
         restore_shape = x.shape[:-1]
 
@@ -317,7 +316,7 @@                 q, k, v,
                 attn_mask=attn_bias,
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
@@ -335,7 +334,7 @@         return x
 
 
-class Downsample2d(nn.Module):
+class Downsample2d(msnn.Cell):
     """A downsample pooling module supporting several maxpool and avgpool modes.
 
     * 'max' - MaxPool2d w/ kernel_size 3, stride 2, padding 1
@@ -375,42 +374,42 @@             self.pool = create_pool2d('avg', 2, padding=padding or 0)
 
         if dim != dim_out:
-            self.expand = nn.Conv2d(dim, dim_out, 1, bias=bias, device=device, dtype=dtype)
-        else:
-            self.expand = nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.expand = nn.Conv2d(dim, dim_out, 1, bias = bias, dtype = dtype)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device' (position 9);
+        else:
+            self.expand = msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.pool(x)  # spatial downsample
         x = self.expand(x)  # expand chs
         return x
 
 
-def _init_transformer(module: nn.Module, name: str, scheme: str = '') -> None:
+def _init_transformer(module: msnn.Cell, name: str, scheme: str = '') -> None:
     """Initialize transformer module weights."""
     if isinstance(module, (nn.Conv2d, nn.Linear)):
         if scheme == 'normal':
-            nn.init.normal_(module.weight, std=.02)
+            nn.init.normal_(module.weight, std=.02)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif scheme == 'trunc_normal':
             trunc_normal_tf_(module.weight, std=.02)
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif scheme == 'xavier_normal':
-            nn.init.xavier_normal_(module.weight)
+            nn.init.xavier_normal_(module.weight)  # 'torch.nn.init.xavier_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             # vit like
-            nn.init.xavier_uniform_(module.weight)
+            nn.init.xavier_uniform_(module.weight)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
                 if 'mlp' in name:
-                    nn.init.normal_(module.bias, std=1e-6)
+                    nn.init.normal_(module.bias, std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 else:
-                    nn.init.zeros_(module.bias)
-
-
-class TransformerBlock2d(nn.Module):
+                    nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+
+class TransformerBlock2d(msnn.Cell):
     """Transformer block with 2D downsampling.
 
     '2D' NCHW tensor layout
@@ -448,13 +447,13 @@ 
         if stride == 2:
             self.shortcut = Downsample2d(dim, dim_out, pool_type=cfg.pool_type, bias=cfg.shortcut_bias, **dd)
-            self.norm1 = nn.Sequential(OrderedDict([
+            self.norm1 = msnn.SequentialCell(OrderedDict([
                 ('norm', norm_layer(dim, **dd)),
                 ('down', Downsample2d(dim, dim, pool_type=cfg.pool_type, **dd)),
             ]))
         else:
             assert dim == dim_out
-            self.shortcut = nn.Identity()
+            self.shortcut = msnn.Identity()
             self.norm1 = norm_layer(dim, **dd)
 
         self.attn = Attention2d(
@@ -468,8 +467,8 @@             proj_drop=cfg.proj_drop,
             **dd,
         )
-        self.ls1 = LayerScale2d(dim_out, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.ls1 = LayerScale2d(dim_out, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.norm2 = norm_layer(dim_out, **dd)
         self.mlp = ConvMlp(
@@ -479,40 +478,40 @@             drop=cfg.proj_drop,
             **dd,
         )
-        self.ls2 = LayerScale2d(dim_out, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.ls2 = LayerScale2d(dim_out, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
     def init_weights(self, scheme: str = '') -> None:
         named_apply(partial(_init_transformer, scheme=scheme), self)
 
-    def forward(self, x: torch.Tensor, shared_rel_pos: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, shared_rel_pos: Optional[ms.Tensor] = None) -> ms.Tensor:
         x = self.shortcut(x) + self.drop_path1(self.ls1(self.attn(self.norm1(x), shared_rel_pos=shared_rel_pos)))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-def _init_conv(module: nn.Module, name: str, scheme: str = '') -> None:
+def _init_conv(module: msnn.Cell, name: str, scheme: str = '') -> None:
     """Initialize convolution module weights."""
     if isinstance(module, nn.Conv2d):
         if scheme == 'normal':
-            nn.init.normal_(module.weight, std=.02)
+            nn.init.normal_(module.weight, std=.02)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif scheme == 'trunc_normal':
             trunc_normal_tf_(module.weight, std=.02)
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif scheme == 'xavier_normal':
-            nn.init.xavier_normal_(module.weight)
+            nn.init.xavier_normal_(module.weight)  # 'torch.nn.init.xavier_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             # efficientnet like
             fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels
             fan_out //= module.groups
-            nn.init.normal_(module.weight, 0, math.sqrt(2.0 / fan_out))
+            nn.init.normal_(module.weight, 0, math.sqrt(2.0 / fan_out))  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def num_groups(group_size: Optional[int], channels: int) -> int:
@@ -525,7 +524,7 @@         return channels // group_size
 
 
-class MbConvBlock(nn.Module):
+class MbConvBlock(msnn.Cell):
     """Pre-Norm Conv Block - 1x1 - kxk - 1x1, w/ inverted bottleneck (expand)."""
 
     def __init__(
@@ -558,7 +557,7 @@             self.shortcut = Downsample2d(
                 in_chs, out_chs, pool_type=cfg.pool_type, bias=cfg.output_bias, padding=cfg.padding, **dd)
         else:
-            self.shortcut = nn.Identity()
+            self.shortcut = msnn.Identity()
 
         assert cfg.stride_mode in ('pool', '1x1', 'dw')
         stride_pool, stride_1, stride_2 = 1, 1, 1
@@ -576,7 +575,7 @@         if stride_pool > 1:
             self.down = Downsample2d(in_chs, in_chs, pool_type=cfg.downsample_pool_type, padding=cfg.padding, **dd)
         else:
-            self.down = nn.Identity()
+            self.down = msnn.Identity()
         self.conv1_1x1 = create_conv2d(in_chs, mid_chs, 1, stride=stride_1, **dd)
         self.norm1 = norm_act_layer(mid_chs, **dd)
 
@@ -608,12 +607,12 @@             self.se = create_attn(cfg.attn_layer, mid_chs, **attn_kwargs, **dd)
 
         self.conv3_1x1 = create_conv2d(mid_chs, out_chs, 1, bias=cfg.output_bias, **dd)
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
     def init_weights(self, scheme: str = '') -> None:
         named_apply(partial(_init_conv, scheme=scheme), self)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = self.shortcut(x)
         x = self.pre_norm(x)
         x = self.down(x)
@@ -636,7 +635,7 @@         return x
 
 
-class ConvNeXtBlock(nn.Module):
+class ConvNeXtBlock(msnn.Cell):
     """ConvNeXt Block."""
 
     def __init__(
@@ -681,7 +680,7 @@         elif in_chs != out_chs:
             self.shortcut = nn.Conv2d(in_chs, out_chs, kernel_size=1, bias=cfg.output_bias, **dd)
         else:
-            self.shortcut = nn.Identity()
+            self.shortcut = msnn.Identity()
 
         assert cfg.stride_mode in ('pool', 'dw')
         stride_pool, stride_dw = 1, 1
@@ -694,7 +693,7 @@         if stride_pool == 2:
             self.down = Downsample2d(in_chs, in_chs, pool_type=cfg.downsample_pool_type, **dd)
         else:
-            self.down = nn.Identity()
+            self.down = msnn.Identity()
 
         self.conv_dw = create_conv2d(
             in_chs,
@@ -715,12 +714,12 @@             **dd,
         )
         if conv_mlp:
-            self.ls = LayerScale2d(out_chs, cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        else:
-            self.ls = LayerScale(out_chs, cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.ls = LayerScale2d(out_chs, cfg.init_values, **dd) if cfg.init_values else msnn.Identity()
+        else:
+            self.ls = LayerScale(out_chs, cfg.init_values, **dd) if cfg.init_values else msnn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = self.shortcut(x)
         x = self.down(x)
         x = self.conv_dw(x)
@@ -739,7 +738,7 @@         return x
 
 
-def window_partition(x: torch.Tensor, window_size: List[int]) -> torch.Tensor:
+def window_partition(x: ms.Tensor, window_size: List[int]) -> ms.Tensor:
     """Partition into non-overlapping windows."""
     B, H, W, C = x.shape
     _assert(H % window_size[0] == 0, f'height ({H}) must be divisible by window ({window_size[0]})')
@@ -750,7 +749,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def window_reverse(windows: torch.Tensor, window_size: List[int], img_size: List[int]) -> torch.Tensor:
+def window_reverse(windows: ms.Tensor, window_size: List[int], img_size: List[int]) -> ms.Tensor:
     """Reverse window partition."""
     H, W = img_size
     C = windows.shape[-1]
@@ -759,7 +758,7 @@     return x
 
 
-def grid_partition(x: torch.Tensor, grid_size: List[int]) -> torch.Tensor:
+def grid_partition(x: ms.Tensor, grid_size: List[int]) -> ms.Tensor:
     """Partition into overlapping windows with grid striding."""
     B, H, W, C = x.shape
     _assert(H % grid_size[0] == 0, f'height {H} must be divisible by grid {grid_size[0]}')
@@ -770,7 +769,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def grid_reverse(windows: torch.Tensor, grid_size: List[int], img_size: List[int]) -> torch.Tensor:
+def grid_reverse(windows: ms.Tensor, grid_size: List[int], img_size: List[int]) -> ms.Tensor:
     """Reverse grid partition."""
     H, W = img_size
     C = windows.shape[-1]
@@ -791,7 +790,7 @@     return rel_pos_cls
 
 
-class PartitionAttentionCl(nn.Module):
+class PartitionAttentionCl(msnn.Cell):
     """Grid or Block partition + Attn + FFN.
 
     NxC 'channels last' tensor layout.
@@ -827,8 +826,8 @@             proj_drop=cfg.proj_drop,
             **dd,
         )
-        self.ls1 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.ls1 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.norm2 = norm_layer(dim, **dd)
         self.mlp = Mlp(
@@ -838,8 +837,8 @@             drop=cfg.proj_drop,
             **dd,
         )
-        self.ls2 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.ls2 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
     def _partition_attn(self, x):
         img_size = x.shape[1:3]
@@ -856,13 +855,13 @@             x = grid_reverse(partitioned, self.partition_size, img_size)
         return x
 
-    def forward(self, x):
+    def construct(self, x):
         x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-class ParallelPartitionAttention(nn.Module):
+class ParallelPartitionAttention(msnn.Cell):
     """Experimental. Grid and Block partition + single FFN.
 
     NxC tensor layout.
@@ -915,8 +914,8 @@             proj_drop=cfg.proj_drop,
             **dd,
         )
-        self.ls1 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.ls1 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.norm2 = norm_layer(dim, **dd)
         self.mlp = Mlp(
@@ -927,10 +926,10 @@             drop=cfg.proj_drop,
             **dd,
         )
-        self.ls2 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def _partition_attn(self, x: torch.Tensor) -> torch.Tensor:
+        self.ls2 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def _partition_attn(self, x: ms.Tensor) -> ms.Tensor:
         img_size = x.shape[1:3]
 
         partitioned_block = window_partition(x, self.partition_size)
@@ -941,15 +940,15 @@         partitioned_grid = self.attn_grid(partitioned_grid)
         x_grid = grid_reverse(partitioned_grid, self.partition_size, img_size)
 
-        return torch.cat([x_window, x_grid], dim=-1)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        return mint.cat([x_window, x_grid], dim=-1)
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-def window_partition_nchw(x: torch.Tensor, window_size: List[int]) -> torch.Tensor:
+def window_partition_nchw(x: ms.Tensor, window_size: List[int]) -> ms.Tensor:
     """Partition windows for NCHW tensors."""
     B, C, H, W = x.shape
     _assert(H % window_size[0] == 0, f'height ({H}) must be divisible by window ({window_size[0]})')
@@ -960,7 +959,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def window_reverse_nchw(windows: torch.Tensor, window_size: List[int], img_size: List[int]) -> torch.Tensor:
+def window_reverse_nchw(windows: ms.Tensor, window_size: List[int], img_size: List[int]) -> ms.Tensor:
     """Reverse window partition for NCHW tensors."""
     H, W = img_size
     C = windows.shape[1]
@@ -969,7 +968,7 @@     return x
 
 
-def grid_partition_nchw(x: torch.Tensor, grid_size: List[int]) -> torch.Tensor:
+def grid_partition_nchw(x: ms.Tensor, grid_size: List[int]) -> ms.Tensor:
     """Grid partition for NCHW tensors."""
     B, C, H, W = x.shape
     _assert(H % grid_size[0] == 0, f'height {H} must be divisible by grid {grid_size[0]}')
@@ -980,7 +979,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def grid_reverse_nchw(windows: torch.Tensor, grid_size: List[int], img_size: List[int]) -> torch.Tensor:
+def grid_reverse_nchw(windows: ms.Tensor, grid_size: List[int], img_size: List[int]) -> ms.Tensor:
     """Reverse grid partition for NCHW tensors."""
     H, W = img_size
     C = windows.shape[1]
@@ -989,7 +988,7 @@     return x
 
 
-class PartitionAttention2d(nn.Module):
+class PartitionAttention2d(msnn.Cell):
     """Grid or Block partition + Attn + FFN.
 
     '2D' NCHW tensor layout.
@@ -1032,8 +1031,8 @@             proj_drop=cfg.proj_drop,
             **dd,
         )
-        self.ls1 = LayerScale2d(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.ls1 = LayerScale2d(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.norm2 = norm_layer(dim, **dd)
         self.mlp = ConvMlp(
@@ -1043,10 +1042,10 @@             drop=cfg.proj_drop,
             **dd,
         )
-        self.ls2 = LayerScale2d(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def _partition_attn(self, x: torch.Tensor) -> torch.Tensor:
+        self.ls2 = LayerScale2d(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def _partition_attn(self, x: ms.Tensor) -> ms.Tensor:
         img_size = x.shape[-2:]
         if self.partition_block:
             partitioned = window_partition_nchw(x, self.partition_size)
@@ -1061,13 +1060,13 @@             x = grid_reverse_nchw(partitioned, self.partition_size, img_size)
         return x
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-class MaxxVitBlock(nn.Module):
+class MaxxVitBlock(msnn.Cell):
     """MaxVit conv, window partition + FFN , grid partition + FFN."""
 
     def __init__(
@@ -1109,7 +1108,7 @@         named_apply(partial(_init_transformer, scheme=scheme), self.attn_grid)
         named_apply(partial(_init_conv, scheme=scheme), self.conv)
 
-    def forward(self, x):
+    def construct(self, x):
         # NCHW format
         x = self.conv(x)
 
@@ -1123,7 +1122,7 @@         return x
 
 
-class ParallelMaxxVitBlock(nn.Module):
+class ParallelMaxxVitBlock(msnn.Cell):
     """MaxVit block with parallel cat(window + grid), one FF.
 
     Experimental timm block.
@@ -1158,7 +1157,7 @@         if num_conv > 1:
             convs = [conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path, **dd)]
             convs += [conv_cls(dim_out, dim_out, cfg=conv_cfg, drop_path=drop_path, **dd)] * (num_conv - 1)
-            self.conv = nn.Sequential(*convs)
+            self.conv = msnn.SequentialCell(*convs)
         else:
             self.conv = conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path, **dd)
         self.attn = ParallelPartitionAttention(dim=dim_out, cfg=transformer_cfg, drop_path=drop_path, **dd)
@@ -1167,7 +1166,7 @@         named_apply(partial(_init_transformer, scheme=scheme), self.attn)
         named_apply(partial(_init_conv, scheme=scheme), self.conv)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.conv(x)
         x = x.permute(0, 2, 3, 1)
         x = self.attn(x)
@@ -1175,7 +1174,7 @@         return x
 
 
-class MaxxVitStage(nn.Module):
+class MaxxVitStage(msnn.Cell):
     """MaxxVit stage consisting of mixed convolution and transformer blocks."""
 
     def __init__(
@@ -1255,9 +1254,9 @@                     **dd,
                 )]
             in_chs = out_chs
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.blocks = msnn.SequentialCell(*blocks)
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
@@ -1265,7 +1264,7 @@         return x
 
 
-class Stem(nn.Module):
+class Stem(msnn.Cell):
     """Stem layer for feature extraction."""
 
     def __init__(
@@ -1308,7 +1307,7 @@     def init_weights(self, scheme: str = '') -> None:
         named_apply(partial(_init_conv, scheme=scheme), self)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.conv1(x)
         x = self.norm1(x)
         x = self.conv2(x)
@@ -1346,7 +1345,7 @@     return cfg
 
 
-class MaxxVit(nn.Module):
+class MaxxVit(msnn.Cell):
     """CoaTNet + MaxVit base model.
 
     Highly configurable for different block compositions, tensor layouts, pooling types.
@@ -1426,11 +1425,11 @@             stride *= stage_stride
             in_chs = out_chs
             self.feature_info += [dict(num_chs=out_chs, reduction=stride, module=f'stages.{i}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)
 
         final_norm_layer = partial(get_norm_layer(cfg.transformer_cfg.norm_layer), eps=cfg.transformer_cfg.norm_eps)
         if cfg.head_hidden_size:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
             self.head_hidden_size = cfg.head_hidden_size
             self.head = NormMlpClassifierHead(
                 self.num_features,
@@ -1458,7 +1457,7 @@         if cfg.weight_init:
             named_apply(partial(self._init_weights, scheme=cfg.weight_init), self)
 
-    def _init_weights(self, module: nn.Module, name: str, scheme: str = '') -> None:
+    def _init_weights(self, module: msnn.Cell, name: str, scheme: str = '') -> None:
         if hasattr(module, 'init_weights'):
             try:
                 module.init_weights(scheme=scheme)
@@ -1485,7 +1484,7 @@             s.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None) -> None:
@@ -1494,13 +1493,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -1556,21 +1555,21 @@         take_indices, max_index = feature_take_indices(len(self.stages) + 1, indices)
         self.stages = self.stages[:max_index]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.head = self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         x = self.stem(x)
         x = self.stages(x)
         x = self.norm(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -2106,7 +2105,7 @@ )
 
 
-def checkpoint_filter_fn(state_dict: Dict[str, torch.Tensor], model: nn.Module) -> Dict[str, torch.Tensor]:
+def checkpoint_filter_fn(state_dict: Dict[str, ms.Tensor], model: msnn.Cell) -> Dict[str, ms.Tensor]:
     """Filter checkpoint state dict for compatibility."""
     model_state_dict = model.state_dict()
     out_dict = {}
