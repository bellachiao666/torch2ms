--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Normalization Free Nets. NFNet, NF-RegNet, NF-ResNet (pre-activation) Models
 
 Paper: `Characterizing signal propagation to close the performance gap in unnormalized ResNets`
@@ -21,8 +26,8 @@ from functools import partial
 from typing import Any, Callable, Dict, Optional, Tuple
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ClassifierHead, DropPath, calculate_drop_path_rates, AvgPool2dSame, ScaledStdConv2d, ScaledStdConv2dSame, \
@@ -61,7 +66,7 @@     act_layer: str = 'silu'
 
 
-class GammaAct(nn.Module):
+class GammaAct(msnn.Cell):
     """Activation function with gamma scaling factor."""
 
     def __init__(self, act_type: str = 'relu', gamma: float = 1.0, inplace: bool = False):
@@ -77,7 +82,7 @@         self.gamma = gamma
         self.inplace = inplace
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -104,7 +109,7 @@     return _create
 
 
-class DownsampleAvg(nn.Module):
+class DownsampleAvg(msnn.Cell):
     """AvgPool downsampling as in 'D' ResNet variants with dilation support."""
 
     def __init__(
@@ -134,10 +139,10 @@             avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
             self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
         else:
-            self.pool = nn.Identity()
+            self.pool = msnn.Identity()
         self.conv = conv_layer(in_chs, out_chs, 1, stride=1, device=device, dtype=dtype)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -150,7 +155,7 @@ 
 
 @register_notrace_module  # reason: mul_ causes FX to drop a relevant node. https://github.com/pytorch/pytorch/issues/68301
-class NormFreeBlock(nn.Module):
+class NormFreeBlock(msnn.Cell):
     """Normalization-Free pre-activation block.
     """
 
@@ -245,10 +250,10 @@             self.attn_last = attn_layer(out_chs, **dd)  # ResNet blocks apply attn after conv3
         else:
             self.attn_last = None
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
-        self.skipinit_gain = nn.Parameter(torch.tensor(0., **dd)) if skipinit else None
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
+        self.skipinit_gain = ms.Parameter(ms.Tensor(0., **dd)) if skipinit else None
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -291,7 +296,7 @@         preact_feature: bool = True,
         device=None,
         dtype=None,
-) -> Tuple[nn.Sequential, int, Dict[str, Any]]:
+) -> Tuple[msnn.SequentialCell, int, Dict[str, Any]]:
     """Create stem module for NFNet models.
 
     Args:
@@ -339,10 +344,10 @@         stem['conv'] = conv_layer(in_chs, out_chs, kernel_size=7, stride=2, **dd)
 
     if 'pool' in stem_type:
-        stem['pool'] = nn.MaxPool2d(3, stride=2, padding=1)
+        stem['pool'] = nn.MaxPool2d(3, stride = 2, padding = 1)
         stem_stride = 4
 
-    return nn.Sequential(stem), stem_stride, stem_feature
+    return msnn.SequentialCell(stem), stem_stride, stem_feature
 
 
 # from https://github.com/deepmind/deepmind-research/tree/master/nfnets
@@ -365,7 +370,7 @@ )
 
 
-class NormFreeNet(nn.Module):
+class NormFreeNet(msnn.Cell):
     """ Normalization-Free Network
 
     As described in :
@@ -483,8 +488,8 @@                 first_dilation = dilation
                 prev_chs = out_chs
             self.feature_info += [dict(num_chs=prev_chs, reduction=net_stride, module=f'stages.{stage_idx}')]
-            stages += [nn.Sequential(*blocks)]
-        self.stages = nn.Sequential(*stages)
+            stages += [msnn.SequentialCell(*blocks)]
+        self.stages = msnn.SequentialCell(*stages)
 
         if cfg.num_features:
             # The paper NFRegNet models have an EfficientNet-like final head convolution.
@@ -493,7 +498,7 @@             self.feature_info[-1] = dict(num_chs=self.num_features, reduction=net_stride, module=f'final_conv')
         else:
             self.num_features = prev_chs
-            self.final_conv = nn.Identity()
+            self.final_conv = msnn.Identity()
         self.final_act = act_layer(inplace=cfg.num_features > 0)
 
         self.head_hidden_size = self.num_features
@@ -508,15 +513,15 @@         for n, m in self.named_modules():
             if 'fc' in n and isinstance(m, nn.Linear):
                 if cfg.zero_init_fc:
-                    nn.init.zeros_(m.weight)
+                    nn.init.zeros_(m.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 else:
-                    nn.init.normal_(m.weight, 0., .01)
+                    nn.init.normal_(m.weight, 0., .01)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 if m.bias is not None:
-                    nn.init.zeros_(m.bias)
+                    nn.init.zeros_(m.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')
+                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='linear')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 if m.bias is not None:
-                    nn.init.zeros_(m.bias)
+                    nn.init.zeros_(m.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def group_matcher(self, coarse: bool = False) -> Dict[str, Any]:
@@ -536,7 +541,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         """Get the classifier head."""
         return self.head.fc
 
@@ -550,7 +555,7 @@         self.num_classes = num_classes
         self.head.reset(num_classes, global_pool)
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers.
 
         Args:
@@ -568,7 +573,7 @@         x = self.final_act(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -580,7 +585,7 @@         """
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
