--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 RDNet
 Copyright (c) 2024-present NAVER Cloud Corp.
@@ -7,8 +12,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Callable, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, calculate_drop_path_rates, NormMlpClassifierHead, ClassifierHead, EffectiveSEModule, \
@@ -21,20 +26,20 @@ __all__ = ["RDNet"]
 
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             inter_chs: int,
             out_chs: int,
-            norm_layer: Type[nn.Module],
-            act_layer: Type[nn.Module],
+            norm_layer: Type[msnn.Cell],
+            act_layer: Type[msnn.Cell],
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.layers = nn.Sequential(
+        self.layers = msnn.SequentialCell(
             nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3, **dd),
             norm_layer(in_chs, **dd),
             nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0, **dd),
@@ -42,24 +47,24 @@             nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0, **dd),
         )
 
-    def forward(self, x):
+    def construct(self, x):
         return self.layers(x)
 
 
-class BlockESE(nn.Module):
+class BlockESE(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             inter_chs: int,
             out_chs: int,
-            norm_layer: Type[nn.Module],
-            act_layer: Type[nn.Module],
+            norm_layer: Type[msnn.Cell],
+            act_layer: Type[msnn.Cell],
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.layers = nn.Sequential(
+        self.layers = msnn.SequentialCell(
             nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3, **dd),
             norm_layer(in_chs, **dd),
             nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0, **dd),
@@ -68,7 +73,7 @@             EffectiveSEModule(out_chs, **dd),
         )
 
-    def forward(self, x):
+    def construct(self, x):
         return self.layers(x)
 
 
@@ -82,7 +87,7 @@         assert False, f"Unknown block type ({block})."
 
 
-class DenseBlock(nn.Module):
+class DenseBlock(msnn.Cell):
     def __init__(
             self,
             num_input_features: int = 64,
@@ -94,8 +99,8 @@             block_idx: int = 0,
             block_type: str = "Block",
             ls_init_value: float = 1e-6,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
     ):
@@ -107,7 +112,7 @@         self.block_idx = block_idx
         self.growth_rate = growth_rate
 
-        self.gamma = nn.Parameter(ls_init_value * torch.ones(growth_rate, **dd)) if ls_init_value > 0 else None
+        self.gamma = ms.Parameter(ls_init_value * mint.ones(growth_rate, **dd)) if ls_init_value > 0 else None
         growth_rate = int(growth_rate)
         inter_chs = int(num_input_features * bottleneck_width_ratio / 8) * 8
 
@@ -122,8 +127,8 @@             **dd,
         )
 
-    def forward(self, x: List[torch.Tensor]) -> torch.Tensor:
-        x = torch.cat(x, 1)
+    def construct(self, x: List[ms.Tensor]) -> ms.Tensor:
+        x = mint.cat(x, 1)
         x = self.layers(x)
 
         if self.gamma is not None:
@@ -133,7 +138,7 @@         return x
 
 
-class DenseStage(nn.Sequential):
+class DenseStage(msnn.SequentialCell):
     def __init__(
             self,
             num_block: int,
@@ -159,15 +164,15 @@             self.add_module(f"dense_block{i}", layer)
         self.num_out_features = num_input_features
 
-    def forward(self, init_feature: torch.Tensor) -> torch.Tensor:
+    def forward(self, init_feature: ms.Tensor) -> ms.Tensor:
         features = [init_feature]
         for module in self:
             new_feature = module(features)
             features.append(new_feature)
-        return torch.cat(features, 1)
-
-
-class RDNet(nn.Module):
+        return mint.cat(features, 1)
+
+
+class RDNet(msnn.Cell):
     def __init__(
             self,
             in_chans: int = 3,  # timm option [--in-chans]
@@ -232,14 +237,14 @@         assert stem_type in ('patch', 'overlap', 'overlap_tiered')
         if stem_type == 'patch':
             # NOTE: this stem is a minimal form of ViT PatchEmbed, as used in SwinTransformer w/ patch_size = 4
-            self.stem = nn.Sequential(
+            self.stem = msnn.SequentialCell(
                 nn.Conv2d(in_chans, num_init_features, kernel_size=patch_size, stride=patch_size, bias=conv_bias, **dd),
                 norm_layer(num_init_features, **dd),
             )
             stem_stride = patch_size
         else:
             mid_chs = make_divisible(num_init_features // 2) if 'tiered' in stem_type else num_init_features
-            self.stem = nn.Sequential(
+            self.stem = msnn.SequentialCell(
                 nn.Conv2d(in_chans, mid_chs, kernel_size=3, stride=2, padding=1, bias=conv_bias, **dd),
                 nn.Conv2d(mid_chs, num_init_features, kernel_size=3, stride=2, padding=1, bias=conv_bias, **dd),
                 norm_layer(num_init_features, **dd),
@@ -299,8 +304,8 @@                         growth_rate=growth_rates[i],
                     )
                 ]
-            dense_stages.append(nn.Sequential(*dense_stage_layers))
-        self.dense_stages = nn.Sequential(*dense_stages)
+            dense_stages.append(msnn.SequentialCell(*dense_stage_layers))
+        self.dense_stages = msnn.SequentialCell(*dense_stages)
         self.num_features = self.head_hidden_size = num_features
 
         # if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets
@@ -315,7 +320,7 @@                 **dd,
             )
         else:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
             self.head = NormMlpClassifierHead(
                 self.num_features,
                 num_classes,
@@ -341,7 +346,7 @@             s.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -350,13 +355,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -412,7 +417,7 @@         max_index = stage_ends[max_index]
         self.dense_stages = self.dense_stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
@@ -426,7 +431,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -434,12 +439,12 @@ 
 def _init_weights(module, name=None, head_init_scale=1.0):
     if isinstance(module, nn.Conv2d):
-        nn.init.kaiming_normal_(module.weight)
+        nn.init.kaiming_normal_(module.weight)  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.BatchNorm2d):
-        nn.init.constant_(module.weight, 1)
-        nn.init.constant_(module.bias, 0)
+        nn.init.constant_(module.weight, 1)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.constant_(module.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Linear):
-        nn.init.constant_(module.bias, 0)
+        nn.init.constant_(module.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if name and 'head.' in name:
             module.weight.data.mul_(head_init_scale)
             module.bias.data.mul_(head_init_scale)
