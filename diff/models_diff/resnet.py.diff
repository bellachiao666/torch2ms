--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """PyTorch ResNet
 
 This started as a copy of https://github.com/pytorch/vision 'resnet.py' (BSD-3-Clause) with
@@ -11,9 +16,8 @@ from functools import partial
 from typing import Any, Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropBlock2d, DropPath, AvgPool2dSame, BlurPool2d, LayerType, create_attn, \
@@ -31,7 +35,7 @@     return padding
 
 
-class BasicBlock(nn.Module):
+class BasicBlock(msnn.Cell):
     """Basic residual block for ResNet.
 
     This is the standard residual block used in ResNet-18 and ResNet-34.
@@ -43,18 +47,18 @@             inplanes: int,
             planes: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             cardinality: int = 1,
             base_width: int = 64,
             reduce_first: int = 1,
             dilation: int = 1,
             first_dilation: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            attn_layer: Optional[Type[nn.Module]] = None,
-            aa_layer: Optional[Type[nn.Module]] = None,
-            drop_block: Optional[Type[nn.Module]] = None,
-            drop_path: Optional[nn.Module] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            attn_layer: Optional[Type[msnn.Cell]] = None,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
+            drop_block: Optional[Type[msnn.Cell]] = None,
+            drop_path: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ) -> None:
@@ -97,7 +101,7 @@             **dd,
         )
         self.bn1 = norm_layer(first_planes, **dd)
-        self.drop_block = drop_block() if drop_block is not None else nn.Identity()
+        self.drop_block = drop_block() if drop_block is not None else msnn.Identity()
         self.act1 = act_layer(inplace=True)
         self.aa = create_aa(aa_layer, channels=first_planes, stride=stride, enable=use_aa, **dd)
 
@@ -123,9 +127,9 @@     def zero_init_last(self) -> None:
         """Initialize the last batch norm layer weights to zero for better convergence."""
         if getattr(self.bn2, 'weight', None) is not None:
-            nn.init.zeros_(self.bn2.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            nn.init.zeros_(self.bn2.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = x
 
         x = self.conv1(x)
@@ -151,7 +155,7 @@         return x
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(msnn.Cell):
     """Bottleneck residual block for ResNet.
 
     This is the bottleneck block used in ResNet-50, ResNet-101, and ResNet-152.
@@ -163,18 +167,18 @@             inplanes: int,
             planes: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             cardinality: int = 1,
             base_width: int = 64,
             reduce_first: int = 1,
             dilation: int = 1,
             first_dilation: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            attn_layer: Optional[Type[nn.Module]] = None,
-            aa_layer: Optional[Type[nn.Module]] = None,
-            drop_block: Optional[Type[nn.Module]] = None,
-            drop_path: Optional[nn.Module] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            attn_layer: Optional[Type[msnn.Cell]] = None,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
+            drop_block: Optional[Type[msnn.Cell]] = None,
+            drop_path: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ) -> None:
@@ -221,7 +225,7 @@             **dd,
         )
         self.bn2 = norm_layer(width, **dd)
-        self.drop_block = drop_block() if drop_block is not None else nn.Identity()
+        self.drop_block = drop_block() if drop_block is not None else msnn.Identity()
         self.act2 = act_layer(inplace=True)
         self.aa = create_aa(aa_layer, channels=width, stride=stride, enable=use_aa, **dd)
 
@@ -239,9 +243,9 @@     def zero_init_last(self) -> None:
         """Initialize the last batch norm layer weights to zero for better convergence."""
         if getattr(self.bn3, 'weight', None) is not None:
-            nn.init.zeros_(self.bn3.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            nn.init.zeros_(self.bn3.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = x
 
         x = self.conv1(x)
@@ -278,17 +282,17 @@         stride: int = 1,
         dilation: int = 1,
         first_dilation: Optional[int] = None,
-        norm_layer: Optional[Type[nn.Module]] = None,
+        norm_layer: Optional[Type[msnn.Cell]] = None,
         device=None,
         dtype=None,
-) -> nn.Module:
+) -> msnn.Cell:
     dd = {'device': device, 'dtype': dtype}
     norm_layer = norm_layer or nn.BatchNorm2d
     kernel_size = 1 if stride == 1 and dilation == 1 else kernel_size
     first_dilation = (first_dilation or dilation) if kernel_size > 1 else 1
     p = get_padding(kernel_size, stride, first_dilation)
 
-    return nn.Sequential(*[
+    return msnn.SequentialCell(*[
         nn.Conv2d(
             in_channels,
             out_channels,
@@ -310,20 +314,20 @@         stride: int = 1,
         dilation: int = 1,
         first_dilation: Optional[int] = None,
-        norm_layer: Optional[Type[nn.Module]] = None,
+        norm_layer: Optional[Type[msnn.Cell]] = None,
         device=None,
         dtype=None,
-) -> nn.Module:
+) -> msnn.Cell:
     dd = {'device': device, 'dtype': dtype}
     norm_layer = norm_layer or nn.BatchNorm2d
     avg_stride = stride if dilation == 1 else 1
     if stride == 1 and dilation == 1:
-        pool = nn.Identity()
+        pool = msnn.Identity()
     else:
         avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
         pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
 
-    return nn.Sequential(*[
+    return msnn.SequentialCell(*[
         pool,
         nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False, **dd),
         norm_layer(out_channels, **dd)
@@ -359,7 +363,7 @@         device=None,
         dtype=None,
         **kwargs,
-) -> Tuple[List[Tuple[str, nn.Module]], List[Dict[str, Any]]]:
+) -> Tuple[List[Tuple[str, msnn.Cell]], List[Dict[str, Any]]]:
     """Create ResNet stages with specified block configurations.
 
     Args:
@@ -428,13 +432,13 @@             inplanes = planes * block_fn.expansion
             net_block_idx += 1
 
-        stages.append((stage_name, nn.Sequential(*blocks)))
+        stages.append((stage_name, msnn.SequentialCell(*blocks)))
         feature_info.append(dict(num_chs=inplanes, reduction=net_stride, module=stage_name))
 
     return stages, feature_info
 
 
-class ResNet(nn.Module):
+class ResNet(msnn.Cell):
     """ResNet / ResNeXt / SE-ResNeXt / SE-Net
 
     This class implements all variants of ResNet, ResNeXt, SE-ResNeXt, and SENet that
@@ -485,7 +489,7 @@             channels: Optional[Tuple[int, ...]] = (64, 128, 256, 512),
             act_layer: LayerType = nn.ReLU,
             norm_layer: LayerType = nn.BatchNorm2d,
-            aa_layer: Optional[Type[nn.Module]] = None,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
             drop_rate: float = 0.0,
             drop_path_rate: float = 0.,
             drop_block_rate: float = 0.,
@@ -542,7 +546,7 @@             stem_chs = (stem_width, stem_width)
             if 'tiered' in stem_type:
                 stem_chs = (3 * (stem_width // 4), stem_width)
-            self.conv1 = nn.Sequential(*[
+            self.conv1 = msnn.SequentialCell(*[
                 nn.Conv2d(in_chans, stem_chs[0], 3, stride=2, padding=1, bias=False, **dd),
                 norm_layer(stem_chs[0], **dd),
                 act_layer(inplace=True),
@@ -558,7 +562,7 @@ 
         # Stem pooling. The name 'maxpool' remains for weight compatibility.
         if replace_stem_pool:
-            self.maxpool = nn.Sequential(*filter(None, [
+            self.maxpool = msnn.SequentialCell(*filter(None, [
                 nn.Conv2d(inplanes, inplanes, 3, stride=1 if aa_layer else 2, padding=1, bias=False, **dd),
                 create_aa(aa_layer, channels=inplanes, stride=2, **dd) if aa_layer is not None else None,
                 norm_layer(inplanes, **dd),
@@ -569,11 +573,11 @@                 if issubclass(aa_layer, nn.AvgPool2d):
                     self.maxpool = aa_layer(2)
                 else:
-                    self.maxpool = nn.Sequential(*[
-                        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
+                    self.maxpool = msnn.SequentialCell(*[
+                        nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1),
                         aa_layer(channels=inplanes, stride=2, **dd)])
             else:
-                self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+                self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
 
         # Feature Blocks
         block_fns = to_ntuple(len(channels))(block)
@@ -615,7 +619,7 @@         """
         for n, m in self.named_modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if zero_init_last:
             for m in self.modules():
                 if hasattr(m, 'zero_init_last'):
@@ -644,7 +648,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self, name_only: bool = False) -> Union[str, nn.Module]:
+    def get_classifier(self, name_only: bool = False) -> Union[str, msnn.Cell]:
         """Get the classifier module.
 
         Args:
@@ -667,13 +671,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """Forward features that returns intermediates.
 
         Args:
@@ -734,12 +738,12 @@         layer_names = ('layer1', 'layer2', 'layer3', 'layer4')
         layer_names = layer_names[max_index:]
         for n in layer_names:
-            setattr(self, n, nn.Identity())
+            setattr(self, n, msnn.Identity())
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers."""
         x = self.conv1(x)
         x = self.bn1(x)
@@ -755,7 +759,7 @@             x = self.layer4(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -767,10 +771,10 @@         """
         x = self.global_pool(x)
         if self.drop_rate:
-            x = F.dropout(x, p=float(self.drop_rate), training=self.training)
+            x = nn.functional.dropout(x, p = float(self.drop_rate), training = self.training)
         return x if pre_logits else self.fc(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = self.forward_features(x)
         x = self.forward_head(x)
