--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 SEResNet implementation from Cadene's pretrained models
 https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py
@@ -15,9 +20,8 @@ from collections import OrderedDict
 from typing import Type, Optional, Tuple
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
@@ -29,23 +33,23 @@ 
 def _weight_init(m):
     if isinstance(m, nn.Conv2d):
-        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(m, nn.BatchNorm2d):
-        nn.init.constant_(m.weight, 1.)
-        nn.init.constant_(m.bias, 0.)
-
-
-class SEModule(nn.Module):
+        nn.init.constant_(m.weight, 1.)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.constant_(m.bias, 0.)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+
+class SEModule(msnn.Cell):
 
     def __init__(self, channels: int, reduction: int, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, **dd)
-        self.relu = nn.ReLU(inplace=True)
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, **dd)
         self.sigmoid = nn.Sigmoid()
 
-    def forward(self, x):
+    def construct(self, x):
         module_input = x
         x = x.mean((2, 3), keepdim=True)
         x = self.fc1(x)
@@ -55,12 +59,12 @@         return module_input * x
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(msnn.Cell):
     """
     Base class for bottlenecks that implements `forward()` method.
     """
 
-    def forward(self, x):
+    def construct(self, x):
         shortcut = x
 
         out = self.conv1(x)
@@ -96,7 +100,7 @@             groups: int,
             reduction: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
@@ -117,7 +121,7 @@         self.bn2 = nn.BatchNorm2d(planes * 4, **dd)
         self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1, bias=False, **dd)
         self.bn3 = nn.BatchNorm2d(planes * 4, **dd)
-        self.relu = nn.ReLU(inplace=True)
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.se_module = SEModule(planes * 4, reduction=reduction, **dd)
         self.downsample = downsample
         self.stride = stride
@@ -138,7 +142,7 @@             groups: int,
             reduction: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
@@ -150,7 +154,7 @@         self.bn2 = nn.BatchNorm2d(planes, **dd)
         self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False, **dd)
         self.bn3 = nn.BatchNorm2d(planes * 4, **dd)
-        self.relu = nn.ReLU(inplace=True)
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.se_module = SEModule(planes * 4, reduction=reduction, **dd)
         self.downsample = downsample
         self.stride = stride
@@ -169,7 +173,7 @@             groups: int,
             reduction: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             base_width: int = 4,
             device=None,
             dtype=None,
@@ -183,13 +187,13 @@         self.bn2 = nn.BatchNorm2d(width, **dd)
         self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False, **dd)
         self.bn3 = nn.BatchNorm2d(planes * 4, **dd)
-        self.relu = nn.ReLU(inplace=True)
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.se_module = SEModule(planes * 4, reduction=reduction, **dd)
         self.downsample = downsample
         self.stride = stride
 
 
-class SEResNetBlock(nn.Module):
+class SEResNetBlock(msnn.Cell):
     expansion = 1
 
     def __init__(
@@ -199,7 +203,7 @@             groups: int,
             reduction: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
@@ -209,12 +213,12 @@         self.bn1 = nn.BatchNorm2d(planes, **dd)
         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False, **dd)
         self.bn2 = nn.BatchNorm2d(planes, **dd)
-        self.relu = nn.ReLU(inplace=True)
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.se_module = SEModule(planes, reduction=reduction, **dd)
         self.downsample = downsample
         self.stride = stride
 
-    def forward(self, x):
+    def construct(self, x):
         shortcut = x
 
         out = self.conv1(x)
@@ -234,11 +238,11 @@         return out
 
 
-class SENet(nn.Module):
+class SENet(msnn.Cell):
 
     def __init__(
             self,
-            block: Type[nn.Module],
+            block: Type[msnn.Cell],
             layers: Tuple[int, ...],
             groups: int,
             reduction: int,
@@ -305,23 +309,23 @@             layer0_modules = [
                 ('conv1', nn.Conv2d(in_chans, 64, 3, stride=2, padding=1, bias=False, **dd)),
                 ('bn1', nn.BatchNorm2d(64, **dd)),
-                ('relu1', nn.ReLU(inplace=True)),
+                ('relu1', nn.ReLU()),
                 ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=False, **dd)),
                 ('bn2', nn.BatchNorm2d(64, **dd)),
-                ('relu2', nn.ReLU(inplace=True)),
+                ('relu2', nn.ReLU()),
                 ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1, bias=False, **dd)),
                 ('bn3', nn.BatchNorm2d(inplanes, **dd)),
-                ('relu3', nn.ReLU(inplace=True)),
-            ]
+                ('relu3', nn.ReLU()),
+            ]  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         else:
             layer0_modules = [
                 ('conv1', nn.Conv2d(in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False, **dd)),
                 ('bn1', nn.BatchNorm2d(inplanes, **dd)),
-                ('relu1', nn.ReLU(inplace=True)),
-            ]
-        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))
+                ('relu1', nn.ReLU()),
+            ]  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.layer0 = msnn.SequentialCell(OrderedDict(layer0_modules))
         # To preserve compatibility with Caffe weights `ceil_mode=True` is used instead of `padding=1`.
-        self.pool0 = nn.MaxPool2d(3, stride=2, ceil_mode=True)
+        self.pool0 = nn.MaxPool2d(3, stride = 2, ceil_mode = True)
         self.feature_info = [dict(num_chs=inplanes, reduction=2, module='layer0')]
         self.layer1 = self._make_layer(
             block,
@@ -386,7 +390,7 @@         dd = {'device': device, 'dtype': dtype}
         downsample = None
         if stride != 1 or self.inplanes != planes * block.expansion:
-            downsample = nn.Sequential(
+            downsample = msnn.SequentialCell(
                 nn.Conv2d(
                     self.inplanes, planes * block.expansion, kernel_size=downsample_kernel_size,
                     stride=stride, padding=downsample_padding, bias=False, **dd),
@@ -398,7 +402,7 @@         for i in range(1, blocks):
             layers.append(block(self.inplanes, planes, groups, reduction, **dd))
 
-        return nn.Sequential(*layers)
+        return msnn.SequentialCell(*layers)
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -410,7 +414,7 @@         assert not enable, 'gradient checkpointing not supported'
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.last_linear
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -430,10 +434,10 @@     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         if self.drop_rate > 0.:
-            x = F.dropout(x, p=self.drop_rate, training=self.training)
+            x = nn.functional.dropout(x, p = self.drop_rate, training = self.training)
         return x if pre_logits else self.last_linear(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
