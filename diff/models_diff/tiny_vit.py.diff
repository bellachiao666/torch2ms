--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ TinyViT
 
 Paper: `TinyViT: Fast Pretraining Distillation for Small Vision Transformers`
@@ -12,9 +17,9 @@ from functools import partial
 from typing import Dict, List, Optional, Tuple, Union, Type, Any
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import LayerNorm2d, NormMlpClassifierHead, DropPath,\
@@ -26,7 +31,7 @@ from ._registry import register_model, generate_default_cfgs
 
 
-class ConvNorm(torch.nn.Sequential):
+class ConvNorm(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
@@ -44,30 +49,28 @@         super().__init__()
         self.conv = nn.Conv2d(in_chs, out_chs, ks, stride, pad, dilation, groups, bias=False, **dd)
         self.bn = nn.BatchNorm2d(out_chs, **dd)
-        torch.nn.init.constant_(self.bn.weight, bn_weight_init)
-        torch.nn.init.constant_(self.bn.bias, 0)
-
-    @torch.no_grad()
+        torch.nn.init.constant_(self.bn.weight, bn_weight_init)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch.nn.init.constant_(self.bn.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
     def fuse(self):
         c, bn = self.conv, self.bn
         w = bn.weight / (bn.running_var + bn.eps) ** 0.5
         w = c.weight * w[:, None, None, None]
         b = bn.bias - bn.running_mean * bn.weight / \
             (bn.running_var + bn.eps) ** 0.5
-        m = torch.nn.Conv2d(
-            w.size(1) * self.conv.groups, w.size(0), w.shape[2:],
-            stride=self.conv.stride, padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups)
+        m = nn.Conv2d(
+            w.size(1) * self.conv.groups, w.size(0), w.shape[2:], stride = self.conv.stride, padding = self.conv.padding, dilation = self.conv.dilation, groups = self.conv.groups)
         m.weight.data.copy_(w)
         m.bias.data.copy_(b)
         return m
 
 
-class PatchEmbed(nn.Module):
+class PatchEmbed(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             device=None,
             dtype=None,
     ):
@@ -78,20 +81,20 @@         self.act = act_layer()
         self.conv2 = ConvNorm(out_chs // 2, out_chs, 3, 2, 1, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         x = self.act(x)
         x = self.conv2(x)
         return x
 
 
-class MBConv(nn.Module):
+class MBConv(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             expand_ratio: float,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             drop_path: float,
             device=None,
             dtype=None,
@@ -105,9 +108,9 @@         self.act2 = act_layer()
         self.conv3 = ConvNorm(mid_chs, out_chs, ks=1, bn_weight_init=0.0, **dd)
         self.act3 = act_layer()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x
         x = self.conv1(x)
         x = self.act1(x)
@@ -120,12 +123,12 @@         return x
 
 
-class PatchMerging(nn.Module):
+class PatchMerging(msnn.Cell):
     def __init__(
             self,
             dim: int,
             out_dim: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             device=None,
             dtype=None,
     ):
@@ -137,7 +140,7 @@         self.act2 = act_layer()
         self.conv3 = ConvNorm(out_dim, out_dim, 1, 1, 0, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         x = self.act1(x)
         x = self.conv2(x)
@@ -146,12 +149,12 @@         return x
 
 
-class ConvLayer(nn.Module):
+class ConvLayer(msnn.Cell):
     def __init__(
             self,
             dim: int,
             depth: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             drop_path: Union[float, List[float]] = 0.,
             conv_expand_ratio: float = 4.,
             device=None,
@@ -161,7 +164,7 @@         super().__init__()
         self.dim = dim
         self.depth = depth
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell(*[
             MBConv(
                 dim,
                 dim,
@@ -173,19 +176,19 @@             for i in range(depth)
         ])
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.blocks(x)
         return x
 
 
-class NormMlp(nn.Module):
+class NormMlp(msnn.Cell):
     def __init__(
             self,
             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop: float = 0.,
             device=None,
             dtype=None,
@@ -201,7 +204,7 @@         self.fc2 = nn.Linear(hidden_features, out_features, **dd)
         self.drop2 = nn.Dropout(drop)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.norm(x)
         x = self.fc1(x)
         x = self.act(x)
@@ -211,9 +214,9 @@         return x
 
 
-class Attention(torch.nn.Module):
+class Attention(msnn.Cell):
     fused_attn: torch.jit.Final[bool]
-    attention_bias_cache: Dict[str, torch.Tensor]
+    attention_bias_cache: Dict[str, ms.Tensor]
 
     def __init__(
             self,
@@ -251,21 +254,22 @@                 if offset not in attention_offsets:
                     attention_offsets[offset] = len(attention_offsets)
                 idxs.append(attention_offsets[offset])
-        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, len(attention_offsets), **dd))
+        self.attention_biases = ms.Parameter(mint.zeros(num_heads, len(attention_offsets), **dd))
         self.register_buffer(
             'attention_bias_idxs',
-            torch.tensor(idxs, device=device, dtype=torch.long).view(N, N),
+            ms.Tensor(idxs, device=device, dtype=ms.int64).view(N, N),
             persistent=False,
         )
         self.attention_bias_cache = {}
 
-    @torch.no_grad()
     def train(self, mode=True):
         super().train(mode)
         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def get_attention_biases(self, device: torch.device) -> ms.Tensor:
         if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
@@ -274,7 +278,7 @@                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
-    def forward(self, x):
+    def construct(self, x):
         attn_bias = self.get_attention_biases(x.device)
         B, N, _ = x.shape
         # Normalization
@@ -288,7 +292,7 @@         v = v.permute(0, 2, 1, 3)
 
         if self.fused_attn:
-            x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_bias)
+            x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_bias)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
@@ -300,7 +304,7 @@         return x
 
 
-class TinyVitBlock(nn.Module):
+class TinyVitBlock(msnn.Cell):
     """ TinyViT Block.
 
     Args:
@@ -324,7 +328,7 @@             drop: float = 0.,
             drop_path: float = 0.,
             local_conv_size: int = 3,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
     ):
@@ -348,7 +352,7 @@             resolution=window_resolution,
             **dd,
         )
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.mlp = NormMlp(
             in_features=dim,
@@ -357,12 +361,12 @@             drop=drop,
             **dd,
         )
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         pad = local_conv_size // 2
         self.local_conv = ConvNorm(dim, dim, ks=local_conv_size, stride=1, pad=pad, groups=dim, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         B, H, W, C = x.shape
         L = H * W
 
@@ -376,7 +380,7 @@             pad_r = (self.window_size - W % self.window_size) % self.window_size
             padding = pad_b > 0 or pad_r > 0
             if padding:
-                x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))
+                x = nn.functional.pad(x, (0, 0, 0, pad_r, 0, pad_b))
 
             # window partition
             pH, pW = H + pad_b, W + pad_r
@@ -410,7 +414,7 @@ register_notrace_module(TinyVitBlock)
 
 
-class TinyVitStage(nn.Module):
+class TinyVitStage(msnn.Cell):
     """ A basic TinyViT layer for one stage.
 
     Args:
@@ -437,9 +441,9 @@             mlp_ratio: float = 4.,
             drop: float = 0.,
             drop_path: Union[float, List[float]] = 0.,
-            downsample: Optional[Type[nn.Module]] = None,
+            downsample: Optional[Type[msnn.Cell]] = None,
             local_conv_size: int = 3,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
     ):
@@ -457,11 +461,11 @@                 **dd,
             )
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
             assert dim == out_dim
 
         # build blocks
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell(*[
             TinyVitBlock(
                 dim=out_dim,
                 num_heads=num_heads,
@@ -475,7 +479,7 @@             )
             for i in range(depth)])
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.downsample(x)
         x = x.permute(0, 2, 3, 1)  # BCHW -> BHWC
         x = self.blocks(x)
@@ -486,7 +490,7 @@         return f"dim={self.out_dim}, depth={self.depth}"
 
 
-class TinyVit(nn.Module):
+class TinyVit(msnn.Cell):
     def __init__(
             self,
             in_chans: int = 3,
@@ -502,7 +506,7 @@             use_checkpoint: bool = False,
             mbconv_expand_ratio: float = 4.0,
             local_conv_size: int = 3,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
     ):
@@ -526,7 +530,7 @@         dpr = calculate_drop_path_rates(drop_path_rate, sum(depths))
 
         # build stages
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         stride = self.patch_embed.stride
         prev_dim = embed_dims[0]
         self.feature_info = []
@@ -581,7 +585,7 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def no_weight_decay_keywords(self):
@@ -607,7 +611,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -616,13 +620,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -685,7 +689,7 @@         x = self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
         return x
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
