--- pytorch+++ mindspore@@ -1,4 +1,9 @@ #!/usr/bin/env python3
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Checkpoint Averaging Script
 
 This script averages all model weights for checkpoints in specified path that match
@@ -11,14 +16,14 @@ 
 Hacked together by / Copyright 2020 Ross Wightman (https://github.com/rwightman)
 """
-import torch
+# import torch
 import argparse
 import os
 import glob
 import hashlib
 from timm.models import load_state_dict
 try:
-    import safetensors.torch
+    # import safetensors.torch
     _has_safetensors = True
 except ImportError:
     _has_safetensors = False
@@ -47,7 +52,7 @@     if not checkpoint_path or not os.path.isfile(checkpoint_path):
         return {}
     print("=> Extracting metric from checkpoint '{}'".format(checkpoint_path))
-    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
+    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)  # 'torch.load' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     metric = None
     if 'metric' in checkpoint:
         metric = checkpoint['metric']
@@ -121,27 +126,27 @@             continue
         for k, v in new_state_dict.items():
             if k not in avg_state_dict:
-                avg_state_dict[k] = v.clone().to(dtype=torch.float64)
+                avg_state_dict[k] = v.clone().to(dtype=ms.float64)
                 avg_counts[k] = 1
             else:
-                avg_state_dict[k] += v.to(dtype=torch.float64)
+                avg_state_dict[k] += v.to(dtype=ms.float64)
                 avg_counts[k] += 1
 
     for k, v in avg_state_dict.items():
         v.div_(avg_counts[k])
 
     # float32 overflow seems unlikely based on weights seen to date, but who knows
-    float32_info = torch.finfo(torch.float32)
+    float32_info = torch.finfo(ms.float32)  # 'torch.finfo' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     final_state_dict = {}
     for k, v in avg_state_dict.items():
         v = v.clamp(float32_info.min, float32_info.max)
-        final_state_dict[k] = v.to(dtype=torch.float32)
+        final_state_dict[k] = v.to(dtype=ms.float32)
 
     if args.safetensors:
         assert _has_safetensors, "`pip install safetensors` to use .safetensors"
         safetensors.torch.save_file(final_state_dict, output)
     else:
-        torch.save(final_state_dict, output)
+        torch.save(final_state_dict, output)  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     with open(output, 'rb') as f:
         sha_hash = hashlib.sha256(f.read()).hexdigest()
