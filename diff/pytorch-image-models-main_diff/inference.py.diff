--- pytorch+++ mindspore@@ -1,4 +1,9 @@ #!/usr/bin/env python3
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """PyTorch Inference Script
 
 An example inference script that outputs top-k class ids for images in a folder into a csv.
@@ -16,7 +21,7 @@ 
 import numpy as np
 import pandas as pd
-import torch
+# import torch
 
 from timm.data import create_dataset, create_loader, resolve_data_config, ImageNetInfo, infer_imagenet_subset
 from timm.layers import apply_test_time_pool
@@ -24,7 +29,7 @@ from timm.utils import AverageMeter, setup_default_logging, set_jit_fuser, ParseKwargs
 
 try:
-    from functorch.compile import memory_efficient_fusion
+    # from functorch.compile import memory_efficient_fusion
     has_functorch = True
 except ImportError as e:
     has_functorch = False
@@ -157,7 +162,7 @@         torch.backends.cuda.matmul.allow_tf32 = True
         torch.backends.cudnn.benchmark = True
 
-    device = torch.device(args.device)
+    device = torch.device(args.device)  # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     model_dtype = None
     if args.model_dtype:
@@ -167,9 +172,9 @@     # resolve AMP arguments based on PyTorch availability
     amp_autocast = suppress
     if args.amp:
-        assert model_dtype is None or model_dtype == torch.float32, 'float32 model dtype must be used with AMP'
+        assert model_dtype is None or model_dtype == ms.float32, 'float32 model dtype must be used with AMP'
         assert args.amp_dtype in ('float16', 'bfloat16')
-        amp_dtype = torch.bfloat16 if args.amp_dtype == 'bfloat16' else torch.float16
+        amp_dtype = ms.bfloat16 if args.amp_dtype == 'bfloat16' else ms.float16
         amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)
         _logger.info('Running inference in mixed precision with native PyTorch AMP.')
     else:
@@ -211,17 +216,17 @@         model = model.to(memory_format=torch.channels_last)
 
     if args.torchscript:
-        model = torch.jit.script(model)
+        model = torch.jit.script(model)  # 'torch.jit.script' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif args.torchcompile:
         assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'
-        torch._dynamo.reset()
-        model = torch.compile(model, backend=args.torchcompile, mode=args.torchcompile_mode)
+        torch._dynamo.reset()  # 'torch._dynamo.reset' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        model = torch.compile(model, backend=args.torchcompile, mode=args.torchcompile_mode)  # 'torch.compile' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif args.aot_autograd:
         assert has_functorch, "functorch is needed for --aot-autograd"
         model = memory_efficient_fusion(model)
 
     if args.num_gpu > 1:
-        model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpu)))
+        model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpu)))  # 'torch.nn.DataParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     root_dir = args.data or args.data_dir
     dataset = create_dataset(
@@ -241,7 +246,7 @@         use_prefetcher=True,
         num_workers=workers,
         device=device,
-        img_dtype=model_dtype or torch.float32,
+        img_dtype=model_dtype or ms.float32,
         **data_config,
     )
 
