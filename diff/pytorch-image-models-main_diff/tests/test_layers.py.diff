--- pytorch+++ mindspore@@ -1,6 +1,10 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import pytest
-import torch
-import torch.nn as nn
+# import torch.nn as nn
 
 from timm.layers import create_act_layer, set_layer_config, get_act_layer, get_act_fn, Attention2d, MultiQueryAttentionV2
 
@@ -12,14 +16,14 @@     importlib.import_module(torch_backend)
 torch_device = os.environ.get('TORCH_DEVICE', 'cpu')
 
-class MLP(nn.Module):
+class MLP(msnn.Cell):
     def __init__(self, act_layer="relu", inplace=True):
         super(MLP, self).__init__()
         self.fc1 = nn.Linear(1000, 100)
         self.act = create_act_layer(act_layer, inplace=inplace)
         self.fc2 = nn.Linear(100, 10)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.fc1(x)
         x = self.act(x)
         x = self.fc2(x)
@@ -27,7 +31,7 @@ 
 
 def _run_act_layer_grad(act_type, inplace=True):
-    x = torch.rand(10, 1000) * 10
+    x = mint.rand(size = (10, 1000)) * 10
     m = MLP(act_layer=act_type, inplace=inplace)
 
     def _run(x, act_layer=''):
@@ -46,12 +50,12 @@     with set_layer_config(scriptable=True):
         out_jit = _run(x, act_type)
 
-    assert torch.isclose(out_jit, out_me)
+    assert mint.isclose(out_jit, out_me)
 
     with set_layer_config(no_jit=True):
         out_basic = _run(x, act_type)
 
-    assert torch.isclose(out_basic, out_jit)
+    assert mint.isclose(out_basic, out_jit)
 
 
 def test_swish_grad():
@@ -84,10 +88,10 @@ 
 
 def test_create_act_layer_inplace_error():
-    class NoInplaceAct(nn.Module):
+    class NoInplaceAct(msnn.Cell):
         def __init__(self):
             super().__init__()
-        def forward(self, x):
+        def construct(self, x):
             return x
     
     # Should recover when inplace arg causes TypeError
@@ -100,10 +104,10 @@     assert create_act_layer(None) is None
     
     # Test TypeError handling for inplace
-    class CustomAct(nn.Module):
+    class CustomAct(msnn.Cell):
         def __init__(self, **kwargs):
             super().__init__()
-        def forward(self, x):
+        def construct(self, x):
             return x
             
     result = create_act_layer(CustomAct, inplace=True)
@@ -127,9 +131,9 @@ def test_mqa_v2(dim, dim_out, use_m):
     mqa = MultiQueryAttentionV2(dim, dim_out)
     
-    x = torch.randn(1, dim, 32, 48)
+    x = mint.randn(size = (1, dim, 32, 48))
     if use_m:
-        m = torch.randn(1, dim, 16, 24)
+        m = mint.randn(size = (1, dim, 16, 24))
     else:
         m = None
         
@@ -143,13 +147,13 @@ @pytest.mark.parametrize("head_first", [True, False])
 @pytest.mark.parametrize("attn_mask", [True, False])
 def test_attn2d(bias, expand_first, head_first, attn_mask):
-    x = torch.randn(1, 128, 32, 48)
+    x = mint.randn(size = (1, 128, 32, 48))
     attn = Attention2d(
         128, 128, num_heads=4, bias=bias, expand_first=expand_first, head_first=head_first
     )
     
     if attn_mask:
-        mask = torch.randint(0, 1, size=(32 * 48, 32 * 48), dtype=torch.float32)
+        mask = mint.randint(0, 1, size = (32 * 48, 32 * 48), dtype = ms.float32)
     else:
         mask = None
     
@@ -157,4 +161,4 @@     attn.fused_attn = False
     o2 = attn(x, mask)
     
-    assert torch.allclose(o1, o2, atol=1e-5), f"{torch.abs(o1 - o2).max()}"
+    assert mint.allclose(o1, o2, atol = 1e-5), f"{mint.abs(o1 - o2).max()}"
