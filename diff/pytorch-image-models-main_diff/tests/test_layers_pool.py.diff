--- pytorch+++ mindspore@@ -1,7 +1,11 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """Tests for timm pooling layers."""
 import pytest
-import torch
-import torch.nn as nn
+# import torch
 
 import importlib
 import os
@@ -19,35 +23,35 @@ 
     def test_adaptive_avgmax_pool2d(self):
         from timm.layers import adaptive_avgmax_pool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         out = adaptive_avgmax_pool2d(x, 1)
         assert out.shape == (2, 64, 1, 1)
         # Should be average of avg and max
         expected = 0.5 * (x.mean(dim=(2, 3), keepdim=True) + x.amax(dim=(2, 3), keepdim=True))
-        assert torch.allclose(out, expected)
+        assert mint.allclose(out, expected)
 
     def test_select_adaptive_pool2d(self):
         from timm.layers import select_adaptive_pool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
 
         out_avg = select_adaptive_pool2d(x, pool_type='avg', output_size=1)
         assert out_avg.shape == (2, 64, 1, 1)
-        assert torch.allclose(out_avg, x.mean(dim=(2, 3), keepdim=True))
+        assert mint.allclose(out_avg, x.mean(dim=(2, 3), keepdim=True))
 
         out_max = select_adaptive_pool2d(x, pool_type='max', output_size=1)
         assert out_max.shape == (2, 64, 1, 1)
-        assert torch.allclose(out_max, x.amax(dim=(2, 3), keepdim=True))
+        assert mint.allclose(out_max, x.amax(dim=(2, 3), keepdim=True))
 
     def test_adaptive_avgmax_pool2d_module(self):
         from timm.layers import AdaptiveAvgMaxPool2d
-        x = torch.randn(2, 64, 14, 14, device=torch_device)
+        x = mint.randn(size = (2, 64, 14, 14))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = AdaptiveAvgMaxPool2d(output_size=1).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64, 1, 1)
 
     def test_select_adaptive_pool2d_module(self):
         from timm.layers import SelectAdaptivePool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
 
         for pool_type in ['avg', 'max', 'avgmax', 'catavgmax']:
             pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True).to(torch_device)
@@ -59,7 +63,7 @@ 
     def test_select_adaptive_pool2d_fast(self):
         from timm.layers import SelectAdaptivePool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
 
         for pool_type in ['fast', 'fastavg', 'fastmax', 'fastavgmax', 'fastcatavgmax']:
             pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True).to(torch_device)
@@ -77,14 +81,14 @@ 
     def test_attention_pool_latent_basic(self):
         from timm.layers import AttentionPoolLatent
-        x = torch.randn(2, 49, 64, device=torch_device)
+        x = mint.randn(size = (2, 49, 64))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = AttentionPoolLatent(in_features=64, num_heads=4).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64)
 
     def test_attention_pool_latent_multi_latent(self):
         from timm.layers import AttentionPoolLatent
-        x = torch.randn(2, 49, 64, device=torch_device)
+        x = mint.randn(size = (2, 49, 64))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = AttentionPoolLatent(
             in_features=64,
             num_heads=4,
@@ -96,7 +100,7 @@ 
     def test_attention_pool2d_basic(self):
         from timm.layers import AttentionPool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = AttentionPool2d(in_features=64, feat_size=7).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64)
@@ -106,13 +110,13 @@         # Test with different spatial sizes (requires pos_embed interpolation)
         pool = AttentionPool2d(in_features=64, feat_size=7).to(torch_device)
         for size in [7, 14]:
-            x = torch.randn(2, 64, size, size, device=torch_device)
+            x = mint.randn(size = (2, 64, size, size))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
             out = pool(x)
             assert out.shape == (2, 64)
 
     def test_rot_attention_pool2d_basic(self):
         from timm.layers import RotAttentionPool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = RotAttentionPool2d(in_features=64, ref_feat_size=7).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64)
@@ -121,13 +125,13 @@         from timm.layers import RotAttentionPool2d
         pool = RotAttentionPool2d(in_features=64, ref_feat_size=7).to(torch_device)
         for size in [7, 14, 10]:
-            x = torch.randn(2, 64, size, size, device=torch_device)
+            x = mint.randn(size = (2, 64, size, size))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
             out = pool(x)
             assert out.shape == (2, 64)
 
     def test_rot_attention_pool2d_rope_types(self):
         from timm.layers import RotAttentionPool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         for rope_type in ['base', 'cat', 'dinov3']:
             pool = RotAttentionPool2d(
                 in_features=64,
@@ -145,7 +149,7 @@ 
     def test_lse_plus_2d_basic(self):
         from timm.layers import LsePlus2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = LsePlus2d().to(torch_device)
         out = pool(x)
         # Default is flatten=True
@@ -153,37 +157,37 @@ 
     def test_lse_plus_2d_no_flatten(self):
         from timm.layers import LsePlus2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = LsePlus2d(flatten=False).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64, 1, 1)
 
     def test_lse_plus_1d_basic(self):
         from timm.layers import LsePlus1d
-        x = torch.randn(2, 49, 64, device=torch_device)
+        x = mint.randn(size = (2, 49, 64))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = LsePlus1d().to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64)
 
     def test_lse_high_r_approximates_max(self):
         from timm.layers import LsePlus2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = LsePlus2d(r=100.0, r_learnable=False).to(torch_device)
         out = pool(x)
         out_max = x.amax(dim=(2, 3))
-        assert torch.allclose(out, out_max, atol=0.1)
+        assert mint.allclose(out, out_max, atol = 0.1)
 
     def test_lse_low_r_approximates_avg(self):
         from timm.layers import LsePlus2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = LsePlus2d(r=0.01, r_learnable=False).to(torch_device)
         out = pool(x)
         out_avg = x.mean(dim=(2, 3))
-        assert torch.allclose(out, out_avg, atol=0.1)
+        assert mint.allclose(out, out_avg, atol = 0.1)
 
     def test_lse_learnable_r_gradient(self):
         from timm.layers import LsePlus2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = LsePlus2d(r=10.0, r_learnable=True).to(torch_device)
         out = pool(x).sum()
         out.backward()
@@ -198,21 +202,21 @@ 
     def test_simpool_2d_basic(self):
         from timm.layers import SimPool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = SimPool2d(dim=64).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64)
 
     def test_simpool_1d_basic(self):
         from timm.layers import SimPool1d
-        x = torch.randn(2, 49, 64, device=torch_device)
+        x = mint.randn(size = (2, 49, 64))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = SimPool1d(dim=64).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64)
 
     def test_simpool_multi_head(self):
         from timm.layers import SimPool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         for num_heads in [1, 2, 4, 8]:
             pool = SimPool2d(dim=64, num_heads=num_heads).to(torch_device)
             out = pool(x)
@@ -220,15 +224,15 @@ 
     def test_simpool_with_gamma(self):
         from timm.layers import SimPool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = SimPool2d(dim=64, gamma=2.0).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64)
-        assert not torch.isnan(out).any()
+        assert not ms.Tensor.isnan(out).any()
 
     def test_simpool_qk_norm(self):
         from timm.layers import SimPool2d
-        x = torch.randn(2, 64, 7, 7, device=torch_device)
+        x = mint.randn(size = (2, 64, 7, 7))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = SimPool2d(dim=64, qk_norm=True).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64)
@@ -251,7 +255,7 @@     ])
     def test_gradient_flow(self, pool_cls, kwargs, input_shape):
         import timm.layers as layers
-        x = torch.randn(*input_shape, device=torch_device, requires_grad=True)
+        x = mint.randn(*input_shape, device=torch_device, requires_grad=True)
         pool = getattr(layers, pool_cls)(**kwargs).to(torch_device)
         out = pool(x)
         loss = out.sum()
@@ -269,13 +273,13 @@     ])
     def test_torchscript(self, pool_cls, kwargs, input_shape):
         import timm.layers as layers
-        x = torch.randn(*input_shape, device=torch_device)
+        x = mint.randn(*input_shape, device=torch_device)
         pool = getattr(layers, pool_cls)(**kwargs).to(torch_device)
         pool.eval()
-        scripted = torch.jit.script(pool)
+        scripted = torch.jit.script(pool)  # 'torch.jit.script' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         out_orig = pool(x)
         out_script = scripted(x)
-        assert torch.allclose(out_orig, out_script, atol=1e-5)
+        assert mint.allclose(out_orig, out_script, atol = 1e-5)
 
     @pytest.mark.parametrize('pool_cls,kwargs,input_shape', [
         ('LsePlus2d', {}, (2, 64, 7, 7)),
@@ -287,13 +291,13 @@     ])
     def test_eval_deterministic(self, pool_cls, kwargs, input_shape):
         import timm.layers as layers
-        x = torch.randn(*input_shape, device=torch_device)
+        x = mint.randn(*input_shape, device=torch_device)
         pool = getattr(layers, pool_cls)(**kwargs).to(torch_device)
         pool.eval()
         with torch.no_grad():
             out1 = pool(x)
             out2 = pool(x)
-        assert torch.allclose(out1, out2)
+        assert mint.allclose(out1, out2)
 
     @pytest.mark.parametrize('pool_cls,kwargs,input_shape', [
         ('LsePlus2d', {}, (2, 64, 7, 7)),
@@ -305,7 +309,7 @@         B, C, _, _ = input_shape
         pool = getattr(layers, pool_cls)(**kwargs).to(torch_device)
         for H, W in [(7, 7), (14, 14), (1, 1), (3, 5)]:
-            x = torch.randn(B, C, H, W, device=torch_device)
+            x = mint.randn(size = (B, C, H, W))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
             out = pool(x)
             assert out.shape[0] == B
             assert out.shape[-1] == C
@@ -318,14 +322,14 @@ 
     def test_blur_pool_2d_basic(self):
         from timm.layers import BlurPool2d
-        x = torch.randn(2, 64, 14, 14, device=torch_device)
+        x = mint.randn(size = (2, 64, 14, 14))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = BlurPool2d(channels=64).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64, 7, 7)
 
     def test_blur_pool_2d_stride(self):
         from timm.layers import BlurPool2d
-        x = torch.randn(2, 64, 28, 28, device=torch_device)
+        x = mint.randn(size = (2, 64, 28, 28))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
         pool = BlurPool2d(channels=64, stride=4).to(torch_device)
         out = pool(x)
         assert out.shape == (2, 64, 8, 8)
@@ -338,21 +342,21 @@ 
     def test_global_pool_nlc(self):
         from timm.layers import global_pool_nlc
-        x = torch.randn(2, 49, 64, device=torch_device)
+        x = mint.randn(size = (2, 49, 64))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
 
         # By default, avg/max excludes first token (num_prefix_tokens=1)
         out_avg = global_pool_nlc(x, pool_type='avg')
         assert out_avg.shape == (2, 64)
-        assert torch.allclose(out_avg, x[:, 1:].mean(dim=1))
+        assert mint.allclose(out_avg, x[:, 1:].mean(dim=1))
 
         out_max = global_pool_nlc(x, pool_type='max')
         assert out_max.shape == (2, 64)
-        assert torch.allclose(out_max, x[:, 1:].amax(dim=1))
+        assert mint.allclose(out_max, x[:, 1:].amax(dim=1))
 
         out_first = global_pool_nlc(x, pool_type='token')
         assert out_first.shape == (2, 64)
-        assert torch.allclose(out_first, x[:, 0])
+        assert mint.allclose(out_first, x[:, 0])
 
         # Test with reduce_include_prefix=True
         out_avg_all = global_pool_nlc(x, pool_type='avg', reduce_include_prefix=True)
-        assert torch.allclose(out_avg_all, x.mean(dim=1))
+        assert mint.allclose(out_avg_all, x.mean(dim=1))
