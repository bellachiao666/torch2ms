--- pytorch+++ mindspore@@ -1,5 +1,10 @@-from torch.nn.modules.batchnorm import BatchNorm2d
-from torchvision.ops.misc import FrozenBatchNorm2d
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# from torch.nn.modules.batchnorm import BatchNorm2d
+# from torchvision.ops.misc import FrozenBatchNorm2d
 
 import timm
 import pytest
@@ -97,7 +102,7 @@     assert len(stats[test_hook.__name__]) > 0
 
 def test_freeze_unfreeze_bn_root():
-    import torch.nn as nn
+    # import torch.nn as nn
     from timm.layers import BatchNormAct2d
     
     # Create batch norm layers
@@ -114,10 +119,9 @@ 
 
 def test_activation_stats_functions():
-    import torch
     
     # Create sample input tensor [batch, channels, height, width]
-    x = torch.randn(2, 3, 4, 4)
+    x = mint.randn(size = (2, 3, 4, 4))
     
     # Test avg_sq_ch_mean
     result1 = avg_sq_ch_mean(None, None, x)
@@ -133,17 +137,17 @@ 
 
 def test_reparameterize_model():
-    import torch.nn as nn
+    # import torch.nn as nn
     
-    class FusableModule(nn.Module):
+    class FusableModule(msnn.Cell):
         def __init__(self):
             super().__init__()
             self.conv = nn.Conv2d(3, 3, 1)
         
         def fuse(self):
-            return nn.Identity()
+            return msnn.Identity()
     
-    class ModelWithFusable(nn.Module):
+    class ModelWithFusable(msnn.Cell):
         def __init__(self):
             super().__init__()
             self.fusable = FusableModule()
@@ -162,9 +166,9 @@ 
 
 def test_get_state_dict_custom_unwrap():
-    import torch.nn as nn
+    # import torch.nn as nn
     
-    class CustomModel(nn.Module):
+    class CustomModel(msnn.Cell):
         def __init__(self):
             super().__init__()
             self.linear = nn.Linear(10, 10)
