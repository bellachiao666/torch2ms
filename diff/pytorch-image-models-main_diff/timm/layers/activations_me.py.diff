--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Activations (memory-efficient w/ custom autograd)
 
 A collection of activations fn and modules with a common interface so that they can
@@ -9,20 +14,20 @@ Hacked together by / Copyright 2020 Ross Wightman
 """
 
-import torch
-from torch import nn as nn
-from torch.nn import functional as F
+# import torch
+# from torch import nn as nn
 
 
 def swish_fwd(x):
-    return x.mul(torch.sigmoid(x))
+    return x.mul(mint.sigmoid(x))
 
 
 def swish_bwd(x, grad_output):
-    x_sigmoid = torch.sigmoid(x)
+    x_sigmoid = mint.sigmoid(x)
     return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))
 
 
+# 'torch.autograd.Function' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class SwishAutoFn(torch.autograd.Function):
     """ optimised Swish w/ memory-efficient checkpoint
     Inspired by conversation btw Jeremy Howard & Adam Pazske
@@ -47,24 +52,25 @@     return SwishAutoFn.apply(x)
 
 
-class SwishMe(nn.Module):
-    def __init__(self, inplace: bool = False):
-        super().__init__()
-
-    def forward(self, x):
+class SwishMe(msnn.Cell):
+    def __init__(self, inplace: bool = False):
+        super().__init__()
+
+    def construct(self, x):
         return SwishAutoFn.apply(x)
 
 
 def mish_fwd(x):
-    return x.mul(torch.tanh(F.softplus(x)))
+    return x.mul(mint.tanh(nn.functional.softplus(x)))
 
 
 def mish_bwd(x, grad_output):
-    x_sigmoid = torch.sigmoid(x)
-    x_tanh_sp = F.softplus(x).tanh()
+    x_sigmoid = mint.sigmoid(x)
+    x_tanh_sp = ms.Tensor.tanh()
     return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))
 
 
+# 'torch.autograd.Function' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class MishAutoFn(torch.autograd.Function):
     """ Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681
     A memory efficient variant of Mish
@@ -84,11 +90,11 @@     return MishAutoFn.apply(x)
 
 
-class MishMe(nn.Module):
-    def __init__(self, inplace: bool = False):
-        super().__init__()
-
-    def forward(self, x):
+class MishMe(msnn.Cell):
+    def __init__(self, inplace: bool = False):
+        super().__init__()
+
+    def construct(self, x):
         return MishAutoFn.apply(x)
 
 
@@ -97,10 +103,11 @@ 
 
 def hard_sigmoid_bwd(x, grad_output):
-    m = torch.ones_like(x) * ((x >= -3.) & (x <= 3.)) / 6.
+    m = mint.ones_like(x) * ((x >= -3.) & (x <= 3.)) / 6.
     return grad_output * m
 
 
+# 'torch.autograd.Function' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class HardSigmoidAutoFn(torch.autograd.Function):
     @staticmethod
     def forward(ctx, x):
@@ -117,11 +124,11 @@     return HardSigmoidAutoFn.apply(x)
 
 
-class HardSigmoidMe(nn.Module):
-    def __init__(self, inplace: bool = False):
-        super().__init__()
-
-    def forward(self, x):
+class HardSigmoidMe(msnn.Cell):
+    def __init__(self, inplace: bool = False):
+        super().__init__()
+
+    def construct(self, x):
         return HardSigmoidAutoFn.apply(x)
 
 
@@ -130,11 +137,12 @@ 
 
 def hard_swish_bwd(x, grad_output):
-    m = torch.ones_like(x) * (x >= 3.)
-    m = torch.where((x >= -3.) & (x <= 3.),  x / 3. + .5, m)
+    m = mint.ones_like(x) * (x >= 3.)
+    m = mint.where((x >= -3.) & (x <= 3.),  x / 3. + .5, m)
     return grad_output * m
 
 
+# 'torch.autograd.Function' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class HardSwishAutoFn(torch.autograd.Function):
     """A memory efficient HardSwish activation"""
     @staticmethod
@@ -149,9 +157,9 @@ 
     @staticmethod
     def symbolic(g, self):
-        input = g.op("Add", self, g.op('Constant', value_t=torch.tensor(3, dtype=torch.float)))
-        hardtanh_ = g.op("Clip", input, g.op('Constant', value_t=torch.tensor(0, dtype=torch.float)), g.op('Constant', value_t=torch.tensor(6, dtype=torch.float)))
-        hardtanh_ = g.op("Div", hardtanh_, g.op('Constant', value_t=torch.tensor(6, dtype=torch.float)))
+        input = g.op("Add", self, g.op('Constant', value_t=ms.Tensor(3, dtype=ms.float32)))
+        hardtanh_ = g.op("Clip", input, g.op('Constant', value_t=ms.Tensor(0, dtype=ms.float32)), g.op('Constant', value_t=ms.Tensor(6, dtype=ms.float32)))
+        hardtanh_ = g.op("Div", hardtanh_, g.op('Constant', value_t=ms.Tensor(6, dtype=ms.float32)))
         return g.op("Mul", self, hardtanh_)
 
 
@@ -159,11 +167,11 @@     return HardSwishAutoFn.apply(x)
 
 
-class HardSwishMe(nn.Module):
-    def __init__(self, inplace: bool = False):
-        super().__init__()
-
-    def forward(self, x):
+class HardSwishMe(msnn.Cell):
+    def __init__(self, inplace: bool = False):
+        super().__init__()
+
+    def construct(self, x):
         return HardSwishAutoFn.apply(x)
 
 
@@ -172,11 +180,12 @@ 
 
 def hard_mish_bwd(x, grad_output):
-    m = torch.ones_like(x) * (x >= -2.)
-    m = torch.where((x >= -2.) & (x <= 0.), x + 1., m)
+    m = mint.ones_like(x) * (x >= -2.)
+    m = mint.where((x >= -2.) & (x <= 0.), x + 1., m)
     return grad_output * m
 
 
+# 'torch.autograd.Function' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class HardMishAutoFn(torch.autograd.Function):
     """ A memory efficient variant of Hard Mish
     Experimental, based on notes by Mish author Diganta Misra at
@@ -197,11 +206,11 @@     return HardMishAutoFn.apply(x)
 
 
-class HardMishMe(nn.Module):
-    def __init__(self, inplace: bool = False):
-        super().__init__()
-
-    def forward(self, x):
+class HardMishMe(msnn.Cell):
+    def __init__(self, inplace: bool = False):
+        super().__init__()
+
+    def construct(self, x):
         return HardMishAutoFn.apply(x)
 
 
