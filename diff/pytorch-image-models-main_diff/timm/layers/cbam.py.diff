--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ CBAM (sort-of) Attention
 
 Experimental impl of CBAM: Convolutional Block Attention Module: https://arxiv.org/abs/1807.06521
@@ -8,17 +13,14 @@ Hacked together by / Copyright 2020 Ross Wightman
 """
 from typing import Optional, Tuple, Type, Union
-
-import torch
-from torch import nn as nn
-import torch.nn.functional as F
+# from torch import nn as nn
 
 from .conv_bn_act import ConvNormAct
 from .create_act import create_act_layer, get_act_layer
 from .helpers import make_divisible
 
 
-class ChannelAttn(nn.Module):
+class ChannelAttn(msnn.Cell):
     """ Original CBAM channel attention module, currently avg + max pool variant only.
     """
     def __init__(
@@ -27,8 +29,8 @@             rd_ratio: float = 1. / 16,
             rd_channels: Optional[int] = None,
             rd_divisor: int = 1,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            gate_layer: Union[str, Type[nn.Module]] = 'sigmoid',
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            gate_layer: Union[str, Type[msnn.Cell]] = 'sigmoid',
             mlp_bias=False,
             device=None,
             dtype=None,
@@ -37,12 +39,12 @@         super().__init__()
         if not rd_channels:
             rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)
-        self.fc1 = nn.Conv2d(channels, rd_channels, 1, bias=mlp_bias, **dd)
+        self.fc1 = nn.Conv2d(channels, rd_channels, 1, bias=mlp_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.act = act_layer(inplace=True)
-        self.fc2 = nn.Conv2d(rd_channels, channels, 1, bias=mlp_bias, **dd)
+        self.fc2 = nn.Conv2d(rd_channels, channels, 1, bias=mlp_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.gate = create_act_layer(gate_layer)
 
-    def forward(self, x):
+    def construct(self, x):
         x_avg = self.fc2(self.act(self.fc1(x.mean((2, 3), keepdim=True))))
         x_max = self.fc2(self.act(self.fc1(x.amax((2, 3), keepdim=True))))
         return x * self.gate(x_avg + x_max)
@@ -57,8 +59,8 @@             rd_ratio: float = 1./16,
             rd_channels: Optional[int] = None,
             rd_divisor: int = 1,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            gate_layer: Union[str, Type[nn.Module]] = 'sigmoid',
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            gate_layer: Union[str, Type[msnn.Cell]] = 'sigmoid',
             mlp_bias: bool = False,
             device=None,
             dtype=None
@@ -69,16 +71,16 @@     def forward(self, x):
         x_pool = 0.5 * x.mean((2, 3), keepdim=True) + 0.5 * x.amax((2, 3), keepdim=True)
         x_attn = self.fc2(self.act(self.fc1(x_pool)))
-        return x * F.sigmoid(x_attn)
+        return x * nn.functional.sigmoid(x_attn)
 
 
-class SpatialAttn(nn.Module):
+class SpatialAttn(msnn.Cell):
     """ Original CBAM spatial attention module
     """
     def __init__(
             self,
             kernel_size: int = 7,
-            gate_layer: Union[str, Type[nn.Module]] = 'sigmoid',
+            gate_layer: Union[str, Type[msnn.Cell]] = 'sigmoid',
             device=None,
             dtype=None,
     ):
@@ -86,19 +88,19 @@         self.conv = ConvNormAct(2, 1, kernel_size, apply_act=False, device=device, dtype=dtype)
         self.gate = create_act_layer(gate_layer)
 
-    def forward(self, x):
-        x_attn = torch.cat([x.mean(dim=1, keepdim=True), x.amax(dim=1, keepdim=True)], dim=1)
+    def construct(self, x):
+        x_attn = mint.cat([x.mean(dim=1, keepdim=True), x.amax(dim=1, keepdim=True)], dim=1)
         x_attn = self.conv(x_attn)
         return x * self.gate(x_attn)
 
 
-class LightSpatialAttn(nn.Module):
+class LightSpatialAttn(msnn.Cell):
     """An experimental 'lightweight' variant that sums avg_pool and max_pool results.
     """
     def __init__(
             self,
             kernel_size: int = 7,
-            gate_layer: Union[str, Type[nn.Module]] = 'sigmoid',
+            gate_layer: Union[str, Type[msnn.Cell]] = 'sigmoid',
             device=None,
             dtype=None,
     ):
@@ -106,13 +108,13 @@         self.conv = ConvNormAct(1, 1, kernel_size, apply_act=False, device=device, dtype=dtype)
         self.gate = create_act_layer(gate_layer)
 
-    def forward(self, x):
+    def construct(self, x):
         x_attn = 0.5 * x.mean(dim=1, keepdim=True) + 0.5 * x.amax(dim=1, keepdim=True)
         x_attn = self.conv(x_attn)
         return x * self.gate(x_attn)
 
 
-class CbamModule(nn.Module):
+class CbamModule(msnn.Cell):
     def __init__(
             self,
             channels: int,
@@ -120,8 +122,8 @@             rd_channels: Optional[int] = None,
             rd_divisor: int = 1,
             spatial_kernel_size: int = 7,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            gate_layer: Union[str, Type[nn.Module]] = 'sigmoid',
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            gate_layer: Union[str, Type[msnn.Cell]] = 'sigmoid',
             mlp_bias: bool = False,
             device=None,
             dtype=None,
@@ -137,16 +139,16 @@             gate_layer=gate_layer,
             mlp_bias=mlp_bias,
             **dd,
-        )
-        self.spatial = SpatialAttn(spatial_kernel_size, gate_layer=gate_layer, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.spatial = SpatialAttn(spatial_kernel_size, gate_layer=gate_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.channel(x)
         x = self.spatial(x)
         return x
 
 
-class LightCbamModule(nn.Module):
+class LightCbamModule(msnn.Cell):
     def __init__(
             self,
             channels: int,
@@ -154,8 +156,8 @@             rd_channels: Optional[int] = None,
             rd_divisor: int = 1,
             spatial_kernel_size: int = 7,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            gate_layer: Union[str, Type[nn.Module]] = 'sigmoid',
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            gate_layer: Union[str, Type[msnn.Cell]] = 'sigmoid',
             mlp_bias: bool = False,
             device=None,
             dtype=None,
@@ -171,10 +173,10 @@             gate_layer=gate_layer,
             mlp_bias=mlp_bias,
             **dd,
-        )
-        self.spatial = LightSpatialAttn(spatial_kernel_size, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.spatial = LightSpatialAttn(spatial_kernel_size, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.channel(x)
         x = self.spatial(x)
         return x
