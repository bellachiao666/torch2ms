--- pytorch+++ mindspore@@ -1,10 +1,14 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Conv2d w/ Same Padding
 
 Hacked together by / Copyright 2020 Ross Wightman
 """
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 from typing import Tuple, Optional, Union
 
 from ._fx import register_notrace_module
@@ -17,7 +21,7 @@ 
 def conv2d_same(
         x,
-        weight: torch.Tensor,
+        weight: ms.Tensor,
         bias: Optional[torch.Tensor] = None,
         stride: Tuple[int, int] = (1, 1),
         padding: Tuple[int, int] = (0, 0),
@@ -25,7 +29,7 @@         groups: int = 1,
 ):
     x = pad_same(x, weight.shape[-2:], stride, dilation)
-    return F.conv2d(x, weight, bias, stride, (0, 0), dilation, groups)
+    return nn.functional.conv2d(x, weight, bias, stride, (0, 0), dilation, groups)
 
 
 @register_notrace_module
@@ -114,15 +118,8 @@             self.pad_input_size = input_size
 
         x = self.pad(x)
-        return F.conv2d(
-            x,
-            self.weight,
-            self.bias,
-            self.stride,
-            self.padding,
-            self.dilation,
-            self.groups,
-        )
+        return nn.functional.conv2d(
+            x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
 
 
 def create_conv2d_pad(in_chs, out_chs, kernel_size, **kwargs):
