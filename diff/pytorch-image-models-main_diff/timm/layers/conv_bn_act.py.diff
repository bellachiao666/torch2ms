--- pytorch+++ mindspore@@ -1,10 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Conv2d + BN + Act
 
 Hacked together by / Copyright 2020 Ross Wightman
 """
 from typing import Any, Dict, Optional, Type
 
-from torch import nn as nn
+# from torch import nn as nn
 
 from .typing import LayerType, PadType
 from .blur_pool import create_aa
@@ -12,7 +17,7 @@ from .create_norm_act import get_norm_act_layer
 
 
-class ConvNormAct(nn.Module):
+class ConvNormAct(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -67,7 +72,7 @@                 **norm_kwargs,
             )
         else:
-            self.bn = nn.Sequential()
+            self.bn = msnn.SequentialCell()
             if drop_layer:
                 norm_kwargs['drop_layer'] = drop_layer
                 self.bn.add_module('drop', drop_layer())
@@ -89,7 +94,7 @@     def out_channels(self):
         return self.conv.out_channels
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv(x)
         x = self.bn(x)
         aa = getattr(self, 'aa', None)
