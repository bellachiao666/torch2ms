--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ 'Fast' Normalization Functions
 
 For GroupNorm and LayerNorm these functions bypass typical AMP upcast to float32.
@@ -8,8 +13,8 @@ """
 from typing import List, Optional
 
-import torch
-from torch.nn import functional as F
+# import torch
+# from torch.nn import functional as F
 
 try:
     from apex.normalization.fused_layer_norm import fused_layer_norm_affine
@@ -32,26 +37,26 @@ 
 def get_autocast_dtype(device: str = 'cuda'):
     try:
-        return torch.get_autocast_dtype(device)
+        return torch.get_autocast_dtype(device)  # 'torch.get_autocast_dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     except (AttributeError, TypeError):
         # dispatch to older device specific fns, only covering cuda/cpu devices here
         if device == 'cpu':
-            return torch.get_autocast_cpu_dtype()
+            return torch.get_autocast_cpu_dtype()  # 'torch.get_autocast_cpu_dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             assert device == 'cuda'
-            return torch.get_autocast_gpu_dtype()
+            return torch.get_autocast_gpu_dtype()  # 'torch.get_autocast_gpu_dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def is_autocast_enabled(device: str = 'cuda'):
     try:
-        return torch.is_autocast_enabled(device)
+        return torch.is_autocast_enabled(device)  # 'torch.is_autocast_enabled' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     except TypeError:
         # dispatch to older device specific fns, only covering cuda/cpu devices here
         if device == 'cpu':
-            return torch.is_autocast_cpu_enabled()
+            return torch.is_autocast_cpu_enabled()  # 'torch.is_autocast_cpu_enabled' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             assert device == 'cuda'
-            return torch.is_autocast_enabled()  # defaults cuda (only cuda on older pytorch)
+            return torch.is_autocast_enabled()  # defaults cuda (only cuda on older pytorch); 'torch.is_autocast_enabled' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def is_fast_norm():
@@ -64,15 +69,16 @@ 
 
 def fast_group_norm(
-    x: torch.Tensor,
+    x: ms.Tensor,
     num_groups: int,
-    weight: Optional[torch.Tensor] = None,
-    bias: Optional[torch.Tensor] = None,
+    weight: Optional[ms.Tensor] = None,
+    bias: Optional[ms.Tensor] = None,
     eps: float = 1e-5
-) -> torch.Tensor:
+) -> ms.Tensor:
+    # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if torch.jit.is_scripting():
         # currently cannot use is_autocast_enabled within torchscript
-        return F.group_norm(x, num_groups, weight, bias, eps)
+        return F.group_norm(x, num_groups, weight, bias, eps)  # 'torch.nn.functional.group_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     if is_autocast_enabled(x.device.type):
         # normally native AMP casts GN inputs to float32
@@ -80,20 +86,22 @@         dt = get_autocast_dtype(x.device.type)
         x, weight, bias = x.to(dt), weight.to(dt), bias.to(dt) if bias is not None else None
 
-    with torch.amp.autocast(device_type=x.device.type, enabled=False):
-        return F.group_norm(x, num_groups, weight, bias, eps)
+    # 'torch.amp.autocast' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    with torch.amp.autocast(device_type=x.device.type, enabled=False):
+        return F.group_norm(x, num_groups, weight, bias, eps)  # 'torch.nn.functional.group_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def fast_layer_norm(
-    x: torch.Tensor,
-    normalized_shape: List[int],
-    weight: Optional[torch.Tensor] = None,
-    bias: Optional[torch.Tensor] = None,
+    x: ms.Tensor,
+    normalized_shape: List[int],
+    weight: Optional[ms.Tensor] = None,
+    bias: Optional[ms.Tensor] = None,
     eps: float = 1e-5
-) -> torch.Tensor:
+) -> ms.Tensor:
+    # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if torch.jit.is_scripting():
         # currently cannot use is_autocast_enabled within torchscript
-        return F.layer_norm(x, normalized_shape, weight, bias, eps)
+        return F.layer_norm(x, normalized_shape, weight, bias, eps)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     if has_apex:
         return fused_layer_norm_affine(x, weight, bias, normalized_shape, eps)
@@ -104,39 +112,42 @@         dt = get_autocast_dtype(x.device.type)
         x, weight, bias = x.to(dt), weight.to(dt), bias.to(dt) if bias is not None else None
 
-    with torch.amp.autocast(device_type=x.device.type, enabled=False):
-        return F.layer_norm(x, normalized_shape, weight, bias, eps)
+    # 'torch.amp.autocast' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    with torch.amp.autocast(device_type=x.device.type, enabled=False):
+        return F.layer_norm(x, normalized_shape, weight, bias, eps)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def rms_norm(
-    x: torch.Tensor,
-    normalized_shape: List[int],
-    weight: Optional[torch.Tensor] = None,
+    x: ms.Tensor,
+    normalized_shape: List[int],
+    weight: Optional[ms.Tensor] = None,
     eps: float = 1e-5,
 ):
     norm_ndim = len(normalized_shape)
     v = x.pow(2)
+    # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if torch.jit.is_scripting():
         # ndim = len(x.shape)
         # dims = list(range(ndim - norm_ndim, ndim))  # this doesn't work on pytorch <= 1.13.x
         # NOTE -ve dims cause torchscript to crash in some cases, out of options to work around
         assert norm_ndim == 1
-        v = torch.mean(v, dim=-1).unsqueeze(-1)  # ts crashes with -ve dim + keepdim=True
+        v = mint.mean(v, dim=-1).unsqueeze(-1)  # ts crashes with -ve dim + keepdim=True
     else:
         dims = tuple(range(-1, -norm_ndim - 1, -1))
-        v = torch.mean(v, dim=dims, keepdim=True)
-    x = x * torch.rsqrt(v + eps)
+        v = mint.mean(v, dim=dims, keepdim=True)
+    x = x * mint.rsqrt(v + eps)
     if weight is not None:
         x = x * weight
     return x
 
 
 def fast_rms_norm(
-    x: torch.Tensor,
-    normalized_shape: List[int],
-    weight: Optional[torch.Tensor] = None,
-    eps: float = 1e-5,
-) -> torch.Tensor:
+    x: ms.Tensor,
+    normalized_shape: List[int],
+    weight: Optional[ms.Tensor] = None,
+    eps: float = 1e-5,
+) -> ms.Tensor:
+    # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if torch.jit.is_scripting():
         # this must be by itself, cannot merge with has_apex_rmsnorm
         return rms_norm(x, normalized_shape, weight, eps)
@@ -153,9 +164,10 @@         dt = get_autocast_dtype(x.device.type)
         x, weight = x.to(dt), weight.to(dt)
 
+    # 'torch.amp.autocast' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     with torch.amp.autocast(device_type=x.device.type, enabled=False):
         if has_torch_rms_norm:
-            x = F.rms_norm(x, normalized_shape, weight, eps)
+            x = F.rms_norm(x, normalized_shape, weight, eps)  # 'torch.nn.functional.rms_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             x = rms_norm(x, normalized_shape, weight, eps)
 
@@ -163,26 +175,27 @@ 
 
 def rms_norm2d(
-    x: torch.Tensor,
-    normalized_shape: List[int],
-    weight: Optional[torch.Tensor] = None,
+    x: ms.Tensor,
+    normalized_shape: List[int],
+    weight: Optional[ms.Tensor] = None,
     eps: float = 1e-5,
 ):
     assert len(normalized_shape) == 1
     v = x.pow(2)
-    v = torch.mean(v, dim=1, keepdim=True)
-    x = x * torch.rsqrt(v + eps)
+    v = mint.mean(v, dim=1, keepdim=True)
+    x = x * mint.rsqrt(v + eps)
     if weight is not None:
         x = x * weight.reshape(1, -1, 1, 1)
     return x
 
 
 def fast_rms_norm2d(
-    x: torch.Tensor,
-    normalized_shape: List[int],
-    weight: Optional[torch.Tensor] = None,
-    eps: float = 1e-5,
-) -> torch.Tensor:
+    x: ms.Tensor,
+    normalized_shape: List[int],
+    weight: Optional[ms.Tensor] = None,
+    eps: float = 1e-5,
+) -> ms.Tensor:
+    # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if torch.jit.is_scripting():
         # this must be by itself, cannot merge with has_apex_rmsnorm
         return rms_norm2d(x, normalized_shape, weight, eps)
@@ -201,6 +214,7 @@         dt = get_autocast_dtype(x.device.type)
         x, weight = x.to(dt), weight.to(dt)
 
+    # 'torch.amp.autocast' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     with torch.amp.autocast(device_type=x.device.type, enabled=False):
         x = rms_norm2d(x, normalized_shape, weight, eps)
 
@@ -208,33 +222,35 @@ 
 
 def simple_norm(
-    x: torch.Tensor,
-    normalized_shape: List[int],
-    weight: Optional[torch.Tensor] = None,
+    x: ms.Tensor,
+    normalized_shape: List[int],
+    weight: Optional[ms.Tensor] = None,
     eps: float = 1e-5,
 ):
     norm_ndim = len(normalized_shape)
+    # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if torch.jit.is_scripting():
         # ndim = len(x.shape)
         # dims = list(range(ndim - norm_ndim, ndim))  # this doesn't work on pytorch <= 1.13.x
         # NOTE -ve dims cause torchscript to crash in some cases, out of options to work around
         assert norm_ndim == 1
-        v = torch.var(x, dim=-1).unsqueeze(-1)  # ts crashes with -ve dim + keepdim=True
+        v = mint.var(x, dim=-1).unsqueeze(-1)  # ts crashes with -ve dim + keepdim=True
     else:
         dims = tuple(range(-1, -norm_ndim - 1, -1))
-        v = torch.var(x, dim=dims, keepdim=True)
-    x = x * torch.rsqrt(v + eps)
+        v = mint.var(x, dim=dims, keepdim=True)
+    x = x * mint.rsqrt(v + eps)
     if weight is not None:
         x = x * weight
     return x
 
 
 def fast_simple_norm(
-    x: torch.Tensor,
-    normalized_shape: List[int],
-    weight: Optional[torch.Tensor] = None,
-    eps: float = 1e-5,
-) -> torch.Tensor:
+    x: ms.Tensor,
+    normalized_shape: List[int],
+    weight: Optional[ms.Tensor] = None,
+    eps: float = 1e-5,
+) -> ms.Tensor:
+    # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if torch.jit.is_scripting():
         # this must be by itself, cannot merge with has_apex_rmsnorm
         return simple_norm(x, normalized_shape, weight, eps)
@@ -245,6 +261,7 @@         dt = get_autocast_dtype(x.device.type)
         x, weight = x.to(dt), weight.to(dt)
 
+    # 'torch.amp.autocast' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     with torch.amp.autocast(device_type=x.device.type, enabled=False):
         x = simple_norm(x, normalized_shape, weight, eps)
     return x
