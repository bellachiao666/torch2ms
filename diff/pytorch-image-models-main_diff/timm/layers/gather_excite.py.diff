--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Gather-Excite Attention Block
 
 Paper: `Gather-Excite: Exploiting Feature Context in CNNs` - https://arxiv.org/abs/1810.12348
@@ -14,8 +19,7 @@ from typing import Optional, Tuple, Type, Union
 import math
 
-from torch import nn as nn
-import torch.nn.functional as F
+# from torch import nn as nn
 
 from .create_act import create_act_layer, get_act_layer
 from .create_conv2d import create_conv2d
@@ -23,7 +27,7 @@ from .mlp import ConvMlp
 
 
-class GatherExcite(nn.Module):
+class GatherExcite(msnn.Cell):
     """ Gather-Excite Attention Module
     """
     def __init__(
@@ -37,9 +41,9 @@             rd_channels: Optional[int] = None,
             rd_divisor: int = 1,
             add_maxpool: bool = False,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            gate_layer: Union[str, Type[nn.Module]] = 'sigmoid',
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            gate_layer: Union[str, Type[msnn.Cell]] = 'sigmoid',
             device=None,
             dtype=None,
     ):
@@ -49,22 +53,22 @@         act_layer = get_act_layer(act_layer)
         self.extent = extent
         if extra_params:
-            self.gather = nn.Sequential()
+            self.gather = msnn.SequentialCell()
             if extent == 0:
                 assert feat_size is not None, 'spatial feature size must be specified for global extent w/ params'
                 self.gather.add_module(
-                    'conv1', create_conv2d(channels, channels, kernel_size=feat_size, stride=1, depthwise=True, *dd))
+                    'conv1', create_conv2d(channels, channels, kernel_size=feat_size, stride=1, depthwise=True, *dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
                 if norm_layer:
-                    self.gather.add_module(f'norm1', nn.BatchNorm2d(channels, *dd))
+                    self.gather.add_module(f'norm1', nn.BatchNorm2d(channels, *dd))  # 存在 *args/**kwargs，需手动确认参数映射;
             else:
                 assert extent % 2 == 0
                 num_conv = int(math.log2(extent))
                 for i in range(num_conv):
                     self.gather.add_module(
                         f'conv{i + 1}',
-                        create_conv2d(channels, channels, kernel_size=3, stride=2, depthwise=True, *dd))
+                        create_conv2d(channels, channels, kernel_size=3, stride=2, depthwise=True, *dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
                     if norm_layer:
-                        self.gather.add_module(f'norm{i + 1}', nn.BatchNorm2d(channels, *dd))
+                        self.gather.add_module(f'norm{i + 1}', nn.BatchNorm2d(channels, *dd))  # 存在 *args/**kwargs，需手动确认参数映射;
                     if i != num_conv - 1:
                         self.gather.add_module(f'act{i + 1}', act_layer(inplace=True))
         else:
@@ -79,10 +83,10 @@ 
         if not rd_channels:
             rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)
-        self.mlp = ConvMlp(channels, rd_channels, act_layer=act_layer, *dd) if use_mlp else nn.Identity()
+        self.mlp = ConvMlp(channels, rd_channels, act_layer=act_layer, *dd) if use_mlp else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.gate = create_act_layer(gate_layer)
 
-    def forward(self, x):
+    def construct(self, x):
         size = x.shape[-2:]
         if self.gather is not None:
             x_ge = self.gather(x)
@@ -94,12 +98,12 @@                     # experimental codepath, may remove or change
                     x_ge = 0.5 * x_ge + 0.5 * x.amax((2, 3), keepdim=True)
             else:
-                x_ge = F.avg_pool2d(
-                    x, kernel_size=self.gk, stride=self.gs, padding=self.gk // 2, count_include_pad=False)
+                x_ge = nn.functional.avg_pool2d(
+                    x, kernel_size = self.gk, stride = self.gs, padding = self.gk // 2, count_include_pad = False)
                 if self.add_maxpool:
                     # experimental codepath, may remove or change
-                    x_ge = 0.5 * x_ge + 0.5 * F.max_pool2d(x, kernel_size=self.gk, stride=self.gs, padding=self.gk // 2)
+                    x_ge = 0.5 * x_ge + 0.5 * nn.functional.max_pool2d(x, kernel_size = self.gk, stride = self.gs, padding = self.gk // 2)
         x_ge = self.mlp(x_ge)
         if x_ge.shape[-1] != 1 or x_ge.shape[-2] != 1:
-            x_ge = F.interpolate(x_ge, size=size)
+            x_ge = nn.functional.interpolate(x_ge, size = size)
         return x * self.gate(x_ge)
