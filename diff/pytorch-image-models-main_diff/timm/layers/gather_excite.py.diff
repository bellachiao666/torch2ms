--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Gather-Excite Attention Block
 
 Paper: `Gather-Excite: Exploiting Feature Context in CNNs` - https://arxiv.org/abs/1810.12348
@@ -14,8 +19,7 @@ from typing import Optional, Tuple, Type, Union
 import math
 
-from torch import nn as nn
-import torch.nn.functional as F
+# from torch import nn as nn
 
 from .create_act import create_act_layer, get_act_layer
 from .create_conv2d import create_conv2d
@@ -23,7 +27,7 @@ from .mlp import ConvMlp
 
 
-class GatherExcite(nn.Module):
+class GatherExcite(msnn.Cell):
     """ Gather-Excite Attention Module
     """
     def __init__(
@@ -49,7 +53,7 @@         act_layer = get_act_layer(act_layer)
         self.extent = extent
         if extra_params:
-            self.gather = nn.Sequential()
+            self.gather = msnn.SequentialCell()
             if extent == 0:
                 assert feat_size is not None, 'spatial feature size must be specified for global extent w/ params'
                 self.gather.add_module(
@@ -79,10 +83,10 @@ 
         if not rd_channels:
             rd_channels = make_divisible(channels * rd_ratio, rd_divisor, round_limit=0.)
-        self.mlp = ConvMlp(channels, rd_channels, act_layer=act_layer, *dd) if use_mlp else nn.Identity()
+        self.mlp = ConvMlp(channels, rd_channels, act_layer=act_layer, *dd) if use_mlp else msnn.Identity()
         self.gate = create_act_layer(gate_layer)
 
-    def forward(self, x):
+    def construct(self, x):
         size = x.shape[-2:]
         if self.gather is not None:
             x_ge = self.gather(x)
@@ -94,12 +98,12 @@                     # experimental codepath, may remove or change
                     x_ge = 0.5 * x_ge + 0.5 * x.amax((2, 3), keepdim=True)
             else:
-                x_ge = F.avg_pool2d(
-                    x, kernel_size=self.gk, stride=self.gs, padding=self.gk // 2, count_include_pad=False)
+                x_ge = nn.functional.avg_pool2d(
+                    x, kernel_size = self.gk, stride = self.gs, padding = self.gk // 2, count_include_pad = False)
                 if self.add_maxpool:
                     # experimental codepath, may remove or change
-                    x_ge = 0.5 * x_ge + 0.5 * F.max_pool2d(x, kernel_size=self.gk, stride=self.gs, padding=self.gk // 2)
+                    x_ge = 0.5 * x_ge + 0.5 * nn.functional.max_pool2d(x, kernel_size = self.gk, stride = self.gs, padding = self.gk // 2)
         x_ge = self.mlp(x_ge)
         if x_ge.shape[-1] != 1 or x_ge.shape[-2] != 1:
-            x_ge = F.interpolate(x_ge, size=size)
+            x_ge = nn.functional.interpolate(x_ge, size = size)
         return x * self.gate(x_ge)
