--- pytorch+++ mindspore@@ -1,20 +1,25 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from typing import Optional
 
-import torch
-from torch import nn
-from torch import nn, Tensor
-from torch.nn.modules.transformer import _get_activation_fn
+# import torch
+# from torch import nn
+# from torch import nn, Tensor
+# from torch.nn.modules.transformer import _get_activation_fn
 
 
 def add_ml_decoder_head(model):
     if hasattr(model, 'global_pool') and hasattr(model, 'fc'):  # most CNN models, like Resnet50
-        model.global_pool = nn.Identity()
+        model.global_pool = msnn.Identity()
         del model.fc
         num_classes = model.num_classes
         num_features = model.num_features
         model.fc = MLDecoder(num_classes=num_classes, initial_num_features=num_features)
     elif hasattr(model, 'global_pool') and hasattr(model, 'classifier'):  # EfficientNet
-        model.global_pool = nn.Identity()
+        model.global_pool = msnn.Identity()
         del model.classifier
         num_classes = model.num_classes
         num_features = model.num_features
@@ -32,36 +37,36 @@     return model
 
 
-class TransformerDecoderLayerOptimal(nn.Module):
+class TransformerDecoderLayerOptimal(msnn.Cell):
     def __init__(self, d_model, nhead=8, dim_feedforward=2048, dropout=0.1, activation="relu",
                  layer_norm_eps=1e-5) -> None:
         super().__init__()
-        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)
+        self.norm1 = nn.LayerNorm(d_model, eps = layer_norm_eps)
         self.dropout = nn.Dropout(dropout)
         self.dropout1 = nn.Dropout(dropout)
         self.dropout2 = nn.Dropout(dropout)
         self.dropout3 = nn.Dropout(dropout)
 
-        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
+        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)  # 'torch.nn.MultiheadAttention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # Implementation of Feedforward model
         self.linear1 = nn.Linear(d_model, dim_feedforward)
         self.linear2 = nn.Linear(dim_feedforward, d_model)
 
-        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)
-        self.norm3 = nn.LayerNorm(d_model, eps=layer_norm_eps)
+        self.norm2 = nn.LayerNorm(d_model, eps = layer_norm_eps)
+        self.norm3 = nn.LayerNorm(d_model, eps = layer_norm_eps)
 
-        self.activation = _get_activation_fn(activation)
+        self.activation = _get_activation_fn(activation)  # 'torch.nn.modules.transformer._get_activation_fn' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __setstate__(self, state):
         if 'activation' not in state:
             state['activation'] = torch.nn.functional.relu
         super(TransformerDecoderLayerOptimal, self).__setstate__(state)
 
-    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,
+    def construct(self, tgt: ms.Tensor, memory: ms.Tensor, tgt_mask: Optional[Tensor] = None,
                 memory_mask: Optional[Tensor] = None,
                 tgt_key_padding_mask: Optional[Tensor] = None,
-                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:
+                memory_key_padding_mask: Optional[Tensor] = None) -> ms.Tensor:
         tgt = tgt + self.dropout1(tgt)
         tgt = self.norm1(tgt)
         tgt2 = self.multihead_attn(tgt, memory, memory)[0]
@@ -87,7 +92,7 @@ #         out = out.view((h.shape[0], self.group_size * self.num_queries))
 #         return out
 
-class MLDecoder(nn.Module):
+class MLDecoder(msnn.Cell):
     def __init__(self, num_classes, num_of_groups=-1, decoder_embedding=768, initial_num_features=2048):
         super().__init__()
         embed_len_decoder = 100 if num_of_groups < 0 else num_of_groups
@@ -105,7 +110,7 @@         dim_feedforward = 2048
         layer_decode = TransformerDecoderLayerOptimal(d_model=decoder_embedding,
                                                       dim_feedforward=dim_feedforward, dropout=decoder_dropout)
-        self.decoder = nn.TransformerDecoder(layer_decode, num_layers=num_layers_decoder)
+        self.decoder = nn.TransformerDecoder(layer_decode, num_layers=num_layers_decoder)  # 'torch.nn.TransformerDecoder' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # non-learnable queries
         self.query_embed = nn.Embedding(embed_len_decoder, decoder_embedding)
@@ -114,19 +119,19 @@         # group fully-connected
         self.num_classes = num_classes
         self.duplicate_factor = int(num_classes / embed_len_decoder + 0.999)
-        self.duplicate_pooling = torch.nn.Parameter(
-            torch.Tensor(embed_len_decoder, decoder_embedding, self.duplicate_factor))
-        self.duplicate_pooling_bias = torch.nn.Parameter(torch.Tensor(num_classes))
-        torch.nn.init.xavier_normal_(self.duplicate_pooling)
-        torch.nn.init.constant_(self.duplicate_pooling_bias, 0)
+        self.duplicate_pooling = ms.Parameter(
+            ms.Tensor(embed_len_decoder, decoder_embedding, self.duplicate_factor))
+        self.duplicate_pooling_bias = ms.Parameter(ms.Tensor(num_classes))
+        torch.nn.init.xavier_normal_(self.duplicate_pooling)  # 'torch.nn.init.xavier_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch.nn.init.constant_(self.duplicate_pooling_bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-    def forward(self, x):
+    def construct(self, x):
         if len(x.shape) == 4:  # [bs,2048, 7,7]
             embedding_spatial = x.flatten(2).transpose(1, 2)
         else:  # [bs, 197,468]
             embedding_spatial = x
         embedding_spatial_786 = self.embed_standart(embedding_spatial)
-        embedding_spatial_786 = torch.nn.functional.relu(embedding_spatial_786, inplace=True)
+        embedding_spatial_786 = nn.functional.relu(embedding_spatial_786, inplace = True)
 
         bs = embedding_spatial_786.shape[0]
         query_embed = self.query_embed.weight
@@ -135,11 +140,11 @@         h = self.decoder(tgt, embedding_spatial_786.transpose(0, 1))  # [embed_len_decoder, batch, 768]
         h = h.transpose(0, 1)
 
-        out_extrap = torch.zeros(h.shape[0], h.shape[1], self.duplicate_factor, device=h.device, dtype=h.dtype)
+        out_extrap = mint.zeros(size = (h.shape[0], h.shape[1], self.duplicate_factor), dtype = h.dtype)  # 'torch.zeros':没有对应的mindspore参数 'device' (position 4);
         for i in range(self.embed_len_decoder):  # group FC
             h_i = h[:, i, :]
             w_i = self.duplicate_pooling[i, :, :]
-            out_extrap[:, i, :] = torch.matmul(h_i, w_i)
+            out_extrap[:, i, :] = mint.matmul(h_i, w_i)
         h_out = out_extrap.flatten(1)[:, :self.num_classes]
         h_out += self.duplicate_pooling_bias
         logits = h_out
