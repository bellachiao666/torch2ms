--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Bilinear-Attention-Transform and Non-Local Attention
 
 Paper: `Non-Local Neural Networks With Grouped Bilinear Attentional Transforms`
@@ -5,10 +10,7 @@ Adapted from original code: https://github.com/BA-Transform/BAT-Image-Classification
 """
 from typing import Optional, Type
-
-import torch
-from torch import nn
-from torch.nn import functional as F
+# from torch import nn
 
 from ._fx import register_notrace_module
 from .conv_bn_act import ConvNormAct
@@ -16,7 +18,7 @@ from .trace_utils import _assert
 
 
-class NonLocalAttn(nn.Module):
+class NonLocalAttn(msnn.Cell):
     """Spatial NL block for image classification.
 
     This was adapted from https://github.com/BA-Transform/BAT-Image-Classification
@@ -39,14 +41,14 @@         if rd_channels is None:
             rd_channels = make_divisible(in_channels * rd_ratio, divisor=rd_divisor)
         self.scale = in_channels ** -0.5 if use_scale else 1.0
-        self.t = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True, **dd)
-        self.p = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True, **dd)
-        self.g = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True, **dd)
-        self.z = nn.Conv2d(rd_channels, in_channels, kernel_size=1, stride=1, bias=True, **dd)
-        self.norm = nn.BatchNorm2d(in_channels, **dd)
+        self.t = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.p = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.g = nn.Conv2d(in_channels, rd_channels, kernel_size=1, stride=1, bias=True, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.z = nn.Conv2d(rd_channels, in_channels, kernel_size=1, stride=1, bias=True, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm = nn.BatchNorm2d(in_channels, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.reset_parameters()
 
-    def forward(self, x):
+    def construct(self, x):
         shortcut = x
 
         t = self.t(x)
@@ -58,9 +60,9 @@         p = p.view(B, C, -1)
         g = g.view(B, C, -1).permute(0, 2, 1)
 
-        att = torch.bmm(t, p) * self.scale
-        att = F.softmax(att, dim=2)
-        x = torch.bmm(att, g)
+        att = mint.bmm(t, p) * self.scale
+        att = nn.functional.softmax(att, dim = 2)
+        x = mint.bmm(att, g)
 
         x = x.permute(0, 2, 1).reshape(B, C, H, W)
         x = self.z(x)
@@ -72,36 +74,36 @@         for name, m in self.named_modules():
             if isinstance(m, nn.Conv2d):
                 nn.init.kaiming_normal_(
-                    m.weight, mode='fan_out', nonlinearity='relu')
+                    m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 if len(list(m.parameters())) > 1:
-                    nn.init.constant_(m.bias, 0.0)
+                    nn.init.constant_(m.bias, 0.0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.BatchNorm2d):
-                nn.init.constant_(m.weight, 0)
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.weight, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.GroupNorm):
-                nn.init.constant_(m.weight, 0)
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.weight, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 @register_notrace_module
-class BilinearAttnTransform(nn.Module):
+class BilinearAttnTransform(msnn.Cell):
 
     def __init__(
             self,
             in_channels: int,
             block_size: int,
             groups: int,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.conv1 = ConvNormAct(in_channels, groups, 1, act_layer=act_layer, norm_layer=norm_layer, **dd)
-        self.conv_p = nn.Conv2d(groups, block_size * block_size * groups, kernel_size=(block_size, 1), **dd)
-        self.conv_q = nn.Conv2d(groups, block_size * block_size * groups, kernel_size=(1, block_size), **dd)
-        self.conv2 = ConvNormAct(in_channels, in_channels, 1, act_layer=act_layer, norm_layer=norm_layer, **dd)
+        self.conv1 = ConvNormAct(in_channels, groups, 1, act_layer=act_layer, norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv_p = nn.Conv2d(groups, block_size * block_size * groups, kernel_size=(block_size, 1), **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv_q = nn.Conv2d(groups, block_size * block_size * groups, kernel_size=(1, block_size), **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv2 = ConvNormAct(in_channels, in_channels, 1, act_layer=act_layer, norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.block_size = block_size
         self.groups = groups
         self.in_channels = in_channels
@@ -112,20 +114,20 @@         if t <= 1:
             return x
         x = x.view(B * C, -1, 1, 1)
-        x = x * torch.eye(t, t, dtype=x.dtype, device=x.device)
+        x = x * mint.eye(t, t, dtype=x.dtype, device=x.device)
         x = x.view(B * C, block_size, block_size, t, t)
-        x = torch.cat(torch.split(x, 1, dim=1), dim=3)
-        x = torch.cat(torch.split(x, 1, dim=2), dim=4)
+        x = mint.cat(mint.split(x, 1, dim=1), dim=3)
+        x = mint.cat(mint.split(x, 1, dim=2), dim=4)
         x = x.view(B, C, block_size * t, block_size * t)
         return x
 
-    def forward(self, x):
+    def construct(self, x):
         _assert(x.shape[-1] % self.block_size == 0, '')
         _assert(x.shape[-2] % self.block_size == 0, '')
         B, C, H, W = x.shape
         out = self.conv1(x)
-        rp = F.adaptive_max_pool2d(out, (self.block_size, 1))
-        cp = F.adaptive_max_pool2d(out, (1, self.block_size))
+        rp = msnn.AdaptiveMaxPool2d(out, (self.block_size, 1))
+        cp = msnn.AdaptiveMaxPool2d(out, (1, self.block_size))
         p = self.conv_p(rp).view(B, self.groups, self.block_size, self.block_size).sigmoid()
         q = self.conv_q(cp).view(B, self.groups, self.block_size, self.block_size).sigmoid()
         p = p / p.sum(dim=3, keepdim=True)
@@ -145,7 +147,7 @@         return y
 
 
-class BatNonLocalAttn(nn.Module):
+class BatNonLocalAttn(msnn.Cell):
     """ BAT
     Adapted from: https://github.com/BA-Transform/BAT-Image-Classification
     """
@@ -159,8 +161,8 @@             rd_channels: Optional[int] = None,
             rd_divisor: int = 8,
             drop_rate: float = 0.2,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
             **_,
@@ -169,7 +171,7 @@         super().__init__()
         if rd_channels is None:
             rd_channels = make_divisible(in_channels * rd_ratio, divisor=rd_divisor)
-        self.conv1 = ConvNormAct(in_channels, rd_channels, 1, act_layer=act_layer, norm_layer=norm_layer, **dd)
+        self.conv1 = ConvNormAct(in_channels, rd_channels, 1, act_layer=act_layer, norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.ba = BilinearAttnTransform(
             rd_channels,
             block_size,
@@ -177,11 +179,11 @@             act_layer=act_layer,
             norm_layer=norm_layer,
             **dd,
-        )
-        self.conv2 = ConvNormAct(rd_channels, in_channels, 1, act_layer=act_layer, norm_layer=norm_layer, **dd)
-        self.dropout = nn.Dropout2d(p=drop_rate)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv2 = ConvNormAct(rd_channels, in_channels, 1, act_layer=act_layer, norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.dropout = nn.Dropout2d(p = drop_rate)
 
-    def forward(self, x):
+    def construct(self, x):
         xl = self.conv1(x)
         y = self.ba(xl)
         y = self.conv2(y)
