--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Padding Helpers
 
 Hacked together by / Copyright 2020 Ross Wightman
@@ -5,8 +10,7 @@ import math
 from typing import List, Tuple, Union
 
-import torch
-import torch.nn.functional as F
+# import torch
 
 from .helpers import to_2tuple
 
@@ -23,7 +27,7 @@ # Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution
 def get_same_padding(x: int, kernel_size: int, stride: int, dilation: int):
     if isinstance(x, torch.Tensor):
-        return torch.clamp(((x / stride).ceil() - 1) * stride + (kernel_size - 1) * dilation + 1 - x, min=0)
+        return mint.clamp(((x / stride).ceil() - 1) * stride + (kernel_size - 1) * dilation + 1 - x, min = 0)
     else:
         return max((math.ceil(x / stride) - 1) * stride + (kernel_size - 1) * dilation + 1 - x, 0)
 
@@ -60,7 +64,7 @@     ih, iw = x.size()[-2:]
     pad_h = get_same_padding(ih, kernel_size[0], stride[0], dilation[0])
     pad_w = get_same_padding(iw, kernel_size[1], stride[1], dilation[1])
-    x = F.pad(x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2), value=value)
+    x = nn.functional.pad(x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2), value = value)
     return x
 
 
