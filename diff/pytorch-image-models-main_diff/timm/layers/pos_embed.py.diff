--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Position Embedding Utilities
 
 Hacked together by / Copyright 2022 Ross Wightman
@@ -6,18 +11,18 @@ import math
 from typing import List, Tuple, Optional, Union
 
-import torch
-import torch.nn.functional as F
+# import torch
 
 from ._fx import register_notrace_function
 
 _logger = logging.getLogger(__name__)
 
 
+# 装饰器 'torch.fx.wrap' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 @torch.fx.wrap
 @register_notrace_function
 def resample_abs_pos_embed(
-        posemb: torch.Tensor,
+        posemb: ms.Tensor,
         new_size: List[int],
         old_size: Optional[List[int]] = None,
         num_prefix_tokens: int = 1,
@@ -45,24 +50,26 @@     orig_dtype = posemb.dtype
     posemb = posemb.float()  # interpolate needs float32
     posemb = posemb.reshape(1, old_size[0], old_size[1], -1).permute(0, 3, 1, 2)
-    posemb = F.interpolate(posemb, size=new_size, mode=interpolation, antialias=antialias)
+    posemb = nn.functional.interpolate(posemb, size=new_size, mode=interpolation, antialias=antialias)
     posemb = posemb.permute(0, 2, 3, 1).reshape(1, -1, embed_dim)
     posemb = posemb.to(orig_dtype)
 
     # add back extra (class, etc) prefix tokens
     if posemb_prefix is not None:
-        posemb = torch.cat([posemb_prefix, posemb], dim=1)
+        posemb = mint.cat([posemb_prefix, posemb], dim=1)
 
+    # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if not torch.jit.is_scripting() and verbose:
         _logger.info(f'Resized position embedding: {old_size} to {new_size}.')
 
     return posemb
 
 
+# 装饰器 'torch.fx.wrap' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 @torch.fx.wrap
 @register_notrace_function
 def resample_abs_pos_embed_nhwc(
-        posemb: torch.Tensor,
+        posemb: ms.Tensor,
         new_size: List[int],
         interpolation: str = 'bicubic',
         antialias: bool = True,
@@ -74,9 +81,10 @@     orig_dtype = posemb.dtype
     posemb = posemb.float()
     posemb = posemb.reshape(1, posemb.shape[-3], posemb.shape[-2], posemb.shape[-1]).permute(0, 3, 1, 2)
-    posemb = F.interpolate(posemb, size=new_size, mode=interpolation, antialias=antialias)
+    posemb = nn.functional.interpolate(posemb, size=new_size, mode=interpolation, antialias=antialias)
     posemb = posemb.permute(0, 2, 3, 1).to(orig_dtype)
 
+    # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if not torch.jit.is_scripting() and verbose:
         _logger.info(f'Resized position embedding: {posemb.shape[-3:-1]} to {new_size}.')
 
