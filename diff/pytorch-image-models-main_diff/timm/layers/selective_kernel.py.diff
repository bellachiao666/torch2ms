--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Selective Kernel Convolution/Attention
 
 Paper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)
@@ -5,9 +10,7 @@ Hacked together by / Copyright 2020 Ross Wightman
 """
 from typing import List, Optional, Tuple, Type, Union
-
-import torch
-from torch import nn as nn
+# from torch import nn as nn
 
 from .conv_bn_act import ConvNormAct
 from .helpers import make_divisible
@@ -21,7 +24,7 @@     assert k >= 3 and k % 2
 
 
-class SelectiveKernelAttn(nn.Module):
+class SelectiveKernelAttn(msnn.Cell):
     def __init__(
             self,
             channels: int,
@@ -45,7 +48,7 @@         self.act = act_layer(inplace=True)
         self.fc_select = nn.Conv2d(attn_channels, channels * num_paths, kernel_size=1, bias=False, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         _assert(x.shape[1] == self.num_paths, '')
         x = x.sum(1).mean((2, 3), keepdim=True)
         x = self.fc_reduce(x)
@@ -54,11 +57,11 @@         x = self.fc_select(x)
         B, C, H, W = x.shape
         x = x.view(B, self.num_paths, C // self.num_paths, H, W)
-        x = torch.softmax(x, dim=1)
+        x = nn.functional.softmax(x, dim = 1)
         return x
 
 
-class SelectiveKernel(nn.Module):
+class SelectiveKernel(msnn.Cell):
 
     def __init__(
             self,
@@ -129,21 +132,21 @@         conv_kwargs = dict(
             stride=stride, groups=groups, act_layer=act_layer, norm_layer=norm_layer,
             aa_layer=aa_layer, drop_layer=drop_layer, **dd)
-        self.paths = nn.ModuleList([
+        self.paths = msnn.CellList([
             ConvNormAct(in_channels, out_channels, kernel_size=k, dilation=d, **conv_kwargs)
             for k, d in zip(kernel_size, dilation)])
 
         attn_channels = rd_channels or make_divisible(out_channels * rd_ratio, divisor=rd_divisor)
         self.attn = SelectiveKernelAttn(out_channels, self.num_paths, attn_channels, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         if self.split_input:
-            x_split = torch.split(x, self.in_channels // self.num_paths, 1)
+            x_split = mint.split(x, self.in_channels // self.num_paths, 1)
             x_paths = [op(x_split[i]) for i, op in enumerate(self.paths)]
         else:
             x_paths = [op(x) for op in self.paths]
-        x = torch.stack(x_paths, dim=1)
+        x = mint.stack(x_paths, dim = 1)
         x_attn = self.attn(x)
         x = x * x_attn
-        x = torch.sum(x, dim=1)
+        x = mint.sum(x)
         return x
