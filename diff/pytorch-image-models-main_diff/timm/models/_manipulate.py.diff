--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import collections.abc
 import math
 import re
@@ -5,10 +10,9 @@ from itertools import chain
 from typing import Any, Callable, Dict, Iterator, Optional, Tuple, Type, Union
 
-import torch
-import torch.utils.checkpoint
-from torch import nn as nn
-from torch import Tensor
+# import torch
+# from torch import nn as nn
+# from torch import Tensor
 
 from timm.layers import use_reentrant_ckpt
 
@@ -17,7 +21,7 @@            'group_with_matcher', 'group_modules', 'group_parameters', 'flatten_modules', 'checkpoint_seq', 'checkpoint']
 
 
-def model_parameters(model: nn.Module, exclude_head: bool = False):
+def model_parameters(model: msnn.Cell, exclude_head: bool = False):
     if exclude_head:
         # FIXME this a bit of a quick and dirty hack to skip classifier head params based on ordering
         return [p for p in model.parameters()][:-2]
@@ -27,10 +31,10 @@ 
 def named_apply(
         fn: Callable,
-        module: nn.Module, name='',
+        module: msnn.Cell, name='',
         depth_first: bool = True,
         include_root: bool = False,
-) -> nn.Module:
+) -> msnn.Cell:
     if not depth_first and include_root:
         fn(module=module, name=name)
     for child_name, child_module in module.named_children():
@@ -42,7 +46,7 @@ 
 
 def named_modules(
-        module: nn.Module,
+        module: msnn.Cell,
         name: str = '',
         depth_first: bool = True,
         include_root: bool = False,
@@ -58,7 +62,7 @@ 
 
 def named_modules_with_params(
-        module: nn.Module,
+        module: msnn.Cell,
         name: str = '',
         depth_first: bool = True,
         include_root: bool = False,
@@ -138,7 +142,7 @@ 
 
 def group_parameters(
-        module: nn.Module,
+        module: msnn.Cell,
         group_matcher,
         return_values: bool = False,
         reverse: bool = False,
@@ -148,7 +152,7 @@ 
 
 def group_modules(
-        module: nn.Module,
+        module: msnn.Cell,
         group_matcher,
         return_values: bool = False,
         reverse: bool = False,
@@ -158,17 +162,17 @@ 
 
 def flatten_modules(
-        named_modules: Iterator[Tuple[str, nn.Module]],
+        named_modules: Iterator[Tuple[str, msnn.Cell]],
         depth: int = 1,
         prefix: Union[str, Tuple[str, ...]] = '',
-        module_types: Union[str, Tuple[Type[nn.Module]]] = 'sequential',
+        module_types: Union[str, Tuple[Type[msnn.Cell]]] = 'sequential',
 ):
     prefix_is_tuple = isinstance(prefix, tuple)
     if isinstance(module_types, str):
         if module_types == 'container':
-            module_types = (nn.Sequential, nn.ModuleList, nn.ModuleDict)
-        else:
-            module_types = (nn.Sequential,)
+            module_types = (msnn.SequentialCell, msnn.CellList, nn.ModuleDict)
+        else:
+            module_types = (msnn.SequentialCell,)
     for name, module in named_modules:
         if depth and isinstance(module, module_types):
             yield from flatten_modules(
@@ -206,7 +210,7 @@         *args,
         use_reentrant=use_reentrant,
         **kwargs,
-    )
+    )  # 'torch.utils.checkpoint.checkpoint' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def checkpoint_seq(
@@ -262,7 +266,7 @@             return _x
         return forward
 
-    if isinstance(functions, torch.nn.Sequential):
+    if isinstance(functions, msnn.SequentialCell):
         functions = functions.children()
     if flatten:
         functions = chain.from_iterable(functions)
@@ -279,13 +283,13 @@             run_function(start, end, functions),
             x,
             use_reentrant=use_reentrant,
-        )
+        )  # 'torch.utils.checkpoint.checkpoint' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if skip_last:
         return run_function(end + 1, len(functions) - 1, functions)(x)
     return x
 
 
-def adapt_input_conv(in_chans: int, conv_weight: Tensor) -> Tensor:
+def adapt_input_conv(in_chans: int, conv_weight: ms.Tensor) -> ms.Tensor:
     conv_type = conv_weight.dtype
     conv_weight = conv_weight.float()  # Some weights are in torch.half, ensure it's float for sum on CPU
     O, I, J, K = conv_weight.shape
