--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Bring-Your-Own-Blocks Network
 
 A flexible network w/ dataclass based config for stacking those NN blocks.
@@ -33,8 +38,8 @@ from functools import partial
 from typing import Tuple, List, Dict, Optional, Union, Any, Callable, Sequence, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
 from timm.layers import (
@@ -70,7 +75,7 @@ 
     Defines configuration for a single block or stage of blocks.
     """
-    type: Union[str, nn.Module]
+    type: Union[str, msnn.Cell]
     d: int  # block depth (number of block repeats in stage)
     c: int  # number of output channels for each block in stage
     s: int = 2  # stride of stage (first block)
@@ -168,9 +173,9 @@             ak = {}
             if i >= d - se:
                 ak['attn_layer'] = 'se'
-            scfg += [ByoBlockCfg(type='one', d=1, c=prev_c, gs=1, block_kwargs=bk, **ak)]  # depthwise block
+            scfg += [ByoBlockCfg(type='one', d=1, c=prev_c, gs=1, block_kwargs=bk, **ak)]  # depthwise block; 存在 *args/**kwargs，未转换，需手动确认参数映射;
             scfg += [ByoBlockCfg(
-                type='one', d=1, c=out_c, gs=0, block_kwargs=dict(kernel_size=1, **bk), **ak)]  # pointwise block
+                type='one', d=1, c=out_c, gs=0, block_kwargs=dict(kernel_size=1, **bk), **ak)]  # pointwise block; 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_c = out_c
         bcfg += [scfg]
     return bcfg
@@ -204,7 +209,7 @@     blocks = []
     for i in range(d):
         block_type = types[1] if i in every else types[0]
-        blocks += [ByoBlockCfg(type=block_type, d=1, **kwargs)]
+        blocks += [ByoBlockCfg(type=block_type, d=1, **kwargs)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return tuple(blocks)
 
 
@@ -246,14 +251,14 @@ @dataclass
 class LayerFn:
     """Container for layer factory functions."""
-    conv_norm_act: Type[nn.Module] = ConvNormAct
-    norm_act: Type[nn.Module] = BatchNormAct2d
-    act: Type[nn.Module] = nn.ReLU
-    attn: Optional[Type[nn.Module]] = None
-    self_attn: Optional[Type[nn.Module]] = None
-
-
-class DownsampleAvg(nn.Module):
+    conv_norm_act: Type[msnn.Cell] = ConvNormAct
+    norm_act: Type[msnn.Cell] = BatchNormAct2d
+    act: Type[msnn.Cell] = nn.ReLU
+    attn: Optional[Type[msnn.Cell]] = None
+    self_attn: Optional[Type[msnn.Cell]] = None
+
+
+class DownsampleAvg(msnn.Cell):
     """Average pool downsampling module.
 
     AvgPool Downsampling as in 'D' ResNet variants.
@@ -288,10 +293,10 @@             avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
             self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
         else:
-            self.pool = nn.Identity()
-        self.conv = layers.conv_norm_act(in_chs, out_chs, 1, apply_act=apply_act, **dd)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.pool = msnn.Identity()
+        self.conv = layers.conv_norm_act(in_chs, out_chs, 1, apply_act=apply_act, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -311,7 +316,7 @@         dilation: Tuple[int, int],
         layers: LayerFn,
         **kwargs,
-) -> Optional[nn.Module]:
+) -> Optional[msnn.Cell]:
     """Create shortcut connection for residual blocks.
 
     Args:
@@ -331,14 +336,14 @@         if not downsample_type:
             return None  # no shortcut
         elif downsample_type == 'avg':
-            return DownsampleAvg(in_chs, out_chs, stride=stride, dilation=dilation[0], **kwargs)
+            return DownsampleAvg(in_chs, out_chs, stride=stride, dilation=dilation[0], **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            return layers.conv_norm_act(in_chs, out_chs, kernel_size=1, stride=stride, dilation=dilation[0], **kwargs)
+            return layers.conv_norm_act(in_chs, out_chs, kernel_size=1, stride=stride, dilation=dilation[0], **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     else:
-        return nn.Identity()  # identity shortcut
-
-
-class BasicBlock(nn.Module):
+        return msnn.Identity()  # identity shortcut
+
+
+class BasicBlock(msnn.Cell):
     """ ResNet Basic Block - kxk + kxk
     """
 
@@ -375,10 +380,10 @@             apply_act=False,
             layers=layers,
             **dd,
-        )
-
-        self.conv1_kxk = layers.conv_norm_act(in_chs, mid_chs, kernel_size, stride=stride, dilation=dilation[0], **dd)
-        self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.conv1_kxk = layers.conv_norm_act(in_chs, mid_chs, kernel_size, stride=stride, dilation=dilation[0], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn = msnn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs)
         self.conv2_kxk = layers.conv_norm_act(
             mid_chs,
             out_chs,
@@ -388,19 +393,19 @@             drop_layer=drop_block,
             apply_act=False,
             **dd,
-        )
-        self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()
-        self.act = nn.Identity() if linear_out else layers.act(inplace=True)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn_last = msnn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else msnn.Identity()
+        self.act = msnn.Identity() if linear_out else layers.act(inplace=True)
 
     def init_weights(self, zero_init_last: bool = False):
         if zero_init_last and self.shortcut is not None and getattr(self.conv2_kxk.bn, 'weight', None) is not None:
-            nn.init.zeros_(self.conv2_kxk.bn.weight)
+            nn.init.zeros_(self.conv2_kxk.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         for attn in (self.attn, self.attn_last):
             if hasattr(attn, 'reset_parameters'):
                 attn.reset_parameters()
 
-    def forward(self, x):
+    def construct(self, x):
         shortcut = x
         x = self.conv1_kxk(x)
         x = self.attn(x)
@@ -412,7 +417,7 @@         return self.act(x)
 
 
-class BottleneckBlock(nn.Module):
+class BottleneckBlock(msnn.Cell):
     """ ResNet-like Bottleneck Block - 1x1 - kxk - 1x1
     """
 
@@ -451,9 +456,9 @@             apply_act=False,
             layers=layers,
             **dd,
-        )
-
-        self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv2_kxk = layers.conv_norm_act(
             mid_chs,
             mid_chs,
@@ -463,7 +468,7 @@             groups=groups,
             drop_layer=drop_block,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if extra_conv:
             self.conv2b_kxk = layers.conv_norm_act(
                 mid_chs,
@@ -472,23 +477,23 @@                 dilation=dilation[1],
                 groups=groups,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.conv2b_kxk = nn.Identity()
-        self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs, **dd)
-        self.conv3_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False, **dd)
-        self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()
-        self.act = nn.Identity() if linear_out else layers.act(inplace=True)
+            self.conv2b_kxk = msnn.Identity()
+        self.attn = msnn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv3_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn_last = msnn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else msnn.Identity()
+        self.act = msnn.Identity() if linear_out else layers.act(inplace=True)
 
     def init_weights(self, zero_init_last: bool = False):
         if zero_init_last and self.shortcut is not None and getattr(self.conv3_1x1.bn, 'weight', None) is not None:
-            nn.init.zeros_(self.conv3_1x1.bn.weight)
+            nn.init.zeros_(self.conv3_1x1.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         for attn in (self.attn, self.attn_last):
             if hasattr(attn, 'reset_parameters'):
                 attn.reset_parameters()
 
-    def forward(self, x):
+    def construct(self, x):
         shortcut = x
         x = self.conv1_1x1(x)
         x = self.conv2_kxk(x)
@@ -502,7 +507,7 @@         return self.act(x)
 
 
-class DarkBlock(nn.Module):
+class DarkBlock(msnn.Cell):
     """ DarkNet-like (1x1 + 3x3 w/ stride) block
 
     The GE-Net impl included a 1x1 + 3x3 block in their search space. It was not used in the feature models.
@@ -546,10 +551,10 @@             apply_act=False,
             layers=layers,
             **dd,
-        )
-
-        self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1, **dd)
-        self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn = msnn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv2_kxk = layers.conv_norm_act(
             mid_chs,
             out_chs,
@@ -560,19 +565,19 @@             drop_layer=drop_block,
             apply_act=False,
             **dd,
-        )
-        self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()
-        self.act = nn.Identity() if linear_out else layers.act(inplace=True)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn_last = msnn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else msnn.Identity()
+        self.act = msnn.Identity() if linear_out else layers.act(inplace=True)
 
     def init_weights(self, zero_init_last: bool = False):
         if zero_init_last and self.shortcut is not None and getattr(self.conv2_kxk.bn, 'weight', None) is not None:
-            nn.init.zeros_(self.conv2_kxk.bn.weight)
+            nn.init.zeros_(self.conv2_kxk.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         for attn in (self.attn, self.attn_last):
             if hasattr(attn, 'reset_parameters'):
                 attn.reset_parameters()
 
-    def forward(self, x):
+    def construct(self, x):
         shortcut = x
         x = self.conv1_1x1(x)
         x = self.attn(x)
@@ -584,7 +589,7 @@         return self.act(x)
 
 
-class EdgeBlock(nn.Module):
+class EdgeBlock(msnn.Cell):
     """ EdgeResidual-like (3x3 + 1x1) block
 
     A two layer block like DarkBlock, but with the order of the 3x3 and 1x1 convs reversed.
@@ -627,7 +632,7 @@             apply_act=False,
             layers=layers,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv1_kxk = layers.conv_norm_act(
             in_chs,
             mid_chs,
@@ -637,21 +642,21 @@             groups=groups,
             drop_layer=drop_block,
             **dd,
-        )
-        self.attn = nn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs, **dd)
-        self.conv2_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False, **dd)
-        self.attn_last = nn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()
-        self.act = nn.Identity() if linear_out else layers.act(inplace=True)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn = msnn.Identity() if attn_last or layers.attn is None else layers.attn(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv2_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn_last = msnn.Identity() if not attn_last or layers.attn is None else layers.attn(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else msnn.Identity()
+        self.act = msnn.Identity() if linear_out else layers.act(inplace=True)
 
     def init_weights(self, zero_init_last: bool = False):
         if zero_init_last and self.shortcut is not None and getattr(self.conv2_1x1.bn, 'weight', None) is not None:
-            nn.init.zeros_(self.conv2_1x1.bn.weight)
+            nn.init.zeros_(self.conv2_1x1.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         for attn in (self.attn, self.attn_last):
             if hasattr(attn, 'reset_parameters'):
                 attn.reset_parameters()
 
-    def forward(self, x):
+    def construct(self, x):
         shortcut = x
         x = self.conv1_kxk(x)
         x = self.attn(x)
@@ -663,7 +668,7 @@         return self.act(x)
 
 
-class RepVggBlock(nn.Module):
+class RepVggBlock(msnn.Cell):
     """ RepVGG Block.
 
     Adapted from impl at https://github.com/DingXiaoH/RepVGG
@@ -701,11 +706,11 @@                 groups=groups,
                 bias=True,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，需手动确认参数映射;
         else:
             self.reparam_conv = None
             use_ident = in_chs == out_chs and stride == 1 and dilation[0] == dilation[1]
-            self.identity = layers.norm_act(out_chs, apply_act=False, **dd) if use_ident else None
+            self.identity = layers.norm_act(out_chs, apply_act=False, **dd) if use_ident else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.conv_kxk = layers.conv_norm_act(
                 in_chs,
                 out_chs,
@@ -716,7 +721,7 @@                 drop_layer=drop_block,
                 apply_act=False,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.conv_1x1 = layers.conv_norm_act(
                 in_chs,
                 out_chs,
@@ -725,22 +730,22 @@                 groups=groups,
                 apply_act=False,
                 **dd,
-            )
-            self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. and use_ident else nn.Identity()
-
-        self.attn = nn.Identity() if layers.attn is None else layers.attn(out_chs, **dd)
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. and use_ident else msnn.Identity()
+
+        self.attn = msnn.Identity() if layers.attn is None else layers.attn(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act = layers.act(inplace=True)
 
     def init_weights(self, zero_init_last: bool = False):
         # NOTE this init overrides that base model init with specific changes for the block type
         for m in self.modules():
             if isinstance(m, nn.BatchNorm2d):
-                nn.init.normal_(m.weight, .1, .1)
-                nn.init.normal_(m.bias, 0, .1)
+                nn.init.normal_(m.weight, .1, .1)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                nn.init.normal_(m.bias, 0, .1)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if hasattr(self.attn, 'reset_parameters'):
             self.attn.reset_parameters()
 
-    def forward(self, x):
+    def construct(self, x):
         if self.reparam_conv is not None:
             return self.act(self.attn(self.reparam_conv(x)))
 
@@ -765,15 +770,7 @@ 
         kernel, bias = self._get_kernel_bias()
         self.reparam_conv = nn.Conv2d(
-            in_channels=self.conv_kxk.conv.in_channels,
-            out_channels=self.conv_kxk.conv.out_channels,
-            kernel_size=self.conv_kxk.conv.kernel_size,
-            stride=self.conv_kxk.conv.stride,
-            padding=self.conv_kxk.conv.padding,
-            dilation=self.conv_kxk.conv.dilation,
-            groups=self.conv_kxk.conv.groups,
-            bias=True,
-        )
+            in_channels = self.conv_kxk.conv.in_channels, out_channels = self.conv_kxk.conv.out_channels, kernel_size = self.conv_kxk.conv.kernel_size, stride = self.conv_kxk.conv.stride, padding = self.conv_kxk.conv.padding, dilation = self.conv_kxk.conv.dilation, groups = self.conv_kxk.conv.groups, bias = True)
         self.reparam_conv.weight.data = kernel
         self.reparam_conv.bias.data = bias
 
@@ -787,7 +784,7 @@         self.__delattr__('identity')
         self.__delattr__('drop_path')
 
-    def _get_kernel_bias(self) -> Tuple[torch.Tensor, torch.Tensor]:
+    def _get_kernel_bias(self) -> Tuple[ms.Tensor, ms.Tensor]:
         """ Method to obtain re-parameterized kernel and bias.
         Reference: https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py#L83
         """
@@ -798,7 +795,7 @@             kernel_1x1, bias_1x1 = self._fuse_bn_tensor(self.conv_1x1)
             # Pad scale branch kernel to match conv branch kernel size.
             pad = self.conv_kxk.conv.kernel_size[0] // 2
-            kernel_1x1 = torch.nn.functional.pad(kernel_1x1, [pad, pad, pad, pad])
+            kernel_1x1 = nn.functional.pad(kernel_1x1, [pad, pad, pad, pad])
 
         # get weights and bias of skip branch
         kernel_identity = 0
@@ -813,7 +810,7 @@         bias_final = bias_conv + bias_1x1 + bias_identity
         return kernel_final, bias_final
 
-    def _fuse_bn_tensor(self, branch) -> Tuple[torch.Tensor, torch.Tensor]:
+    def _fuse_bn_tensor(self, branch) -> Tuple[ms.Tensor, ms.Tensor]:
         """ Method to fuse batchnorm layer with preceding conv layer.
         Reference: https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py#L95
         """
@@ -830,7 +827,7 @@                 in_chs = self.conv_kxk.conv.in_channels
                 input_dim = in_chs // self.groups
                 kernel_size = self.conv_kxk.conv.kernel_size
-                kernel_value = torch.zeros_like(self.conv_kxk.conv.weight)
+                kernel_value = mint.zeros_like(self.conv_kxk.conv.weight)
                 for i in range(in_chs):
                     kernel_value[i, i % input_dim, kernel_size[0] // 2, kernel_size[1] // 2] = 1
                 self.id_tensor = kernel_value
@@ -845,7 +842,7 @@         return kernel * t, beta - running_mean * gamma / std
 
 
-class MobileOneBlock(nn.Module):
+class MobileOneBlock(msnn.Cell):
     """ MobileOne building block.
 
         This block has a multi-branched architecture at train-time
@@ -891,13 +888,13 @@                 groups=groups,
                 bias=True,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，需手动确认参数映射;
         else:
             self.reparam_conv = None
 
             # Re-parameterizable skip connection
             use_ident = in_chs == out_chs and stride == 1 and dilation[0] == dilation[1]
-            self.identity = layers.norm_act(out_chs, apply_act=False, **dd) if use_ident else None
+            self.identity = layers.norm_act(out_chs, apply_act=False, **dd) if use_ident else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
             # Re-parameterizable conv branches
             convs = []
@@ -910,8 +907,8 @@                     groups=groups,
                     apply_act=False,
                     **dd,
-                ))
-            self.conv_kxk = nn.ModuleList(convs)
+                ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.conv_kxk = msnn.CellList(convs)
 
             # Re-parameterizable scale branch
             self.conv_scale = None
@@ -924,13 +921,13 @@                     groups=groups,
                     apply_act=False,
                     **dd,
-                )
-            self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. and use_ident else nn.Identity()
-
-        self.attn = nn.Identity() if layers.attn is None else layers.attn(out_chs, **dd)
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. and use_ident else msnn.Identity()
+
+        self.attn = msnn.Identity() if layers.attn is None else layers.attn(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act = layers.act(inplace=True)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """ Apply forward pass. """
         # Inference mode forward pass.
         if self.reparam_conv is not None:
@@ -967,14 +964,7 @@ 
         kernel, bias = self._get_kernel_bias()
         self.reparam_conv = nn.Conv2d(
-            in_channels=self.conv_kxk[0].conv.in_channels,
-            out_channels=self.conv_kxk[0].conv.out_channels,
-            kernel_size=self.conv_kxk[0].conv.kernel_size,
-            stride=self.conv_kxk[0].conv.stride,
-            padding=self.conv_kxk[0].conv.padding,
-            dilation=self.conv_kxk[0].conv.dilation,
-            groups=self.conv_kxk[0].conv.groups,
-            bias=True)
+            in_channels = self.conv_kxk[0].conv.in_channels, out_channels = self.conv_kxk[0].conv.out_channels, kernel_size = self.conv_kxk[0].conv.kernel_size, stride = self.conv_kxk[0].conv.stride, padding = self.conv_kxk[0].conv.padding, dilation = self.conv_kxk[0].conv.dilation, groups = self.conv_kxk[0].conv.groups, bias = True)
         self.reparam_conv.weight.data = kernel
         self.reparam_conv.bias.data = bias
 
@@ -988,7 +978,7 @@         self.__delattr__('identity')
         self.__delattr__('drop_path')
 
-    def _get_kernel_bias(self) -> Tuple[torch.Tensor, torch.Tensor]:
+    def _get_kernel_bias(self) -> Tuple[ms.Tensor, ms.Tensor]:
         """ Method to obtain re-parameterized kernel and bias.
         Reference: https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py#L83
         """
@@ -999,7 +989,7 @@             kernel_scale, bias_scale = self._fuse_bn_tensor(self.conv_scale)
             # Pad scale branch kernel to match conv branch kernel size.
             pad = self.conv_kxk[0].conv.kernel_size[0] // 2
-            kernel_scale = torch.nn.functional.pad(kernel_scale, [pad, pad, pad, pad])
+            kernel_scale = nn.functional.pad(kernel_scale, [pad, pad, pad, pad])
 
         # get weights and bias of skip branch
         kernel_identity = 0
@@ -1019,7 +1009,7 @@         bias_final = bias_conv + bias_scale + bias_identity
         return kernel_final, bias_final
 
-    def _fuse_bn_tensor(self, branch) -> Tuple[torch.Tensor, torch.Tensor]:
+    def _fuse_bn_tensor(self, branch) -> Tuple[ms.Tensor, ms.Tensor]:
         """ Method to fuse batchnorm layer with preceding conv layer.
         Reference: https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py#L95
         """
@@ -1036,7 +1026,7 @@                 in_chs = self.conv_kxk[0].conv.in_channels
                 input_dim = in_chs // self.groups
                 kernel_size = self.conv_kxk[0].conv.kernel_size
-                kernel_value = torch.zeros_like(self.conv_kxk[0].conv.weight)
+                kernel_value = mint.zeros_like(self.conv_kxk[0].conv.weight)
                 for i in range(in_chs):
                     kernel_value[i, i % input_dim, kernel_size[0] // 2, kernel_size[1] // 2] = 1
                 self.id_tensor = kernel_value
@@ -1051,7 +1041,7 @@         return kernel * t, beta - running_mean * gamma / std
 
 
-class SelfAttnBlock(nn.Module):
+class SelfAttnBlock(msnn.Cell):
     """ ResNet-like Bottleneck Block - 1x1 - optional kxk - self attn - 1x1
     """
 
@@ -1091,9 +1081,9 @@             apply_act=False,
             layers=layers,
             **dd,
-        )
-
-        self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.conv1_1x1 = layers.conv_norm_act(in_chs, mid_chs, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if extra_conv:
             self.conv2_kxk = layers.conv_norm_act(
                 mid_chs,
@@ -1104,25 +1094,25 @@                 groups=groups,
                 drop_layer=drop_block,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             stride = 1  # striding done via conv if enabled
         else:
-            self.conv2_kxk = nn.Identity()
+            self.conv2_kxk = msnn.Identity()
         opt_kwargs = {} if feat_size is None else dict(feat_size=feat_size)
         # FIXME need to dilate self attn to have dilated network support, moop moop
-        self.self_attn = layers.self_attn(mid_chs, stride=stride, **opt_kwargs, **dd)
-        self.post_attn = layers.norm_act(mid_chs, **dd) if post_attn_na else nn.Identity()
-        self.conv3_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()
-        self.act = nn.Identity() if linear_out else layers.act(inplace=True)
+        self.self_attn = layers.self_attn(mid_chs, stride=stride, **opt_kwargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.post_attn = layers.norm_act(mid_chs, **dd) if post_attn_na else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv3_1x1 = layers.conv_norm_act(mid_chs, out_chs, 1, apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0. else msnn.Identity()
+        self.act = msnn.Identity() if linear_out else layers.act(inplace=True)
 
     def init_weights(self, zero_init_last: bool = False):
         if zero_init_last and self.shortcut is not None and getattr(self.conv3_1x1.bn, 'weight', None) is not None:
-            nn.init.zeros_(self.conv3_1x1.bn.weight)
+            nn.init.zeros_(self.conv3_1x1.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if hasattr(self.self_attn, 'reset_parameters'):
             self.self_attn.reset_parameters()
 
-    def forward(self, x):
+    def construct(self, x):
         shortcut = x
         x = self.conv1_1x1(x)
         x = self.conv2_kxk(x)
@@ -1146,18 +1136,18 @@ )
 
 
-def register_block(block_type: str, block_fn: nn.Module):
+def register_block(block_type: str, block_fn: msnn.Cell):
     _block_registry[block_type] = block_fn
 
 
-def create_block(block: Union[str, nn.Module], **kwargs):
-    if isinstance(block, (nn.Module, partial)):
-        return block(**kwargs)
+def create_block(block: Union[str, msnn.Cell], **kwargs):
+    if isinstance(block, (msnn.Cell, partial)):
+        return block(**kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     assert block in _block_registry, f'Unknown block type ({block}'
-    return _block_registry[block](**kwargs)
-
-
-class Stem(nn.Sequential):
+    return _block_registry[block](**kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+
+class Stem(msnn.SequentialCell):
 
     def __init__(
             self,
@@ -1204,7 +1194,7 @@             if i > 0 and s > 1:
                 last_feat_idx = i - 1
                 self.feature_info.append(dict(num_chs=prev_chs, reduction=curr_stride, module=prev_feat, stage=0))
-            self.add_module(conv_name, layer_fn(prev_chs, ch, kernel_size=kernel_size, stride=s, **dd))
+            self.add_module(conv_name, layer_fn(prev_chs, ch, kernel_size=kernel_size, stride=s, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_chs = ch
             curr_stride *= s
             prev_feat = conv_name
@@ -1219,9 +1209,9 @@             elif pool == 'avg2':
                 self.add_module('pool', nn.AvgPool2d(2))
             elif 'max' in pool:
-                self.add_module('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
+                self.add_module('pool', nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1))
             elif 'avg' in pool:
-                self.add_module('pool', nn.AvgPool2d(kernel_size=3, stride=2, padding=1, count_include_pad=False))
+                self.add_module('pool', nn.AvgPool2d(kernel_size = 3, stride = 2, padding = 1, count_include_pad = False))
             curr_stride *= 2
             prev_feat = 'pool'
 
@@ -1229,8 +1219,8 @@         self.feature_info.append(dict(num_chs=prev_chs, reduction=curr_stride, module=prev_feat, stage=0))
         assert curr_stride == stride
 
-    def forward_intermediates(self, x) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
-        intermediate: Optional[torch.Tensor] = None
+    def forward_intermediates(self, x) -> Tuple[ms.Tensor, Optional[ms.Tensor]]:
+        intermediate: Optional[ms.Tensor] = None
         for i, m in enumerate(self):
             x = m(x)
             if self.last_feat_idx is not None and i == self.last_feat_idx:
@@ -1254,32 +1244,32 @@     if 'quad' in stem_type:
         # based on NFNet stem, stack of 4 3x3 convs
         num_act = 2 if 'quad2' in stem_type else None
-        stem = Stem(in_chs, out_chs, num_rep=4, num_act=num_act, pool=pool_type, layers=layers, **dd)
+        stem = Stem(in_chs, out_chs, num_rep=4, num_act=num_act, pool=pool_type, layers=layers, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif 'tiered' in stem_type:
         # 3x3 stack of 3 convs as in my ResNet-T
-        stem = Stem(in_chs, (3 * out_chs // 8, out_chs // 2, out_chs), pool=pool_type, layers=layers, **dd)
+        stem = Stem(in_chs, (3 * out_chs // 8, out_chs // 2, out_chs), pool=pool_type, layers=layers, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif 'deep' in stem_type:
         # 3x3 stack of 3 convs as in ResNet-D
-        stem = Stem(in_chs, out_chs, num_rep=3, chs_decay=1.0, pool=pool_type, layers=layers, **dd)
+        stem = Stem(in_chs, out_chs, num_rep=3, chs_decay=1.0, pool=pool_type, layers=layers, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif 'rep' in stem_type:
-        stem = RepVggBlock(in_chs, out_chs, stride=2, layers=layers, **dd)
+        stem = RepVggBlock(in_chs, out_chs, stride=2, layers=layers, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif 'one' in stem_type:
-        stem = MobileOneBlock(in_chs, out_chs, kernel_size=3, stride=2, layers=layers, **dd)
+        stem = MobileOneBlock(in_chs, out_chs, kernel_size=3, stride=2, layers=layers, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif '7x7' in stem_type:
         # 7x7 stem conv as in ResNet
         if pool_type:
-            stem = Stem(in_chs, out_chs, 7, num_rep=1, pool=pool_type, layers=layers, **dd)
+            stem = Stem(in_chs, out_chs, 7, num_rep=1, pool=pool_type, layers=layers, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            stem = layers.conv_norm_act(in_chs, out_chs, 7, stride=2, **dd)
+            stem = layers.conv_norm_act(in_chs, out_chs, 7, stride=2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     else:
         if isinstance(out_chs, (tuple, list)):
-            stem = Stem(in_chs, out_chs, 3, pool=pool_type, layers=layers, **dd)
+            stem = Stem(in_chs, out_chs, 3, pool=pool_type, layers=layers, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             # 3x3 stem conv as in RegNet is the default
             if pool_type:
-                stem = Stem(in_chs, out_chs, 3, num_rep=1, pool=pool_type, layers=layers, **dd)
+                stem = Stem(in_chs, out_chs, 3, num_rep=1, pool=pool_type, layers=layers, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
-                stem = layers.conv_norm_act(in_chs, out_chs, 3, stride=2, **dd)
+                stem = layers.conv_norm_act(in_chs, out_chs, 3, stride=2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     if isinstance(stem, Stem):
         feature_info = [dict(f, module='.'.join([feat_prefix, f['module']])) for f in stem.feature_info]
@@ -1317,7 +1307,7 @@         else:
             attn_kwargs = override_kwargs(block_cfg.attn_kwargs, model_cfg.attn_kwargs)
             attn_layer = block_cfg.attn_layer or model_cfg.attn_layer
-            attn_layer = partial(get_attn(attn_layer), **attn_kwargs) if attn_layer is not None else None
+            attn_layer = partial(get_attn(attn_layer), **attn_kwargs) if attn_layer is not None else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         layer_fns = replace(layer_fns, attn=attn_layer)
 
     # override self-attn layer / args with block local cfg
@@ -1331,7 +1321,7 @@             self_attn_kwargs = override_kwargs(block_cfg.self_attn_kwargs, model_cfg.self_attn_kwargs)
             self_attn_layer = block_cfg.self_attn_layer or model_cfg.self_attn_layer
             self_attn_layer = partial(get_attn(self_attn_layer), **self_attn_kwargs) \
-                if self_attn_layer is not None else None
+                if self_attn_layer is not None else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         layer_fns = replace(layer_fns, self_attn=self_attn_layer)
 
     block_kwargs['layers'] = layer_fns
@@ -1428,17 +1418,17 @@                 # add feat_size arg for blocks that support/need it
                 block_kwargs['feat_size'] = feat_size
             block_kwargs_fn(block_kwargs, block_cfg=block_cfg, model_cfg=cfg)
-            blocks += [create_block(block_cfg.type, **block_kwargs)]
+            blocks += [create_block(block_cfg.type, **block_kwargs)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             first_dilation = dilation
             prev_chs = out_chs
             if stride > 1 and block_idx == 0:
                 feat_size = reduce_feat_size(feat_size, stride)
 
-        stages += [nn.Sequential(*blocks)]
+        stages += [msnn.SequentialCell(*blocks)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         prev_feat = dict(num_chs=prev_chs, reduction=net_stride, module=f'stages.{stage_idx}', stage=stage_idx + 1)
 
     feature_info.append(prev_feat)
-    return nn.Sequential(*stages), feature_info, feat_size
+    return msnn.SequentialCell(*stages), feature_info, feat_size  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def get_layer_fns(cfg: ByoModelCfg, allow_aa: bool = True):
@@ -1448,13 +1438,13 @@         conv_norm_act = partial(ConvNormAct, norm_layer=cfg.norm_layer, act_layer=act, aa_layer=cfg.aa_layer)
     else:
         conv_norm_act = partial(ConvNormAct, norm_layer=cfg.norm_layer, act_layer=act)
-    attn = partial(get_attn(cfg.attn_layer), **cfg.attn_kwargs) if cfg.attn_layer else None
-    self_attn = partial(get_attn(cfg.self_attn_layer), **cfg.self_attn_kwargs) if cfg.self_attn_layer else None
+    attn = partial(get_attn(cfg.attn_layer), **cfg.attn_kwargs) if cfg.attn_layer else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    self_attn = partial(get_attn(cfg.self_attn_layer), **cfg.self_attn_kwargs) if cfg.self_attn_layer else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     layer_fn = LayerFn(conv_norm_act=conv_norm_act, norm_act=norm_act, act=act, attn=attn, self_attn=self_attn)
     return layer_fn
 
 
-class ByobNet(nn.Module):
+class ByobNet(msnn.Cell):
     """Bring-your-own-blocks Network.
 
     A flexible network backbone that allows building model stem + blocks via
@@ -1501,7 +1491,7 @@         self.drop_rate = drop_rate
         self.grad_checkpointing = False
 
-        cfg = replace(cfg, **kwargs)  # overlay kwargs onto cfg
+        cfg = replace(cfg, **kwargs)  # overlay kwargs onto cfg; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stem_layers = get_layer_fns(cfg, allow_aa=False)  # keep aa off for stem-layers
         stage_layers = get_layer_fns(cfg)
         if cfg.fixed_input_size:
@@ -1520,7 +1510,7 @@             pool_type=cfg.stem_pool,
             layers=stem_layers,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info.extend(stem_feat[:-1])
         feat_size = reduce_feat_size(feat_size, stride=stem_feat[-1]['reduction'])
 
@@ -1534,17 +1524,17 @@             layers=stage_layers,
             feat_size=feat_size,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info.extend(stage_feat[:-1])
         reduction = stage_feat[-1]['reduction']
 
         prev_chs = stage_feat[-1]['num_chs']
         if cfg.num_features:
             self.num_features = int(round(cfg.width_factor * cfg.num_features))
-            self.final_conv = stage_layers.conv_norm_act(prev_chs, self.num_features, 1, **dd)
+            self.final_conv = stage_layers.conv_norm_act(prev_chs, self.num_features, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.num_features = prev_chs
-            self.final_conv = nn.Identity()
+            self.final_conv = msnn.Identity()
         self.feature_info += [
             dict(num_chs=self.num_features, reduction=reduction, module='final_conv', stage=len(self.stages))]
         self.stage_ends = [f['stage'] for f in self.feature_info]
@@ -1563,7 +1553,7 @@                 act_layer=cfg.act_layer,
                 drop_rate=self.drop_rate,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.head_hidden_size = self.head.hidden_size
         elif cfg.head_type == 'attn_abs':
             if global_pool is None:
@@ -1578,7 +1568,7 @@                 drop_rate=self.drop_rate,
                 qkv_separate=True,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.head_hidden_size = self.head.embed_dim
         elif cfg.head_type == 'attn_rot':
             if global_pool is None:
@@ -1593,7 +1583,7 @@                 drop_rate=self.drop_rate,
                 qkv_separate=True,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.head_hidden_size = self.head.embed_dim
         else:
             if global_pool is None:
@@ -1605,13 +1595,13 @@                 pool_type=global_pool,
                 drop_rate=self.drop_rate,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.global_pool = global_pool
 
         # init weights
         named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse: bool = False) -> Dict[str, Any]:
         """Group matcher for parameter groups.
 
@@ -1630,7 +1620,7 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True) -> None:
         """Enable or disable gradient checkpointing.
 
@@ -1639,8 +1629,8 @@         """
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         """Get classifier module.
 
         Returns:
@@ -1660,14 +1650,14 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
             exclude_final_conv: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -1696,12 +1686,14 @@         if feat_idx in take_indices:
             intermediates.append(x if x_inter is None else x_inter)
         last_idx = self.stage_ends[-1]
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             stages = self.stages
         else:
             stages = self.stages[:max_index]
         for stage in stages:
             feat_idx += 1
+            # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if self.grad_checkpointing and not torch.jit.is_scripting():
                 x = checkpoint_seq(stage, x)
             else:
@@ -1740,12 +1732,12 @@         max_index = self.stage_ends[max_index]
         self.stages = self.stages[:max_index]  # truncate blocks w/ stem as idx 0
         if max_index < self.stage_ends[-1]:
-            self.final_conv = nn.Identity()
+            self.final_conv = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction.
 
         Args:
@@ -1755,6 +1747,7 @@             Feature tensor.
         """
         x = self.stem(x)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stages, x)
         else:
@@ -1762,7 +1755,7 @@         x = self.final_conv(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through head.
 
         Args:
@@ -1774,7 +1767,7 @@         """
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -1788,7 +1781,7 @@         return x
 
 
-def _init_weights(module: nn.Module, name: str = '', zero_init_last: bool = False) -> None:
+def _init_weights(module: msnn.Cell, name: str = '', zero_init_last: bool = False) -> None:
     """Initialize weights.
 
     Args:
@@ -1803,12 +1796,12 @@         if module.bias is not None:
             module.bias.data.zero_()
     elif isinstance(module, nn.Linear):
-        nn.init.normal_(module.weight, mean=0.0, std=0.01)
+        nn.init.normal_(module.weight, mean=0.0, std=0.01)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.BatchNorm2d):
-        nn.init.ones_(module.weight)
-        nn.init.zeros_(module.bias)
+        nn.init.ones_(module.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         module.init_weights(zero_init_last=zero_init_last)
 
@@ -2387,10 +2380,10 @@ 
 
 def _convert_openai_clip(
-        state_dict: Dict[str, torch.Tensor],
+        state_dict: Dict[str, ms.Tensor],
         model: ByobNet,
         prefix: str = 'visual.',
-) -> Dict[str, torch.Tensor]:
+) -> Dict[str, ms.Tensor]:
     model_has_attn_pool = isinstance(model.head, (RotAttentionPool2d, AttentionPool2d))
     import re
 
@@ -2430,7 +2423,7 @@ 
 
 def checkpoint_filter_fn(
-        state_dict: Dict[str, torch.Tensor],
+        state_dict: Dict[str, ms.Tensor],
         model: ByobNet
 ):
     if 'visual.conv1.weight' in state_dict:
@@ -2455,7 +2448,7 @@         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(flatten_sequential=True),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url: str = '', **kwargs) -> Dict[str, Any]:
@@ -2799,7 +2792,7 @@     """ GEResNet-Large (GENet-Large from official impl)
     `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090
     """
-    return _create_byobnet('gernet_l', pretrained=pretrained, **kwargs)
+    return _create_byobnet('gernet_l', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2807,7 +2800,7 @@     """ GEResNet-Medium (GENet-Normal from official impl)
     `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090
     """
-    return _create_byobnet('gernet_m', pretrained=pretrained, **kwargs)
+    return _create_byobnet('gernet_m', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2815,7 +2808,7 @@     """ EResNet-Small (GENet-Small from official impl)
     `Neural Architecture Design for GPU-Efficient Networks` - https://arxiv.org/abs/2006.14090
     """
-    return _create_byobnet('gernet_s', pretrained=pretrained, **kwargs)
+    return _create_byobnet('gernet_s', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2823,7 +2816,7 @@     """ RepVGG-A0
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_a0', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_a0', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2831,7 +2824,7 @@     """ RepVGG-A1
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_a1', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_a1', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2839,7 +2832,7 @@     """ RepVGG-A2
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_a2', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_a2', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2847,7 +2840,7 @@     """ RepVGG-B0
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_b0', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_b0', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2855,7 +2848,7 @@     """ RepVGG-B1
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_b1', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_b1', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2863,7 +2856,7 @@     """ RepVGG-B1g4
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_b1g4', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_b1g4', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2871,7 +2864,7 @@     """ RepVGG-B2
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_b2', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_b2', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2879,7 +2872,7 @@     """ RepVGG-B2g4
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_b2g4', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_b2g4', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2887,7 +2880,7 @@     """ RepVGG-B3
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_b3', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_b3', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2895,7 +2888,7 @@     """ RepVGG-B3g4
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_b3g4', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_b3g4', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2903,277 +2896,277 @@     """ RepVGG-D2se
     `Making VGG-style ConvNets Great Again` - https://arxiv.org/abs/2101.03697
     """
-    return _create_byobnet('repvgg_d2se', pretrained=pretrained, **kwargs)
+    return _create_byobnet('repvgg_d2se', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet51q(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('resnet51q', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet51q', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet61q(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('resnet61q', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet61q', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnext26ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('resnext26ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnext26ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def gcresnext26ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('gcresnext26ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('gcresnext26ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def seresnext26ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('seresnext26ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('seresnext26ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def eca_resnext26ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('eca_resnext26ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('eca_resnext26ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def bat_resnext26ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('bat_resnext26ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('bat_resnext26ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet32ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('resnet32ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet32ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet33ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('resnet33ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet33ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def gcresnet33ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('gcresnet33ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('gcresnet33ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def seresnet33ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('seresnet33ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('seresnet33ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def eca_resnet33ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('eca_resnet33ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('eca_resnet33ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def gcresnet50t(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('gcresnet50t', pretrained=pretrained, **kwargs)
+    return _create_byobnet('gcresnet50t', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def gcresnext50ts(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('gcresnext50ts', pretrained=pretrained, **kwargs)
+    return _create_byobnet('gcresnext50ts', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetz_b16(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('regnetz_b16', pretrained=pretrained, **kwargs)
+    return _create_byobnet('regnetz_b16', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetz_c16(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('regnetz_c16', pretrained=pretrained, **kwargs)
+    return _create_byobnet('regnetz_c16', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetz_d32(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('regnetz_d32', pretrained=pretrained, **kwargs)
+    return _create_byobnet('regnetz_d32', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetz_d8(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('regnetz_d8', pretrained=pretrained, **kwargs)
+    return _create_byobnet('regnetz_d8', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetz_e8(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('regnetz_e8', pretrained=pretrained, **kwargs)
+    return _create_byobnet('regnetz_e8', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetz_b16_evos(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('regnetz_b16_evos', pretrained=pretrained, **kwargs)
+    return _create_byobnet('regnetz_b16_evos', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetz_c16_evos(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('regnetz_c16_evos', pretrained=pretrained, **kwargs)
+    return _create_byobnet('regnetz_c16_evos', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetz_d8_evos(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('regnetz_d8_evos', pretrained=pretrained, **kwargs)
+    return _create_byobnet('regnetz_d8_evos', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobileone_s0(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('mobileone_s0', pretrained=pretrained, **kwargs)
+    return _create_byobnet('mobileone_s0', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobileone_s1(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('mobileone_s1', pretrained=pretrained, **kwargs)
+    return _create_byobnet('mobileone_s1', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobileone_s2(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('mobileone_s2', pretrained=pretrained, **kwargs)
+    return _create_byobnet('mobileone_s2', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobileone_s3(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('mobileone_s3', pretrained=pretrained, **kwargs)
+    return _create_byobnet('mobileone_s3', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobileone_s4(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('mobileone_s4', pretrained=pretrained, **kwargs)
+    return _create_byobnet('mobileone_s4', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet50_clip(pretrained=False, **kwargs) -> ByobNet:
     """ OpenAI Modified ResNet-50 CLIP image tower
     """
-    return _create_byobnet('resnet50_clip', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet50_clip', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet101_clip(pretrained=False, **kwargs) -> ByobNet:
     """ OpenAI Modified ResNet-101 CLIP image tower
     """
-    return _create_byobnet('resnet101_clip', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet101_clip', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet50x4_clip(pretrained=False, **kwargs) -> ByobNet:
     """ OpenAI Modified ResNet-50x4 CLIP image tower
     """
-    return _create_byobnet('resnet50x4_clip', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet50x4_clip', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet50x16_clip(pretrained=False, **kwargs) -> ByobNet:
     """ OpenAI Modified ResNet-50x16 CLIP image tower
     """
-    return _create_byobnet('resnet50x16_clip', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet50x16_clip', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet50x64_clip(pretrained=False, **kwargs) -> ByobNet:
     """ OpenAI Modified ResNet-50x64 CLIP image tower
     """
-    return _create_byobnet('resnet50x64_clip', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet50x64_clip', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet50_clip_gap(pretrained=False, **kwargs) -> ByobNet:
     """ OpenAI Modified ResNet-50 CLIP image tower w/ avg pool (no attention pool)
     """
-    return _create_byobnet('resnet50_clip_gap', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet50_clip_gap', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet101_clip_gap(pretrained=False, **kwargs) -> ByobNet:
     """ OpenAI Modified ResNet-101 CLIP image tower w/ avg pool (no attention pool)
     """
-    return _create_byobnet('resnet101_clip_gap', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet101_clip_gap', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet50x4_clip_gap(pretrained=False, **kwargs) -> ByobNet:
     """ OpenAI Modified ResNet-50x4 CLIP image tower w/ avg pool (no attention pool)
     """
-    return _create_byobnet('resnet50x4_clip_gap', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet50x4_clip_gap', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet50x16_clip_gap(pretrained=False, **kwargs) -> ByobNet:
     """ OpenAI Modified ResNet-50x16 CLIP image tower w/ avg pool (no attention pool)
     """
-    return _create_byobnet('resnet50x16_clip_gap', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet50x16_clip_gap', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet50x64_clip_gap(pretrained=False, **kwargs) -> ByobNet:
     """ OpenAI Modified ResNet-50x64 CLIP image tower w/ avg pool (no attention pool)
     """
-    return _create_byobnet('resnet50x64_clip_gap', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet50x64_clip_gap', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnet50_mlp(pretrained=False, **kwargs) -> ByobNet:
     """
     """
-    return _create_byobnet('resnet50_mlp', pretrained=pretrained, **kwargs)
+    return _create_byobnet('resnet50_mlp', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def test_byobnet(pretrained=False, **kwargs) -> ByobNet:
     """ Minimal test ResNet (BYOB based) model.
     """
-    return _create_byobnet('test_byobnet', pretrained=pretrained, **kwargs)
+    return _create_byobnet('test_byobnet', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
