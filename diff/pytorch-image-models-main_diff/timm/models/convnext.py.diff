--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ ConvNeXt
 
 Papers:
@@ -40,8 +45,8 @@ from functools import partial
 from typing import Callable, Dict, List, Optional, Tuple, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
 from timm.layers import (
@@ -73,7 +78,7 @@ __all__ = ['ConvNeXt']  # model_registry will add each entrypoint fn to this
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
     """Downsample module for ConvNeXt."""
 
     def __init__(
@@ -100,21 +105,21 @@             avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
             self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
         else:
-            self.pool = nn.Identity()
+            self.pool = msnn.Identity()
 
         if in_chs != out_chs:
-            self.conv = create_conv2d(in_chs, out_chs, 1, stride=1, **dd)
+            self.conv = create_conv2d(in_chs, out_chs, 1, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.conv = nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.conv = msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = self.pool(x)
         x = self.conv(x)
         return x
 
 
-class ConvNeXtBlock(nn.Module):
+class ConvNeXtBlock(msnn.Cell):
     """ConvNeXt Block.
 
     There are two equivalent implementations:
@@ -179,22 +184,22 @@             depthwise=True,
             bias=conv_bias,
             **dd,
-        )
-        self.norm = norm_layer(out_chs, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm = norm_layer(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.mlp = mlp_layer(
             out_chs,
             int(mlp_ratio * out_chs),
             act_layer=act_layer,
             **dd,
-        )
-        self.gamma = nn.Parameter(ls_init_value * torch.ones(out_chs, **dd)) if ls_init_value is not None else None
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.gamma = ms.Parameter(ls_init_value * mint.ones(out_chs, **dd)) if ls_init_value is not None else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if in_chs != out_chs or stride != 1 or dilation[0] != dilation[1]:
-            self.shortcut = Downsample(in_chs, out_chs, stride=stride, dilation=dilation[0], **dd)
+            self.shortcut = Downsample(in_chs, out_chs, stride=stride, dilation=dilation[0], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.shortcut = nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.shortcut = msnn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         shortcut = x
         x = self.conv_dw(x)
@@ -213,7 +218,7 @@         return x
 
 
-class ConvNeXtStage(nn.Module):
+class ConvNeXtStage(msnn.Cell):
     """ConvNeXt stage (multiple blocks)."""
 
     def __init__(
@@ -260,7 +265,8 @@         if in_chs != out_chs or stride > 1 or dilation[0] != dilation[1]:
             ds_ks = 2 if stride > 1 or dilation[0] != dilation[1] else 1
             pad = 'same' if dilation[1] > 1 else 0  # same padding needed if dilation used
-            self.downsample = nn.Sequential(
+            self.downsample = msnn.SequentialCell(
+                [
                 norm_layer(in_chs, **dd),
                 create_conv2d(
                     in_chs,
@@ -271,11 +277,11 @@                     padding=pad,
                     bias=conv_bias,
                     **dd,
-                ),
-            )
+                )
+            ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             in_chs = out_chs
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         drop_path_rates = drop_path_rates or [0.] * depth
         stage_blocks = []
@@ -293,13 +299,14 @@                 act_layer=act_layer,
                 norm_layer=norm_layer if conv_mlp else norm_layer_cl,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             in_chs = out_chs
-        self.blocks = nn.Sequential(*stage_blocks)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.blocks = msnn.SequentialCell(*stage_blocks)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = self.downsample(x)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
@@ -335,7 +342,7 @@     return norm_layer, norm_layer_cl
 
 
-class ConvNeXt(nn.Module):
+class ConvNeXt(msnn.Cell):
     """ConvNeXt model architecture.
 
     A PyTorch impl of : `A ConvNet for the 2020s`  - https://arxiv.org/pdf/2201.03545.pdf
@@ -404,22 +411,23 @@         assert stem_type in ('patch', 'overlap', 'overlap_tiered', 'overlap_act')
         if stem_type == 'patch':
             # NOTE: this stem is a minimal form of ViT PatchEmbed, as used in SwinTransformer w/ patch_size = 4
-            self.stem = nn.Sequential(
+            self.stem = msnn.SequentialCell(
+                [
                 nn.Conv2d(in_chans, dims[0], kernel_size=patch_size, stride=patch_size, bias=conv_bias, **dd),
-                norm_layer(dims[0], **dd),
-            )
+                norm_layer(dims[0], **dd)
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
             stem_stride = patch_size
         else:
             mid_chs = make_divisible(dims[0] // 2) if 'tiered' in stem_type else dims[0]
-            self.stem = nn.Sequential(*filter(None, [
+            self.stem = msnn.SequentialCell(*filter(None, [
                 nn.Conv2d(in_chans, mid_chs, kernel_size=3, stride=2, padding=1, bias=conv_bias, **dd),
                 act_layer() if 'act' in stem_type else None,
                 nn.Conv2d(mid_chs, dims[0], kernel_size=3, stride=2, padding=1, bias=conv_bias, **dd),
                 norm_layer(dims[0], **dd),
-            ]))
+            ]))  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
             stem_stride = 4
 
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         dp_rates = calculate_drop_path_rates(drop_path_rate, depths, stagewise=True)
         stages = []
         prev_chs = dims[0]
@@ -450,27 +458,27 @@                 norm_layer=norm_layer,
                 norm_layer_cl=norm_layer_cl,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_chs = out_chs
             # NOTE feature_info use currently assumes stage 0 == stride 1, rest are stride 2
             self.feature_info += [dict(num_chs=prev_chs, reduction=curr_stride, module=f'stages.{i}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.num_features = self.head_hidden_size = prev_chs
 
         # if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets
         # otherwise pool -> norm -> fc, the default ConvNeXt ordering (pretrained FB weights)
         if head_norm_first:
             assert not head_hidden_size
-            self.norm_pre = norm_layer(self.num_features, **dd)
+            self.norm_pre = norm_layer(self.num_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.head = ClassifierHead(
                 self.num_features,
                 num_classes,
                 pool_type=global_pool,
                 drop_rate=self.drop_rate,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
             self.head = NormMlpClassifierHead(
                 self.num_features,
                 num_classes,
@@ -480,11 +488,11 @@                 norm_layer=norm_layer,
                 act_layer='gelu',
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.head_hidden_size = self.head.num_features
         named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse: bool = False) -> Dict[str, Union[str, List]]:
         """Create regex patterns for parameter grouping.
 
@@ -503,7 +511,7 @@             ]
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True) -> None:
         """Enable or disable gradient checkpointing.
 
@@ -513,8 +521,8 @@         for s in self.stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         """Get the classifier module."""
         return self.head.fc
 
@@ -530,13 +538,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """Forward features that returns intermediates.
 
         Args:
@@ -558,6 +566,7 @@         x = self.stem(x)
 
         last_idx = len(self.stages) - 1
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             stages = self.stages
         else:
@@ -597,19 +606,19 @@         take_indices, max_index = feature_take_indices(len(self.stages), indices)
         self.stages = self.stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers."""
         x = self.stem(x)
         x = self.stages(x)
         x = self.norm_pre(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -621,14 +630,14 @@         """
         return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-def _init_weights(module: nn.Module, name: Optional[str] = None, head_init_scale: float = 1.0) -> None:
+def _init_weights(module: msnn.Cell, name: Optional[str] = None, head_init_scale: float = 1.0) -> None:
     """Initialize model weights.
 
     Args:
@@ -639,10 +648,10 @@     if isinstance(module, nn.Conv2d):
         trunc_normal_(module.weight, std=.02)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Linear):
         trunc_normal_(module.weight, std=.02)
-        nn.init.zeros_(module.bias)
+        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if name and 'head.' in name:
             module.weight.data.mul_(head_init_scale)
             module.bias.data.mul_(head_init_scale)
@@ -660,12 +669,12 @@         out_dict = {k.replace('visual.trunk.', ''): v for k, v in state_dict.items() if k.startswith('visual.trunk.')}
         if 'visual.head.proj.weight' in state_dict:
             out_dict['head.fc.weight'] = state_dict['visual.head.proj.weight']
-            out_dict['head.fc.bias'] = torch.zeros(state_dict['visual.head.proj.weight'].shape[0])
+            out_dict['head.fc.bias'] = mint.zeros(state_dict['visual.head.proj.weight'].shape[0])
         elif 'visual.head.mlp.fc1.weight' in state_dict:
             out_dict['head.pre_logits.fc.weight'] = state_dict['visual.head.mlp.fc1.weight']
             out_dict['head.pre_logits.fc.bias'] = state_dict['visual.head.mlp.fc1.bias']
             out_dict['head.fc.weight'] = state_dict['visual.head.mlp.fc2.weight']
-            out_dict['head.fc.bias'] = torch.zeros(state_dict['visual.head.mlp.fc2.weight'].shape[0])
+            out_dict['head.fc.bias'] = mint.zeros(state_dict['visual.head.mlp.fc2.weight'].shape[0])
         return out_dict
 
     import re
@@ -700,7 +709,7 @@         ConvNeXt, variant, pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(out_indices=(0, 1, 2, 3), flatten_sequential=True),
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1154,7 +1163,7 @@ def convnext_zepto_rms(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant (NOTE: still tweaking depths, will vary between 3-4M param, current is 3.7M
     model_args = dict(depths=(2, 2, 4, 2), dims=(32, 64, 128, 256), conv_mlp=True, norm_layer='simplenorm')
-    model = _create_convnext('convnext_zepto_rms', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_zepto_rms', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1163,7 +1172,7 @@     # timm femto variant (NOTE: still tweaking depths, will vary between 3-4M param, current is 3.7M
     model_args = dict(
         depths=(2, 2, 4, 2), dims=(32, 64, 128, 256), conv_mlp=True, norm_layer='simplenorm', stem_type='overlap_act')
-    model = _create_convnext('convnext_zepto_rms_ols', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_zepto_rms_ols', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1171,7 +1180,7 @@ def convnext_atto(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant (NOTE: still tweaking depths, will vary between 3-4M param, current is 3.7M
     model_args = dict(depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), conv_mlp=True)
-    model = _create_convnext('convnext_atto', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_atto', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1179,7 +1188,7 @@ def convnext_atto_ols(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant with overlapping 3x3 conv stem, wider than non-ols femto above, current param count 3.7M
     model_args = dict(depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), conv_mlp=True, stem_type='overlap_tiered')
-    model = _create_convnext('convnext_atto_ols', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_atto_ols', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1187,7 +1196,7 @@ def convnext_atto_rms(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant (NOTE: still tweaking depths, will vary between 3-4M param, current is 3.7M
     model_args = dict(depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), conv_mlp=True, norm_layer='rmsnorm2d')
-    model = _create_convnext('convnext_atto_rms', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_atto_rms', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1195,7 +1204,7 @@ def convnext_femto(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant
     model_args = dict(depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), conv_mlp=True)
-    model = _create_convnext('convnext_femto', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_femto', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1203,7 +1212,7 @@ def convnext_femto_ols(pretrained=False, **kwargs) -> ConvNeXt:
     # timm femto variant
     model_args = dict(depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), conv_mlp=True, stem_type='overlap_tiered')
-    model = _create_convnext('convnext_femto_ols', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_femto_ols', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1211,7 +1220,7 @@ def convnext_pico(pretrained=False, **kwargs) -> ConvNeXt:
     # timm pico variant
     model_args = dict(depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), conv_mlp=True)
-    model = _create_convnext('convnext_pico', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_pico', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1219,7 +1228,7 @@ def convnext_pico_ols(pretrained=False, **kwargs) -> ConvNeXt:
     # timm nano variant with overlapping 3x3 conv stem
     model_args = dict(depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), conv_mlp=True,  stem_type='overlap_tiered')
-    model = _create_convnext('convnext_pico_ols', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_pico_ols', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1227,7 +1236,7 @@ def convnext_nano(pretrained=False, **kwargs) -> ConvNeXt:
     # timm nano variant with standard stem and head
     model_args = dict(depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), conv_mlp=True)
-    model = _create_convnext('convnext_nano', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_nano', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1235,7 +1244,7 @@ def convnext_nano_ols(pretrained=False, **kwargs) -> ConvNeXt:
     # experimental nano variant with overlapping conv stem
     model_args = dict(depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), conv_mlp=True, stem_type='overlap')
-    model = _create_convnext('convnext_nano_ols', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_nano_ols', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1243,56 +1252,56 @@ def convnext_tiny_hnf(pretrained=False, **kwargs) -> ConvNeXt:
     # experimental tiny variant with norm before pooling in head (head norm first)
     model_args = dict(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), head_norm_first=True, conv_mlp=True)
-    model = _create_convnext('convnext_tiny_hnf', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_tiny_hnf', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnext_tiny(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768))
-    model = _create_convnext('convnext_tiny', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_tiny', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnext_small(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768])
-    model = _create_convnext('convnext_small', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_small', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnext_base(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024])
-    model = _create_convnext('convnext_base', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_base', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnext_large(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536])
-    model = _create_convnext('convnext_large', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_large', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnext_large_mlp(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], head_hidden_size=1536)
-    model = _create_convnext('convnext_large_mlp', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_large_mlp', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnext_xlarge(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048])
-    model = _create_convnext('convnext_xlarge', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_xlarge', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnext_xxlarge(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[3, 4, 30, 3], dims=[384, 768, 1536, 3072], norm_eps=kwargs.pop('norm_eps', 1e-5))
-    model = _create_convnext('convnext_xxlarge', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnext_xxlarge', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1301,7 +1310,7 @@     # timm femto variant (NOTE: still tweaking depths, will vary between 3-4M param, current is 3.7M
     model_args = dict(
         depths=(2, 2, 6, 2), dims=(40, 80, 160, 320), use_grn=True, ls_init_value=None, conv_mlp=True)
-    model = _create_convnext('convnextv2_atto', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnextv2_atto', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1310,7 +1319,7 @@     # timm femto variant
     model_args = dict(
         depths=(2, 2, 6, 2), dims=(48, 96, 192, 384), use_grn=True, ls_init_value=None, conv_mlp=True)
-    model = _create_convnext('convnextv2_femto', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnextv2_femto', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1319,7 +1328,7 @@     # timm pico variant
     model_args = dict(
         depths=(2, 2, 6, 2), dims=(64, 128, 256, 512), use_grn=True, ls_init_value=None, conv_mlp=True)
-    model = _create_convnext('convnextv2_pico', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnextv2_pico', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1328,56 +1337,56 @@     # timm nano variant with standard stem and head
     model_args = dict(
         depths=(2, 2, 8, 2), dims=(80, 160, 320, 640), use_grn=True, ls_init_value=None, conv_mlp=True)
-    model = _create_convnext('convnextv2_nano', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnextv2_nano', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnextv2_tiny(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=(3, 3, 9, 3), dims=(96, 192, 384, 768), use_grn=True, ls_init_value=None)
-    model = _create_convnext('convnextv2_tiny', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnextv2_tiny', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnextv2_small(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], use_grn=True, ls_init_value=None)
-    model = _create_convnext('convnextv2_small', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnextv2_small', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnextv2_base(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], use_grn=True, ls_init_value=None)
-    model = _create_convnext('convnextv2_base', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnextv2_base', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnextv2_large(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], use_grn=True, ls_init_value=None)
-    model = _create_convnext('convnextv2_large', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnextv2_large', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def convnextv2_huge(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[3, 3, 27, 3], dims=[352, 704, 1408, 2816], use_grn=True, ls_init_value=None)
-    model = _create_convnext('convnextv2_huge', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('convnextv2_huge', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def test_convnext(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[1, 2, 4, 2], dims=[24, 32, 48, 64], norm_eps=kwargs.pop('norm_eps', 1e-5), act_layer='gelu_tanh')
-    model = _create_convnext('test_convnext', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('test_convnext', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def test_convnext2(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(depths=[1, 1, 1, 1], dims=[32, 64, 96, 128], norm_eps=kwargs.pop('norm_eps', 1e-5), act_layer='gelu_tanh')
-    model = _create_convnext('test_convnext2', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('test_convnext2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1385,7 +1394,7 @@ def test_convnext3(pretrained=False, **kwargs) -> ConvNeXt:
     model_args = dict(
         depths=[1, 1, 1, 1], dims=[32, 64, 96, 128], norm_eps=kwargs.pop('norm_eps', 1e-5), kernel_sizes=(7, 5, 5, 3), act_layer='silu')
-    model = _create_convnext('test_convnext3', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_convnext('test_convnext3', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
