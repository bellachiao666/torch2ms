--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """PyTorch CspNet
 
 A PyTorch implementation of Cross Stage Partial Networks including:
@@ -15,9 +20,7 @@ from dataclasses import dataclass, asdict, replace
 from functools import partial
 from typing import Any, Dict, List, Optional, Tuple, Type, Union
-
-import torch
-import torch.nn as nn
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ClassifierHead, ConvNormAct, DropPath, calculate_drop_path_rates, get_attn, create_act_layer, make_divisible
@@ -132,7 +135,7 @@     )
 
 
-class BottleneckBlock(nn.Module):
+class BottleneckBlock(msnn.Cell):
     """ ResNe(X)t Bottleneck Block
     """
 
@@ -143,11 +146,11 @@             dilation: int = 1,
             bottle_ratio: float = 0.25,
             groups: int = 1,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             attn_last: bool = False,
-            attn_layer: Optional[Type[nn.Module]] = None,
-            drop_block: Optional[Type[nn.Module]] = None,
+            attn_layer: Optional[Type[msnn.Cell]] = None,
+            drop_block: Optional[Type[msnn.Cell]] = None,
             drop_path: float = 0.,
             device=None,
             dtype=None,
@@ -159,7 +162,7 @@         attn_last = attn_layer is not None and attn_last
         attn_first = attn_layer is not None and not attn_last
 
-        self.conv1 = ConvNormAct(in_chs, mid_chs, kernel_size=1, **ckwargs, **dd)
+        self.conv1 = ConvNormAct(in_chs, mid_chs, kernel_size=1, **ckwargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv2 = ConvNormAct(
             mid_chs,
             mid_chs,
@@ -169,17 +172,17 @@             drop_layer=drop_block,
             **ckwargs,
             **dd,
-        )
-        self.attn2 = attn_layer(mid_chs, act_layer=act_layer, **dd) if attn_first else nn.Identity()
-        self.conv3 = ConvNormAct(mid_chs, out_chs, kernel_size=1, apply_act=False, **ckwargs, **dd)
-        self.attn3 = attn_layer(out_chs, act_layer=act_layer, **dd) if attn_last else nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn2 = attn_layer(mid_chs, act_layer=act_layer, **dd) if attn_first else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv3 = ConvNormAct(mid_chs, out_chs, kernel_size=1, apply_act=False, **ckwargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn3 = attn_layer(out_chs, act_layer=act_layer, **dd) if attn_last else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path else msnn.Identity()
         self.act3 = create_act_layer(act_layer)
 
     def zero_init_last(self):
-        nn.init.zeros_(self.conv3.bn.weight)
-
-    def forward(self, x):
+        nn.init.zeros_(self.conv3.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x):
         shortcut = x
         x = self.conv1(x)
         x = self.conv2(x)
@@ -193,7 +196,7 @@         return x
 
 
-class DarkBlock(nn.Module):
+class DarkBlock(msnn.Cell):
     """ DarkNet Block
     """
 
@@ -204,10 +207,10 @@             dilation: int = 1,
             bottle_ratio: float = 0.5,
             groups: int = 1,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            attn_layer: Optional[Type[nn.Module]] = None,
-            drop_block: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            attn_layer: Optional[Type[msnn.Cell]] = None,
+            drop_block: Optional[Type[msnn.Cell]] = None,
             drop_path: float = 0.,
             device=None,
             dtype=None,
@@ -217,8 +220,8 @@         mid_chs = int(round(out_chs * bottle_ratio))
         ckwargs = dict(act_layer=act_layer, norm_layer=norm_layer)
 
-        self.conv1 = ConvNormAct(in_chs, mid_chs, kernel_size=1, **ckwargs, **dd)
-        self.attn = attn_layer(mid_chs, act_layer=act_layer, **dd) if attn_layer is not None else nn.Identity()
+        self.conv1 = ConvNormAct(in_chs, mid_chs, kernel_size=1, **ckwargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn = attn_layer(mid_chs, act_layer=act_layer, **dd) if attn_layer is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv2 = ConvNormAct(
             mid_chs,
             out_chs,
@@ -228,13 +231,13 @@             drop_layer=drop_block,
             **ckwargs,
             **dd,
-        )
-        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path else msnn.Identity()
 
     def zero_init_last(self):
-        nn.init.zeros_(self.conv2.bn.weight)
-
-    def forward(self, x):
+        nn.init.zeros_(self.conv2.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x):
         shortcut = x
         x = self.conv1(x)
         x = self.attn(x)
@@ -243,7 +246,7 @@         return x
 
 
-class EdgeBlock(nn.Module):
+class EdgeBlock(msnn.Cell):
     """ EdgeResidual / Fused-MBConv / MobileNetV1-like 3x3 + 1x1 block (w/ activated output)
     """
 
@@ -254,10 +257,10 @@             dilation: int = 1,
             bottle_ratio: float = 0.5,
             groups: int = 1,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            attn_layer: Optional[Type[nn.Module]] = None,
-            drop_block: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            attn_layer: Optional[Type[msnn.Cell]] = None,
+            drop_block: Optional[Type[msnn.Cell]] = None,
             drop_path: float = 0.,
             device=None,
             dtype=None,
@@ -276,15 +279,15 @@             drop_layer=drop_block,
             **ckwargs,
             **dd,
-        )
-        self.attn = attn_layer(mid_chs, act_layer=act_layer, **dd) if attn_layer is not None else nn.Identity()
-        self.conv2 = ConvNormAct(mid_chs, out_chs, kernel_size=1, **ckwargs, **dd)
-        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn = attn_layer(mid_chs, act_layer=act_layer, **dd) if attn_layer is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv2 = ConvNormAct(mid_chs, out_chs, kernel_size=1, **ckwargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path else msnn.Identity()
 
     def zero_init_last(self):
-        nn.init.zeros_(self.conv2.bn.weight)
-
-    def forward(self, x):
+        nn.init.zeros_(self.conv2.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x):
         shortcut = x
         x = self.conv1(x)
         x = self.attn(x)
@@ -293,7 +296,7 @@         return x
 
 
-class CrossStage(nn.Module):
+class CrossStage(msnn.Cell):
     """Cross Stage."""
     def __init__(
             self,
@@ -311,7 +314,7 @@             down_growth: bool = False,
             cross_linear: bool = False,
             block_dpr: Optional[List[float]] = None,
-            block_fn: Type[nn.Module] = BottleneckBlock,
+            block_fn: Type[msnn.Cell] = BottleneckBlock,
             device=None,
             dtype=None,
             **block_kwargs,
@@ -327,10 +330,11 @@ 
         if stride != 1 or first_dilation != dilation:
             if avg_down:
-                self.conv_down = nn.Sequential(
-                    nn.AvgPool2d(2) if stride == 2 else nn.Identity(),  # FIXME dilation handling
+                self.conv_down = msnn.SequentialCell(
+                    [
+                    nn.AvgPool2d(2) if stride == 2 else msnn.Identity(),
                     ConvNormAct(in_chs, out_chs, kernel_size=1, stride=1, groups=groups, **conv_kwargs, **dd)
-                )
+                ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
                 self.conv_down = ConvNormAct(
                     in_chs,
@@ -342,10 +346,10 @@                     aa_layer=aa_layer,
                     **conv_kwargs,
                     **dd,
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_chs = down_chs
         else:
-            self.conv_down = nn.Identity()
+            self.conv_down = msnn.Identity()
             prev_chs = in_chs
 
         # FIXME this 1x1 expansion is pushed down into the cross and block paths in the darknet cfgs. Also,
@@ -358,10 +362,10 @@             apply_act=not cross_linear,
             **conv_kwargs,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         prev_chs = exp_chs // 2  # output of conv_exp is always split in two
 
-        self.blocks = nn.Sequential()
+        self.blocks = msnn.SequentialCell()
         for i in range(depth):
             self.blocks.add_module(str(i), block_fn(
                 in_chs=prev_chs,
@@ -372,24 +376,24 @@                 drop_path=block_dpr[i] if block_dpr is not None else 0.,
                 **block_kwargs,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_chs = block_out_chs
 
         # transition convs
-        self.conv_transition_b = ConvNormAct(prev_chs, exp_chs // 2, kernel_size=1, **conv_kwargs, **dd)
-        self.conv_transition = ConvNormAct(exp_chs, out_chs, kernel_size=1, **conv_kwargs, **dd)
-
-    def forward(self, x):
+        self.conv_transition_b = ConvNormAct(prev_chs, exp_chs // 2, kernel_size=1, **conv_kwargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv_transition = ConvNormAct(exp_chs, out_chs, kernel_size=1, **conv_kwargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.conv_down(x)
         x = self.conv_exp(x)
         xs, xb = x.split(self.expand_chs // 2, dim=1)
         xb = self.blocks(xb)
         xb = self.conv_transition_b(xb).contiguous()
-        out = self.conv_transition(torch.cat([xs, xb], dim=1))
+        out = self.conv_transition(mint.cat([xs, xb], dim=1))
         return out
 
 
-class CrossStage3(nn.Module):
+class CrossStage3(msnn.Cell):
     """Cross Stage 3.
     Similar to CrossStage, but with only one transition conv for the output.
     """
@@ -409,7 +413,7 @@             down_growth: bool = False,
             cross_linear: bool = False,
             block_dpr: Optional[List[float]] = None,
-            block_fn: Type[nn.Module] = BottleneckBlock,
+            block_fn: Type[msnn.Cell] = BottleneckBlock,
             device=None,
             dtype=None,
             **block_kwargs,
@@ -425,10 +429,11 @@ 
         if stride != 1 or first_dilation != dilation:
             if avg_down:
-                self.conv_down = nn.Sequential(
-                    nn.AvgPool2d(2) if stride == 2 else nn.Identity(),  # FIXME dilation handling
+                self.conv_down = msnn.SequentialCell(
+                    [
+                    nn.AvgPool2d(2) if stride == 2 else msnn.Identity(),
                     ConvNormAct(in_chs, out_chs, kernel_size=1, stride=1, groups=groups, **conv_kwargs, **dd)
-                )
+                ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
                 self.conv_down = ConvNormAct(
                     in_chs,
@@ -440,7 +445,7 @@                     aa_layer=aa_layer,
                     **conv_kwargs,
                     **dd,
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_chs = down_chs
         else:
             self.conv_down = None
@@ -454,10 +459,10 @@             apply_act=not cross_linear,
             **conv_kwargs,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         prev_chs = exp_chs // 2  # expanded output is split in 2 for blocks and cross stage
 
-        self.blocks = nn.Sequential()
+        self.blocks = msnn.SequentialCell()
         for i in range(depth):
             self.blocks.add_module(str(i), block_fn(
                 in_chs=prev_chs,
@@ -468,22 +473,22 @@                 drop_path=block_dpr[i] if block_dpr is not None else 0.,
                 **block_kwargs,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_chs = block_out_chs
 
         # transition convs
-        self.conv_transition = ConvNormAct(exp_chs, out_chs, kernel_size=1, **conv_kwargs, **dd)
-
-    def forward(self, x):
+        self.conv_transition = ConvNormAct(exp_chs, out_chs, kernel_size=1, **conv_kwargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.conv_down(x)
         x = self.conv_exp(x)
         x1, x2 = x.split(self.expand_chs // 2, dim=1)
         x1 = self.blocks(x1)
-        out = self.conv_transition(torch.cat([x1, x2], dim=1))
+        out = self.conv_transition(mint.cat([x1, x2], dim=1))
         return out
 
 
-class DarkStage(nn.Module):
+class DarkStage(msnn.Cell):
     """DarkNet stage."""
 
     def __init__(
@@ -498,7 +503,7 @@             groups: int = 1,
             first_dilation: Optional[int] = None,
             avg_down: bool = False,
-            block_fn: Type[nn.Module] = BottleneckBlock,
+            block_fn: Type[msnn.Cell] = BottleneckBlock,
             block_dpr: Optional[List[float]] = None,
             device=None,
             dtype=None,
@@ -511,10 +516,11 @@         aa_layer = block_kwargs.pop('aa_layer', None)
 
         if avg_down:
-            self.conv_down = nn.Sequential(
-                nn.AvgPool2d(2) if stride == 2 else nn.Identity(),   # FIXME dilation handling
+            self.conv_down = msnn.SequentialCell(
+                [
+                nn.AvgPool2d(2) if stride == 2 else msnn.Identity(),
                 ConvNormAct(in_chs, out_chs, kernel_size=1, stride=1, groups=groups, **conv_kwargs, **dd)
-            )
+            ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.conv_down = ConvNormAct(
                 in_chs,
@@ -526,11 +532,11 @@                 aa_layer=aa_layer,
                 **conv_kwargs,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         prev_chs = out_chs
         block_out_chs = int(round(out_chs * block_ratio))
-        self.blocks = nn.Sequential()
+        self.blocks = msnn.SequentialCell()
         for i in range(depth):
             self.blocks.add_module(str(i), block_fn(
                 in_chs=prev_chs,
@@ -541,10 +547,10 @@                 drop_path=block_dpr[i] if block_dpr is not None else 0.,
                 **block_kwargs,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_chs = block_out_chs
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv_down(x)
         x = self.blocks(x)
         return x
@@ -557,14 +563,14 @@         stride: int = 2,
         pool: str = '',
         padding: str = '',
-        act_layer: Type[nn.Module] = nn.ReLU,
-        norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-        aa_layer: Optional[Type[nn.Module]] = None,
+        act_layer: Type[msnn.Cell] = nn.ReLU,
+        norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+        aa_layer: Optional[Type[msnn.Cell]] = None,
         device=None,
         dtype=None,
 ):
     dd = {'device': device, 'dtype': dtype}
-    stem = nn.Sequential()
+    stem = msnn.SequentialCell()
     feature_info = []
     if not isinstance(out_chs, (tuple, list)):
         out_chs = [out_chs]
@@ -587,7 +593,7 @@             act_layer=act_layer,
             norm_layer=norm_layer,
             **dd,
-        ))
+        ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stem_stride *= conv_stride
         prev_chs = chs
         prev_feat = dict(num_chs=prev_chs, reduction=stem_stride, module='.'.join(['stem', conv_name]))
@@ -596,11 +602,11 @@         if prev_feat is not None:
             feature_info.append(prev_feat)
         if aa_layer is not None:
-            stem.add_module('pool', nn.MaxPool2d(kernel_size=3, stride=1, padding=1))
-            stem.add_module('aa', aa_layer(channels=prev_chs, stride=2, **dd))
+            stem.add_module('pool', nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1))
+            stem.add_module('aa', aa_layer(channels=prev_chs, stride=2, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             pool_name = 'aa'
         else:
-            stem.add_module('pool', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
+            stem.add_module('pool', nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1))
             pool_name = 'pool'
         stem_stride *= 2
         prev_feat = dict(num_chs=prev_chs, reduction=stem_stride, module='.'.join(['stem', pool_name]))
@@ -640,7 +646,7 @@     if attn_layer is not None:
         attn_layer = get_attn(attn_layer)
         if attn_kwargs:
-            attn_layer = partial(attn_layer, **attn_kwargs)
+            attn_layer = partial(attn_layer, **attn_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return attn_layer, stage_args
 
 
@@ -657,7 +663,7 @@     num_stages = len(cfg.stages.depth)
     cfg_dict['block_dpr'] = [None] * num_stages if not drop_path_rate else \
         calculate_drop_path_rates(drop_path_rate, cfg.stages.depth, stagewise=True)
-    stage_args = [dict(zip(cfg_dict.keys(), values)) for values in zip(*cfg_dict.values())]
+    stage_args = [dict(zip(cfg_dict.keys(), values)) for values in zip(*cfg_dict.values())]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     block_kwargs = dict(
         act_layer=cfg.act_layer,
         norm_layer=cfg.norm_layer,
@@ -693,15 +699,15 @@             attn_layer=attn_fn,  # will be passed through stage as block_kwargs
             **block_kwargs,
             **dd,
-        )]
+        )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         prev_chs = stage_args['out_chs']
         prev_feat = dict(num_chs=prev_chs, reduction=net_stride, module=f'stages.{stage_idx}')
 
     feature_info.append(prev_feat)
-    return nn.Sequential(*stages), feature_info
-
-
-class CspNet(nn.Module):
+    return msnn.SequentialCell(*stages), feature_info  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+
+class CspNet(msnn.Cell):
     """Cross Stage Partial base model.
 
     Paper: `CSPNet: A New Backbone that can Enhance Learning Capability of CNN` - https://arxiv.org/abs/1911.11929
@@ -743,7 +749,7 @@         self.drop_rate = drop_rate
         assert output_stride in (8, 16, 32)
 
-        cfg = replace(cfg, **kwargs)  # overlay kwargs onto cfg
+        cfg = replace(cfg, **kwargs)  # overlay kwargs onto cfg; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         layer_args = dict(
             act_layer=cfg.act_layer,
             norm_layer=cfg.norm_layer,
@@ -752,7 +758,7 @@         self.feature_info = []
 
         # Construct the stem
-        self.stem, stem_feat_info = create_csp_stem(in_chans, **asdict(cfg.stem), **layer_args, **dd)
+        self.stem, stem_feat_info = create_csp_stem(in_chans, **asdict(cfg.stem), **layer_args, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info.extend(stem_feat_info[:-1])
 
         # Construct the stages
@@ -762,7 +768,7 @@             output_stride=output_stride,
             stem_feat=stem_feat_info[-1],
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         prev_chs = stage_feat_info[-1]['num_chs']
         self.feature_info.extend(stage_feat_info)
 
@@ -774,11 +780,11 @@             pool_type=global_pool,
             drop_rate=drop_rate,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(
             stem=r'^stem',
@@ -790,12 +796,12 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         assert not enable, 'gradient checkpointing not supported'
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -810,7 +816,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -818,13 +824,13 @@ 
 def _init_weights(module, name, zero_init_last=False):
     if isinstance(module, nn.Conv2d):
-        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
+        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Linear):
-        nn.init.normal_(module.weight, mean=0.0, std=0.01)
+        nn.init.normal_(module.weight, mean=0.0, std=0.01)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif zero_init_last and hasattr(module, 'zero_init_last'):
         module.zero_init_last()
 
@@ -1001,7 +1007,7 @@         CspNet, variant, pretrained,
         model_cfg=model_cfgs[variant],
         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url='', **kwargs):
@@ -1094,114 +1100,114 @@ 
 @register_model
 def cspresnet50(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cspresnet50', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cspresnet50', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cspresnet50d(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cspresnet50d', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cspresnet50d', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cspresnet50w(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cspresnet50w', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cspresnet50w', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cspresnext50(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cspresnext50', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cspresnext50', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cspdarknet53(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cspdarknet53', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cspdarknet53', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def darknet17(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('darknet17', pretrained=pretrained, **kwargs)
+    return _create_cspnet('darknet17', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def darknet21(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('darknet21', pretrained=pretrained, **kwargs)
+    return _create_cspnet('darknet21', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def sedarknet21(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('sedarknet21', pretrained=pretrained, **kwargs)
+    return _create_cspnet('sedarknet21', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def darknet53(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('darknet53', pretrained=pretrained, **kwargs)
+    return _create_cspnet('darknet53', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def darknetaa53(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('darknetaa53', pretrained=pretrained, **kwargs)
+    return _create_cspnet('darknetaa53', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3darknet_s(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3darknet_s', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3darknet_s', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3darknet_m(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3darknet_m', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3darknet_m', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3darknet_l(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3darknet_l', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3darknet_l', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3darknet_x(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3darknet_x', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3darknet_x', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3darknet_focus_s(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3darknet_focus_s', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3darknet_focus_s', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3darknet_focus_m(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3darknet_focus_m', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3darknet_focus_m', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3darknet_focus_l(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3darknet_focus_l', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3darknet_focus_l', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3darknet_focus_x(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3darknet_focus_x', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3darknet_focus_x', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3sedarknet_l(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3sedarknet_l', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3sedarknet_l', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3sedarknet_x(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3sedarknet_x', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3sedarknet_x', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3sedarknet_xdw(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3sedarknet_xdw', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3sedarknet_xdw', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3edgenet_x(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3edgenet_x', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3edgenet_x', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def cs3se_edgenet_x(pretrained=False, **kwargs) -> CspNet:
-    return _create_cspnet('cs3se_edgenet_x', pretrained=pretrained, **kwargs)
+    return _create_cspnet('cs3se_edgenet_x', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
