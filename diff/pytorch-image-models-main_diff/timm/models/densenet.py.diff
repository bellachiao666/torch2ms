--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """Pytorch Densenet implementation w/ tweaks
 This file is a copy of https://github.com/pytorch/vision 'densenet.py' (BSD-3-Clause) with
 fixed kwargs passthrough and addition of dynamic global avg/max pool.
@@ -6,10 +11,9 @@ from collections import OrderedDict
 from typing import Any, Dict, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.jit.annotations import List
+# import torch
+# import torch.nn as nn
+# from torch.jit.annotations import List
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import BatchNormAct2d, get_norm_act_layer, BlurPool2d, create_classifier
@@ -20,7 +24,7 @@ __all__ = ['DenseNet']
 
 
-class DenseLayer(nn.Module):
+class DenseLayer(msnn.Cell):
     """Dense layer for DenseNet.
 
     Implements the bottleneck layer with 1x1 and 3x3 convolutions.
@@ -31,7 +35,7 @@             num_input_features: int,
             growth_rate: int,
             bn_size: int,
-            norm_layer: Type[nn.Module] = BatchNormAct2d,
+            norm_layer: Type[msnn.Cell] = BatchNormAct2d,
             drop_rate: float = 0.,
             grad_checkpointing: bool = False,
             device=None,
@@ -49,50 +53,57 @@         """
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.add_module('norm1', norm_layer(num_input_features, **dd)),
+        self.add_module('norm1', norm_layer(num_input_features, **dd)),  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.add_module('conv1', nn.Conv2d(
-            num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False, **dd)),
-        self.add_module('norm2', norm_layer(bn_size * growth_rate, **dd)),
+            num_input_features, bn_size * growth_rate, kernel_size=1, stride=1, bias=False, **dd)),  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.add_module('norm2', norm_layer(bn_size * growth_rate, **dd)),  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.add_module('conv2', nn.Conv2d(
-            bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False, **dd)),
+            bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False, **dd)),  # 存在 *args/**kwargs，需手动确认参数映射;
         self.drop_rate = float(drop_rate)
         self.grad_checkpointing = grad_checkpointing
 
-    def bottleneck_fn(self, xs: List[torch.Tensor]) -> torch.Tensor:
+    # 'torch.jit.annotations.List' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def bottleneck_fn(self, xs: List[ms.Tensor]) -> ms.Tensor:
         """Bottleneck function for concatenated features."""
-        concated_features = torch.cat(xs, 1)
+        concated_features = mint.cat(xs, 1)
         bottleneck_output = self.conv1(self.norm1(concated_features))  # noqa: T484
         return bottleneck_output
 
     # todo: rewrite when torchscript supports any
-    def any_requires_grad(self, x: List[torch.Tensor]) -> bool:
+    # 'torch.jit.annotations.List' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def any_requires_grad(self, x: List[ms.Tensor]) -> bool:
         """Check if any tensor in list requires gradient."""
         for tensor in x:
             if tensor.requires_grad:
                 return True
         return False
 
+    # 'torch.jit.annotations.List' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 装饰器 'torch.jit.unused' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.unused  # noqa: T484
-    def call_checkpoint_bottleneck(self, x: List[torch.Tensor]) -> torch.Tensor:
+    def call_checkpoint_bottleneck(self, x: List[ms.Tensor]) -> ms.Tensor:
         """Call bottleneck function with gradient checkpointing."""
         def closure(*xs):
             return self.bottleneck_fn(xs)
 
-        return checkpoint(closure, *x)
-
+        return checkpoint(closure, *x)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    # 装饰器 'torch.jit._overload_method' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit._overload_method  # noqa: F811
-    def forward(self, x):
+    def construct(self, x):
         # type: (List[torch.Tensor]) -> (torch.Tensor)
         pass
 
+    # 装饰器 'torch.jit._overload_method' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit._overload_method  # noqa: F811
-    def forward(self, x):
+    def construct(self, x):
         # type: (torch.Tensor) -> (torch.Tensor)
         pass
 
     # torchscript does not yet support *args, so we overload method
     # allowing it to take either a List[Tensor] or single Tensor
-    def forward(self, x: Union[torch.Tensor, List[torch.Tensor]]) -> torch.Tensor:  # noqa: F811
+    # 'torch.jit.annotations.List' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def construct(self, x: Union[ms.Tensor, List[ms.Tensor]]) -> ms.Tensor:  # noqa: F811
         """Forward pass.
 
         Args:
@@ -101,12 +112,13 @@         Returns:
             New features to be concatenated.
         """
-        if isinstance(x, torch.Tensor):
+        if isinstance(x, ms.Tensor):
             prev_features = [x]
         else:
             prev_features = x
 
         if self.grad_checkpointing and self.any_requires_grad(prev_features):
+            # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if torch.jit.is_scripting():
                 raise Exception("Memory Efficient not supported in JIT")
             bottleneck_output = self.call_checkpoint_bottleneck(prev_features)
@@ -115,10 +127,11 @@ 
         new_features = self.conv2(self.norm2(bottleneck_output))
         if self.drop_rate > 0:
-            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)
+            new_features = nn.functional.dropout(new_features, p = self.drop_rate, training = self.training)
         return new_features
 
 
+# 'torch.nn.ModuleDict' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class DenseBlock(nn.ModuleDict):
     """DenseNet Block.
 
@@ -132,7 +145,7 @@             num_input_features: int,
             bn_size: int,
             growth_rate: int,
-            norm_layer: Type[nn.Module] = BatchNormAct2d,
+            norm_layer: Type[msnn.Cell] = BatchNormAct2d,
             drop_rate: float = 0.,
             grad_checkpointing: bool = False,
             device=None,
@@ -160,10 +173,10 @@                 drop_rate=drop_rate,
                 grad_checkpointing=grad_checkpointing,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.add_module('denselayer%d' % (i + 1), layer)
 
-    def forward(self, init_features: torch.Tensor) -> torch.Tensor:
+    def forward(self, init_features: ms.Tensor) -> ms.Tensor:
         """Forward pass through all layers in the block.
 
         Args:
@@ -176,10 +189,10 @@         for name, layer in self.items():
             new_features = layer(features)
             features.append(new_features)
-        return torch.cat(features, 1)
-
-
-class DenseTransition(nn.Sequential):
+        return mint.cat(features, 1)
+
+
+class DenseTransition(msnn.SequentialCell):
     """Transition layer between DenseNet blocks.
 
     Reduces feature dimensions and spatial resolution.
@@ -189,8 +202,8 @@             self,
             num_input_features: int,
             num_output_features: int,
-            norm_layer: Type[nn.Module] = BatchNormAct2d,
-            aa_layer: Optional[Type[nn.Module]] = None,
+            norm_layer: Type[msnn.Cell] = BatchNormAct2d,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ) -> None:
@@ -204,16 +217,16 @@         """
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.add_module('norm', norm_layer(num_input_features, **dd))
+        self.add_module('norm', norm_layer(num_input_features, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.add_module('conv', nn.Conv2d(
-            num_input_features, num_output_features, kernel_size=1, stride=1, bias=False, **dd))
+            num_input_features, num_output_features, kernel_size=1, stride=1, bias=False, **dd))  # 存在 *args/**kwargs，需手动确认参数映射;
         if aa_layer is not None:
-            self.add_module('pool', aa_layer(num_output_features, stride=2, **dd))
+            self.add_module('pool', aa_layer(num_output_features, stride=2, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))
-
-
-class DenseNet(nn.Module):
+            self.add_module('pool', nn.AvgPool2d(kernel_size = 2, stride = 2))
+
+
+class DenseNet(msnn.Cell):
     """Densenet-BC model class.
 
     Based on `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`_
@@ -241,7 +254,7 @@             stem_type: str = '',
             act_layer: str = 'relu',
             norm_layer: str = 'batchnorm2d',
-            aa_layer: Optional[Type[nn.Module]] = None,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
             drop_rate: float = 0.,
             proj_drop_rate: float = 0.,
             memory_efficient: bool = False,
@@ -276,17 +289,18 @@         deep_stem = 'deep' in stem_type  # 3x3 deep stem
         num_init_features = growth_rate * 2
         if aa_layer is None:
-            stem_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+            stem_pool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
         else:
-            stem_pool = nn.Sequential(*[
-                nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
-                aa_layer(channels=num_init_features, stride=2, **dd)])
+            stem_pool = msnn.SequentialCell(*[
+                nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1),
+                aa_layer(channels=num_init_features, stride=2, **dd)])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if deep_stem:
             stem_chs_1 = stem_chs_2 = growth_rate
             if 'tiered' in stem_type:
                 stem_chs_1 = 3 * (growth_rate // 4)
                 stem_chs_2 = num_init_features if 'narrow' in stem_type else 6 * (growth_rate // 4)
-            self.features = nn.Sequential(OrderedDict([
+            self.features = msnn.SequentialCell([
+                OrderedDict([
                 ('conv0', nn.Conv2d(in_chans, stem_chs_1, 3, stride=2, padding=1, bias=False, **dd)),
                 ('norm0', norm_layer(stem_chs_1, **dd)),
                 ('conv1', nn.Conv2d(stem_chs_1, stem_chs_2, 3, stride=1, padding=1, bias=False, **dd)),
@@ -294,13 +308,16 @@                 ('conv2', nn.Conv2d(stem_chs_2, num_init_features, 3, stride=1, padding=1, bias=False, **dd)),
                 ('norm2', norm_layer(num_init_features, **dd)),
                 ('pool0', stem_pool),
-            ]))
+            ])
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.features = nn.Sequential(OrderedDict([
+            self.features = msnn.SequentialCell([
+                OrderedDict([
                 ('conv0', nn.Conv2d(in_chans, num_init_features, kernel_size=7, stride=2, padding=3, bias=False, **dd)),
                 ('norm0', norm_layer(num_init_features, **dd)),
                 ('pool0', stem_pool),
-            ]))
+            ])
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info = [
             dict(num_chs=num_init_features, reduction=2, module=f'features.norm{2 if deep_stem else 0}')]
         current_stride = 4
@@ -317,7 +334,7 @@                 drop_rate=proj_drop_rate,
                 grad_checkpointing=memory_efficient,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             module_name = f'denseblock{(i + 1)}'
             self.features.add_module(module_name, block)
             num_features = num_features + num_layers * growth_rate
@@ -332,12 +349,12 @@                     norm_layer=norm_layer,
                     aa_layer=transition_aa_layer,
                     **dd,
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
                 self.features.add_module(f'transition{i + 1}', trans)
                 num_features = num_features // 2
 
         # Final batch norm
-        self.features.add_module('norm5', norm_layer(num_features, **dd))
+        self.features.add_module('norm5', norm_layer(num_features, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.feature_info += [dict(num_chs=num_features, reduction=current_stride, module='features.norm5')]
         self.num_features = self.head_hidden_size = num_features
@@ -348,7 +365,7 @@             self.num_classes,
             pool_type=global_pool,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.global_pool = global_pool
         self.head_drop = nn.Dropout(drop_rate)
         self.classifier = classifier
@@ -356,14 +373,14 @@         # Official init from torch repo.
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight)
+                nn.init.kaiming_normal_(m.weight)  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.BatchNorm2d):
-                nn.init.constant_(m.weight, 1)
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.weight, 1)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.Linear):
-                nn.init.constant_(m.bias, 0)
-
-    @torch.jit.ignore
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def group_matcher(self, coarse: bool = False) -> Dict[str, Any]:
         """Group parameters for optimization."""
         matcher = dict(
@@ -375,15 +392,15 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True) -> None:
         """Enable or disable gradient checkpointing."""
         for b in self.features.modules():
             if isinstance(b, DenseLayer):
                 b.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         """Get the classifier head."""
         return self.classifier
 
@@ -398,11 +415,11 @@         self.global_pool, self.classifier = create_classifier(
             self.num_features, self.num_classes, pool_type=global_pool)
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers."""
         return self.features(x)
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -416,7 +433,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.classifier(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -430,7 +447,7 @@         return x
 
 
-def _filter_torchvision_pretrained(state_dict: dict) -> Dict[str, torch.Tensor]:
+def _filter_torchvision_pretrained(state_dict: dict) -> Dict[str, ms.Tensor]:
     """Filter torchvision pretrained state dict for compatibility.
 
     Args:
@@ -479,7 +496,7 @@         feature_cfg=dict(flatten_sequential=True),
         pretrained_filter_fn=_filter_torchvision_pretrained,
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url: str = '', **kwargs) -> Dict[str, Any]:
@@ -514,7 +531,7 @@     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model_args = dict(growth_rate=32, block_config=(6, 12, 24, 16))
-    model = _create_densenet('densenet121', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_densenet('densenet121', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -524,7 +541,7 @@     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model_args = dict(growth_rate=32, block_config=(6, 12, 24, 16), stem_type='deep', aa_layer=BlurPool2d)
-    model = _create_densenet('densenetblur121d', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_densenet('densenetblur121d', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -534,7 +551,7 @@     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model_args = dict(growth_rate=32, block_config=(6, 12, 32, 32))
-    model = _create_densenet('densenet169', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_densenet('densenet169', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -544,7 +561,7 @@     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model_args = dict(growth_rate=32, block_config=(6, 12, 48, 32))
-    model = _create_densenet('densenet201', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_densenet('densenet201', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -554,7 +571,7 @@     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model_args = dict(growth_rate=48, block_config=(6, 12, 36, 24))
-    model = _create_densenet('densenet161', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_densenet('densenet161', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -564,7 +581,7 @@     `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`
     """
     model_args = dict(growth_rate=48, block_config=(6, 12, 64, 48), stem_type='deep')
-    model = _create_densenet('densenet264d', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_densenet('densenet264d', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
