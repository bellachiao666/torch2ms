--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """Pytorch Densenet implementation w/ tweaks
 This file is a copy of https://github.com/pytorch/vision 'densenet.py' (BSD-3-Clause) with
 fixed kwargs passthrough and addition of dynamic global avg/max pool.
@@ -6,10 +11,9 @@ from collections import OrderedDict
 from typing import Any, Dict, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.jit.annotations import List
+# import torch
+# import torch.nn as nn
+# from torch.jit.annotations import List
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import BatchNormAct2d, get_norm_act_layer, BlurPool2d, create_classifier
@@ -20,7 +24,7 @@ __all__ = ['DenseNet']
 
 
-class DenseLayer(nn.Module):
+class DenseLayer(msnn.Cell):
     """Dense layer for DenseNet.
 
     Implements the bottleneck layer with 1x1 and 3x3 convolutions.
@@ -58,13 +62,15 @@         self.drop_rate = float(drop_rate)
         self.grad_checkpointing = grad_checkpointing
 
-    def bottleneck_fn(self, xs: List[torch.Tensor]) -> torch.Tensor:
+    # 类型标注 'torch.jit.annotations.List' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def bottleneck_fn(self, xs: List[torch.Tensor]) -> ms.Tensor:
         """Bottleneck function for concatenated features."""
-        concated_features = torch.cat(xs, 1)
+        concated_features = mint.cat(xs, 1)
         bottleneck_output = self.conv1(self.norm1(concated_features))  # noqa: T484
         return bottleneck_output
 
     # todo: rewrite when torchscript supports any
+    # 类型标注 'torch.jit.annotations.List' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def any_requires_grad(self, x: List[torch.Tensor]) -> bool:
         """Check if any tensor in list requires gradient."""
         for tensor in x:
@@ -72,8 +78,9 @@                 return True
         return False
 
+    # 类型标注 'torch.jit.annotations.List' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.unused  # noqa: T484
-    def call_checkpoint_bottleneck(self, x: List[torch.Tensor]) -> torch.Tensor:
+    def call_checkpoint_bottleneck(self, x: List[torch.Tensor]) -> ms.Tensor:
         """Call bottleneck function with gradient checkpointing."""
         def closure(*xs):
             return self.bottleneck_fn(xs)
@@ -81,18 +88,18 @@         return checkpoint(closure, *x)
 
     @torch.jit._overload_method  # noqa: F811
-    def forward(self, x):
+    def construct(self, x):
         # type: (List[torch.Tensor]) -> (torch.Tensor)
         pass
 
     @torch.jit._overload_method  # noqa: F811
-    def forward(self, x):
+    def construct(self, x):
         # type: (torch.Tensor) -> (torch.Tensor)
         pass
 
     # torchscript does not yet support *args, so we overload method
     # allowing it to take either a List[Tensor] or single Tensor
-    def forward(self, x: Union[torch.Tensor, List[torch.Tensor]]) -> torch.Tensor:  # noqa: F811
+    def construct(self, x: Union[torch.Tensor, List[torch.Tensor]]) -> ms.Tensor:  # noqa: F811
         """Forward pass.
 
         Args:
@@ -115,10 +122,11 @@ 
         new_features = self.conv2(self.norm2(bottleneck_output))
         if self.drop_rate > 0:
-            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)
+            new_features = nn.functional.dropout(new_features, p = self.drop_rate, training = self.training)
         return new_features
 
 
+# 'torch.nn.ModuleDict' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class DenseBlock(nn.ModuleDict):
     """DenseNet Block.
 
@@ -163,7 +171,7 @@             )
             self.add_module('denselayer%d' % (i + 1), layer)
 
-    def forward(self, init_features: torch.Tensor) -> torch.Tensor:
+    def forward(self, init_features: ms.Tensor) -> ms.Tensor:
         """Forward pass through all layers in the block.
 
         Args:
@@ -176,10 +184,10 @@         for name, layer in self.items():
             new_features = layer(features)
             features.append(new_features)
-        return torch.cat(features, 1)
-
-
-class DenseTransition(nn.Sequential):
+        return mint.cat(features, 1)
+
+
+class DenseTransition(msnn.SequentialCell):
     """Transition layer between DenseNet blocks.
 
     Reduces feature dimensions and spatial resolution.
@@ -210,10 +218,10 @@         if aa_layer is not None:
             self.add_module('pool', aa_layer(num_output_features, stride=2, **dd))
         else:
-            self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))
-
-
-class DenseNet(nn.Module):
+            self.add_module('pool', nn.AvgPool2d(kernel_size = 2, stride = 2))
+
+
+class DenseNet(msnn.Cell):
     """Densenet-BC model class.
 
     Based on `"Densely Connected Convolutional Networks" <https://arxiv.org/pdf/1608.06993.pdf>`_
@@ -276,17 +284,18 @@         deep_stem = 'deep' in stem_type  # 3x3 deep stem
         num_init_features = growth_rate * 2
         if aa_layer is None:
-            stem_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+            stem_pool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
         else:
-            stem_pool = nn.Sequential(*[
-                nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
+            stem_pool = msnn.SequentialCell(*[
+                nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1),
                 aa_layer(channels=num_init_features, stride=2, **dd)])
         if deep_stem:
             stem_chs_1 = stem_chs_2 = growth_rate
             if 'tiered' in stem_type:
                 stem_chs_1 = 3 * (growth_rate // 4)
                 stem_chs_2 = num_init_features if 'narrow' in stem_type else 6 * (growth_rate // 4)
-            self.features = nn.Sequential(OrderedDict([
+            self.features = msnn.SequentialCell([
+                OrderedDict([
                 ('conv0', nn.Conv2d(in_chans, stem_chs_1, 3, stride=2, padding=1, bias=False, **dd)),
                 ('norm0', norm_layer(stem_chs_1, **dd)),
                 ('conv1', nn.Conv2d(stem_chs_1, stem_chs_2, 3, stride=1, padding=1, bias=False, **dd)),
@@ -294,13 +303,16 @@                 ('conv2', nn.Conv2d(stem_chs_2, num_init_features, 3, stride=1, padding=1, bias=False, **dd)),
                 ('norm2', norm_layer(num_init_features, **dd)),
                 ('pool0', stem_pool),
-            ]))
+            ])
+            ])
         else:
-            self.features = nn.Sequential(OrderedDict([
+            self.features = msnn.SequentialCell([
+                OrderedDict([
                 ('conv0', nn.Conv2d(in_chans, num_init_features, kernel_size=7, stride=2, padding=3, bias=False, **dd)),
                 ('norm0', norm_layer(num_init_features, **dd)),
                 ('pool0', stem_pool),
-            ]))
+            ])
+            ])
         self.feature_info = [
             dict(num_chs=num_init_features, reduction=2, module=f'features.norm{2 if deep_stem else 0}')]
         current_stride = 4
@@ -356,12 +368,12 @@         # Official init from torch repo.
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight)
+                nn.init.kaiming_normal_(m.weight)  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.BatchNorm2d):
-                nn.init.constant_(m.weight, 1)
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.weight, 1)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.Linear):
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def group_matcher(self, coarse: bool = False) -> Dict[str, Any]:
@@ -382,6 +394,7 @@             if isinstance(b, DenseLayer):
                 b.grad_checkpointing = enable
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         """Get the classifier head."""
@@ -398,11 +411,11 @@         self.global_pool, self.classifier = create_classifier(
             self.num_features, self.num_classes, pool_type=global_pool)
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers."""
         return self.features(x)
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -416,7 +429,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.classifier(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
