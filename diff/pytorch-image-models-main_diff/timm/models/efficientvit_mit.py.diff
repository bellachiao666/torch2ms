--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ EfficientViT (by MIT Song Han's Lab)
 
 Paper: `Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition`
@@ -10,9 +15,8 @@ from typing import List, Optional, Tuple, Type, Union
 from functools import partial
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectAdaptivePool2d, create_conv2d, GELUTanh
@@ -46,7 +50,7 @@         return kernel_size // 2
 
 
-class ConvNormAct(nn.Module):
+class ConvNormAct(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -64,7 +68,7 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.dropout = nn.Dropout(dropout, inplace=False)
+        self.dropout = nn.Dropout(dropout, inplace = False)
         self.conv = create_conv2d(
             in_channels,
             out_channels,
@@ -75,10 +79,10 @@             bias=bias,
             **dd,
         )
-        self.norm = norm_layer(num_features=out_channels, **dd) if norm_layer else nn.Identity()
-        self.act = act_layer(inplace=True) if act_layer is not None else nn.Identity()
-
-    def forward(self, x):
+        self.norm = norm_layer(num_features=out_channels, **dd) if norm_layer else msnn.Identity()
+        self.act = act_layer(inplace=True) if act_layer is not None else msnn.Identity()
+
+    def construct(self, x):
         x = self.dropout(x)
         x = self.conv(x)
         x = self.norm(x)
@@ -86,7 +90,7 @@         return x
 
 
-class DSConv(nn.Module):
+class DSConv(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -126,13 +130,13 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.depth_conv(x)
         x = self.point_conv(x)
         return x
 
 
-class ConvBlock(nn.Module):
+class ConvBlock(msnn.Cell):
     def __init__(
         self,
             in_channels: int,
@@ -175,13 +179,13 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         x = self.conv2(x)
         return x
 
 
-class MBConv(nn.Module):
+class MBConv(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -234,14 +238,14 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.inverted_conv(x)
         x = self.depth_conv(x)
         x = self.point_conv(x)
         return x
 
 
-class FusedMBConv(nn.Module):
+class FusedMBConv(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -285,13 +289,13 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.spatial_conv(x)
         x = self.point_conv(x)
         return x
 
 
-class LiteMLA(nn.Module):
+class LiteMLA(msnn.Cell):
     """Lightweight multi-scale linear attention"""
 
     def __init__(
@@ -329,9 +333,10 @@             act_layer=act_layer[0],
             **dd,
         )
-        self.aggreg = nn.ModuleList([
-            nn.Sequential(
-                nn.Conv2d(
+        self.aggreg = msnn.CellList([
+            msnn.SequentialCell(
+                [
+            nn.Conv2d(
                     3 * total_dim,
                     3 * total_dim,
                     scale,
@@ -340,8 +345,8 @@                     bias=use_bias[0],
                     **dd,
                 ),
-                nn.Conv2d(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0], **dd),
-            )
+            nn.Conv2d(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0], **dd)
+        ])
             for scale in scales
         ])
         self.kernel_func = kernel_func(inplace=False)
@@ -364,7 +369,7 @@         out = out[..., :-1] / (out[..., -1:] + self.eps)
         return out.to(dtype)
 
-    def forward(self, x):
+    def construct(self, x):
         B, _, H, W = x.shape
 
         # generate multi-scale q, k, v
@@ -372,14 +377,14 @@         multi_scale_qkv = [qkv]
         for op in self.aggreg:
             multi_scale_qkv.append(op(qkv))
-        multi_scale_qkv = torch.cat(multi_scale_qkv, dim=1)
+        multi_scale_qkv = mint.cat(multi_scale_qkv, dim = 1)
         multi_scale_qkv = multi_scale_qkv.reshape(B, -1, 3 * self.dim, H * W).transpose(-1, -2)
         q, k, v = multi_scale_qkv.chunk(3, dim=-1)
 
         # lightweight global attention
         q = self.kernel_func(q)
         k = self.kernel_func(k)
-        v = F.pad(v, (0, 1), mode="constant", value=1.)
+        v = nn.functional.pad(v, (0, 1), mode = "constant", value = 1.)
 
         if not torch.jit.is_scripting():
             with torch.autocast(device_type=v.device.type, enabled=False):
@@ -396,7 +401,7 @@ register_notrace_module(LiteMLA)
 
 
-class EfficientVitBlock(nn.Module):
+class EfficientVitBlock(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -419,7 +424,7 @@                 norm_layer=(None, norm_layer),
                 **dd,
             ),
-            nn.Identity(),
+            msnn.Identity(),
         )
         self.local_module = ResidualBlock(
             MBConv(
@@ -431,16 +436,16 @@                 act_layer=(act_layer, act_layer, None),
                 **dd,
             ),
-            nn.Identity(),
-        )
-
-    def forward(self, x):
+            msnn.Identity(),
+        )
+
+    def construct(self, x):
         x = self.context_module(x)
         x = self.local_module(x)
         return x
 
 
-class ResidualBlock(nn.Module):
+class ResidualBlock(msnn.Cell):
     def __init__(
             self,
             main: Optional[nn.Module],
@@ -448,11 +453,11 @@             pre_norm: Optional[nn.Module] = None,
     ):
         super().__init__()
-        self.pre_norm = pre_norm if pre_norm is not None else nn.Identity()
+        self.pre_norm = pre_norm if pre_norm is not None else msnn.Identity()
         self.main = main
         self.shortcut = shortcut
 
-    def forward(self, x):
+    def construct(self, x):
         res = self.main(self.pre_norm(x))
         if self.shortcut is not None:
             res = res + self.shortcut(x)
@@ -520,7 +525,7 @@     return block
 
 
-class Stem(nn.Sequential):
+class Stem(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
@@ -561,12 +566,12 @@                     block_type=block_type,
                     **dd,
                 ),
-                nn.Identity(),
+                msnn.Identity(),
             ))
             stem_block += 1
 
 
-class EfficientVitStage(nn.Module):
+class EfficientVitStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -623,16 +628,18 @@                         act_layer=act_layer,
                         **dd,
                     ),
-                    nn.Identity(),
+                    msnn.Identity(),
                 ))
 
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x):
         return self.blocks(x)
 
 
-class EfficientVitLargeStage(nn.Module):
+class EfficientVitLargeStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -692,16 +699,18 @@                         block_type='default' if fewer_norm else 'fused',
                         **dd,
                     ),
-                    nn.Identity(),
+                    msnn.Identity(),
                 ))
 
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x):
         return self.blocks(x)
 
 
-class ClassifierHead(nn.Module):
+class ClassifierHead(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -723,24 +732,25 @@         assert pool_type, 'Cannot disable pooling'
         self.in_conv = ConvNormAct(in_channels, widths[0], 1, norm_layer=norm_layer, act_layer=act_layer, **dd)
         self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True)
-        self.classifier = nn.Sequential(
+        self.classifier = msnn.SequentialCell(
+            [
             nn.Linear(widths[0], widths[1], bias=False, **dd),
             nn.LayerNorm(widths[1], eps=norm_eps, **dd),
-            act_layer(inplace=True) if act_layer is not None else nn.Identity(),
-            nn.Dropout(dropout, inplace=False),
-            nn.Linear(widths[1], num_classes, bias=True, **dd) if num_classes > 0 else nn.Identity(),
-        )
+            act_layer(inplace=True) if act_layer is not None else msnn.Identity(),
+            nn.Dropout(dropout, inplace = False),
+            nn.Linear(widths[1], num_classes, bias=True, **dd) if num_classes > 0 else msnn.Identity()
+        ])
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None):
         if pool_type is not None:
             assert pool_type, 'Cannot disable pooling'
             self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True,)
         if num_classes > 0:
-            self.classifier[-1] = nn.Linear(self.num_features, num_classes, bias=True)
+            self.classifier[-1] = nn.Linear(self.num_features, num_classes, bias = True)
         else:
-            self.classifier[-1] = nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
+            self.classifier[-1] = msnn.Identity()
+
+    def construct(self, x, pre_logits: bool = False):
         x = self.in_conv(x)
         x = self.global_pool(x)
         if pre_logits:
@@ -754,7 +764,7 @@         return x
 
 
-class EfficientVit(nn.Module):
+class EfficientVit(msnn.Cell):
     def __init__(
             self,
             in_chans: int = 3,
@@ -783,7 +793,7 @@ 
         # stages
         self.feature_info = []
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         in_channels = widths[0]
         for i, (w, d) in enumerate(zip(widths[1:], depths[1:])):
             self.stages.append(EfficientVitStage(
@@ -827,6 +837,7 @@     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         return self.head.classifier[-1]
@@ -837,7 +848,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
@@ -906,13 +917,13 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-class EfficientVitLarge(nn.Module):
+class EfficientVitLarge(msnn.Cell):
     def __init__(
         self,
         in_chans: int = 3,
@@ -943,7 +954,7 @@ 
         # stages
         self.feature_info = []
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         in_channels = widths[0]
         for i, (w, d) in enumerate(zip(widths[1:], depths[1:])):
             self.stages.append(EfficientVitLargeStage(
@@ -989,6 +1000,7 @@     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         return self.head.classifier[-1]
@@ -999,7 +1011,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
@@ -1068,7 +1080,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
