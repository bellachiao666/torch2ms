--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ EVA
 
 EVA ViT from https://github.com/baaivision/EVA , paper: https://arxiv.org/abs/2211.07636
@@ -67,9 +72,9 @@ from functools import partial
 from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
 from timm.layers import (
@@ -101,7 +106,7 @@ __all__ = ['Eva']
 
 
-class EvaAttention(nn.Module):
+class EvaAttention(msnn.Cell):
     """ EVA Attention with ROPE, no k-bias, and fused/unfused qkv options
     """
     fused_attn: torch.jit.Final[bool]
@@ -160,9 +165,9 @@             self.qkv = nn.Linear(dim, attn_dim * 3, bias=False, **dd)
             self.q_proj = self.k_proj = self.v_proj = None
             if qkv_bias:
-                self.q_bias = nn.Parameter(torch.zeros(attn_dim, **dd))
-                self.register_buffer('k_bias', torch.zeros(attn_dim, **dd), persistent=False)
-                self.v_bias = nn.Parameter(torch.zeros(attn_dim, **dd))
+                self.q_bias = ms.Parameter(mint.zeros(attn_dim, **dd))
+                self.register_buffer('k_bias', mint.zeros(attn_dim, **dd), persistent=False)
+                self.v_bias = ms.Parameter(mint.zeros(attn_dim, **dd))
             else:
                 self.q_bias = self.k_bias = self.v_bias = None
         else:
@@ -171,14 +176,14 @@             self.v_proj = nn.Linear(dim, attn_dim, bias=qkv_bias, **dd)
             self.qkv = None
             self.q_bias = self.k_bias = self.v_bias = None
-        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()
-        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()
+        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else msnn.Identity()
+        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else msnn.Identity()
         self.attn_drop = nn.Dropout(attn_drop)
-        self.norm = norm_layer(attn_dim, **dd) if scale_norm else nn.Identity()
+        self.norm = norm_layer(attn_dim, **dd) if scale_norm else msnn.Identity()
         self.proj = nn.Linear(attn_dim, dim, **dd)
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(
+    def construct(
             self,
             x,
             rope: Optional[torch.Tensor] = None,
@@ -200,12 +205,12 @@             if self.q_bias is None:
                 qkv = self.qkv(x)
             else:
-                qkv_bias = torch.cat((self.q_bias, self.k_bias, self.v_bias))
+                qkv_bias = mint.cat((self.q_bias, self.k_bias, self.v_bias))
                 if self.qkv_bias_separate:
                     qkv = self.qkv(x)
                     qkv += qkv_bias
                 else:
-                    qkv = F.linear(x, weight=self.qkv.weight, bias=qkv_bias)
+                    qkv = nn.functional.linear(x, weight = self.qkv.weight, bias = qkv_bias)
             qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
             q, k, v = qkv.unbind(0)  # B, num_heads, N, head_dim
         else:
@@ -218,15 +223,15 @@         if rope is not None:
             npt = self.num_prefix_tokens
             half = getattr(self, 'rotate_half', False)
-            q = torch.cat([q[:, :, :npt, :], apply_rot_embed_cat(q[:, :, npt:, :], rope, half=half)], dim=2).type_as(v)
-            k = torch.cat([k[:, :, :npt, :], apply_rot_embed_cat(k[:, :, npt:, :], rope, half=half)], dim=2).type_as(v)
+            q = mint.cat([q[:, :, :npt, :], apply_rot_embed_cat(q[:, :, npt:, :], rope, half=half)], dim = 2).type_as(v)
+            k = mint.cat([k[:, :, :npt, :], apply_rot_embed_cat(k[:, :, npt:, :], rope, half=half)], dim = 2).type_as(v)
 
         if self.fused_attn:
             x = F.scaled_dot_product_attention(
                 q, k, v,
                 attn_mask=attn_mask,
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = (q @ k.transpose(-2, -1))
@@ -243,7 +248,7 @@         return x
 
 
-class EvaBlock(nn.Module):
+class EvaBlock(msnn.Cell):
 
     def __init__(
             self,
@@ -310,8 +315,8 @@             rotate_half=rotate_half,
             **dd,
         )
-        self.gamma_1 = nn.Parameter(init_values * torch.ones(dim, **dd)) if init_values is not None else None
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.gamma_1 = ms.Parameter(init_values * mint.ones(dim, **dd)) if init_values is not None else None
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.norm2 = norm_layer(dim, **dd)
         hidden_features = int(dim * mlp_ratio)
@@ -346,15 +351,15 @@                 drop=proj_drop,
                 **dd,
             )
-        self.gamma_2 = nn.Parameter(init_values * torch.ones(dim, **dd)) if init_values is not None else None
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(
+        self.gamma_2 = ms.Parameter(init_values * mint.ones(dim, **dd)) if init_values is not None else None
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             rope: Optional[torch.Tensor] = None,
             attn_mask: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
+    ) -> ms.Tensor:
         if self.gamma_1 is None:
             x = x + self.drop_path1(self.attn(self.norm1(x), rope=rope, attn_mask=attn_mask))
             x = x + self.drop_path2(self.mlp(self.norm2(x)))
@@ -364,7 +369,7 @@         return x
 
 
-class EvaBlockPostNorm(nn.Module):
+class EvaBlockPostNorm(msnn.Cell):
     """ EVA block w/ post-norm and support for swiglu, MLP norm scale, ROPE. """
     def __init__(
             self,
@@ -431,7 +436,7 @@             **dd,
         )
         self.norm1 = norm_layer(dim, **dd)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         hidden_features = int(dim * mlp_ratio)
         if swiglu_mlp:
@@ -466,20 +471,20 @@                 **dd,
             )
         self.norm2 = norm_layer(dim, **dd)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             rope: Optional[torch.Tensor] = None,
             attn_mask: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
+    ) -> ms.Tensor:
         x = x + self.drop_path1(self.norm1(self.attn(x, rope=rope, attn_mask=attn_mask)))
         x = x + self.drop_path2(self.norm2(self.mlp(x)))
         return x
 
 
-class Eva(nn.Module):
+class Eva(msnn.Cell):
     """ Eva Vision Transformer w/ Abs & Rotary Pos Embed
 
     This class implements the EVA and EVA02 models that were based on the BEiT ViT variant
@@ -622,13 +627,13 @@         num_patches = self.patch_embed.num_patches
         r = self.patch_embed.feat_ratio() if hasattr(self.patch_embed, 'feat_ratio') else patch_size
 
-        self.cls_token = nn.Parameter(torch.empty(1, 1, embed_dim, **dd)) if class_token else None
-        self.reg_token = nn.Parameter(torch.empty(1, num_reg_tokens, embed_dim, **dd)) if num_reg_tokens else None
+        self.cls_token = ms.Parameter(mint.empty(1, 1, embed_dim, **dd)) if class_token else None
+        self.reg_token = ms.Parameter(mint.empty(1, num_reg_tokens, embed_dim, **dd)) if num_reg_tokens else None
         self.cls_embed = class_token and self.reg_token is None
 
         num_pos_tokens = num_patches if no_embed_class else num_patches + self.num_prefix_tokens
-        self.pos_embed = nn.Parameter(torch.empty(1, num_pos_tokens, embed_dim, **dd)) if use_abs_pos_emb else None
-        self.pos_drop = nn.Dropout(p=pos_drop_rate)
+        self.pos_embed = ms.Parameter(mint.empty(1, num_pos_tokens, embed_dim, **dd)) if use_abs_pos_emb else None
+        self.pos_drop = nn.Dropout(p = pos_drop_rate)
         if patch_drop_rate > 0:
             self.patch_drop = PatchDropoutWithIndices(patch_drop_rate, num_prefix_tokens=self.num_prefix_tokens)
         else:
@@ -661,11 +666,11 @@         else:
             self.rope = None
 
-        self.norm_pre = norm_layer(embed_dim, **dd) if activate_pre_norm else nn.Identity()
+        self.norm_pre = norm_layer(embed_dim, **dd) if activate_pre_norm else msnn.Identity()
 
         dpr = calculate_drop_path_rates(drop_path_rate, depth)  # stochastic depth decay rule
         block_fn = EvaBlockPostNorm if use_post_norm else EvaBlock
-        self.blocks = nn.ModuleList([
+        self.blocks = msnn.CellList([
             block_fn(
                 dim=embed_dim,
                 num_heads=num_heads,
@@ -690,7 +695,7 @@         self.feature_info = [
             dict(module=f'blocks.{i}', num_chs=embed_dim, reduction=r) for i in range(depth)]
 
-        self.norm = norm_layer(embed_dim, **dd) if activate_post_norm else nn.Identity()
+        self.norm = norm_layer(embed_dim, **dd) if activate_post_norm else msnn.Identity()
 
         if global_pool == 'map':
             self.attn_pool = AttentionPoolLatent(
@@ -703,9 +708,9 @@             )
         else:
             self.attn_pool = None
-        self.fc_norm = norm_layer(embed_dim, **dd) if activate_fc_norm else nn.Identity()
+        self.fc_norm = norm_layer(embed_dim, **dd) if activate_fc_norm else msnn.Identity()
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(embed_dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(embed_dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()
 
         self.init_weights(head_init_scale=head_init_scale)
 
@@ -732,6 +737,7 @@             rescale(layer.attn.proj.weight.data, layer_id + 1)
             rescale(layer.mlp.fc2.weight.data, layer_id + 1)
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def _init_weights(self, m: nn.Module) -> None:
         """Initialize weights for Linear layers.
 
@@ -741,7 +747,7 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if m.bias is not None:
-                nn.init.zeros_(m.bias)
+                nn.init.zeros_(m.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def no_weight_decay(self) -> Set[str]:
@@ -765,6 +771,7 @@         )
         return matcher
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         return self.head
@@ -779,7 +786,7 @@         self.num_classes = num_classes
         if global_pool is not None:
             self.global_pool = global_pool
-        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else msnn.Identity()
 
     def set_input_size(
             self,
@@ -799,7 +806,7 @@             num_prefix_tokens = 0 if self.no_embed_class else self.num_prefix_tokens
             num_new_tokens = self.patch_embed.num_patches + num_prefix_tokens
             if num_new_tokens != self.pos_embed.shape[1]:
-                self.pos_embed = nn.Parameter(resample_abs_pos_embed(
+                self.pos_embed = ms.Parameter(resample_abs_pos_embed(
                     self.pos_embed,
                     new_size=self.patch_embed.grid_size,
                     old_size=prev_grid_size,
@@ -840,11 +847,11 @@             if pos_embed is not None:
                 x = x + pos_embed
             if to_cat:
-                x = torch.cat(to_cat + [x], dim=1)
+                x = mint.cat(to_cat + [x], dim = 1)
         else:
             # pos_embed has entry for class / reg token, concat then add
             if to_cat:
-                x = torch.cat(to_cat + [x], dim=1)
+                x = mint.cat(to_cat + [x], dim = 1)
             if pos_embed is not None:
                 x = x + pos_embed
 
@@ -867,7 +874,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             return_prefix_tokens: bool = False,
             norm: bool = False,
@@ -949,14 +956,14 @@         take_indices, max_index = feature_take_indices(len(self.blocks), indices)
         self.blocks = self.blocks[:max_index + 1]  # truncate blocks
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.attn_pool = None
-            self.fc_norm = nn.Identity()
+            self.fc_norm = msnn.Identity()
             self.reset_classifier(0, '')
         return take_indices
 
-    def pool(self, x: torch.Tensor, pool_type: Optional[str] = None) -> torch.Tensor:
+    def pool(self, x: ms.Tensor, pool_type: Optional[str] = None) -> ms.Tensor:
         if self.attn_pool is not None:
             x = self.attn_pool(x)
             return x
@@ -964,7 +971,7 @@         x = global_pool_nlc(x, pool_type=pool_type, num_prefix_tokens=self.num_prefix_tokens)
         return x
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers.
 
         Args:
@@ -996,7 +1003,7 @@         x = self.norm(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -1011,7 +1018,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -1025,6 +1032,7 @@         return x
 
 
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def _convert_pe(
     state_dict: Dict[str, torch.Tensor],
     model: nn.Module,
@@ -1084,7 +1092,7 @@         elif k == 'proj':
             k = 'head.weight'
             v = v.transpose(0, 1)
-            out_dict['head.bias'] = torch.zeros(v.shape[0])
+            out_dict['head.bias'] = mint.zeros(v.shape[0])
         elif k == 'class_embedding':
             k = 'cls_token'
             v = v.unsqueeze(0).unsqueeze(1)
@@ -1095,6 +1103,7 @@     return out_dict
 
 
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def checkpoint_filter_fn(
         state_dict: Dict[str, torch.Tensor],
         model: nn.Module,
