--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ HRNet
 
 Copied from https://github.com/HRNet/HRNet-Image-Classification
@@ -11,8 +16,8 @@ import logging
 from typing import Dict, List, Type, Optional, Tuple
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
@@ -354,11 +359,11 @@ )
 
 
-class HighResolutionModule(nn.Module):
+class HighResolutionModule(msnn.Cell):
     def __init__(
             self,
             num_branches: int,
-            block_types: Type[nn.Module],
+            block_types: Type[msnn.Cell],
             num_blocks: Tuple[int, ...],
             num_in_chs: List[int],
             num_channels: Tuple[int, ...],
@@ -389,9 +394,9 @@             num_blocks,
             num_channels,
             **dd,
-        )
-        self.fuse_layers = self._make_fuse_layers(**dd)
-        self.fuse_act = nn.ReLU(False)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.fuse_layers = self._make_fuse_layers(**dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.fuse_act = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
     def _check_branches(self, num_branches, block_types, num_blocks, num_in_chs, num_channels):
         error_msg = ''
@@ -409,7 +414,8 @@         dd = {'device': device, 'dtype': dtype}
         downsample = None
         if stride != 1 or self.num_in_chs[branch_index] != num_channels[branch_index] * block_type.expansion:
-            downsample = nn.Sequential(
+            downsample = msnn.SequentialCell(
+                [
                 nn.Conv2d(
                     self.num_in_chs[branch_index],
                     num_channels[branch_index] * block_type.expansion,
@@ -418,28 +424,28 @@                     bias=False,
                     **dd,
                 ),
-                nn.BatchNorm2d(num_channels[branch_index] * block_type.expansion, momentum=_BN_MOMENTUM, **dd),
-            )
-
-        layers = [block_type(self.num_in_chs[branch_index], num_channels[branch_index], stride, downsample, **dd)]
+                nn.BatchNorm2d(num_channels[branch_index] * block_type.expansion, momentum=_BN_MOMENTUM, **dd)
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;
+
+        layers = [block_type(self.num_in_chs[branch_index], num_channels[branch_index], stride, downsample, **dd)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.num_in_chs[branch_index] = num_channels[branch_index] * block_type.expansion
         for i in range(1, num_blocks[branch_index]):
-            layers.append(block_type(self.num_in_chs[branch_index], num_channels[branch_index], **dd))
-
-        return nn.Sequential(*layers)
+            layers.append(block_type(self.num_in_chs[branch_index], num_channels[branch_index], **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        return msnn.SequentialCell(*layers)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def _make_branches(self, num_branches, block_type, num_blocks, num_channels, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         branches = []
         for i in range(num_branches):
-            branches.append(self._make_one_branch(i, block_type, num_blocks, num_channels, **dd))
-
-        return nn.ModuleList(branches)
+            branches.append(self._make_one_branch(i, block_type, num_blocks, num_channels, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        return msnn.CellList(branches)
 
     def _make_fuse_layers(self, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         if self.num_branches == 1:
-            return nn.Identity()
+            return msnn.Identity()
 
         num_branches = self.num_branches
         num_in_chs = self.num_in_chs
@@ -448,37 +454,41 @@             fuse_layer = []
             for j in range(num_branches):
                 if j > i:
-                    fuse_layer.append(nn.Sequential(
+                    fuse_layer.append(msnn.SequentialCell(
+                        [
                         nn.Conv2d(num_in_chs[j], num_in_chs[i], 1, 1, 0, bias=False, **dd),
                         nn.BatchNorm2d(num_in_chs[i], momentum=_BN_MOMENTUM, **dd),
-                        nn.Upsample(scale_factor=2 ** (j - i), mode='nearest')))
+                        nn.Upsample(scale_factor = 2 ** (j - i), mode = 'nearest')
+                    ]))  # 存在 *args/**kwargs，需手动确认参数映射;
                 elif j == i:
-                    fuse_layer.append(nn.Identity())
+                    fuse_layer.append(msnn.Identity())
                 else:
                     conv3x3s = []
                     for k in range(i - j):
                         if k == i - j - 1:
                             num_out_chs_conv3x3 = num_in_chs[i]
-                            conv3x3s.append(nn.Sequential(
+                            conv3x3s.append(msnn.SequentialCell(
+                                [
                                 nn.Conv2d(num_in_chs[j], num_out_chs_conv3x3, 3, 2, 1, bias=False, **dd),
                                 nn.BatchNorm2d(num_out_chs_conv3x3, momentum=_BN_MOMENTUM, **dd)
-                            ))
+                            ]))  # 存在 *args/**kwargs，需手动确认参数映射;
                         else:
                             num_out_chs_conv3x3 = num_in_chs[j]
-                            conv3x3s.append(nn.Sequential(
+                            conv3x3s.append(msnn.SequentialCell(
+                                [
                                 nn.Conv2d(num_in_chs[j], num_out_chs_conv3x3, 3, 2, 1, bias=False, **dd),
                                 nn.BatchNorm2d(num_out_chs_conv3x3, momentum=_BN_MOMENTUM, **dd),
-                                nn.ReLU(False)
-                            ))
-                    fuse_layer.append(nn.Sequential(*conv3x3s))
-            fuse_layers.append(nn.ModuleList(fuse_layer))
-
-        return nn.ModuleList(fuse_layers)
+                                nn.ReLU()
+                            ]))  # 存在 *args/**kwargs，需手动确认参数映射;; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+                    fuse_layer.append(msnn.SequentialCell(*conv3x3s))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            fuse_layers.append(msnn.CellList(fuse_layer))
+
+        return msnn.CellList(fuse_layers)
 
     def get_num_in_chs(self):
         return self.num_in_chs
 
-    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:
+    def construct(self, x: List[ms.Tensor]) -> List[ms.Tensor]:
         if self.num_branches == 1:
             return [self.branches[0](x[0])]
 
@@ -497,30 +507,32 @@         return x_fuse
 
 
-class SequentialList(nn.Sequential):
+class SequentialList(msnn.SequentialCell):
 
     def __init__(self, *args):
-        super().__init__(*args)
-
+        super().__init__(*args)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    # 装饰器 'torch.jit._overload_method' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit._overload_method  # noqa: F811
     def forward(self, x):
         # type: (List[torch.Tensor]) -> (List[torch.Tensor])
         pass
 
+    # 装饰器 'torch.jit._overload_method' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit._overload_method  # noqa: F811
     def forward(self, x):
         # type: (torch.Tensor) -> (List[torch.Tensor])
         pass
 
-    def forward(self, x) -> List[torch.Tensor]:
+    def forward(self, x) -> List[ms.Tensor]:
         for module in self:
             x = module(x)
         return x
 
 
 @torch.jit.interface
-class ModuleInterface(torch.nn.Module):
-    def forward(self, input: torch.Tensor) -> torch.Tensor: # `input` has a same name in Sequential forward
+class ModuleInterface(msnn.Cell):
+    def construct(self, input: ms.Tensor) -> ms.Tensor: # `input` has a same name in Sequential forward
         pass
 
 
@@ -530,7 +542,7 @@ }
 
 
-class HighResolutionNet(nn.Module):
+class HighResolutionNet(msnn.Cell):
 
     def __init__(
             self,
@@ -550,42 +562,42 @@         self.num_classes = num_classes
         assert output_stride == 32  # FIXME support dilation
 
-        cfg.update(**kwargs)
+        cfg.update(**kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stem_width = cfg['stem_width']
-        self.conv1 = nn.Conv2d(in_chans, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **dd)
-        self.bn1 = nn.BatchNorm2d(stem_width, momentum=_BN_MOMENTUM, **dd)
-        self.act1 = nn.ReLU(inplace=True)
-        self.conv2 = nn.Conv2d(stem_width, 64, kernel_size=3, stride=2, padding=1, bias=False, **dd)
-        self.bn2 = nn.BatchNorm2d(64, momentum=_BN_MOMENTUM, **dd)
-        self.act2 = nn.ReLU(inplace=True)
+        self.conv1 = nn.Conv2d(in_chans, stem_width, kernel_size=3, stride=2, padding=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn1 = nn.BatchNorm2d(stem_width, momentum=_BN_MOMENTUM, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.act1 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.conv2 = nn.Conv2d(stem_width, 64, kernel_size=3, stride=2, padding=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn2 = nn.BatchNorm2d(64, momentum=_BN_MOMENTUM, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.act2 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         self.stage1_cfg = cfg['stage1']
         num_channels = self.stage1_cfg['num_channels'][0]
         block_type = block_types_dict[self.stage1_cfg['block_type']]
         num_blocks = self.stage1_cfg['num_blocks'][0]
-        self.layer1 = self._make_layer(block_type, 64, num_channels, num_blocks, **dd)
+        self.layer1 = self._make_layer(block_type, 64, num_channels, num_blocks, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stage1_out_channel = block_type.expansion * num_channels
 
         self.stage2_cfg = cfg['stage2']
         num_channels = self.stage2_cfg['num_channels']
         block_type = block_types_dict[self.stage2_cfg['block_type']]
         num_channels = [num_channels[i] * block_type.expansion for i in range(len(num_channels))]
-        self.transition1 = self._make_transition_layer([stage1_out_channel], num_channels, **dd)
-        self.stage2, pre_stage_channels = self._make_stage(self.stage2_cfg, num_channels, **dd)
+        self.transition1 = self._make_transition_layer([stage1_out_channel], num_channels, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.stage2, pre_stage_channels = self._make_stage(self.stage2_cfg, num_channels, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.stage3_cfg = cfg['stage3']
         num_channels = self.stage3_cfg['num_channels']
         block_type = block_types_dict[self.stage3_cfg['block_type']]
         num_channels = [num_channels[i] * block_type.expansion for i in range(len(num_channels))]
-        self.transition2 = self._make_transition_layer(pre_stage_channels, num_channels, **dd)
-        self.stage3, pre_stage_channels = self._make_stage(self.stage3_cfg, num_channels, **dd)
+        self.transition2 = self._make_transition_layer(pre_stage_channels, num_channels, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.stage3, pre_stage_channels = self._make_stage(self.stage3_cfg, num_channels, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.stage4_cfg = cfg['stage4']
         num_channels = self.stage4_cfg['num_channels']
         block_type = block_types_dict[self.stage4_cfg['block_type']]
         num_channels = [num_channels[i] * block_type.expansion for i in range(len(num_channels))]
-        self.transition3 = self._make_transition_layer(pre_stage_channels, num_channels, **dd)
-        self.stage4, pre_stage_channels = self._make_stage(self.stage4_cfg, num_channels, multi_scale_output=True, **dd)
+        self.transition3 = self._make_transition_layer(pre_stage_channels, num_channels, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.stage4, pre_stage_channels = self._make_stage(self.stage4_cfg, num_channels, multi_scale_output=True, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.head = head
         self.head_channels = None  # set if _make_head called
@@ -597,24 +609,24 @@                 pre_stage_channels,
                 conv_bias=head_conv_bias,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.global_pool, self.head_drop, self.classifier = create_classifier(
                 self.num_features,
                 self.num_classes,
                 pool_type=global_pool,
                 drop_rate=drop_rate,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             if head == 'incre':
                 self.num_features = self.head_hidden_size = 2048
-                self.incre_modules, _, _ = self._make_head(pre_stage_channels, incre_only=True, **dd)
+                self.incre_modules, _, _ = self._make_head(pre_stage_channels, incre_only=True, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
                 self.num_features = self.head_hidden_size = 256
                 self.incre_modules = None
-            self.global_pool = nn.Identity()
-            self.head_drop = nn.Identity()
-            self.classifier = nn.Identity()
+            self.global_pool = msnn.Identity()
+            self.head_drop = msnn.Identity()
+            self.classifier = msnn.Identity()
 
         curr_stride = 2
         # module names aren't actually valid here, hook or FeatureNet based extraction would not work
@@ -635,8 +647,8 @@         # from C, 2C, 4C, 8C to 128, 256, 512, 1024
         incre_modules = []
         for i, channels in enumerate(pre_stage_channels):
-            incre_modules.append(self._make_layer(head_block_type, channels, self.head_channels[i], 1, stride=1, **dd))
-        incre_modules = nn.ModuleList(incre_modules)
+            incre_modules.append(self._make_layer(head_block_type, channels, self.head_channels[i], 1, stride=1, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        incre_modules = msnn.CellList(incre_modules)
         if incre_only:
             return incre_modules, None, None
 
@@ -645,7 +657,8 @@         for i in range(len(pre_stage_channels) - 1):
             in_channels = self.head_channels[i] * head_block_type.expansion
             out_channels = self.head_channels[i + 1] * head_block_type.expansion
-            downsamp_module = nn.Sequential(
+            downsamp_module = msnn.SequentialCell(
+                [
                 nn.Conv2d(
                     in_channels=in_channels,
                     out_channels=out_channels,
@@ -656,12 +669,13 @@                     **dd,
                 ),
                 nn.BatchNorm2d(out_channels, momentum=_BN_MOMENTUM, **dd),
-                nn.ReLU(inplace=True)
-            )
+                nn.ReLU()
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
             downsamp_modules.append(downsamp_module)
-        downsamp_modules = nn.ModuleList(downsamp_modules)
-
-        final_layer = nn.Sequential(
+        downsamp_modules = msnn.CellList(downsamp_modules)
+
+        final_layer = msnn.SequentialCell(
+            [
             nn.Conv2d(
                 in_channels=self.head_channels[3] * head_block_type.expansion,
                 out_channels=self.num_features,
@@ -672,8 +686,8 @@                 **dd,
             ),
             nn.BatchNorm2d(self.num_features, momentum=_BN_MOMENTUM, **dd),
-            nn.ReLU(inplace=True)
-        )
+            nn.ReLU()
+        ])  # 存在 *args/**kwargs，需手动确认参数映射;; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         return incre_modules, downsamp_modules, final_layer
 
@@ -686,40 +700,45 @@         for i in range(num_branches_cur):
             if i < num_branches_pre:
                 if num_channels_cur_layer[i] != num_channels_pre_layer[i]:
-                    transition_layers.append(nn.Sequential(
+                    transition_layers.append(msnn.SequentialCell(
+                        [
                         nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False, **dd),
                         nn.BatchNorm2d(num_channels_cur_layer[i], momentum=_BN_MOMENTUM, **dd),
-                        nn.ReLU(inplace=True)))
+                        nn.ReLU()
+                    ]))  # 存在 *args/**kwargs，需手动确认参数映射;; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
                 else:
-                    transition_layers.append(nn.Identity())
+                    transition_layers.append(msnn.Identity())
             else:
                 conv3x3s = []
                 for j in range(i + 1 - num_branches_pre):
                     _in_chs = num_channels_pre_layer[-1]
                     _out_chs = num_channels_cur_layer[i] if j == i - num_branches_pre else _in_chs
-                    conv3x3s.append(nn.Sequential(
+                    conv3x3s.append(msnn.SequentialCell(
+                        [
                         nn.Conv2d(_in_chs, _out_chs, 3, 2, 1, bias=False, **dd),
                         nn.BatchNorm2d(_out_chs, momentum=_BN_MOMENTUM, **dd),
-                        nn.ReLU(inplace=True)))
-                transition_layers.append(nn.Sequential(*conv3x3s))
-
-        return nn.ModuleList(transition_layers)
+                        nn.ReLU()
+                    ]))  # 存在 *args/**kwargs，需手动确认参数映射;; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+                transition_layers.append(msnn.SequentialCell(*conv3x3s))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        return msnn.CellList(transition_layers)
 
     def _make_layer(self, block_type, inplanes, planes, block_types, stride=1, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         downsample = None
         if stride != 1 or inplanes != planes * block_type.expansion:
-            downsample = nn.Sequential(
+            downsample = msnn.SequentialCell(
+                [
                 nn.Conv2d(inplanes, planes * block_type.expansion, kernel_size=1, stride=stride, bias=False, **dd),
-                nn.BatchNorm2d(planes * block_type.expansion, momentum=_BN_MOMENTUM, **dd),
-            )
-
-        layers = [block_type(inplanes, planes, stride, downsample, **dd)]
+                nn.BatchNorm2d(planes * block_type.expansion, momentum=_BN_MOMENTUM, **dd)
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;
+
+        layers = [block_type(inplanes, planes, stride, downsample, **dd)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         inplanes = planes * block_type.expansion
         for i in range(1, block_types):
-            layers.append(block_type(inplanes, planes, **dd))
-
-        return nn.Sequential(*layers)
+            layers.append(block_type(inplanes, planes, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        return msnn.SequentialCell(*layers)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def _make_stage(self, layer_config, num_in_chs, multi_scale_output=True, device=None, dtype=None):
         num_modules = layer_config['num_modules']
@@ -746,19 +765,19 @@             ))
             num_in_chs = modules[-1].get_num_in_chs()
 
-        return SequentialList(*modules), num_in_chs
-
-    @torch.jit.ignore
+        return SequentialList(*modules), num_in_chs  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def init_weights(self):
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
                 nn.init.kaiming_normal_(
-                    m.weight, mode='fan_out', nonlinearity='relu')
+                    m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.BatchNorm2d):
-                nn.init.constant_(m.weight, 1)
-                nn.init.constant_(m.bias, 0)
-
-    @torch.jit.ignore
+                nn.init.constant_(m.weight, 1)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(
             stem=r'^conv[12]|bn[12]',
@@ -770,12 +789,12 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         assert not enable, "gradient checkpointing not supported"
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.classifier
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -783,16 +802,16 @@         self.global_pool, self.classifier = create_classifier(
             self.num_features, self.num_classes, pool_type=global_pool)
 
-    def stages(self, x) -> List[torch.Tensor]:
+    def stages(self, x) -> List[ms.Tensor]:
         x = self.layer1(x)
 
         xl = [t(x) for i, t in enumerate(self.transition1)]
         yl = self.stage2(xl)
 
-        xl = [t(yl[-1]) if not isinstance(t, nn.Identity) else yl[i] for i, t in enumerate(self.transition2)]
+        xl = [t(yl[-1]) if not isinstance(t, msnn.Identity) else yl[i] for i, t in enumerate(self.transition2)]
         yl = self.stage3(xl)
 
-        xl = [t(yl[-1]) if not isinstance(t, nn.Identity) else yl[i] for i, t in enumerate(self.transition3)]
+        xl = [t(yl[-1]) if not isinstance(t, msnn.Identity) else yl[i] for i, t in enumerate(self.transition3)]
         yl = self.stage4(xl)
         return yl
 
@@ -827,7 +846,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.classifier(x)
 
-    def forward(self, x):
+    def construct(self, x):
         y = self.forward_features(x)
         x = self.forward_head(y)
         return x
@@ -866,14 +885,14 @@             drop_rate=drop_rate,
             head=feature_location,
             **kwargs,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info = FeatureInfo(self.feature_info, out_indices)
         self._out_idx = {f['index'] for f in self.feature_info.get_dicts()}
 
     def forward_features(self, x):
         assert False, 'Not supported'
 
-    def forward(self, x) -> List[torch.Tensor]:
+    def forward(self, x) -> List[ms.Tensor]:
         out = []
         x = self.conv1(x)
         x = self.bn1(x)
@@ -914,7 +933,7 @@         pretrained_strict=pretrained_strict,
         kwargs_filter=kwargs_filter,
         **model_kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     if features_only:
         model.pretrained_cfg = pretrained_cfg_for_features(model.default_cfg)
         model.default_cfg = model.pretrained_cfg  # backwards compat
@@ -963,57 +982,57 @@ 
 @register_model
 def hrnet_w18_small(pretrained=False, **kwargs) -> HighResolutionNet:
-    return _create_hrnet('hrnet_w18_small', pretrained, **kwargs)
+    return _create_hrnet('hrnet_w18_small', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hrnet_w18_small_v2(pretrained=False, **kwargs) -> HighResolutionNet:
-    return _create_hrnet('hrnet_w18_small_v2', pretrained, **kwargs)
+    return _create_hrnet('hrnet_w18_small_v2', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hrnet_w18(pretrained=False, **kwargs) -> HighResolutionNet:
-    return _create_hrnet('hrnet_w18', pretrained, **kwargs)
+    return _create_hrnet('hrnet_w18', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hrnet_w30(pretrained=False, **kwargs) -> HighResolutionNet:
-    return _create_hrnet('hrnet_w30', pretrained, **kwargs)
+    return _create_hrnet('hrnet_w30', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hrnet_w32(pretrained=False, **kwargs) -> HighResolutionNet:
-    return _create_hrnet('hrnet_w32', pretrained, **kwargs)
+    return _create_hrnet('hrnet_w32', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hrnet_w40(pretrained=False, **kwargs) -> HighResolutionNet:
-    return _create_hrnet('hrnet_w40', pretrained, **kwargs)
+    return _create_hrnet('hrnet_w40', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hrnet_w44(pretrained=False, **kwargs) -> HighResolutionNet:
-    return _create_hrnet('hrnet_w44', pretrained, **kwargs)
+    return _create_hrnet('hrnet_w44', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hrnet_w48(pretrained=False, **kwargs) -> HighResolutionNet:
-    return _create_hrnet('hrnet_w48', pretrained, **kwargs)
+    return _create_hrnet('hrnet_w48', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hrnet_w64(pretrained=False, **kwargs) -> HighResolutionNet:
-    return _create_hrnet('hrnet_w64', pretrained, **kwargs)
+    return _create_hrnet('hrnet_w64', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hrnet_w18_ssld(pretrained=False, **kwargs) -> HighResolutionNet:
     kwargs.setdefault('head_conv_bias', False)
-    return _create_hrnet('hrnet_w18_ssld', cfg_variant='hrnet_w18', pretrained=pretrained, **kwargs)
+    return _create_hrnet('hrnet_w18_ssld', cfg_variant='hrnet_w18', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hrnet_w48_ssld(pretrained=False, **kwargs) -> HighResolutionNet:
     kwargs.setdefault('head_conv_bias', False)
-    return _create_hrnet('hrnet_w48_ssld', cfg_variant='hrnet_w48', pretrained=pretrained, **kwargs)
-
+    return _create_hrnet('hrnet_w48_ssld', cfg_variant='hrnet_w48', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
