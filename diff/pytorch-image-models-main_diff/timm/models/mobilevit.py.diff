--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ MobileViT
 
 Paper:
@@ -16,9 +21,9 @@ import math
 from typing import Callable, Tuple, Optional, Type
 
-import torch
-import torch.nn.functional as F
-from torch import nn
+# import torch
+# import torch.nn.functional as F
+# from torch import nn
 
 from timm.layers import to_2tuple, make_divisible, GroupNorm1, ConvMlp, DropPath, is_exportable
 from ._builder import build_model_with_cfg
@@ -162,7 +167,7 @@ 
 
 @register_notrace_module
-class MobileVitBlock(nn.Module):
+class MobileVitBlock(msnn.Cell):
     """ MobileViT block
         Paper: https://arxiv.org/abs/2110.02178?context=cs.LG
     """
@@ -208,7 +213,8 @@         )
         self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False, **dd)
 
-        self.transformer = nn.Sequential(*[
+        self.transformer = msnn.SequentialCell([
+            [
             TransformerBlock(
                 transformer_dim,
                 mlp_ratio=mlp_ratio,
@@ -222,6 +228,7 @@                 **dd,
             )
             for _ in range(transformer_depth)
+        ]
         ])
         self.norm = transformer_norm_layer(transformer_dim, **dd)
 
@@ -235,7 +242,7 @@         self.patch_size = to_2tuple(patch_size)
         self.patch_area = self.patch_size[0] * self.patch_size[1]
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = x
 
         # Local representation
@@ -251,7 +258,7 @@         interpolate = False
         if new_h != H or new_w != W:
             # Note: Padding can be done, but then it needs to be handled in attention function.
-            x = F.interpolate(x, size=(new_h, new_w), mode="bilinear", align_corners=False)
+            x = nn.functional.interpolate(x, size = (new_h, new_w), mode = "bilinear", align_corners = False)
             interpolate = True
 
         # [B, C, H, W] --> [B * C * n_h, n_w, p_h, p_w]
@@ -270,15 +277,15 @@         # [B*C*n_h, n_w, p_h, p_w] --> [B*C*n_h, p_h, n_w, p_w] --> [B, C, H, W]
         x = x.transpose(1, 2).reshape(B, C, num_patch_h * patch_h, num_patch_w * patch_w)
         if interpolate:
-            x = F.interpolate(x, size=(H, W), mode="bilinear", align_corners=False)
+            x = nn.functional.interpolate(x, size = (H, W), mode = "bilinear", align_corners = False)
 
         x = self.conv_proj(x)
         if self.conv_fusion is not None:
-            x = self.conv_fusion(torch.cat((shortcut, x), dim=1))
+            x = self.conv_fusion(mint.cat((shortcut, x), dim = 1))
         return x
 
 
-class LinearSelfAttention(nn.Module):
+class LinearSelfAttention(msnn.Cell):
     """
     This layer applies a self-attention with linear complexity, as described in `https://arxiv.org/abs/2206.02680`
     This layer can be used for self- as well as cross-attention.
@@ -328,7 +335,7 @@         )
         self.out_drop = nn.Dropout(proj_drop)
 
-    def _forward_self_attn(self, x: torch.Tensor) -> torch.Tensor:
+    def _forward_self_attn(self, x: ms.Tensor) -> ms.Tensor:
         # [B, C, P, N] --> [B, h + 2d, P, N]
         qkv = self.qkv_proj(x)
 
@@ -338,7 +345,7 @@         query, key, value = qkv.split([1, self.embed_dim, self.embed_dim], dim=1)
 
         # apply softmax along N dimension
-        context_scores = F.softmax(query, dim=-1)
+        context_scores = nn.functional.softmax(query, dim = -1)
         context_scores = self.attn_drop(context_scores)
 
         # Compute context vector
@@ -347,13 +354,13 @@ 
         # combine context vector with values
         # [B, d, P, N] * [B, d, P, 1] --> [B, d, P, N]
-        out = F.relu(value) * context_vector.expand_as(value)
+        out = nn.functional.relu(value) * context_vector.expand_as(value)
         out = self.out_proj(out)
         out = self.out_drop(out)
         return out
 
     @torch.jit.ignore()
-    def _forward_cross_attn(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def _forward_cross_attn(self, x: ms.Tensor, x_prev: Optional[torch.Tensor] = None) -> ms.Tensor:
         # x --> [B, C, P, N]
         # x_prev = [B, C, P, M]
         batch_size, in_dim, kv_patch_area, kv_num_patches = x.shape
@@ -365,23 +372,17 @@ 
         # compute query, key, and value
         # [B, C, P, M] --> [B, 1 + d, P, M]
-        qk = F.conv2d(
-            x_prev,
-            weight=self.qkv_proj.weight[:self.embed_dim + 1],
-            bias=self.qkv_proj.bias[:self.embed_dim + 1],
-        )
+        qk = nn.functional.conv2d(
+            x_prev, weight = self.qkv_proj.weight[:self.embed_dim + 1], bias = self.qkv_proj.bias[:self.embed_dim + 1])
 
         # [B, 1 + d, P, M] --> [B, 1, P, M], [B, d, P, M]
         query, key = qk.split([1, self.embed_dim], dim=1)
         # [B, C, P, N] --> [B, d, P, N]
-        value = F.conv2d(
-            x,
-            weight=self.qkv_proj.weight[self.embed_dim + 1],
-            bias=self.qkv_proj.bias[self.embed_dim + 1] if self.qkv_proj.bias is not None else None,
-        )
+        value = nn.functional.conv2d(
+            x, weight = self.qkv_proj.weight[self.embed_dim + 1], bias = self.qkv_proj.bias[self.embed_dim + 1] if self.qkv_proj.bias is not None else None)
 
         # apply softmax along M dimension
-        context_scores = F.softmax(query, dim=-1)
+        context_scores = nn.functional.softmax(query, dim = -1)
         context_scores = self.attn_drop(context_scores)
 
         # compute context vector
@@ -390,19 +391,19 @@ 
         # combine context vector with values
         # [B, d, P, N] * [B, d, P, 1] --> [B, d, P, N]
-        out = F.relu(value) * context_vector.expand_as(value)
+        out = nn.functional.relu(value) * context_vector.expand_as(value)
         out = self.out_proj(out)
         out = self.out_drop(out)
         return out
 
-    def forward(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, x_prev: Optional[torch.Tensor] = None) -> ms.Tensor:
         if x_prev is None:
             return self._forward_self_attn(x)
         else:
             return self._forward_cross_attn(x, x_prev=x_prev)
 
 
-class LinearTransformerBlock(nn.Module):
+class LinearTransformerBlock(msnn.Cell):
     """
     This class defines the pre-norm transformer encoder with linear self-attention in `MobileViTv2 paper <>`_
     Args:
@@ -448,7 +449,7 @@             **dd)
         self.drop_path2 = DropPath(drop_path)
 
-    def forward(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, x_prev: Optional[torch.Tensor] = None) -> ms.Tensor:
         if x_prev is None:
             # self-attention
             x = x + self.drop_path1(self.attn(self.norm1(x)))
@@ -465,7 +466,7 @@ 
 
 @register_notrace_module
-class MobileVitV2Block(nn.Module):
+class MobileVitV2Block(msnn.Cell):
     """
     This class defines the `MobileViTv2 block <>`_
     """
@@ -509,7 +510,8 @@         )
         self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False, **dd)
 
-        self.transformer = nn.Sequential(*[
+        self.transformer = msnn.SequentialCell([
+            [
             LinearTransformerBlock(
                 transformer_dim,
                 mlp_ratio=mlp_ratio,
@@ -521,6 +523,7 @@                 **dd,
             )
             for _ in range(transformer_depth)
+        ]
         ])
         self.norm = transformer_norm_layer(transformer_dim, **dd)
 
@@ -530,14 +533,14 @@         self.patch_area = self.patch_size[0] * self.patch_size[1]
         self.coreml_exportable = is_exportable()
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         B, C, H, W = x.shape
         patch_h, patch_w = self.patch_size
         new_h, new_w = math.ceil(H / patch_h) * patch_h, math.ceil(W / patch_w) * patch_w
         num_patch_h, num_patch_w = new_h // patch_h, new_w // patch_w  # n_h, n_w
         num_patches = num_patch_h * num_patch_w  # N
         if new_h != H or new_w != W:
-            x = F.interpolate(x, size=(new_h, new_w), mode="bilinear", align_corners=True)
+            x = nn.functional.interpolate(x, size = (new_h, new_w), mode = "bilinear", align_corners = True)
 
         # Local representation
         x = self.conv_kxk(x)
@@ -546,7 +549,7 @@         # Unfold (feature map -> patches), [B, C, H, W] -> [B, C, P, N]
         C = x.shape[1]
         if self.coreml_exportable:
-            x = F.unfold(x, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))
+            x = nn.functional.unfold(x, kernel_size = (patch_h, patch_w), stride = (patch_h, patch_w))
         else:
             x = x.reshape(B, C, num_patch_h, patch_h, num_patch_w, patch_w).permute(0, 1, 3, 5, 2, 4)
         x = x.reshape(B, C, -1, num_patches)
@@ -559,7 +562,7 @@         if self.coreml_exportable:
             # adopted from https://github.com/apple/ml-cvnets/blob/main/cvnets/modules/mobilevit_block.py#L609-L624
             x = x.reshape(B, C * patch_h * patch_w, num_patch_h, num_patch_w)
-            x = F.pixel_shuffle(x, upscale_factor=patch_h)
+            x = F.pixel_shuffle(x, upscale_factor=patch_h)  # 'torch.nn.functional.pixel_shuffle' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             x = x.reshape(B, C, patch_h, patch_w, num_patch_h, num_patch_w).permute(0, 1, 4, 2, 5, 3)
             x = x.reshape(B, C, num_patch_h * patch_h, num_patch_w * patch_w)
