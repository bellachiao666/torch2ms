--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ MobileViT
 
 Paper:
@@ -15,10 +20,8 @@ #
 import math
 from typing import Callable, Tuple, Optional, Type
-
-import torch
-import torch.nn.functional as F
-from torch import nn
+# import torch.nn.functional as F
+# from torch import nn
 
 from timm.layers import to_2tuple, make_divisible, GroupNorm1, ConvMlp, DropPath, is_exportable
 from ._builder import build_model_with_cfg
@@ -162,7 +165,7 @@ 
 
 @register_notrace_module
-class MobileVitBlock(nn.Module):
+class MobileVitBlock(msnn.Cell):
     """ MobileViT block
         Paper: https://arxiv.org/abs/2110.02178?context=cs.LG
     """
@@ -185,7 +188,7 @@             no_fusion: bool = False,
             drop_path_rate: float = 0.,
             layers: LayerFn = None,
-            transformer_norm_layer: Type[nn.Module] = nn.LayerNorm,
+            transformer_norm_layer: Type[msnn.Cell] = nn.LayerNorm,
             device=None,
             dtype=None,
             **kwargs,  # eat unused args
@@ -205,10 +208,10 @@             groups=groups,
             dilation=dilation[0],
             **dd,
-        )
-        self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False, **dd)
-
-        self.transformer = nn.Sequential(*[
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+        self.transformer = msnn.SequentialCell(*[
             TransformerBlock(
                 transformer_dim,
                 mlp_ratio=mlp_ratio,
@@ -222,20 +225,20 @@                 **dd,
             )
             for _ in range(transformer_depth)
-        ])
-        self.norm = transformer_norm_layer(transformer_dim, **dd)
-
-        self.conv_proj = layers.conv_norm_act(transformer_dim, out_chs, kernel_size=1, stride=1, **dd)
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm = transformer_norm_layer(transformer_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.conv_proj = layers.conv_norm_act(transformer_dim, out_chs, kernel_size=1, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         if no_fusion:
             self.conv_fusion = None
         else:
-            self.conv_fusion = layers.conv_norm_act(in_chs + out_chs, out_chs, kernel_size=kernel_size, stride=1, **dd)
+            self.conv_fusion = layers.conv_norm_act(in_chs + out_chs, out_chs, kernel_size=kernel_size, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.patch_size = to_2tuple(patch_size)
         self.patch_area = self.patch_size[0] * self.patch_size[1]
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = x
 
         # Local representation
@@ -251,7 +254,7 @@         interpolate = False
         if new_h != H or new_w != W:
             # Note: Padding can be done, but then it needs to be handled in attention function.
-            x = F.interpolate(x, size=(new_h, new_w), mode="bilinear", align_corners=False)
+            x = nn.functional.interpolate(x, size = (new_h, new_w), mode = "bilinear", align_corners = False)
             interpolate = True
 
         # [B, C, H, W] --> [B * C * n_h, n_w, p_h, p_w]
@@ -270,15 +273,15 @@         # [B*C*n_h, n_w, p_h, p_w] --> [B*C*n_h, p_h, n_w, p_w] --> [B, C, H, W]
         x = x.transpose(1, 2).reshape(B, C, num_patch_h * patch_h, num_patch_w * patch_w)
         if interpolate:
-            x = F.interpolate(x, size=(H, W), mode="bilinear", align_corners=False)
+            x = nn.functional.interpolate(x, size = (H, W), mode = "bilinear", align_corners = False)
 
         x = self.conv_proj(x)
         if self.conv_fusion is not None:
-            x = self.conv_fusion(torch.cat((shortcut, x), dim=1))
+            x = self.conv_fusion(mint.cat((shortcut, x), dim=1))
         return x
 
 
-class LinearSelfAttention(nn.Module):
+class LinearSelfAttention(msnn.Cell):
     """
     This layer applies a self-attention with linear complexity, as described in `https://arxiv.org/abs/2206.02680`
     This layer can be used for self- as well as cross-attention.
@@ -317,7 +320,7 @@             bias=bias,
             kernel_size=1,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn_drop = nn.Dropout(attn_drop)
         self.out_proj = nn.Conv2d(
             in_channels=embed_dim,
@@ -325,10 +328,10 @@             bias=bias,
             kernel_size=1,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
         self.out_drop = nn.Dropout(proj_drop)
 
-    def _forward_self_attn(self, x: torch.Tensor) -> torch.Tensor:
+    def _forward_self_attn(self, x: ms.Tensor) -> ms.Tensor:
         # [B, C, P, N] --> [B, h + 2d, P, N]
         qkv = self.qkv_proj(x)
 
@@ -338,7 +341,7 @@         query, key, value = qkv.split([1, self.embed_dim, self.embed_dim], dim=1)
 
         # apply softmax along N dimension
-        context_scores = F.softmax(query, dim=-1)
+        context_scores = nn.functional.softmax(query, dim = -1)
         context_scores = self.attn_drop(context_scores)
 
         # Compute context vector
@@ -347,13 +350,13 @@ 
         # combine context vector with values
         # [B, d, P, N] * [B, d, P, 1] --> [B, d, P, N]
-        out = F.relu(value) * context_vector.expand_as(value)
+        out = nn.functional.relu(value) * context_vector.expand_as(value)
         out = self.out_proj(out)
         out = self.out_drop(out)
         return out
 
-    @torch.jit.ignore()
-    def _forward_cross_attn(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:
+    @ms.jit()
+    def _forward_cross_attn(self, x: ms.Tensor, x_prev: Optional[ms.Tensor] = None) -> ms.Tensor:
         # x --> [B, C, P, N]
         # x_prev = [B, C, P, M]
         batch_size, in_dim, kv_patch_area, kv_num_patches = x.shape
@@ -365,23 +368,17 @@ 
         # compute query, key, and value
         # [B, C, P, M] --> [B, 1 + d, P, M]
-        qk = F.conv2d(
-            x_prev,
-            weight=self.qkv_proj.weight[:self.embed_dim + 1],
-            bias=self.qkv_proj.bias[:self.embed_dim + 1],
-        )
+        qk = nn.functional.conv2d(
+            x_prev, weight = self.qkv_proj.weight[:self.embed_dim + 1], bias = self.qkv_proj.bias[:self.embed_dim + 1])
 
         # [B, 1 + d, P, M] --> [B, 1, P, M], [B, d, P, M]
         query, key = qk.split([1, self.embed_dim], dim=1)
         # [B, C, P, N] --> [B, d, P, N]
-        value = F.conv2d(
-            x,
-            weight=self.qkv_proj.weight[self.embed_dim + 1],
-            bias=self.qkv_proj.bias[self.embed_dim + 1] if self.qkv_proj.bias is not None else None,
-        )
+        value = nn.functional.conv2d(
+            x, weight = self.qkv_proj.weight[self.embed_dim + 1], bias = self.qkv_proj.bias[self.embed_dim + 1] if self.qkv_proj.bias is not None else None)
 
         # apply softmax along M dimension
-        context_scores = F.softmax(query, dim=-1)
+        context_scores = nn.functional.softmax(query, dim = -1)
         context_scores = self.attn_drop(context_scores)
 
         # compute context vector
@@ -390,19 +387,19 @@ 
         # combine context vector with values
         # [B, d, P, N] * [B, d, P, 1] --> [B, d, P, N]
-        out = F.relu(value) * context_vector.expand_as(value)
+        out = nn.functional.relu(value) * context_vector.expand_as(value)
         out = self.out_proj(out)
         out = self.out_drop(out)
         return out
 
-    def forward(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, x_prev: Optional[ms.Tensor] = None) -> ms.Tensor:
         if x_prev is None:
             return self._forward_self_attn(x)
         else:
             return self._forward_cross_attn(x, x_prev=x_prev)
 
 
-class LinearTransformerBlock(nn.Module):
+class LinearTransformerBlock(msnn.Cell):
     """
     This class defines the pre-norm transformer encoder with linear self-attention in `MobileViTv2 paper <>`_
     Args:
@@ -425,8 +422,8 @@             drop: float = 0.0,
             attn_drop: float = 0.0,
             drop_path: float = 0.0,
-            act_layer: Optional[Type[nn.Module]] = None,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Optional[Type[msnn.Cell]] = None,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ) -> None:
@@ -435,20 +432,20 @@         act_layer = act_layer or nn.SiLU
         norm_layer = norm_layer or GroupNorm1
 
-        self.norm1 = norm_layer(embed_dim, **dd)
-        self.attn = LinearSelfAttention(embed_dim=embed_dim, attn_drop=attn_drop, proj_drop=drop, **dd)
+        self.norm1 = norm_layer(embed_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn = LinearSelfAttention(embed_dim=embed_dim, attn_drop=attn_drop, proj_drop=drop, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.drop_path1 = DropPath(drop_path)
 
-        self.norm2 = norm_layer(embed_dim, **dd)
+        self.norm2 = norm_layer(embed_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.mlp = ConvMlp(
             in_features=embed_dim,
             hidden_features=int(embed_dim * mlp_ratio),
             act_layer=act_layer,
             drop=drop,
-            **dd)
+            **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.drop_path2 = DropPath(drop_path)
 
-    def forward(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, x_prev: Optional[ms.Tensor] = None) -> ms.Tensor:
         if x_prev is None:
             # self-attention
             x = x + self.drop_path1(self.attn(self.norm1(x)))
@@ -465,7 +462,7 @@ 
 
 @register_notrace_module
-class MobileVitV2Block(nn.Module):
+class MobileVitV2Block(msnn.Cell):
     """
     This class defines the `MobileViTv2 block <>`_
     """
@@ -486,7 +483,7 @@             drop: int = 0.,
             drop_path_rate: float = 0.,
             layers: LayerFn = None,
-            transformer_norm_layer: Type[nn.Module] = GroupNorm1,
+            transformer_norm_layer: Type[msnn.Cell] = GroupNorm1,
             device=None,
             dtype=None,
             **kwargs,  # eat unused args
@@ -506,10 +503,10 @@             groups=groups,
             dilation=dilation[0],
             **dd,
-        )
-        self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False, **dd)
-
-        self.transformer = nn.Sequential(*[
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+        self.transformer = msnn.SequentialCell(*[
             LinearTransformerBlock(
                 transformer_dim,
                 mlp_ratio=mlp_ratio,
@@ -521,23 +518,23 @@                 **dd,
             )
             for _ in range(transformer_depth)
-        ])
-        self.norm = transformer_norm_layer(transformer_dim, **dd)
-
-        self.conv_proj = layers.conv_norm_act(transformer_dim, out_chs, kernel_size=1, stride=1, apply_act=False, **dd)
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm = transformer_norm_layer(transformer_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.conv_proj = layers.conv_norm_act(transformer_dim, out_chs, kernel_size=1, stride=1, apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.patch_size = to_2tuple(patch_size)
         self.patch_area = self.patch_size[0] * self.patch_size[1]
         self.coreml_exportable = is_exportable()
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         B, C, H, W = x.shape
         patch_h, patch_w = self.patch_size
         new_h, new_w = math.ceil(H / patch_h) * patch_h, math.ceil(W / patch_w) * patch_w
         num_patch_h, num_patch_w = new_h // patch_h, new_w // patch_w  # n_h, n_w
         num_patches = num_patch_h * num_patch_w  # N
         if new_h != H or new_w != W:
-            x = F.interpolate(x, size=(new_h, new_w), mode="bilinear", align_corners=True)
+            x = nn.functional.interpolate(x, size = (new_h, new_w), mode = "bilinear", align_corners = True)
 
         # Local representation
         x = self.conv_kxk(x)
@@ -546,7 +543,7 @@         # Unfold (feature map -> patches), [B, C, H, W] -> [B, C, P, N]
         C = x.shape[1]
         if self.coreml_exportable:
-            x = F.unfold(x, kernel_size=(patch_h, patch_w), stride=(patch_h, patch_w))
+            x = nn.functional.unfold(x, kernel_size = (patch_h, patch_w), stride = (patch_h, patch_w))
         else:
             x = x.reshape(B, C, num_patch_h, patch_h, num_patch_w, patch_w).permute(0, 1, 3, 5, 2, 4)
         x = x.reshape(B, C, -1, num_patches)
@@ -559,7 +556,7 @@         if self.coreml_exportable:
             # adopted from https://github.com/apple/ml-cvnets/blob/main/cvnets/modules/mobilevit_block.py#L609-L624
             x = x.reshape(B, C * patch_h * patch_w, num_patch_h, num_patch_w)
-            x = F.pixel_shuffle(x, upscale_factor=patch_h)
+            x = F.pixel_shuffle(x, upscale_factor=patch_h)  # 'torch.nn.functional.pixel_shuffle' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             x = x.reshape(B, C, patch_h, patch_w, num_patch_h, num_patch_w).permute(0, 1, 4, 2, 5, 3)
             x = x.reshape(B, C, num_patch_h * patch_h, num_patch_w * patch_w)
@@ -577,7 +574,7 @@         ByobNet, variant, pretrained,
         model_cfg=model_cfgs[variant] if not cfg_variant else model_cfgs[cfg_variant],
         feature_cfg=dict(flatten_sequential=True),
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _create_mobilevit2(variant, cfg_variant=None, pretrained=False, **kwargs):
@@ -585,7 +582,7 @@         ByobNet, variant, pretrained,
         model_cfg=model_cfgs[variant] if not cfg_variant else model_cfgs[cfg_variant],
         feature_cfg=dict(flatten_sequential=True),
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url='', **kwargs):
@@ -651,52 +648,52 @@ 
 @register_model
 def mobilevit_xxs(pretrained=False, **kwargs) -> ByobNet:
-    return _create_mobilevit('mobilevit_xxs', pretrained=pretrained, **kwargs)
+    return _create_mobilevit('mobilevit_xxs', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobilevit_xs(pretrained=False, **kwargs) -> ByobNet:
-    return _create_mobilevit('mobilevit_xs', pretrained=pretrained, **kwargs)
+    return _create_mobilevit('mobilevit_xs', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobilevit_s(pretrained=False, **kwargs) -> ByobNet:
-    return _create_mobilevit('mobilevit_s', pretrained=pretrained, **kwargs)
+    return _create_mobilevit('mobilevit_s', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobilevitv2_050(pretrained=False, **kwargs) -> ByobNet:
-    return _create_mobilevit('mobilevitv2_050', pretrained=pretrained, **kwargs)
+    return _create_mobilevit('mobilevitv2_050', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobilevitv2_075(pretrained=False, **kwargs) -> ByobNet:
-    return _create_mobilevit('mobilevitv2_075', pretrained=pretrained, **kwargs)
+    return _create_mobilevit('mobilevitv2_075', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobilevitv2_100(pretrained=False, **kwargs) -> ByobNet:
-    return _create_mobilevit('mobilevitv2_100', pretrained=pretrained, **kwargs)
+    return _create_mobilevit('mobilevitv2_100', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobilevitv2_125(pretrained=False, **kwargs) -> ByobNet:
-    return _create_mobilevit('mobilevitv2_125', pretrained=pretrained, **kwargs)
+    return _create_mobilevit('mobilevitv2_125', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobilevitv2_150(pretrained=False, **kwargs) -> ByobNet:
-    return _create_mobilevit('mobilevitv2_150', pretrained=pretrained, **kwargs)
+    return _create_mobilevit('mobilevitv2_150', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobilevitv2_175(pretrained=False, **kwargs) -> ByobNet:
-    return _create_mobilevit('mobilevitv2_175', pretrained=pretrained, **kwargs)
+    return _create_mobilevit('mobilevitv2_175', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mobilevitv2_200(pretrained=False, **kwargs) -> ByobNet:
-    return _create_mobilevit('mobilevitv2_200', pretrained=pretrained, **kwargs)
+    return _create_mobilevit('mobilevitv2_200', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 register_model_deprecations(__name__, {
