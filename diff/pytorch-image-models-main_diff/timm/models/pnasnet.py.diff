--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
  pnasnet5large implementation grabbed from Cadene's pretrained models
  Additional credit to https://github.com/creafz
@@ -8,9 +13,7 @@ from collections import OrderedDict
 from functools import partial
 from typing import Type
-
-import torch
-import torch.nn as nn
+# import torch.nn as nn
 
 from timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier
 from ._builder import build_model_with_cfg
@@ -19,7 +22,7 @@ __all__ = ['PNASNet5Large']
 
 
-class SeparableConv2d(nn.Module):
+class SeparableConv2d(msnn.Cell):
 
     def __init__(
             self,
@@ -41,22 +44,22 @@             padding=padding,
             groups=in_channels,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.pointwise_conv2d = create_conv2d(
             in_channels,
             out_channels,
             kernel_size=1,
             padding=padding,
             **dd,
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.depthwise_conv2d(x)
         x = self.pointwise_conv2d(x)
         return x
 
 
-class BranchSeparables(nn.Module):
+class BranchSeparables(msnn.Cell):
 
     def __init__(
             self,
@@ -80,8 +83,8 @@             stride=stride,
             padding=padding,
             **dd,
-        )
-        self.bn_sep_1 = nn.BatchNorm2d(middle_channels, eps=0.001, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.bn_sep_1 = nn.BatchNorm2d(middle_channels, eps=0.001, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.act_2 = nn.ReLU()
         self.separable_2 = SeparableConv2d(
             middle_channels,
@@ -90,10 +93,10 @@             stride=1,
             padding=padding,
             **dd,
-        )
-        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, **dd)
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.act_1(x)
         x = self.separable_1(x)
         x = self.bn_sep_1(x)
@@ -103,7 +106,7 @@         return x
 
 
-class ActConvBn(nn.Module):
+class ActConvBn(msnn.Cell):
 
     def __init__(
             self,
@@ -125,17 +128,17 @@             stride=stride,
             padding=padding,
             **dd,
-        )
-        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, **dd)
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.act(x)
         x = self.conv(x)
         x = self.bn(x)
         return x
 
 
-class FactorizedReduction(nn.Module):
+class FactorizedReduction(msnn.Cell):
 
     def __init__(
             self,
@@ -148,26 +151,30 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.act = nn.ReLU()
-        self.path_1 = nn.Sequential(OrderedDict([
-            ('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False)),
+        self.path_1 = msnn.SequentialCell([
+            OrderedDict([
+            ('avgpool', nn.AvgPool2d(1, stride = 2, count_include_pad = False)),
             ('conv', create_conv2d(in_channels, out_channels // 2, kernel_size=1, padding=padding, **dd)),
-        ]))
-        self.path_2 = nn.Sequential(OrderedDict([
+        ])
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.path_2 = msnn.SequentialCell([
+            OrderedDict([
             ('pad', nn.ZeroPad2d((-1, 1, -1, 1))),  # shift
-            ('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False)),
+            ('avgpool', nn.AvgPool2d(1, stride = 2, count_include_pad = False)),
             ('conv', create_conv2d(in_channels, out_channels // 2, kernel_size=1, padding=padding, **dd)),
-        ]))
-        self.final_path_bn = nn.BatchNorm2d(out_channels, eps=0.001, **dd)
-
-    def forward(self, x):
+        ])
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.final_path_bn = nn.BatchNorm2d(out_channels, eps=0.001, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.act(x)
         x_path1 = self.path_1(x)
         x_path2 = self.path_2(x)
-        out = self.final_path_bn(torch.cat([x_path1, x_path2], 1))
+        out = self.final_path_bn(mint.cat([x_path1, x_path2], 1))
         return out
 
 
-class CellBase(nn.Module):
+class CellBase(msnn.Cell):
 
     def cell_forward(self, x_left, x_right):
         x_comb_iter_0_left = self.comb_iter_0_left(x_left)
@@ -193,7 +200,7 @@             x_comb_iter_4_right = x_right
         x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right
 
-        x_out = torch.cat([x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
+        x_out = mint.cat([x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
         return x_out
 
 
@@ -211,33 +218,35 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, kernel_size=1, padding=pad_type, **dd)
+        self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, kernel_size=1, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.comb_iter_0_left = BranchSeparables(
-            in_chs_left, out_chs_left, kernel_size=5, stride=2, stem_cell=True, padding=pad_type, **dd)
-        self.comb_iter_0_right = nn.Sequential(OrderedDict([
+            in_chs_left, out_chs_left, kernel_size=5, stride=2, stem_cell=True, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.comb_iter_0_right = msnn.SequentialCell([
+            OrderedDict([
             ('max_pool', create_pool2d('max', 3, stride=2, padding=pad_type)),
             ('conv', create_conv2d(in_chs_left, out_chs_left, kernel_size=1, padding=pad_type, **dd)),
             ('bn', nn.BatchNorm2d(out_chs_left, eps=0.001, **dd)),
-        ]))
+        ])
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;; 存在 *args/**kwargs，需手动确认参数映射;
 
         self.comb_iter_1_left = BranchSeparables(
-            out_chs_right, out_chs_right, kernel_size=7, stride=2, padding=pad_type, **dd)
+            out_chs_right, out_chs_right, kernel_size=7, stride=2, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.comb_iter_1_right = create_pool2d('max', 3, stride=2, padding=pad_type)
 
         self.comb_iter_2_left = BranchSeparables(
-            out_chs_right, out_chs_right, kernel_size=5, stride=2, padding=pad_type, **dd)
+            out_chs_right, out_chs_right, kernel_size=5, stride=2, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.comb_iter_2_right = BranchSeparables(
-            out_chs_right, out_chs_right, kernel_size=3, stride=2, padding=pad_type, **dd)
+            out_chs_right, out_chs_right, kernel_size=3, stride=2, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.comb_iter_3_left = BranchSeparables(
-            out_chs_right, out_chs_right, kernel_size=3, padding=pad_type, **dd)
+            out_chs_right, out_chs_right, kernel_size=3, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.comb_iter_3_right = create_pool2d('max', 3, stride=2, padding=pad_type)
 
         self.comb_iter_4_left = BranchSeparables(
-            in_chs_right, out_chs_right, kernel_size=3, stride=2, stem_cell=True, padding=pad_type, **dd)
+            in_chs_right, out_chs_right, kernel_size=3, stride=2, stem_cell=True, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.comb_iter_4_right = ActConvBn(
-            out_chs_right, out_chs_right, kernel_size=1, stride=2, padding=pad_type, **dd)
+            out_chs_right, out_chs_right, kernel_size=1, stride=2, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def forward(self, x_left):
         x_right = self.conv_1x1(x_left)
@@ -271,32 +280,32 @@         # of the left input of a cell approximately by a factor of 2.
         self.match_prev_layer_dimensions = match_prev_layer_dims
         if match_prev_layer_dims:
-            self.conv_prev_1x1 = FactorizedReduction(in_chs_left, out_chs_left, padding=pad_type, **dd)
+            self.conv_prev_1x1 = FactorizedReduction(in_chs_left, out_chs_left, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.conv_prev_1x1 = ActConvBn(in_chs_left, out_chs_left, kernel_size=1, padding=pad_type, **dd)
-        self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, kernel_size=1, padding=pad_type, **dd)
+            self.conv_prev_1x1 = ActConvBn(in_chs_left, out_chs_left, kernel_size=1, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, kernel_size=1, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.comb_iter_0_left = BranchSeparables(
-            out_chs_left, out_chs_left, kernel_size=5, stride=stride, padding=pad_type, **dd)
+            out_chs_left, out_chs_left, kernel_size=5, stride=stride, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.comb_iter_0_right = create_pool2d('max', 3, stride=stride, padding=pad_type)
 
         self.comb_iter_1_left = BranchSeparables(
-            out_chs_right, out_chs_right, kernel_size=7, stride=stride, padding=pad_type, **dd)
+            out_chs_right, out_chs_right, kernel_size=7, stride=stride, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.comb_iter_1_right = create_pool2d('max', 3, stride=stride, padding=pad_type)
 
         self.comb_iter_2_left = BranchSeparables(
-            out_chs_right, out_chs_right, kernel_size=5, stride=stride, padding=pad_type, **dd)
+            out_chs_right, out_chs_right, kernel_size=5, stride=stride, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.comb_iter_2_right = BranchSeparables(
-            out_chs_right, out_chs_right, kernel_size=3, stride=stride, padding=pad_type, **dd)
-
-        self.comb_iter_3_left = BranchSeparables(out_chs_right, out_chs_right, kernel_size=3, **dd)
+            out_chs_right, out_chs_right, kernel_size=3, stride=stride, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.comb_iter_3_left = BranchSeparables(out_chs_right, out_chs_right, kernel_size=3, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.comb_iter_3_right = create_pool2d('max', 3, stride=stride, padding=pad_type)
 
         self.comb_iter_4_left = BranchSeparables(
-            out_chs_left, out_chs_left, kernel_size=3, stride=stride, padding=pad_type, **dd)
+            out_chs_left, out_chs_left, kernel_size=3, stride=stride, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if is_reduction:
             self.comb_iter_4_right = ActConvBn(
-                out_chs_right, out_chs_right, kernel_size=1, stride=stride, padding=pad_type, **dd)
+                out_chs_right, out_chs_right, kernel_size=1, stride=stride, padding=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.comb_iter_4_right = None
 
@@ -307,7 +316,7 @@         return x_out
 
 
-class PNASNet5Large(nn.Module):
+class PNASNet5Large(msnn.Cell):
     def __init__(
             self,
             num_classes: int = 1000,
@@ -327,45 +336,45 @@ 
         self.conv_0 = ConvNormAct(
             in_chans, 96, kernel_size=3, stride=2, padding=0,
-            norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.1), apply_act=False, **dd)
+            norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.1), apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.cell_stem_0 = CellStem0(
-            in_chs_left=96, out_chs_left=54, in_chs_right=96, out_chs_right=54, pad_type=pad_type, **dd)
+            in_chs_left=96, out_chs_left=54, in_chs_right=96, out_chs_right=54, pad_type=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.cell_stem_1 = Cell(
             in_chs_left=96, out_chs_left=108, in_chs_right=270, out_chs_right=108, pad_type=pad_type,
-            match_prev_layer_dims=True, is_reduction=True, **dd)
+            match_prev_layer_dims=True, is_reduction=True, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.cell_0 = Cell(
             in_chs_left=270, out_chs_left=216, in_chs_right=540, out_chs_right=216, pad_type=pad_type,
-            match_prev_layer_dims=True, **dd)
+            match_prev_layer_dims=True, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.cell_1 = Cell(
-            in_chs_left=540, out_chs_left=216, in_chs_right=1080, out_chs_right=216, pad_type=pad_type, **dd)
+            in_chs_left=540, out_chs_left=216, in_chs_right=1080, out_chs_right=216, pad_type=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.cell_2 = Cell(
-            in_chs_left=1080, out_chs_left=216, in_chs_right=1080, out_chs_right=216, pad_type=pad_type, **dd)
+            in_chs_left=1080, out_chs_left=216, in_chs_right=1080, out_chs_right=216, pad_type=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.cell_3 = Cell(
-            in_chs_left=1080, out_chs_left=216, in_chs_right=1080, out_chs_right=216, pad_type=pad_type, **dd)
+            in_chs_left=1080, out_chs_left=216, in_chs_right=1080, out_chs_right=216, pad_type=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.cell_4 = Cell(
             in_chs_left=1080, out_chs_left=432, in_chs_right=1080, out_chs_right=432, pad_type=pad_type,
-            is_reduction=True, **dd)
+            is_reduction=True, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.cell_5 = Cell(
             in_chs_left=1080, out_chs_left=432, in_chs_right=2160, out_chs_right=432, pad_type=pad_type,
-            match_prev_layer_dims=True, **dd)
+            match_prev_layer_dims=True, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.cell_6 = Cell(
-            in_chs_left=2160, out_chs_left=432, in_chs_right=2160, out_chs_right=432, pad_type=pad_type, **dd)
+            in_chs_left=2160, out_chs_left=432, in_chs_right=2160, out_chs_right=432, pad_type=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.cell_7 = Cell(
-            in_chs_left=2160, out_chs_left=432, in_chs_right=2160, out_chs_right=432, pad_type=pad_type, **dd)
+            in_chs_left=2160, out_chs_left=432, in_chs_right=2160, out_chs_right=432, pad_type=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.cell_8 = Cell(
             in_chs_left=2160, out_chs_left=864, in_chs_right=2160, out_chs_right=864, pad_type=pad_type,
-            is_reduction=True, **dd)
+            is_reduction=True, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.cell_9 = Cell(
             in_chs_left=2160, out_chs_left=864, in_chs_right=4320, out_chs_right=864, pad_type=pad_type,
-            match_prev_layer_dims=True, **dd)
+            match_prev_layer_dims=True, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.cell_10 = Cell(
-            in_chs_left=4320, out_chs_left=864, in_chs_right=4320, out_chs_right=864, pad_type=pad_type, **dd)
+            in_chs_left=4320, out_chs_left=864, in_chs_right=4320, out_chs_right=864, pad_type=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.cell_11 = Cell(
-            in_chs_left=4320, out_chs_left=864, in_chs_right=4320, out_chs_right=864, pad_type=pad_type, **dd)
+            in_chs_left=4320, out_chs_left=864, in_chs_right=4320, out_chs_right=864, pad_type=pad_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act = nn.ReLU()
         self.feature_info = [
             dict(num_chs=96, reduction=2, module='conv_0'),
@@ -376,25 +385,25 @@         ]
 
         self.global_pool, self.head_drop, self.last_linear = create_classifier(
-            self.num_features, self.num_classes, pool_type=global_pool, drop_rate=drop_rate, **dd)
-
-    @torch.jit.ignore
+            self.num_features, self.num_classes, pool_type=global_pool, drop_rate=drop_rate, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(stem=r'^conv_0|cell_stem_[01]', blocks=r'^cell_(\d+)')
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         assert not enable, 'gradient checkpointing not supported'
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.last_linear
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg', device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         self.num_classes = num_classes
         self.global_pool, self.last_linear = create_classifier(
-            self.num_features, self.num_classes, pool_type=global_pool, **dd)
+            self.num_features, self.num_classes, pool_type=global_pool, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def forward_features(self, x):
         x_conv_0 = self.conv_0(x)
@@ -420,7 +429,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.last_linear(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -433,7 +442,7 @@         pretrained,
         feature_cfg=dict(feature_cls='hook', no_rewrite=True),  # not possible to re-write this model
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 default_cfgs = generate_default_cfgs({
@@ -459,5 +468,5 @@     `"Progressive Neural Architecture Search"
     <https://arxiv.org/abs/1712.00559>`_ paper.
     """
-    model_kwargs = dict(pad_type='same', **kwargs)
-    return _create_pnasnet('pnasnet5large', pretrained, **model_kwargs)
+    model_kwargs = dict(pad_type='same', **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_pnasnet('pnasnet5large', pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
