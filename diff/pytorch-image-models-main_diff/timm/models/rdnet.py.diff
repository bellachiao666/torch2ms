--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 RDNet
 Copyright (c) 2024-present NAVER Cloud Corp.
@@ -7,8 +12,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Callable, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, calculate_drop_path_rates, NormMlpClassifierHead, ClassifierHead, EffectiveSEModule, \
@@ -21,54 +26,56 @@ __all__ = ["RDNet"]
 
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             inter_chs: int,
             out_chs: int,
-            norm_layer: Type[nn.Module],
-            act_layer: Type[nn.Module],
+            norm_layer: Type[msnn.Cell],
+            act_layer: Type[msnn.Cell],
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.layers = nn.Sequential(
+        self.layers = msnn.SequentialCell(
+            [
+            nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3, **dd),
+            norm_layer(in_chs, **dd),
+            nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0, **dd),
+            act_layer(),
+            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0, **dd)
+        ])  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
+        return self.layers(x)
+
+
+class BlockESE(msnn.Cell):
+    def __init__(
+            self,
+            in_chs: int,
+            inter_chs: int,
+            out_chs: int,
+            norm_layer: Type[msnn.Cell],
+            act_layer: Type[msnn.Cell],
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+        self.layers = msnn.SequentialCell(
+            [
             nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3, **dd),
             norm_layer(in_chs, **dd),
             nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0, **dd),
             act_layer(),
             nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0, **dd),
-        )
-
-    def forward(self, x):
-        return self.layers(x)
-
-
-class BlockESE(nn.Module):
-    def __init__(
-            self,
-            in_chs: int,
-            inter_chs: int,
-            out_chs: int,
-            norm_layer: Type[nn.Module],
-            act_layer: Type[nn.Module],
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-        self.layers = nn.Sequential(
-            nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3, **dd),
-            norm_layer(in_chs, **dd),
-            nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0, **dd),
-            act_layer(),
-            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0, **dd),
-            EffectiveSEModule(out_chs, **dd),
-        )
-
-    def forward(self, x):
+            EffectiveSEModule(out_chs, **dd)
+        ])  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         return self.layers(x)
 
 
@@ -82,7 +89,7 @@         assert False, f"Unknown block type ({block})."
 
 
-class DenseBlock(nn.Module):
+class DenseBlock(msnn.Cell):
     def __init__(
             self,
             num_input_features: int = 64,
@@ -94,8 +101,8 @@             block_idx: int = 0,
             block_type: str = "Block",
             ls_init_value: float = 1e-6,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
     ):
@@ -107,7 +114,7 @@         self.block_idx = block_idx
         self.growth_rate = growth_rate
 
-        self.gamma = nn.Parameter(ls_init_value * torch.ones(growth_rate, **dd)) if ls_init_value > 0 else None
+        self.gamma = ms.Parameter(ls_init_value * mint.ones(growth_rate, **dd)) if ls_init_value > 0 else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         growth_rate = int(growth_rate)
         inter_chs = int(num_input_features * bottleneck_width_ratio / 8) * 8
 
@@ -120,10 +127,10 @@             norm_layer=norm_layer,
             act_layer=act_layer,
             **dd,
-        )
-
-    def forward(self, x: List[torch.Tensor]) -> torch.Tensor:
-        x = torch.cat(x, 1)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: List[ms.Tensor]) -> ms.Tensor:
+        x = mint.cat(x, 1)
         x = self.layers(x)
 
         if self.gamma is not None:
@@ -133,7 +140,7 @@         return x
 
 
-class DenseStage(nn.Sequential):
+class DenseStage(msnn.SequentialCell):
     def __init__(
             self,
             num_block: int,
@@ -154,20 +161,20 @@                 block_idx=i,
                 **dd,
                 **kwargs,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             num_input_features += growth_rate
             self.add_module(f"dense_block{i}", layer)
         self.num_out_features = num_input_features
 
-    def forward(self, init_feature: torch.Tensor) -> torch.Tensor:
+    def forward(self, init_feature: ms.Tensor) -> ms.Tensor:
         features = [init_feature]
         for module in self:
             new_feature = module(features)
             features.append(new_feature)
-        return torch.cat(features, 1)
-
-
-class RDNet(nn.Module):
+        return mint.cat(features, 1)
+
+
+class RDNet(msnn.Cell):
     def __init__(
             self,
             in_chans: int = 3,  # timm option [--in-chans]
@@ -232,18 +239,20 @@         assert stem_type in ('patch', 'overlap', 'overlap_tiered')
         if stem_type == 'patch':
             # NOTE: this stem is a minimal form of ViT PatchEmbed, as used in SwinTransformer w/ patch_size = 4
-            self.stem = nn.Sequential(
+            self.stem = msnn.SequentialCell(
+                [
                 nn.Conv2d(in_chans, num_init_features, kernel_size=patch_size, stride=patch_size, bias=conv_bias, **dd),
-                norm_layer(num_init_features, **dd),
-            )
+                norm_layer(num_init_features, **dd)
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
             stem_stride = patch_size
         else:
             mid_chs = make_divisible(num_init_features // 2) if 'tiered' in stem_type else num_init_features
-            self.stem = nn.Sequential(
+            self.stem = msnn.SequentialCell(
+                [
                 nn.Conv2d(in_chans, mid_chs, kernel_size=3, stride=2, padding=1, bias=conv_bias, **dd),
                 nn.Conv2d(mid_chs, num_init_features, kernel_size=3, stride=2, padding=1, bias=conv_bias, **dd),
-                norm_layer(num_init_features, **dd),
-            )
+                norm_layer(num_init_features, **dd)
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
             stem_stride = 4
 
         # features
@@ -263,7 +272,7 @@                     curr_stride *= 2
                     k_size = stride = 2
 
-                dense_stage_layers.append(norm_layer(num_features, **dd))
+                dense_stage_layers.append(norm_layer(num_features, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
                 dense_stage_layers.append(nn.Conv2d(
                     num_features,
                     compressed_num_features,
@@ -271,7 +280,7 @@                     stride=stride,
                     padding=0,
                     **dd,
-                ))
+                ))  # 存在 *args/**kwargs，需手动确认参数映射;
                 num_features = compressed_num_features
 
             stage = DenseStage(
@@ -286,7 +295,7 @@                 norm_layer=norm_layer,
                 act_layer=act_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             dense_stage_layers.append(stage)
             num_features += num_blocks_list[i] * growth_rates[i]
 
@@ -299,23 +308,23 @@                         growth_rate=growth_rates[i],
                     )
                 ]
-            dense_stages.append(nn.Sequential(*dense_stage_layers))
-        self.dense_stages = nn.Sequential(*dense_stages)
+            dense_stages.append(msnn.SequentialCell(*dense_stage_layers))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.dense_stages = msnn.SequentialCell(*dense_stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.num_features = self.head_hidden_size = num_features
 
         # if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets
         # otherwise pool -> norm -> fc, the default RDNet ordering (pretrained NV weights)
         if head_norm_first:
-            self.norm_pre = norm_layer(self.num_features, **dd)
+            self.norm_pre = norm_layer(self.num_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.head = ClassifierHead(
                 self.num_features,
                 num_classes,
                 pool_type=global_pool,
                 drop_rate=self.drop_rate,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
             self.head = NormMlpClassifierHead(
                 self.num_features,
                 num_classes,
@@ -323,11 +332,11 @@                 drop_rate=self.drop_rate,
                 norm_layer=norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         assert not coarse, "coarse grouping is not implemented for RDNet"
         return dict(
@@ -335,13 +344,13 @@             blocks=r'^dense_stages\.(\d+)',
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         for s in self.dense_stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -350,13 +359,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -378,6 +387,7 @@         x = self.stem(x)
 
         last_idx = len(self.dense_stages) - 1
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             dense_stages = self.dense_stages
         else:
@@ -412,7 +422,7 @@         max_index = stage_ends[max_index]
         self.dense_stages = self.dense_stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
@@ -426,7 +436,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -434,12 +444,12 @@ 
 def _init_weights(module, name=None, head_init_scale=1.0):
     if isinstance(module, nn.Conv2d):
-        nn.init.kaiming_normal_(module.weight)
+        nn.init.kaiming_normal_(module.weight)  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.BatchNorm2d):
-        nn.init.constant_(module.weight, 1)
-        nn.init.constant_(module.bias, 0)
+        nn.init.constant_(module.weight, 1)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.constant_(module.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Linear):
-        nn.init.constant_(module.bias, 0)
+        nn.init.constant_(module.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if name and 'head.' in name:
             module.weight.data.mul_(head_init_scale)
             module.bias.data.mul_(head_init_scale)
@@ -466,7 +476,7 @@         RDNet, variant, pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(out_indices=(0, 1, 2, 3), flatten_sequential=True),
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -511,7 +521,7 @@         "transition_compression_ratio": 0.5,
         "block_type": ["Block"] + ["Block"] + ["BlockESE"] * 4 + ["BlockESE"],
     }
-    model = _create_rdnet("rdnet_tiny", pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_rdnet("rdnet_tiny", pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -526,7 +536,7 @@         "transition_compression_ratio": 0.5,
         "block_type": ["Block"] + ["Block"] + ["BlockESE"] * (n_layer - 4) + ["BlockESE"] * 2,
     }
-    model = _create_rdnet("rdnet_small", pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_rdnet("rdnet_small", pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -541,7 +551,7 @@         "transition_compression_ratio": 0.5,
         "block_type": ["Block"] + ["Block"] + ["BlockESE"] * (n_layer - 4) + ["BlockESE"] * 2,
     }
-    model = _create_rdnet("rdnet_base", pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_rdnet("rdnet_base", pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -556,5 +566,5 @@         "transition_compression_ratio": 0.5,
         "block_type": ["Block"] + ["Block"] + ["BlockESE"] * (n_layer - 4) + ["BlockESE"] * 2,
     }
-    model = _create_rdnet("rdnet_large", pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_rdnet("rdnet_large", pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
