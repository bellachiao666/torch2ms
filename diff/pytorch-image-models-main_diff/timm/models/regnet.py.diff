--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """RegNet X, Y, Z, and more
 
 Paper: `Designing Network Design Spaces` - https://arxiv.org/abs/2003.13678
@@ -28,8 +33,8 @@ from functools import partial
 from typing import Any, Callable, Dict, List, Optional, Union, Tuple, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ClassifierHead, AvgPool2dSame, ConvNormAct, SEModule, DropPath, GroupNormAct, calculate_drop_path_rates
@@ -128,11 +133,11 @@     # TODO dWr scaling?
     # depth = int(depth * (scale ** 0.1))
     # width_scale = scale ** 0.4  # dWr scale, exp 0.8 / 2, applied to both group and layer widths
-    widths_cont = torch.arange(depth, dtype=torch.float32) * width_slope + width_initial
-    width_exps = torch.round(torch.log(widths_cont / width_initial) / math.log(width_mult))
-    widths = torch.round((width_initial * torch.pow(width_mult, width_exps)) / quant) * quant
-    num_stages, max_stage = len(torch.unique(widths)), int(width_exps.max().item()) + 1
-    groups = torch.tensor([group_size for _ in range(num_stages)], dtype=torch.int32)
+    widths_cont = mint.arange(depth, dtype=ms.float32) * width_slope + width_initial
+    width_exps = ms.Tensor.round(mint.log(widths_cont / width_initial) / math.log(width_mult))
+    widths = ms.Tensor.round((width_initial * mint.pow(width_mult, width_exps)) / quant) * quant
+    num_stages, max_stage = len(mint.unique(widths)), int(width_exps.max().item()) + 1
+    groups = ms.Tensor([group_size for _ in range(num_stages)], dtype=ms.int32)
     return widths.int().tolist(), num_stages, groups.tolist()
 
 
@@ -142,11 +147,11 @@         kernel_size: int = 1,
         stride: int = 1,
         dilation: int = 1,
-        norm_layer: Optional[Type[nn.Module]] = None,
+        norm_layer: Optional[Type[msnn.Cell]] = None,
         preact: bool = False,
         device=None,
         dtype=None,
-) -> nn.Module:
+) -> msnn.Cell:
     """Create convolutional downsampling module.
 
     Args:
@@ -173,7 +178,7 @@             stride=stride,
             dilation=dilation,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     else:
         return ConvNormAct(
             in_chs,
@@ -184,7 +189,7 @@             norm_layer=norm_layer,
             apply_act=False,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def downsample_avg(
@@ -193,11 +198,11 @@         kernel_size: int = 1,
         stride: int = 1,
         dilation: int = 1,
-        norm_layer: Optional[Type[nn.Module]] = None,
+        norm_layer: Optional[Type[msnn.Cell]] = None,
         preact: bool = False,
         device=None,
         dtype=None,
-) -> nn.Sequential:
+) -> msnn.SequentialCell:
     """Create average pool downsampling module.
 
     AvgPool Downsampling as in 'D' ResNet variants. This is not in RegNet space but I might experiment.
@@ -217,15 +222,15 @@     dd = {'device': device, 'dtype': dtype}
     norm_layer = norm_layer or nn.BatchNorm2d
     avg_stride = stride if dilation == 1 else 1
-    pool = nn.Identity()
+    pool = msnn.Identity()
     if stride > 1 or dilation > 1:
         avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
         pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
     if preact:
-        conv = create_conv2d(in_chs, out_chs, 1, stride=1, **dd)
+        conv = create_conv2d(in_chs, out_chs, 1, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     else:
-        conv = ConvNormAct(in_chs, out_chs, 1, stride=1, norm_layer=norm_layer, apply_act=False, **dd)
-    return nn.Sequential(*[pool, conv])
+        conv = ConvNormAct(in_chs, out_chs, 1, stride=1, norm_layer=norm_layer, apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return msnn.SequentialCell(*[pool, conv])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def create_shortcut(
@@ -235,11 +240,11 @@         kernel_size: int,
         stride: int,
         dilation: Tuple[int, int] = (1, 1),
-        norm_layer: Optional[Type[nn.Module]] = None,
+        norm_layer: Optional[Type[msnn.Cell]] = None,
         preact: bool = False,
         device=None,
         dtype=None,
-) -> Optional[nn.Module]:
+) -> Optional[msnn.Cell]:
     """Create shortcut connection for residual blocks.
 
     Args:
@@ -258,18 +263,18 @@     dd = {'device': device, 'dtype': dtype}
     assert downsample_type in ('avg', 'conv1x1', '', None)
     if in_chs != out_chs or stride != 1 or dilation[0] != dilation[1]:
-        dargs = dict(stride=stride, dilation=dilation[0], norm_layer=norm_layer, preact=preact, **dd)
+        dargs = dict(stride=stride, dilation=dilation[0], norm_layer=norm_layer, preact=preact, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if not downsample_type:
             return None  # no shortcut, no downsample
         elif downsample_type == 'avg':
-            return downsample_avg(in_chs, out_chs, **dargs)
+            return downsample_avg(in_chs, out_chs, **dargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            return downsample_conv(in_chs, out_chs, kernel_size=kernel_size, **dargs)
+            return downsample_conv(in_chs, out_chs, kernel_size=kernel_size, **dargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     else:
-        return nn.Identity()  # identity shortcut (no downsample)
-
-
-class Bottleneck(nn.Module):
+        return msnn.Identity()  # identity shortcut (no downsample)
+
+
+class Bottleneck(msnn.Cell):
     """RegNet Bottleneck block.
 
     This is almost exactly the same as a ResNet Bottleneck. The main difference is the SE block is moved from
@@ -287,9 +292,9 @@             se_ratio: float = 0.25,
             downsample: str = 'conv1x1',
             linear_out: bool = False,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            drop_block: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            drop_block: Optional[Type[msnn.Cell]] = None,
             drop_path_rate: float = 0.,
             device=None,
             dtype=None,
@@ -318,7 +323,7 @@         groups = bottleneck_chs // group_size
 
         cargs = dict(act_layer=act_layer, norm_layer=norm_layer)
-        self.conv1 = ConvNormAct(in_chs, bottleneck_chs, kernel_size=1, **cargs, **dd)
+        self.conv1 = ConvNormAct(in_chs, bottleneck_chs, kernel_size=1, **cargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv2 = ConvNormAct(
             bottleneck_chs,
             bottleneck_chs,
@@ -329,14 +334,14 @@             drop_layer=drop_block,
             **cargs,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if se_ratio:
             se_channels = int(round(in_chs * se_ratio))
-            self.se = SEModule(bottleneck_chs, rd_channels=se_channels, act_layer=act_layer, **dd)
+            self.se = SEModule(bottleneck_chs, rd_channels=se_channels, act_layer=act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.se = nn.Identity()
-        self.conv3 = ConvNormAct(bottleneck_chs, out_chs, kernel_size=1, apply_act=False, **cargs, **dd)
-        self.act3 = nn.Identity() if linear_out else act_layer()
+            self.se = msnn.Identity()
+        self.conv3 = ConvNormAct(bottleneck_chs, out_chs, kernel_size=1, apply_act=False, **cargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.act3 = msnn.Identity() if linear_out else act_layer()
         self.downsample = create_shortcut(
             downsample,
             in_chs,
@@ -346,14 +351,14 @@             dilation=dilation,
             norm_layer=norm_layer,
             **dd,
-        )
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
 
     def zero_init_last(self) -> None:
         """Zero-initialize the last batch norm in the block."""
-        nn.init.zeros_(self.conv3.bn.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        nn.init.zeros_(self.conv3.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -375,7 +380,7 @@         return x
 
 
-class PreBottleneck(nn.Module):
+class PreBottleneck(msnn.Cell):
     """Pre-activation RegNet Bottleneck block.
 
     Similar to Bottleneck but with pre-activation normalization.
@@ -392,9 +397,9 @@             se_ratio: float = 0.25,
             downsample: str = 'conv1x1',
             linear_out: bool = False,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            drop_block: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            drop_block: Optional[Type[msnn.Cell]] = None,
             drop_path_rate: float = 0.,
             device=None,
             dtype=None,
@@ -422,9 +427,9 @@         bottleneck_chs = int(round(out_chs * bottle_ratio))
         groups = bottleneck_chs // group_size
 
-        self.norm1 = norm_act_layer(in_chs, **dd)
-        self.conv1 = create_conv2d(in_chs, bottleneck_chs, kernel_size=1, **dd)
-        self.norm2 = norm_act_layer(bottleneck_chs, **dd)
+        self.norm1 = norm_act_layer(in_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv1 = create_conv2d(in_chs, bottleneck_chs, kernel_size=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm2 = norm_act_layer(bottleneck_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv2 = create_conv2d(
             bottleneck_chs,
             bottleneck_chs,
@@ -433,14 +438,14 @@             dilation=dilation[0],
             groups=groups,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if se_ratio:
             se_channels = int(round(in_chs * se_ratio))
-            self.se = SEModule(bottleneck_chs, rd_channels=se_channels, act_layer=act_layer, **dd)
+            self.se = SEModule(bottleneck_chs, rd_channels=se_channels, act_layer=act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.se = nn.Identity()
-        self.norm3 = norm_act_layer(bottleneck_chs, **dd)
-        self.conv3 = create_conv2d(bottleneck_chs, out_chs, kernel_size=1, **dd)
+            self.se = msnn.Identity()
+        self.norm3 = norm_act_layer(bottleneck_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv3 = create_conv2d(bottleneck_chs, out_chs, kernel_size=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.downsample = create_shortcut(
             downsample,
             in_chs,
@@ -450,14 +455,14 @@             dilation=dilation,
             preact=True,
             **dd,
-        )
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
 
     def zero_init_last(self) -> None:
         """Zero-initialize the last batch norm (no-op for pre-activation)."""
         pass
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -481,7 +486,7 @@         return x
 
 
-class RegStage(nn.Module):
+class RegStage(msnn.Cell):
     """RegNet stage (sequence of blocks with the same output shape).
 
     A stage consists of multiple bottleneck blocks with the same output dimensions.
@@ -495,7 +500,7 @@             stride: int,
             dilation: int,
             drop_path_rates: Optional[List[float]] = None,
-            block_fn: Type[nn.Module] = Bottleneck,
+            block_fn: Type[msnn.Cell] = Bottleneck,
             **block_kwargs,
     ):
         """Initialize RegNet stage.
@@ -530,10 +535,10 @@                     drop_path_rate=dpr,
                     **block_kwargs,
                 )
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             first_dilation = dilation
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through all blocks in the stage.
 
         Args:
@@ -542,6 +547,7 @@         Returns:
             Output tensor.
         """
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.children(), x)
         else:
@@ -550,7 +556,7 @@         return x
 
 
-class RegNet(nn.Module):
+class RegNet(msnn.Cell):
     """RegNet-X, Y, and Z Models.
 
     Paper: https://arxiv.org/abs/2003.13678
@@ -589,15 +595,15 @@         self.num_classes = num_classes
         self.drop_rate = drop_rate
         assert output_stride in (8, 16, 32)
-        cfg = replace(cfg, **kwargs)  # update cfg with extra passed kwargs
+        cfg = replace(cfg, **kwargs)  # update cfg with extra passed kwargs; 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # Construct the stem
         stem_width = cfg.stem_width
         na_args = dict(act_layer=cfg.act_layer, norm_layer=cfg.norm_layer)
         if cfg.preact:
-            self.stem = create_conv2d(in_chans, stem_width, 3, stride=2, **dd)
+            self.stem = create_conv2d(in_chans, stem_width, 3, stride=2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.stem = ConvNormAct(in_chans, stem_width, 3, stride=2, **na_args, **dd)
+            self.stem = ConvNormAct(in_chans, stem_width, 3, stride=2, **na_args, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info = [dict(num_chs=stem_width, reduction=2, module='stem')]
 
         # Construct the stages
@@ -621,18 +627,18 @@                     **common_args,
                     **dd,
                 )
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_width = stage_args['out_chs']
             curr_stride *= stage_args['stride']
             self.feature_info += [dict(num_chs=prev_width, reduction=curr_stride, module=stage_name)]
 
         # Construct the head
         if cfg.num_features:
-            self.final_conv = ConvNormAct(prev_width, cfg.num_features, kernel_size=1, **na_args, **dd)
+            self.final_conv = ConvNormAct(prev_width, cfg.num_features, kernel_size=1, **na_args, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.num_features = cfg.num_features
         else:
             final_act = cfg.linear_out or cfg.preact
-            self.final_conv = get_act_layer(cfg.act_layer)() if final_act else nn.Identity()
+            self.final_conv = get_act_layer(cfg.act_layer)() if final_act else msnn.Identity()
             self.num_features = prev_width
         self.head_hidden_size = self.num_features
         self.head = ClassifierHead(
@@ -641,7 +647,7 @@             pool_type=global_pool,
             drop_rate=drop_rate,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)
 
@@ -667,7 +673,7 @@         widths, num_stages, stage_gs = generate_regnet(cfg.wa, cfg.w0, cfg.wm, cfg.depth, cfg.group_size)
 
         # Convert to per stage format
-        stage_widths, stage_depths = torch.unique(torch.tensor(widths), return_counts=True)
+        stage_widths, stage_depths = mint.unique(ms.Tensor(widths), return_counts=True)
         stage_widths, stage_depths = stage_widths.tolist(), stage_depths.tolist()
         stage_br = [cfg.bottle_ratio for _ in range(num_stages)]
         stage_strides = []
@@ -701,7 +707,7 @@         )
         return per_stage_args, common_args
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse: bool = False) -> Dict[str, Any]:
         """Group parameters for optimization."""
         return dict(
@@ -709,14 +715,14 @@             blocks=r'^s(\d+)' if coarse else r'^s(\d+)\.b(\d+)',
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True) -> None:
         """Enable or disable gradient checkpointing."""
         for s in list(self.children())[1:-1]:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         """Get the classifier head."""
         return self.head.fc
 
@@ -732,13 +738,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -798,14 +804,14 @@         layer_names = ('s1', 's2', 's3', 's4')
         layer_names = layer_names[max_index:]
         for n in layer_names:
-            setattr(self, n, nn.Identity())
+            setattr(self, n, msnn.Identity())
         if max_index < 4:
-            self.final_conv = nn.Identity()
+            self.final_conv = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers.
 
         Args:
@@ -822,7 +828,7 @@         x = self.final_conv(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -834,7 +840,7 @@         """
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -848,7 +854,7 @@         return x
 
 
-def _init_weights(module: nn.Module, name: str = '', zero_init_last: bool = False) -> None:
+def _init_weights(module: msnn.Cell, name: str = '', zero_init_last: bool = False) -> None:
     """Initialize module weights.
 
     Args:
@@ -863,9 +869,9 @@         if module.bias is not None:
             module.bias.data.zero_()
     elif isinstance(module, nn.Linear):
-        nn.init.normal_(module.weight, mean=0.0, std=0.01)
+        nn.init.normal_(module.weight, mean=0.0, std=0.01)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif zero_init_last and hasattr(module, 'zero_init_last'):
         module.zero_init_last()
 
@@ -1015,7 +1021,7 @@         RegNet, variant, pretrained,
         model_cfg=model_cfgs[variant],
         pretrained_filter_fn=_filter_fn,
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url: str = '', **kwargs) -> Dict[str, Any]:
@@ -1262,199 +1268,199 @@ @register_model
 def regnetx_002(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-200MF"""
-    return _create_regnet('regnetx_002', pretrained, **kwargs)
+    return _create_regnet('regnetx_002', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_004(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-400MF"""
-    return _create_regnet('regnetx_004', pretrained, **kwargs)
+    return _create_regnet('regnetx_004', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_004_tv(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-400MF w/ torchvision group rounding"""
-    return _create_regnet('regnetx_004_tv', pretrained, **kwargs)
+    return _create_regnet('regnetx_004_tv', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_006(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-600MF"""
-    return _create_regnet('regnetx_006', pretrained, **kwargs)
+    return _create_regnet('regnetx_006', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_008(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-800MF"""
-    return _create_regnet('regnetx_008', pretrained, **kwargs)
+    return _create_regnet('regnetx_008', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_016(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-1.6GF"""
-    return _create_regnet('regnetx_016', pretrained, **kwargs)
+    return _create_regnet('regnetx_016', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_032(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-3.2GF"""
-    return _create_regnet('regnetx_032', pretrained, **kwargs)
+    return _create_regnet('regnetx_032', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_040(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-4.0GF"""
-    return _create_regnet('regnetx_040', pretrained, **kwargs)
+    return _create_regnet('regnetx_040', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_064(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-6.4GF"""
-    return _create_regnet('regnetx_064', pretrained, **kwargs)
+    return _create_regnet('regnetx_064', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_080(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-8.0GF"""
-    return _create_regnet('regnetx_080', pretrained, **kwargs)
+    return _create_regnet('regnetx_080', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_120(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-12GF"""
-    return _create_regnet('regnetx_120', pretrained, **kwargs)
+    return _create_regnet('regnetx_120', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_160(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-16GF"""
-    return _create_regnet('regnetx_160', pretrained, **kwargs)
+    return _create_regnet('regnetx_160', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetx_320(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetX-32GF"""
-    return _create_regnet('regnetx_320', pretrained, **kwargs)
+    return _create_regnet('regnetx_320', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_002(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-200MF"""
-    return _create_regnet('regnety_002', pretrained, **kwargs)
+    return _create_regnet('regnety_002', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_004(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-400MF"""
-    return _create_regnet('regnety_004', pretrained, **kwargs)
+    return _create_regnet('regnety_004', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_006(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-600MF"""
-    return _create_regnet('regnety_006', pretrained, **kwargs)
+    return _create_regnet('regnety_006', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_008(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-800MF"""
-    return _create_regnet('regnety_008', pretrained, **kwargs)
+    return _create_regnet('regnety_008', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_008_tv(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-800MF w/ torchvision group rounding"""
-    return _create_regnet('regnety_008_tv', pretrained, **kwargs)
+    return _create_regnet('regnety_008_tv', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_016(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-1.6GF"""
-    return _create_regnet('regnety_016', pretrained, **kwargs)
+    return _create_regnet('regnety_016', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_032(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-3.2GF"""
-    return _create_regnet('regnety_032', pretrained, **kwargs)
+    return _create_regnet('regnety_032', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_040(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-4.0GF"""
-    return _create_regnet('regnety_040', pretrained, **kwargs)
+    return _create_regnet('regnety_040', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_064(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-6.4GF"""
-    return _create_regnet('regnety_064', pretrained, **kwargs)
+    return _create_regnet('regnety_064', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_080(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-8.0GF"""
-    return _create_regnet('regnety_080', pretrained, **kwargs)
+    return _create_regnet('regnety_080', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_080_tv(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-8.0GF w/ torchvision group rounding"""
-    return _create_regnet('regnety_080_tv', pretrained, **kwargs)
+    return _create_regnet('regnety_080_tv', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_120(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-12GF"""
-    return _create_regnet('regnety_120', pretrained, **kwargs)
+    return _create_regnet('regnety_120', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_160(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-16GF"""
-    return _create_regnet('regnety_160', pretrained, **kwargs)
+    return _create_regnet('regnety_160', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_320(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-32GF"""
-    return _create_regnet('regnety_320', pretrained, **kwargs)
+    return _create_regnet('regnety_320', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_640(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-64GF"""
-    return _create_regnet('regnety_640', pretrained, **kwargs)
+    return _create_regnet('regnety_640', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_1280(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-128GF"""
-    return _create_regnet('regnety_1280', pretrained, **kwargs)
+    return _create_regnet('regnety_1280', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_2560(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-256GF"""
-    return _create_regnet('regnety_2560', pretrained, **kwargs)
+    return _create_regnet('regnety_2560', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnety_040_sgn(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetY-4.0GF w/ GroupNorm """
-    return _create_regnet('regnety_040_sgn', pretrained, **kwargs)
+    return _create_regnet('regnety_040_sgn', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetv_040(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetV-4.0GF (pre-activation)"""
-    return _create_regnet('regnetv_040', pretrained, **kwargs)
+    return _create_regnet('regnetv_040', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def regnetv_064(pretrained: bool = False, **kwargs) -> RegNet:
     """RegNetV-6.4GF (pre-activation)"""
-    return _create_regnet('regnetv_064', pretrained, **kwargs)
+    return _create_regnet('regnetv_064', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1463,7 +1469,7 @@     NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py
     but it's not clear it is equivalent to paper model as not detailed in the paper.
     """
-    return _create_regnet('regnetz_005', pretrained, zero_init_last=False, **kwargs)
+    return _create_regnet('regnetz_005', pretrained, zero_init_last=False, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1472,7 +1478,7 @@     NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py
     but it's not clear it is equivalent to paper model as not detailed in the paper.
     """
-    return _create_regnet('regnetz_040', pretrained, zero_init_last=False, **kwargs)
+    return _create_regnet('regnetz_040', pretrained, zero_init_last=False, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1481,7 +1487,7 @@     NOTE: config found in https://github.com/facebookresearch/ClassyVision/blob/main/classy_vision/models/regnet.py
     but it's not clear it is equivalent to paper model as not detailed in the paper.
     """
-    return _create_regnet('regnetz_040_h', pretrained, zero_init_last=False, **kwargs)
+    return _create_regnet('regnetz_040_h', pretrained, zero_init_last=False, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 register_model_deprecations(__name__, {
