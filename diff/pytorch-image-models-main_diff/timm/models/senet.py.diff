--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 SEResNet implementation from Cadene's pretrained models
 https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py
@@ -14,10 +19,7 @@ import math
 from collections import OrderedDict
 from typing import Type, Optional, Tuple
-
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
@@ -29,23 +31,23 @@ 
 def _weight_init(m):
     if isinstance(m, nn.Conv2d):
-        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(m, nn.BatchNorm2d):
-        nn.init.constant_(m.weight, 1.)
-        nn.init.constant_(m.bias, 0.)
-
-
-class SEModule(nn.Module):
+        nn.init.constant_(m.weight, 1.)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.constant_(m.bias, 0.)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+
+class SEModule(msnn.Cell):
 
     def __init__(self, channels: int, reduction: int, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, **dd)
-        self.relu = nn.ReLU(inplace=True)
-        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, **dd)
+        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.sigmoid = nn.Sigmoid()
 
-    def forward(self, x):
+    def construct(self, x):
         module_input = x
         x = x.mean((2, 3), keepdim=True)
         x = self.fc1(x)
@@ -55,12 +57,12 @@         return module_input * x
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(msnn.Cell):
     """
     Base class for bottlenecks that implements `forward()` method.
     """
 
-    def forward(self, x):
+    def construct(self, x):
         shortcut = x
 
         out = self.conv1(x)
@@ -96,14 +98,14 @@             groups: int,
             reduction: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False, **dd)
-        self.bn1 = nn.BatchNorm2d(planes * 2, **dd)
+        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn1 = nn.BatchNorm2d(planes * 2, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.conv2 = nn.Conv2d(
             planes * 2,
             planes * 4,
@@ -113,12 +115,12 @@             groups=groups,
             bias=False,
             **dd,
-        )
-        self.bn2 = nn.BatchNorm2d(planes * 4, **dd)
-        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1, bias=False, **dd)
-        self.bn3 = nn.BatchNorm2d(planes * 4, **dd)
-        self.relu = nn.ReLU(inplace=True)
-        self.se_module = SEModule(planes * 4, reduction=reduction, **dd)
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn2 = nn.BatchNorm2d(planes * 4, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn3 = nn.BatchNorm2d(planes * 4, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.se_module = SEModule(planes * 4, reduction=reduction, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.downsample = downsample
         self.stride = stride
 
@@ -138,20 +140,20 @@             groups: int,
             reduction: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False, stride=stride, **dd)
-        self.bn1 = nn.BatchNorm2d(planes, **dd)
-        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False, **dd)
-        self.bn2 = nn.BatchNorm2d(planes, **dd)
-        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False, **dd)
-        self.bn3 = nn.BatchNorm2d(planes * 4, **dd)
-        self.relu = nn.ReLU(inplace=True)
-        self.se_module = SEModule(planes * 4, reduction=reduction, **dd)
+        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False, stride=stride, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn1 = nn.BatchNorm2d(planes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn2 = nn.BatchNorm2d(planes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn3 = nn.BatchNorm2d(planes * 4, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.se_module = SEModule(planes * 4, reduction=reduction, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.downsample = downsample
         self.stride = stride
 
@@ -169,7 +171,7 @@             groups: int,
             reduction: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             base_width: int = 4,
             device=None,
             dtype=None,
@@ -177,19 +179,19 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         width = math.floor(planes * (base_width / 64)) * groups
-        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False, stride=1, **dd)
-        self.bn1 = nn.BatchNorm2d(width, **dd)
-        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, groups=groups, bias=False, **dd)
-        self.bn2 = nn.BatchNorm2d(width, **dd)
-        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False, **dd)
-        self.bn3 = nn.BatchNorm2d(planes * 4, **dd)
-        self.relu = nn.ReLU(inplace=True)
-        self.se_module = SEModule(planes * 4, reduction=reduction, **dd)
+        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False, stride=1, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn1 = nn.BatchNorm2d(width, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, groups=groups, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn2 = nn.BatchNorm2d(width, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn3 = nn.BatchNorm2d(planes * 4, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.se_module = SEModule(planes * 4, reduction=reduction, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.downsample = downsample
         self.stride = stride
 
 
-class SEResNetBlock(nn.Module):
+class SEResNetBlock(msnn.Cell):
     expansion = 1
 
     def __init__(
@@ -199,22 +201,22 @@             groups: int,
             reduction: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, padding=1, stride=stride, bias=False, **dd)
-        self.bn1 = nn.BatchNorm2d(planes, **dd)
-        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False, **dd)
-        self.bn2 = nn.BatchNorm2d(planes, **dd)
-        self.relu = nn.ReLU(inplace=True)
-        self.se_module = SEModule(planes, reduction=reduction, **dd)
+        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, padding=1, stride=stride, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn1 = nn.BatchNorm2d(planes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, groups=groups, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn2 = nn.BatchNorm2d(planes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.se_module = SEModule(planes, reduction=reduction, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.downsample = downsample
         self.stride = stride
 
-    def forward(self, x):
+    def construct(self, x):
         shortcut = x
 
         out = self.conv1(x)
@@ -234,11 +236,11 @@         return out
 
 
-class SENet(nn.Module):
+class SENet(msnn.Cell):
 
     def __init__(
             self,
-            block: Type[nn.Module],
+            block: Type[msnn.Cell],
             layers: Tuple[int, ...],
             groups: int,
             reduction: int,
@@ -305,23 +307,25 @@             layer0_modules = [
                 ('conv1', nn.Conv2d(in_chans, 64, 3, stride=2, padding=1, bias=False, **dd)),
                 ('bn1', nn.BatchNorm2d(64, **dd)),
-                ('relu1', nn.ReLU(inplace=True)),
+                ('relu1', nn.ReLU()),
                 ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=False, **dd)),
                 ('bn2', nn.BatchNorm2d(64, **dd)),
-                ('relu2', nn.ReLU(inplace=True)),
+                ('relu2', nn.ReLU()),
                 ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1, bias=False, **dd)),
                 ('bn3', nn.BatchNorm2d(inplanes, **dd)),
-                ('relu3', nn.ReLU(inplace=True)),
-            ]
+                ('relu3', nn.ReLU()),
+            ]  # 存在 *args/**kwargs，需手动确认参数映射;; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         else:
             layer0_modules = [
                 ('conv1', nn.Conv2d(in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False, **dd)),
                 ('bn1', nn.BatchNorm2d(inplanes, **dd)),
-                ('relu1', nn.ReLU(inplace=True)),
-            ]
-        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))
+                ('relu1', nn.ReLU()),
+            ]  # 存在 *args/**kwargs，需手动确认参数映射;; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.layer0 = msnn.SequentialCell([
+            OrderedDict(layer0_modules)
+        ])
         # To preserve compatibility with Caffe weights `ceil_mode=True` is used instead of `padding=1`.
-        self.pool0 = nn.MaxPool2d(3, stride=2, ceil_mode=True)
+        self.pool0 = nn.MaxPool2d(3, stride = 2, ceil_mode = True)
         self.feature_info = [dict(num_chs=inplanes, reduction=2, module='layer0')]
         self.layer1 = self._make_layer(
             block,
@@ -332,7 +336,7 @@             downsample_kernel_size=1,
             downsample_padding=0,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info += [dict(num_chs=64 * block.expansion, reduction=4, module='layer1')]
         self.layer2 = self._make_layer(
             block,
@@ -344,7 +348,7 @@             downsample_kernel_size=downsample_kernel_size,
             downsample_padding=downsample_padding,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info += [dict(num_chs=128 * block.expansion, reduction=8, module='layer2')]
         self.layer3 = self._make_layer(
             block,
@@ -356,7 +360,7 @@             downsample_kernel_size=downsample_kernel_size,
             downsample_padding=downsample_padding,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info += [dict(num_chs=256 * block.expansion, reduction=16, module='layer3')]
         self.layer4 = self._make_layer(
             block,
@@ -368,7 +372,7 @@             downsample_kernel_size=downsample_kernel_size,
             downsample_padding=downsample_padding,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info += [dict(num_chs=512 * block.expansion, reduction=32, module='layer4')]
         self.num_features = self.head_hidden_size = 512 * block.expansion
         self.global_pool, self.last_linear = create_classifier(
@@ -376,7 +380,7 @@             self.num_classes,
             pool_type=global_pool,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         for m in self.modules():
             _weight_init(m)
@@ -386,31 +390,32 @@         dd = {'device': device, 'dtype': dtype}
         downsample = None
         if stride != 1 or self.inplanes != planes * block.expansion:
-            downsample = nn.Sequential(
+            downsample = msnn.SequentialCell(
+                [
                 nn.Conv2d(
                     self.inplanes, planes * block.expansion, kernel_size=downsample_kernel_size,
                     stride=stride, padding=downsample_padding, bias=False, **dd),
-                nn.BatchNorm2d(planes * block.expansion, **dd),
-            )
-
-        layers = [block(self.inplanes, planes, groups, reduction, stride, downsample, **dd)]
+                nn.BatchNorm2d(planes * block.expansion, **dd)
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;
+
+        layers = [block(self.inplanes, planes, groups, reduction, stride, downsample, **dd)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.inplanes = planes * block.expansion
         for i in range(1, blocks):
-            layers.append(block(self.inplanes, planes, groups, reduction, **dd))
-
-        return nn.Sequential(*layers)
-
-    @torch.jit.ignore
+            layers.append(block(self.inplanes, planes, groups, reduction, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        return msnn.SequentialCell(*layers)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(stem=r'^layer0', blocks=r'^layer(\d+)' if coarse else r'^layer(\d+)\.(\d+)')
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         assert not enable, 'gradient checkpointing not supported'
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.last_linear
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -430,17 +435,17 @@     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         if self.drop_rate > 0.:
-            x = F.dropout(x, p=self.drop_rate, training=self.training)
+            x = nn.functional.dropout(x, p = self.drop_rate, training = self.training)
         return x if pre_logits else self.last_linear(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
 def _create_senet(variant, pretrained=False, **kwargs):
-    return build_model_with_cfg(SENet, variant, pretrained, **kwargs)
+    return build_model_with_cfg(SENet, variant, pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url='', **kwargs):
@@ -481,35 +486,35 @@ def legacy_seresnet18(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNetBlock, layers=[2, 2, 2, 2], groups=1, reduction=16)
-    return _create_senet('legacy_seresnet18', pretrained, **dict(model_args, **kwargs))
+    return _create_senet('legacy_seresnet18', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def legacy_seresnet34(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNetBlock, layers=[3, 4, 6, 3], groups=1, reduction=16)
-    return _create_senet('legacy_seresnet34', pretrained, **dict(model_args, **kwargs))
+    return _create_senet('legacy_seresnet34', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def legacy_seresnet50(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNetBottleneck, layers=[3, 4, 6, 3], groups=1, reduction=16)
-    return _create_senet('legacy_seresnet50', pretrained, **dict(model_args, **kwargs))
+    return _create_senet('legacy_seresnet50', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def legacy_seresnet101(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNetBottleneck, layers=[3, 4, 23, 3], groups=1, reduction=16)
-    return _create_senet('legacy_seresnet101', pretrained, **dict(model_args, **kwargs))
+    return _create_senet('legacy_seresnet101', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def legacy_seresnet152(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNetBottleneck, layers=[3, 8, 36, 3], groups=1, reduction=16)
-    return _create_senet('legacy_seresnet152', pretrained, **dict(model_args, **kwargs))
+    return _create_senet('legacy_seresnet152', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -517,25 +522,25 @@     model_args = dict(
         block=SEBottleneck, layers=[3, 8, 36, 3], groups=64, reduction=16,
         downsample_kernel_size=3, downsample_padding=1,  inplanes=128, input_3x3=True)
-    return _create_senet('legacy_senet154', pretrained, **dict(model_args, **kwargs))
+    return _create_senet('legacy_senet154', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def legacy_seresnext26_32x4d(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNeXtBottleneck, layers=[2, 2, 2, 2], groups=32, reduction=16)
-    return _create_senet('legacy_seresnext26_32x4d', pretrained, **dict(model_args, **kwargs))
+    return _create_senet('legacy_seresnext26_32x4d', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def legacy_seresnext50_32x4d(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNeXtBottleneck, layers=[3, 4, 6, 3], groups=32, reduction=16)
-    return _create_senet('legacy_seresnext50_32x4d', pretrained, **dict(model_args, **kwargs))
+    return _create_senet('legacy_seresnext50_32x4d', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def legacy_seresnext101_32x4d(pretrained=False, **kwargs) -> SENet:
     model_args = dict(
         block=SEResNeXtBottleneck, layers=[3, 4, 23, 3], groups=32, reduction=16)
-    return _create_senet('legacy_seresnext101_32x4d', pretrained, **dict(model_args, **kwargs))
+    return _create_senet('legacy_seresnext101_32x4d', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
