--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Swin Transformer V2
 
 A PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`
@@ -31,9 +36,8 @@ import math
 from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, calculate_drop_path_rates, Mlp, ClassifierHead, to_2tuple, _assert, ndgrid
@@ -48,17 +52,17 @@ _logger = logging.getLogger(__name__)
 
 
-def bchw_to_bhwc(x: torch.Tensor) -> torch.Tensor:
+def bchw_to_bhwc(x: ms.Tensor) -> ms.Tensor:
     """Permutes a tensor from the shape (B, C, H, W) to (B, H, W, C)."""
     return x.permute(0, 2, 3, 1)
 
 
-def bhwc_to_bchw(x: torch.Tensor) -> torch.Tensor:
+def bhwc_to_bchw(x: ms.Tensor) -> ms.Tensor:
     """Permutes a tensor from the shape (B, H, W, C) to (B, C, H, W)."""
     return x.permute(0, 3, 1, 2)
 
 
-def window_partition(x: torch.Tensor, window_size: Tuple[int, int]) -> torch.Tensor:
+def window_partition(x: ms.Tensor, window_size: Tuple[int, int]) -> ms.Tensor:
     """Partition into non-overlapping windows.
 
     Args:
@@ -75,7 +79,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def window_reverse(windows: torch.Tensor, window_size: Tuple[int, int], img_size: Tuple[int, int]) -> torch.Tensor:
+def window_reverse(windows: ms.Tensor, window_size: Tuple[int, int], img_size: Tuple[int, int]) -> ms.Tensor:
     """Merge windows back to feature map.
 
     Args:
@@ -93,7 +97,7 @@     return x
 
 
-class WindowMultiHeadAttention(nn.Module):
+class WindowMultiHeadAttention(msnn.Cell):
     r"""This class implements window-based Multi-Head-Attention with log-spaced continuous position bias.
 
     Args:
@@ -127,9 +131,9 @@         self.num_heads: int = num_heads
         self.sequential_attn: bool = sequential_attn
 
-        self.qkv = nn.Linear(in_features=dim, out_features=dim * 3, bias=True, **dd)
+        self.qkv = nn.Linear(in_features=dim, out_features=dim * 3, bias=True, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn_drop = nn.Dropout(drop_attn)
-        self.proj = nn.Linear(in_features=dim, out_features=dim, bias=True, **dd)
+        self.proj = nn.Linear(in_features=dim, out_features=dim, bias=True, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.proj_drop = nn.Dropout(drop_proj)
         # meta network for positional encodings
         self.meta_mlp = Mlp(
@@ -139,21 +143,21 @@             act_layer=nn.ReLU,
             drop=(0.125, 0.),  # FIXME should there be stochasticity, appears to 'overfit' without?
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         # NOTE old checkpoints used inverse of logit_scale ('tau') following the paper, see conversion fn
-        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones(num_heads, **dd)))
+        self.logit_scale = ms.Parameter(mint.log(10 * mint.ones(num_heads, **dd)))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self._make_pair_wise_relative_positions()
 
     def _make_pair_wise_relative_positions(self) -> None:
         """Initialize the pair-wise relative positions to compute the positional biases."""
         device = self.logit_scale.device
-        coordinates = torch.stack(ndgrid(
-            torch.arange(self.window_size[0], device=device, dtype=torch.float32),
-            torch.arange(self.window_size[1], device=device, dtype=torch.float32),
+        coordinates = mint.stack(ndgrid(
+            mint.arange(self.window_size[0], device=device, dtype=ms.float32),
+            mint.arange(self.window_size[1], device=device, dtype=ms.float32),
         )).flatten(1)
         relative_coordinates = coordinates[:, :, None] - coordinates[:, None, :]
         relative_coordinates = relative_coordinates.permute(1, 2, 0).reshape(-1, 2).float()
-        relative_coordinates_log = torch.sign(relative_coordinates) * torch.log(
+        relative_coordinates_log = mint.sign(relative_coordinates) * mint.log(
             1.0 + relative_coordinates.abs())
         self.register_buffer(
             "relative_coordinates_log",
@@ -172,7 +176,7 @@             self.window_size = window_size
             self._make_pair_wise_relative_positions()
 
-    def _relative_positional_encodings(self) -> torch.Tensor:
+    def _relative_positional_encodings(self) -> ms.Tensor:
         """Compute the relative positional encodings.
 
         Returns:
@@ -186,7 +190,7 @@         relative_position_bias = relative_position_bias.unsqueeze(0)
         return relative_position_bias
 
-    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, mask: Optional[ms.Tensor] = None) -> ms.Tensor:
         """Forward pass of window multi-head self-attention.
 
         Args:
@@ -202,8 +206,8 @@         query, key, value = qkv.unbind(0)
 
         # compute attention map with scaled cosine attention
-        attn = (F.normalize(query, dim=-1) @ F.normalize(key, dim=-1).transpose(-2, -1))
-        logit_scale = torch.clamp(self.logit_scale.reshape(1, self.num_heads, 1, 1), max=math.log(1. / 0.01)).exp()
+        attn = (nn.functional.normalize(query, dim = -1) @ mint.transpose(-2, -1))
+        logit_scale = mint.clamp(self.logit_scale.reshape(1, self.num_heads, 1, 1), max=math.log(1. / 0.01)).exp()
         attn = attn * logit_scale
         attn = attn + self._relative_positional_encodings()
 
@@ -222,7 +226,7 @@         return x
 
 
-class SwinTransformerV2CrBlock(nn.Module):
+class SwinTransformerV2CrBlock(msnn.Cell):
     r"""This class implements the Swin transformer block.
 
     Args:
@@ -256,7 +260,7 @@             drop_path: float = 0.0,
             extra_norm: bool = False,
             sequential_attn: bool = False,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
             device=None,
             dtype=None,
     ):
@@ -280,9 +284,9 @@             drop_proj=proj_drop,
             sequential_attn=sequential_attn,
             **dd,
-        )
-        self.norm1 = norm_layer(dim, **dd)
-        self.drop_path1 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else msnn.Identity()
 
         # mlp branch
         self.mlp = Mlp(
@@ -291,19 +295,19 @@             drop=proj_drop,
             out_features=dim,
             **dd,
-        )
-        self.norm2 = norm_layer(dim, **dd)
-        self.drop_path2 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else msnn.Identity()
 
         # Extra main branch norm layer mentioned for Huge/Giant models in V2 paper.
         # Also being used as final network norm and optional stage ending norm while still in a C-last format.
-        self.norm3 = norm_layer(dim, **dd) if extra_norm else nn.Identity()
+        self.norm3 = norm_layer(dim, **dd) if extra_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.register_buffer(
             "attn_mask",
             None if self.dynamic_mask else self.get_attn_mask(**dd),
             persistent=False,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.init_weights()
 
     def _calc_window_shift(
@@ -323,20 +327,22 @@         shift_size = [0 if f <= w else s for f, w, s in zip(self.feat_size, window_size, target_shift_size)]
         return tuple(window_size), tuple(shift_size)
 
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def get_attn_mask(
             self,
-            x: Optional[torch.Tensor] = None,
+            x: Optional[ms.Tensor] = None,
             device: Optional[torch.device] = None,
             dtype: Optional[torch.dtype] = None,
-    ) -> Optional[torch.Tensor]:
+    ) -> Optional[ms.Tensor]:
         """Method generates the attention mask used in shift case."""
         # Make masks for shift case
         if any(self.shift_size):
             # calculate attention mask for SW-MSA
             if x is None:
-                img_mask = torch.zeros((1, *self.feat_size, 1), device=device, dtype=dtype)  # 1 H W 1
+                img_mask = mint.zeros((1, *self.feat_size, 1), device=device, dtype=dtype)  # 1 H W 1
             else:
-                img_mask = torch.zeros((1, x.shape[1], x.shape[2], 1), device=x.device, dtype=x.dtype)  # 1 H W 1
+                img_mask = mint.zeros((1, x.shape[1], x.shape[2], 1), device=x.device, dtype=x.dtype)  # 1 H W 1
             cnt = 0
             for h in (
                     (0, -self.window_size[0]),
@@ -361,8 +367,8 @@     def init_weights(self):
         # extra, module specific weight init
         if self.init_values is not None:
-            nn.init.constant_(self.norm1.weight, self.init_values)
-            nn.init.constant_(self.norm2.weight, self.init_values)
+            nn.init.constant_(self.norm1.weight, self.init_values)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(self.norm2.weight, self.init_values)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def set_input_size(self, feat_size: Tuple[int, int], window_size: Tuple[int, int]) -> None:
         """Method updates the image resolution to be processed and window size and so the pair-wise relative positions.
@@ -394,11 +400,11 @@             # FIXME PyTorch XLA needs cat impl, roll not lowered
             # x = torch.cat([x[:, sh:], x[:, :sh]], dim=1)
             # x = torch.cat([x[:, :, sw:], x[:, :, :sw]], dim=2)
-            x = torch.roll(x, shifts=(-sh, -sw), dims=(1, 2))
+            x = mint.roll(x, shifts=(-sh, -sw), dims=(1, 2))
 
         pad_h = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]
         pad_w = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]
-        x = torch.nn.functional.pad(x, (0, 0, 0, pad_w, 0, pad_h))
+        x = nn.functional.pad(x, (0, 0, 0, pad_w, 0, pad_h))
         _, Hp, Wp, _ = x.shape
 
         # partition windows
@@ -422,11 +428,11 @@             # FIXME PyTorch XLA needs cat impl, roll not lowered
             # x = torch.cat([x[:, -sh:], x[:, :-sh]], dim=1)
             # x = torch.cat([x[:, :, -sw:], x[:, :, :-sw]], dim=2)
-            x = torch.roll(x, shifts=(sh, sw), dims=(1, 2))
+            x = mint.roll(x, shifts=(sh, sw), dims=(1, 2))
 
         return x
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass of Swin Transformer V2 block.
 
         Args:
@@ -446,7 +452,7 @@         return x
 
 
-class PatchMerging(nn.Module):
+class PatchMerging(msnn.Cell):
     """Patch merging layer.
 
     This class implements the patch merging as a strided convolution with a normalization before.
@@ -455,7 +461,7 @@     def __init__(
             self,
             dim: int,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
             device=None,
             dtype=None,
     ) -> None:
@@ -467,10 +473,10 @@         """
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.norm = norm_layer(4 * dim, **dd)
-        self.reduction = nn.Linear(in_features=4 * dim, out_features=2 * dim, bias=False, **dd)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.norm = norm_layer(4 * dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.reduction = nn.Linear(in_features=4 * dim, out_features=2 * dim, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass of patch merging.
 
         Args:
@@ -491,7 +497,7 @@         return x
 
 
-class PatchEmbed(nn.Module):
+class PatchEmbed(msnn.Cell):
     """2D Image to Patch Embedding."""
     def __init__(
             self,
@@ -499,7 +505,7 @@             patch_size: Union[int, Tuple[int, int]] = 16,
             in_chans: int = 3,
             embed_dim: int = 768,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             strict_img_size: bool = True,
             device=None,
             dtype=None,
@@ -524,8 +530,8 @@         self.num_patches = self.grid_size[0] * self.grid_size[1]
         self.strict_img_size = strict_img_size
 
-        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, **dd)
-        self.norm = norm_layer(embed_dim, **dd) if norm_layer else nn.Identity()
+        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm = norm_layer(embed_dim, **dd) if norm_layer else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def set_input_size(self, img_size: Tuple[int, int]) -> None:
         """Update input image size.
@@ -539,7 +545,7 @@             self.grid_size = (img_size[0] // self.patch_size[0], img_size[1] // self.patch_size[1])
             self.num_patches = self.grid_size[0] * self.grid_size[1]
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass of patch embedding.
 
         Args:
@@ -557,7 +563,7 @@         return x
 
 
-class SwinTransformerV2CrStage(nn.Module):
+class SwinTransformerV2CrStage(msnn.Cell):
     r"""This class implements a stage of the Swin transformer including multiple layers.
 
     Args:
@@ -592,7 +598,7 @@             proj_drop: float = 0.0,
             drop_attn: float = 0.0,
             drop_path: Union[List[float], float] = 0.0,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
             extra_norm_period: int = 0,
             extra_norm_stage: bool = False,
             sequential_attn: bool = False,
@@ -606,10 +612,10 @@         self.feat_size: Tuple[int, int] = (feat_size[0] // 2, feat_size[1] // 2) if downscale else feat_size
 
         if downscale:
-            self.downsample = PatchMerging(embed_dim, norm_layer=norm_layer, **dd)
+            self.downsample = PatchMerging(embed_dim, norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             embed_dim = embed_dim * 2
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         def _extra_norm(index):
             i = index + 1
@@ -617,7 +623,7 @@                 return True
             return i == depth if extra_norm_stage else False
 
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell(*[
             SwinTransformerV2CrBlock(
                 dim=embed_dim,
                 num_heads=num_heads,
@@ -637,7 +643,7 @@                 **dd,
             )
             for index in range(depth)]
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def set_input_size(
             self,
@@ -658,7 +664,7 @@                 window_size=window_size,
             )
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
         Args:
             x (torch.Tensor): Input tensor of the shape [B, C, H, W] or [B, L, C]
@@ -669,6 +675,7 @@         x = self.downsample(x)
         for block in self.blocks:
             # Perform checkpointing if utilized
+            # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if self.grad_checkpointing and not torch.jit.is_scripting():
                 x = checkpoint(block, x)
             else:
@@ -677,7 +684,7 @@         return x
 
 
-class SwinTransformerV2Cr(nn.Module):
+class SwinTransformerV2Cr(msnn.Cell):
     r""" Swin Transformer V2
         A PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`  -
           https://arxiv.org/pdf/2111.09883
@@ -722,7 +729,7 @@             proj_drop_rate: float = 0.0,
             attn_drop_rate: float = 0.0,
             drop_path_rate: float = 0.0,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
             extra_norm_period: int = 0,
             extra_norm_stage: bool = False,
             sequential_attn: bool = False,
@@ -749,7 +756,7 @@             norm_layer=norm_layer,
             strict_img_size=strict_img_size,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         grid_size = self.patch_embed.grid_size
         if window_size is None:
             self.window_size = tuple([s // window_ratio for s in grid_size])
@@ -780,12 +787,12 @@                 sequential_attn=sequential_attn,
                 norm_layer=norm_layer,
                 **dd,
-            )]
+            )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             if stage_idx != 0:
                 in_dim *= 2
                 in_scale *= 2
             self.feature_info += [dict(num_chs=in_dim, reduction=4 * in_scale, module=f'stages.{stage_idx}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.head = ClassifierHead(
             self.num_features,
@@ -793,7 +800,7 @@             pool_type=global_pool,
             drop_rate=drop_rate,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # current weight init skips custom init and uses pytorch layer defaults, seems to work well
         # FIXME more experiments needed
@@ -830,7 +837,7 @@                 always_partition=always_partition,
             )
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^patch_embed',  # stem and embed
@@ -840,13 +847,13 @@             ]
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         for s in self.stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore()
-    def get_classifier(self) -> nn.Module:
+    @ms.jit()
+    def get_classifier(self) -> msnn.Cell:
         """Method returns the classification head of the model.
         Returns:
             head (nn.Module): Current classification head
@@ -865,13 +872,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -891,6 +898,7 @@         # forward pass
         x = self.patch_embed(x)
 
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             stages = self.stages
         else:
@@ -919,7 +927,7 @@             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         x = self.patch_embed(x)
         x = self.stages(x)
         return x
@@ -927,25 +935,25 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-def init_weights(module: nn.Module, name: str = ''):
+def init_weights(module: msnn.Cell, name: str = ''):
     # FIXME WIP determining if there's a better weight init
     if isinstance(module, nn.Linear):
         if 'qkv' in name:
             # treat the weights of Q, K, V separately
             val = math.sqrt(6. / float(module.weight.shape[0] // 3 + module.weight.shape[1]))
-            nn.init.uniform_(module.weight, -val, val)
+            nn.init.uniform_(module.weight, -val, val)  # 'torch.nn.init.uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif 'head' in name:
-            nn.init.zeros_(module.weight)
+            nn.init.zeros_(module.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
-            nn.init.xavier_uniform_(module.weight)
+            nn.init.xavier_uniform_(module.weight)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
@@ -960,7 +968,7 @@     for k, v in state_dict.items():
         if 'tau' in k:
             # convert old tau based checkpoints -> logit_scale (inverse)
-            v = torch.log(1 / v)
+            v = mint.log(1 / v)
             k = k.replace('tau', 'logit_scale')
         k = k.replace('head.', 'head.fc.')
         out_dict[k] = v
@@ -976,7 +984,7 @@         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         **kwargs
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1057,7 +1065,7 @@         depths=(2, 2, 6, 2),
         num_heads=(3, 6, 12, 24),
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1068,7 +1076,7 @@         depths=(2, 2, 6, 2),
         num_heads=(3, 6, 12, 24),
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1083,7 +1091,7 @@         num_heads=(3, 6, 12, 24),
         extra_norm_stage=True,
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_tiny_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1094,7 +1102,7 @@         depths=(2, 2, 18, 2),
         num_heads=(3, 6, 12, 24),
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_small_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_small_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1105,7 +1113,7 @@         depths=(2, 2, 18, 2),
         num_heads=(3, 6, 12, 24),
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_small_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_small_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1117,7 +1125,7 @@         num_heads=(3, 6, 12, 24),
         extra_norm_stage=True,
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_small_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_small_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1129,7 +1137,7 @@         num_heads=(3, 6, 12, 24),
         extra_norm_stage=True,
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_small_ns_256', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_small_ns_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1140,7 +1148,7 @@         depths=(2, 2, 18, 2),
         num_heads=(4, 8, 16, 32),
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_base_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_base_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1151,7 +1159,7 @@         depths=(2, 2, 18, 2),
         num_heads=(4, 8, 16, 32),
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_base_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_base_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1163,7 +1171,7 @@         num_heads=(4, 8, 16, 32),
         extra_norm_stage=True,
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_base_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_base_ns_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1174,7 +1182,7 @@         depths=(2, 2, 18, 2),
         num_heads=(6, 12, 24, 48),
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_large_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_large_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1185,7 +1193,7 @@         depths=(2, 2, 18, 2),
         num_heads=(6, 12, 24, 48),
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_large_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_large_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1197,7 +1205,7 @@         num_heads=(11, 22, 44, 88),  # head count not certain for Huge, 384 & 224 trying diff values
         extra_norm_period=6,
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_huge_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_huge_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1209,7 +1217,7 @@         num_heads=(8, 16, 32, 64),  # head count not certain for Huge, 384 & 224 trying diff values
         extra_norm_period=6,
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_huge_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_huge_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1221,7 +1229,7 @@         num_heads=(16, 32, 64, 128),
         extra_norm_period=6,
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_giant_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_giant_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1233,4 +1241,4 @@         num_heads=(16, 32, 64, 128),
         extra_norm_period=6,
     )
-    return _create_swin_transformer_v2_cr('swinv2_cr_giant_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_swin_transformer_v2_cr('swinv2_cr_giant_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
