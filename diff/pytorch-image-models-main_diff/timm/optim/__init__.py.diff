--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from .adabelief import AdaBelief
 from .adafactor import Adafactor
 from .adafactor_bv import AdafactorBigVision
@@ -21,12 +26,8 @@ from .rmsprop_tf import RMSpropTF
 from .sgdp import SGDP
 from .sgdw import SGDW
-
-# bring common torch.optim Optimizers into timm.optim namespace for consistency
-from torch.optim import Adadelta, Adagrad, Adamax, Adam, AdamW, RMSprop, SGD
 try:
-    # in case any very old torch versions being used
-    from torch.optim import NAdam, RAdam
+    pass
 except ImportError:
     pass
 
