--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Adafactor Optimizer
 
 Lifted from https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py
@@ -13,11 +18,12 @@ import math
 from typing import Optional, Tuple
 
-import torch
+# import torch
 
 from ._types import ParamsT
 
 
+# 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class Adafactor(torch.optim.Optimizer):
     """Implements Adafactor algorithm.
 
@@ -122,8 +128,10 @@         # from our dim heuristic, always dim_col < dim_row, so col reduction dim for factored row = dim_col
         r_factor = (exp_avg_sq_row / exp_avg_sq_row.mean(dim=dim_col, keepdim=True)).rsqrt_().unsqueeze(dim_row)
         c_factor = exp_avg_sq_col.unsqueeze(dim_col).rsqrt()
-        return torch.mul(r_factor, c_factor)
-
+        return mint.mul(r_factor, c_factor)
+
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 装饰器 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.no_grad()
     def step(self, closure=None):
         """Performs a single optimization step.
@@ -132,6 +140,7 @@         """
         loss = None
         if closure is not None:
+            # 'torch.enable_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             with torch.enable_grad():
                 loss = closure()
 
@@ -140,7 +149,7 @@                 if p.grad is None:
                     continue
                 grad = p.grad
-                if grad.dtype in {torch.float16, torch.bfloat16}:
+                if grad.dtype in {ms.float16, ms.bfloat16}:
                     grad = grad.float()
                 if grad.is_sparse:
                     raise RuntimeError('Adafactor does not support sparse gradients.')
@@ -158,15 +167,15 @@ 
                     if use_first_moment:
                         # Exponential moving average of gradient values
-                        state['exp_avg'] = torch.zeros_like(grad)
+                        state['exp_avg'] = mint.zeros_like(grad)
                     if factored_dims is not None:
                         dim_col, dim_row = factored_dims
                         def _remove_dim(shape, dim):
                             return shape[:dim] + shape[dim + 1:]
-                        state['exp_avg_sq_row'] = torch.zeros(_remove_dim(grad.shape, dim_row)).to(grad)
-                        state['exp_avg_sq_col'] = torch.zeros(_remove_dim(grad.shape, dim_col)).to(grad)
+                        state['exp_avg_sq_row'] = mint.zeros(_remove_dim(grad.shape, dim_row)).to(grad)
+                        state['exp_avg_sq_col'] = mint.zeros(_remove_dim(grad.shape, dim_col)).to(grad)
                     else:
-                        state['exp_avg_sq'] = torch.zeros_like(grad)
+                        state['exp_avg_sq'] = mint.zeros_like(grad)
 
                     state['RMS'] = 0
                 else:
@@ -179,7 +188,7 @@                         state['exp_avg_sq'] = state['exp_avg_sq'].to(grad)
 
                 p_fp32 = p
-                if p.dtype in {torch.float16, torch.bfloat16}:
+                if p.dtype in {ms.float16, ms.bfloat16}:
                     p_fp32 = p_fp32.float()
 
                 state['step'] += 1
@@ -223,7 +232,7 @@                     p_fp32.add_(p_fp32, alpha=-group['weight_decay'] * lr_t)
 
                 p_fp32.add_(-update)
-                if p.dtype in {torch.float16, torch.bfloat16}:
+                if p.dtype in {ms.float16, ms.bfloat16}:
                     p.copy_(p_fp32)
 
         return loss
