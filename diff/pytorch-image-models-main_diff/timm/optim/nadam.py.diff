--- pytorch+++ mindspore@@ -1,9 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import math
 
-import torch
-from torch.optim.optimizer import Optimizer
+# import torch
+# from torch.optim.optimizer import Optimizer
 
 
+# 'torch.optim.optimizer.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class NAdamLegacy(Optimizer):
     """Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).
 
@@ -49,6 +55,8 @@         )
         super(NAdamLegacy, self).__init__(params, defaults)
 
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 装饰器 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.no_grad()
     def step(self, closure=None):
         """Performs a single optimization step.
@@ -59,6 +67,7 @@         """
         loss = None
         if closure is not None:
+            # 'torch.enable_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             with torch.enable_grad():
                 loss = closure()
 
@@ -73,8 +82,8 @@                 if len(state) == 0:
                     state['step'] = 0
                     state['m_schedule'] = 1.
-                    state['exp_avg'] = torch.zeros_like(p)
-                    state['exp_avg_sq'] = torch.zeros_like(p)
+                    state['exp_avg'] = mint.zeros_like(p)
+                    state['exp_avg_sq'] = mint.zeros_like(p)
 
                 # Warming momentum schedule
                 m_schedule = state['m_schedule']
