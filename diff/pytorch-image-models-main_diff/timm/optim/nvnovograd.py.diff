--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Nvidia NovoGrad Optimizer.
 Original impl by Nvidia from Jasper example:
     - https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechRecognition/Jasper
@@ -5,11 +10,12 @@     - https://arxiv.org/abs/1905.11286
 """
 
-import torch
-from torch.optim.optimizer import Optimizer
+# import torch
+# from torch.optim.optimizer import Optimizer
 import math
 
 
+# 'torch.optim.optimizer.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class NvNovoGrad(Optimizer):
     """
     Implements Novograd algorithm.
@@ -91,12 +97,12 @@                 if len(state) == 0:
                     state['step'] = 0
                     # Exponential moving average of gradient values
-                    state['exp_avg'] = torch.zeros_like(p)
+                    state['exp_avg'] = mint.zeros_like(p)
                     # Exponential moving average of squared gradient values
-                    state['exp_avg_sq'] = torch.zeros([]).to(state['exp_avg'].device)
+                    state['exp_avg_sq'] = mint.zeros([]).to(state['exp_avg'].device)
                     if amsgrad:
                         # Maintains max of all exp. moving avg. of sq. grad. values
-                        state['max_exp_avg_sq'] = torch.zeros([]).to(state['exp_avg'].device)
+                        state['max_exp_avg_sq'] = mint.zeros([]).to(state['exp_avg'].device)
 
                 exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                 if amsgrad:
@@ -105,7 +111,7 @@ 
                 state['step'] += 1
 
-                norm = torch.sum(torch.pow(grad, 2))
+                norm = mint.sum(mint.pow(grad, 2))
 
                 if exp_avg_sq == 0:
                     exp_avg_sq.copy_(norm)
@@ -114,7 +120,7 @@ 
                 if amsgrad:
                     # Maintains the maximum of all 2nd moment running avg. till now
-                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)
+                    mint.max(max_exp_avg_sq, exp_avg_sq)
                     # Use the max. for normalizing running avg. of gradient
                     denom = max_exp_avg_sq.sqrt().add_(group['eps'])
                 else:
