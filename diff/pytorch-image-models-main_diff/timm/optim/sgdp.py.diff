--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 SGDP Optimizer Implementation copied from https://github.com/clovaai/AdamP/blob/master/adamp/sgdp.py
 
@@ -8,14 +13,14 @@ MIT license
 """
 
-import torch
-import torch.nn.functional as F
-from torch.optim.optimizer import Optimizer, required
+# import torch
+# from torch.optim.optimizer import Optimizer, required
 import math
 
 from .adamp import projection
 
 
+# 'torch.optim.optimizer.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class SGDP(Optimizer):
     def __init__(
             self,
@@ -41,10 +46,13 @@         )
         super(SGDP, self).__init__(params, defaults)
 
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 装饰器 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.no_grad()
     def step(self, closure=None):
         loss = None
         if closure is not None:
+            # 'torch.enable_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             with torch.enable_grad():
                 loss = closure()
 
@@ -62,7 +70,7 @@ 
                 # State initialization
                 if len(state) == 0:
-                    state['momentum'] = torch.zeros_like(p)
+                    state['momentum'] = mint.zeros_like(p)
 
                 # SGD
                 buf = state['momentum']
