--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Checkpoint Saver
 
 Track top-n training checkpoints and maintain recovery checkpoints on specified intervals.
@@ -11,7 +16,7 @@ import os
 import shutil
 
-import torch
+# import torch
 
 from .model import unwrap_model, get_state_dict
 
@@ -101,7 +106,7 @@             save_state['state_dict_ema'] = get_state_dict(self.model_ema, self.unwrap_fn)
         if metric is not None:
             save_state['metric'] = metric
-        torch.save(save_state, save_path)
+        torch.save(save_state, save_path)  # 'torch.save' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def _cleanup_checkpoints(self, trim=0):
         trim = min(len(self.checkpoint_files), trim)
