--- pytorch+++ mindspore@@ -1,8 +1,13 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ CUDA / AMP utils
 
 Hacked together by / Copyright 2020 Ross Wightman
 """
-import torch
+# import torch
 
 try:
     from apex import amp
@@ -48,9 +53,9 @@ 
     def __init__(self, device='cuda'):
         try:
-            self._scaler = torch.amp.GradScaler(device=device)
+            self._scaler = torch.amp.GradScaler(device=device)  # 'torch.amp.GradScaler' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         except (AttributeError, TypeError) as e:
-            self._scaler = torch.cuda.amp.GradScaler()
+            self._scaler = torch.cuda.amp.GradScaler()  # 'torch.cuda.amp.GradScaler' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __call__(
             self,
