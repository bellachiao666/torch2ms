--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Exponential Moving Average (EMA) of model updates
 
 Hacked together by / Copyright 2020 Ross Wightman
@@ -7,8 +12,7 @@ from copy import deepcopy
 from typing import Optional
 
-import torch
-import torch.nn as nn
+# import torch
 
 _logger = logging.getLogger(__name__)
 
@@ -50,7 +54,7 @@             p.requires_grad_(False)
 
     def _load_checkpoint(self, checkpoint_path):
-        checkpoint = torch.load(checkpoint_path, map_location='cpu')
+        checkpoint = torch.load(checkpoint_path, map_location='cpu')  # 'torch.load' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         assert isinstance(checkpoint, dict)
         if 'state_dict_ema' in checkpoint:
             new_state_dict = OrderedDict()
@@ -69,6 +73,7 @@     def update(self, model):
         # correct a mismatch in state dict keys
         needs_module = hasattr(model, 'module') and not self.ema_has_module
+        # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         with torch.no_grad():
             msd = model.state_dict()
             for k, ema_v in self.ema.state_dict().items():
@@ -80,7 +85,7 @@                 ema_v.copy_(ema_v * self.decay + (1. - self.decay) * model_v)
 
 
-class ModelEmaV2(nn.Module):
+class ModelEmaV2(msnn.Cell):
     """ Model Exponential Moving Average V2
 
     Keep a moving average of everything in the model state_dict (parameters and buffers).
@@ -114,6 +119,7 @@             self.module.to(device=device)
 
     def _update(self, model, update_fn):
+        # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         with torch.no_grad():
             for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):
                 if self.device is not None:
@@ -126,11 +132,11 @@     def set(self, model):
         self._update(model, update_fn=lambda e, m: m)
 
-    def forward(self, *args, **kwargs):
-        return self.module(*args, **kwargs)
-
-
-class ModelEmaV3(nn.Module):
+    def construct(self, *args, **kwargs):
+        return self.module(*args, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+
+class ModelEmaV3(msnn.Cell):
     """ Model Exponential Moving Average V3
 
     Keep a moving average of everything in the model state_dict (parameters and buffers).
@@ -153,6 +159,7 @@     This class is sensitive where it is initialized in the sequence of model init,
     GPU assignment and distributed training wrappers.
     """
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             model,
@@ -202,6 +209,8 @@ 
         return decay
 
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 装饰器 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.no_grad()
     def update(self, model, step: Optional[int] = None):
         decay = self.get_decay(step)
@@ -223,10 +232,10 @@                     ema_v.copy_(model_v)
 
             if hasattr(torch, '_foreach_lerp_'):
-                torch._foreach_lerp_(ema_lerp_values, model_lerp_values, weight=1. - decay)
+                torch._foreach_lerp_(ema_lerp_values, model_lerp_values, weight=1. - decay)  # 'torch._foreach_lerp_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
-                torch._foreach_mul_(ema_lerp_values, scalar=decay)
-                torch._foreach_add_(ema_lerp_values, model_lerp_values, alpha=1. - decay)
+                torch._foreach_mul_(ema_lerp_values, scalar=decay)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                torch._foreach_add_(ema_lerp_values, model_lerp_values, alpha=1. - decay)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):
                 if ema_v.is_floating_point():
@@ -240,10 +249,10 @@         model_params = tuple(model.parameters())
         if self.foreach:
             if hasattr(torch, '_foreach_lerp_'):
-                torch._foreach_lerp_(ema_params, model_params, weight=1. - decay)
+                torch._foreach_lerp_(ema_params, model_params, weight=1. - decay)  # 'torch._foreach_lerp_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
-                torch._foreach_mul_(ema_params, scalar=decay)
-                torch._foreach_add_(ema_params, model_params, alpha=1 - decay)
+                torch._foreach_mul_(ema_params, scalar=decay)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                torch._foreach_add_(ema_params, model_params, alpha=1 - decay)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             for ema_p, model_p in zip(ema_params, model_params):
                 ema_p.lerp_(model_p.to(device=self.device), weight=1. - decay)
@@ -251,10 +260,12 @@         for ema_b, model_b in zip(self.module.buffers(), model.buffers()):
             ema_b.copy_(model_b.to(device=self.device))
 
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 装饰器 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.no_grad()
     def set(self, model):
         for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):
             ema_v.copy_(model_v.to(device=self.device))
 
-    def forward(self, *args, **kwargs):
-        return self.module(*args, **kwargs)+    def construct(self, *args, **kwargs):
+        return self.module(*args, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;