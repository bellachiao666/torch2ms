--- pytorch+++ mindspore@@ -1,6 +1,11 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from typing import Optional, Tuple, List
 
-import torch
+# import torch
 
 
 def onnx_forward(onnx_file, example_input):
@@ -15,9 +20,9 @@ 
 
 def onnx_export(
-        model: torch.nn.Module,
+        model: msnn.Cell,
         output_file: str,
-        example_input: Optional[torch.Tensor] = None,
+        example_input: Optional[ms.Tensor] = None,
         training: bool = False,
         verbose: bool = False,
         check: bool = True,
@@ -45,7 +50,7 @@         if not input_size:
             assert hasattr(model, 'default_cfg'), 'Cannot file model default config, input size must be provided'
             input_size = model.default_cfg.get('input_size')
-        example_input = torch.randn((batch_size,) + input_size, requires_grad=training)
+        example_input = mint.randn((batch_size,) + input_size, requires_grad=training)
 
     # Run model once before export trace, sets padding for models with Conv2dSameExport. This means
     # that the padding for models with Conv2dSameExport (most models with tf_ prefix) is fixed for
@@ -54,6 +59,7 @@     # Opset >= 11 should allow for dynamic padding, however I cannot get it to work due to
     # issues in the tracing of the dynamic padding or errors attempting to export the model after jit
     # scripting it (an approach that should work). Perhaps in a future PyTorch or ONNX versions...
+    # 'torch.inference_mode' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     with torch.inference_mode():
         original_out = model(example_input)
 
@@ -71,15 +77,15 @@         export_type = torch.onnx.OperatorExportTypes.ONNX
 
     if use_dynamo:
-        export_options = torch.onnx.ExportOptions(dynamic_shapes=dynamic_size)
+        export_options = torch.onnx.ExportOptions(dynamic_shapes=dynamic_size)  # 'torch.onnx.ExportOptions' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         export_output = torch.onnx.dynamo_export(
             model,
             example_input,
             export_options=export_options,
-        )
+        )  # 'torch.onnx.dynamo_export' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         export_output.save(output_file)
     else:
-        torch.onnx.export(
+        ms.jit(
             model,
             example_input,
             output_file,
