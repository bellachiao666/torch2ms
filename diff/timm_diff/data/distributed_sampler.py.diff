--- pytorch+++ mindspore@@ -1,9 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import math
-import torch
-from torch.utils.data import Sampler
-import torch.distributed as dist
+# import torch
+# from torch.utils.data import Sampler
+# import torch.distributed as dist
 
 
+# 'torch.utils.data.Sampler' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class OrderedDistributedSampler(Sampler):
     """Sampler that restricts data loading to a subset of the dataset.
     It is especially useful in conjunction with
@@ -23,11 +29,11 @@         if num_replicas is None:
             if not dist.is_available():
                 raise RuntimeError("Requires distributed package to be available")
-            num_replicas = dist.get_world_size()
+            num_replicas = mint.distributed.get_world_size()
         if rank is None:
             if not dist.is_available():
                 raise RuntimeError("Requires distributed package to be available")
-            rank = dist.get_rank()
+            rank = mint.distributed.get_rank()
         self.dataset = dataset
         self.num_replicas = num_replicas
         self.rank = rank
@@ -51,6 +57,7 @@         return self.num_samples
 
 
+# 'torch.utils.data.Sampler' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class RepeatAugSampler(Sampler):
     """Sampler that restricts data loading to a subset of the dataset for distributed,
     with repeated augmentation.
@@ -75,11 +82,11 @@         if num_replicas is None:
             if not dist.is_available():
                 raise RuntimeError("Requires distributed package to be available")
-            num_replicas = dist.get_world_size()
+            num_replicas = mint.distributed.get_world_size()
         if rank is None:
             if not dist.is_available():
                 raise RuntimeError("Requires distributed package to be available")
-            rank = dist.get_rank()
+            rank = mint.distributed.get_rank()
         self.dataset = dataset
         self.num_replicas = num_replicas
         self.rank = rank
@@ -100,20 +107,20 @@ 
     def __iter__(self):
         # deterministically shuffle based on epoch
-        g = torch.Generator()
+        g = torch.Generator()  # 'torch.Generator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         g.manual_seed(self.epoch)
         if self.shuffle:
-            indices = torch.randperm(len(self.dataset), generator=g)
+            indices = mint.randperm(len(self.dataset), generator=g)
         else:
-            indices = torch.arange(start=0, end=len(self.dataset))
+            indices = mint.arange(start=0, end=len(self.dataset))
 
         # produce repeats e.g. [0, 0, 0, 1, 1, 1, 2, 2, 2....]
         if isinstance(self.num_repeats, float) and not self.num_repeats.is_integer():
             # resample for repeats w/ non-integer ratio
             repeat_size = math.ceil(self.num_repeats * len(self.dataset))
-            indices = indices[torch.tensor([int(i // self.num_repeats) for i in range(repeat_size)])]
+            indices = indices[ms.Tensor([int(i // self.num_repeats) for i in range(repeat_size)])]
         else:
-            indices = torch.repeat_interleave(indices, repeats=int(self.num_repeats), dim=0)
+            indices = mint.repeat_interleave(indices, repeats=int(self.num_repeats), dim=0)
         indices = indices.tolist()  # leaving as tensor thrashes dataloader memory
         # add extra samples to make it evenly divisible
         padding_size = self.total_size - len(indices)
