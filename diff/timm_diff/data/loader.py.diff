--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Loader Factory, Fast Collate, CUDA Prefetcher
 
 Prefetcher and Fast Collate inspired by NVIDIA APEX example at
@@ -12,8 +17,7 @@ from itertools import repeat
 from typing import Callable, Optional, Tuple, Union
 
-import torch
-import torch.utils.data
+# import torch
 import numpy as np
 
 from .constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
@@ -36,28 +40,28 @@         is_np = isinstance(batch[0][0], np.ndarray)
         inner_tuple_size = len(batch[0][0])
         flattened_batch_size = batch_size * inner_tuple_size
-        targets = torch.zeros(flattened_batch_size, dtype=torch.int64)
-        tensor = torch.zeros((flattened_batch_size, *batch[0][0][0].shape), dtype=torch.uint8)
+        targets = mint.zeros(flattened_batch_size, dtype=ms.int64)
+        tensor = mint.zeros((flattened_batch_size, *batch[0][0][0].shape), dtype=ms.uint8)
         for i in range(batch_size):
             assert len(batch[i][0]) == inner_tuple_size  # all input tensor tuples must be same length
             for j in range(inner_tuple_size):
                 targets[i + j * batch_size] = batch[i][1]
                 if is_np:
-                    tensor[i + j * batch_size] += torch.from_numpy(batch[i][0][j])
+                    tensor[i + j * batch_size] += torch.from_numpy(batch[i][0][j])  # 'torch.from_numpy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 else:
                     tensor[i + j * batch_size] += batch[i][0][j]
         return tensor, targets
     elif isinstance(batch[0][0], np.ndarray):
-        targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)
+        targets = ms.Tensor([b[1] for b in batch], dtype=ms.int64)
         assert len(targets) == batch_size
-        tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)
+        tensor = mint.zeros((batch_size, *batch[0][0].shape), dtype=ms.uint8)
         for i in range(batch_size):
-            tensor[i] += torch.from_numpy(batch[i][0])
+            tensor[i] += torch.from_numpy(batch[i][0])  # 'torch.from_numpy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return tensor, targets
-    elif isinstance(batch[0][0], torch.Tensor):
-        targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)
+    elif isinstance(batch[0][0], ms.Tensor):
+        targets = ms.Tensor([b[1] for b in batch], dtype=ms.int64)
         assert len(targets) == batch_size
-        tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)
+        tensor = mint.zeros((batch_size, *batch[0][0].shape), dtype=ms.uint8)
         for i in range(batch_size):
             tensor[i].copy_(batch[i][0])
         return tensor, targets
@@ -101,11 +105,11 @@         self.device = device
         if fp16:
             # fp16 arg is deprecated, but will override dtype arg if set for bwd compat
-            img_dtype = torch.float16
-        self.img_dtype = img_dtype or torch.float32
-        self.mean = torch.tensor(
+            img_dtype = ms.float16
+        self.img_dtype = img_dtype or ms.float32
+        self.mean = ms.Tensor(
             [x * 255 for x in mean], device=device, dtype=img_dtype).view(normalization_shape)
-        self.std = torch.tensor(
+        self.std = ms.Tensor(
             [x * 255 for x in std], device=device, dtype=img_dtype).view(normalization_shape)
         if re_prob > 0.:
             self.random_erasing = RandomErasing(
@@ -117,16 +121,16 @@             )
         else:
             self.random_erasing = None
-        self.is_cuda = device.type == 'cuda' and torch.cuda.is_available()
-        self.is_npu = device.type == 'npu' and torch.npu.is_available()
+        self.is_cuda = device.type == 'cuda' and torch.cuda.is_available()  # 'torch.cuda.is_available' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        self.is_npu = device.type == 'npu' and torch.npu.is_available()  # 'torch.npu.is_available' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __iter__(self):
         first = True
         if self.is_cuda:
-            stream = torch.cuda.Stream(device=self.device)
+            stream = torch.cuda.Stream(device=self.device)  # 'torch.cuda.Stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             stream_context = partial(torch.cuda.stream, stream=stream)
         elif self.is_npu:
-            stream = torch.npu.Stream(device=self.device)
+            stream = torch.npu.Stream(device=self.device)  # 'torch.npu.Stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             stream_context = partial(torch.npu.stream, stream=stream)
         else:
             stream = None
@@ -148,9 +152,9 @@ 
             if stream is not None:
                 if self.is_cuda:
-                    torch.cuda.current_stream(device=self.device).wait_stream(stream)
+                    torch.cuda.current_stream(device=self.device).wait_stream(stream)  # 'torch.cuda.current_stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.cuda.current_stream.wait_stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 elif self.is_npu:
-                    torch.npu.current_stream(device=self.device).wait_stream(stream)
+                    torch.npu.current_stream(device=self.device).wait_stream(stream)  # 'torch.npu.current_stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.npu.current_stream.wait_stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
             input = next_input
             target = next_target
@@ -182,12 +186,12 @@ 
 
 def _worker_init(worker_id, worker_seeding='all'):
-    worker_info = torch.utils.data.get_worker_info()
+    worker_info = torch.utils.data.get_worker_info()  # 'torch.utils.data.get_worker_info' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     assert worker_info.id == worker_id
     if isinstance(worker_seeding, Callable):
         seed = worker_seeding(worker_info)
         random.seed(seed)
-        torch.manual_seed(seed)
+        torch.manual_seed(seed)  # 'torch.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         np.random.seed(seed % (2 ** 32 - 1))
     else:
         assert worker_seeding in ('all', 'part')
@@ -230,7 +234,7 @@         collate_fn: Optional[Callable] = None,
         pin_memory: bool = False,
         fp16: bool = False,  # deprecated, use img_dtype
-        img_dtype: torch.dtype = torch.float32,
+        img_dtype: torch.dtype = ms.float32,
         device: torch.device = torch.device('cuda'),
         use_prefetcher: bool = True,
         use_multi_epochs_loader: bool = False,
@@ -328,7 +332,7 @@             if num_aug_repeats:
                 sampler = RepeatAugSampler(dataset, num_repeats=num_aug_repeats)
             else:
-                sampler = torch.utils.data.distributed.DistributedSampler(dataset)
+                sampler = torch.utils.data.distributed.DistributedSampler(dataset)  # 'torch.utils.data.distributed.DistributedSampler' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             # This will add extra duplicate entries to result in equal num
             # of samples per-process, will slightly alter validation results
@@ -355,10 +359,10 @@         persistent_workers=persistent_workers
     )
     try:
-        loader = loader_class(dataset, **loader_args)
+        loader = loader_class(dataset, **loader_args)  # 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
     except TypeError as e:
         loader_args.pop('persistent_workers')  # only in Pytorch 1.7+
-        loader = loader_class(dataset, **loader_args)
+        loader = loader_class(dataset, **loader_args)  # 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
     if use_prefetcher:
         prefetch_re_prob = re_prob if is_training and not no_aug else 0.
         loader = PrefetchLoader(
@@ -378,10 +382,11 @@     return loader
 
 
+# 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class MultiEpochsDataLoader(torch.utils.data.DataLoader):
 
     def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
+        super().__init__(*args, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self._DataLoader__initialized = False
         if self.batch_sampler is None:
             self.sampler = _RepeatSampler(self.sampler)
