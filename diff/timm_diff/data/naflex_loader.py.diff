--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """NaFlex data loader for dynamic sequence length training.
 
 This module provides a specialized data loader for Vision Transformer models that supports:
@@ -15,7 +20,7 @@ from typing import Callable, Dict, Iterator, List, Optional, Tuple, Union
 
 
-import torch
+# import torch
 
 from .constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from .loader import _worker_init, adapt_to_chs
@@ -27,6 +32,8 @@ class NaFlexPrefetchLoader:
     """Data prefetcher for NaFlex format which normalizes patches."""
 
+    # 类型标注 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 类型标注 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             loader: torch.utils.data.DataLoader,
@@ -56,17 +63,17 @@         """
         self.loader = loader
         self.device = device
-        self.img_dtype = img_dtype or torch.float32
+        self.img_dtype = img_dtype or ms.float32
 
         # Create mean/std tensors for normalization (will be applied to patches)
         mean = adapt_to_chs(mean, channels)
         std = adapt_to_chs(std, channels)
         normalization_shape = (1, 1, channels)
         self.channels = channels
-        self.mean = torch.tensor(
-            [x * 255 for x in mean], device=device, dtype=self.img_dtype).view(normalization_shape)
-        self.std = torch.tensor(
-            [x * 255 for x in std], device=device, dtype=self.img_dtype).view(normalization_shape)
+        self.mean = ms.Tensor(
+            [x * 255 for x in mean], dtype = self.img_dtype).view(normalization_shape)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;; 'torch.tensor':没有对应的mindspore参数 'device' (position 2);
+        self.std = ms.Tensor(
+            [x * 255 for x in std], dtype = self.img_dtype).view(normalization_shape)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;; 'torch.tensor':没有对应的mindspore参数 'device' (position 2);
 
         if re_prob > 0.:
             self.random_erasing = PatchRandomErasing(
@@ -80,8 +87,8 @@             self.random_erasing = None
 
         # Check for CUDA/NPU availability
-        self.is_cuda = device.type == 'cuda' and torch.cuda.is_available()
-        self.is_npu = device.type == 'npu' and torch.npu.is_available()
+        self.is_cuda = device.type == 'cuda' and torch.cuda.is_available()  # 'torch.cuda.is_available' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        self.is_npu = device.type == 'npu' and torch.npu.is_available()  # 'torch.npu.is_available' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __iter__(self) -> Iterator[Tuple[Dict[str, torch.Tensor], torch.Tensor]]:
         """Iterate through the loader with prefetching and normalization.
@@ -91,10 +98,10 @@         """
         first = True
         if self.is_cuda:
-            stream = torch.cuda.Stream(device=self.device)
+            stream = torch.cuda.Stream(device=self.device)  # 'torch.cuda.Stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             stream_context = partial(torch.cuda.stream, stream=stream)
         elif self.is_npu:
-            stream = torch.npu.Stream(device=self.device)
+            stream = torch.npu.Stream(device=self.device)  # 'torch.npu.Stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             stream_context = partial(torch.npu.stream, stream=stream)
         else:
             stream = None
@@ -152,9 +159,9 @@ 
             if stream is not None:
                 if self.is_cuda:
-                    torch.cuda.current_stream(device=self.device).wait_stream(stream)
+                    torch.cuda.current_stream(device=self.device).wait_stream(stream)  # 'torch.cuda.current_stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.cuda.current_stream.wait_stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 elif self.is_npu:
-                    torch.npu.current_stream(device=self.device).wait_stream(stream)
+                    torch.npu.current_stream(device=self.device).wait_stream(stream)  # 'torch.npu.current_stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.npu.current_stream.wait_stream' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
             input_dict = next_input_dict
             target = next_target
@@ -188,6 +195,7 @@         return self.loader.dataset
 
 
+# 类型标注 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def create_naflex_loader(
         dataset,
         patch_size: Optional[Union[Tuple[int, int], int]] = None,
@@ -231,7 +239,7 @@         epoch: int = 0,
         use_prefetcher: bool = True,
         pin_memory: bool = True,
-        img_dtype: torch.dtype = torch.float32,
+        img_dtype: torch.dtype = ms.float32,
         device: Union[str, torch.device] = torch.device('cuda'),
         persistent_workers: bool = True,
         worker_seeding: str = 'all',
@@ -352,7 +360,7 @@             pin_memory=pin_memory,
             worker_init_fn=partial(_worker_init, worker_seeding=worker_seeding),
             persistent_workers=persistent_workers
-        )
+        )  # 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         if use_prefetcher:
             loader = NaFlexPrefetchLoader(
@@ -400,7 +408,7 @@             collate_fn=collate_fn,
             pin_memory=pin_memory,
             drop_last=False,
-        )
+        )  # 'torch.utils.data.DataLoader' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         if use_prefetcher:
             loader = NaFlexPrefetchLoader(
