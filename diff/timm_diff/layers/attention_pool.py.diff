--- pytorch+++ mindspore@@ -1,8 +1,13 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from typing import Optional, Type
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from .attention import maybe_add_mask
 from .config import use_fused_attn
@@ -10,10 +15,10 @@ from .weight_init import trunc_normal_tf_
 
 
-class AttentionPoolLatent(nn.Module):
+class AttentionPoolLatent(msnn.Cell):
     """ Attention pooling w/ latent query
     """
-    fused_attn: torch.jit.Final[bool]
+    fused_attn: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -29,8 +34,8 @@             latent_dim: int = None,
             pos_embed: str = '',
             pool_type: str = 'token',
-            norm_layer: Optional[Type[nn.Module]] = None,
-            act_layer: Optional[Type[nn.Module]] = nn.GELU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
+            act_layer: Optional[Type[msnn.Cell]] = nn.GELU,
             drop: float = 0.0,
             device = None,
             dtype = None
@@ -49,28 +54,28 @@ 
         if pos_embed == 'abs':
             assert feat_size is not None
-            self.pos_embed = nn.Parameter(torch.zeros(feat_size, in_features, **dd))
+            self.pos_embed = ms.Parameter(mint.zeros(feat_size, in_features, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.pos_embed = None
 
         self.latent_dim = latent_dim or embed_dim
         self.latent_len = latent_len
-        self.latent = nn.Parameter(torch.zeros(1, self.latent_len, embed_dim, **dd))
+        self.latent = ms.Parameter(mint.zeros(1, self.latent_len, embed_dim, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
-        self.q = nn.Linear(embed_dim, embed_dim, bias=qkv_bias, **dd)
-        self.kv = nn.Linear(embed_dim, embed_dim * 2, bias=qkv_bias, **dd)
+        self.q = nn.Linear(embed_dim, embed_dim, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.kv = nn.Linear(embed_dim, embed_dim * 2, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         if qk_norm:
             qk_norm_layer = norm_layer or nn.LayerNorm
-            self.q_norm = qk_norm_layer(self.head_dim, **dd)
-            self.k_norm = qk_norm_layer(self.head_dim, **dd)
+            self.q_norm = qk_norm_layer(self.head_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.k_norm = qk_norm_layer(self.head_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.q_norm = nn.Identity()
-            self.k_norm = nn.Identity()
-        self.proj = nn.Linear(embed_dim, embed_dim, **dd)
+            self.q_norm = msnn.Identity()
+            self.k_norm = msnn.Identity()
+        self.proj = nn.Linear(embed_dim, embed_dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.proj_drop = nn.Dropout(drop)
 
-        self.norm = norm_layer(out_features, **dd) if norm_layer is not None else nn.Identity()
-        self.mlp = Mlp(embed_dim, int(embed_dim * mlp_ratio), act_layer=act_layer, **dd)
+        self.norm = norm_layer(out_features, **dd) if norm_layer is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.mlp = Mlp(embed_dim, int(embed_dim * mlp_ratio), act_layer=act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.init_weights()
 
@@ -79,7 +84,7 @@             trunc_normal_tf_(self.pos_embed, std=self.pos_embed.shape[1] ** -0.5)
         trunc_normal_tf_(self.latent, std=self.latent_dim ** -0.5)
 
-    def forward(self, x, attn_mask: Optional[torch.Tensor] = None):
+    def construct(self, x, attn_mask: Optional[ms.Tensor] = None):
         B, N, C = x.shape
 
         if self.pos_embed is not None:
@@ -95,7 +100,7 @@         q, k = self.q_norm(q), self.k_norm(k)
 
         if self.fused_attn:
-            x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)
+            x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
