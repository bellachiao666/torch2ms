--- pytorch+++ mindspore@@ -1,8 +1,13 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from typing import Optional, Type
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from .attention import maybe_add_mask
 from .config import use_fused_attn
@@ -10,7 +15,7 @@ from .weight_init import trunc_normal_tf_
 
 
-class AttentionPoolLatent(nn.Module):
+class AttentionPoolLatent(msnn.Cell):
     """ Attention pooling w/ latent query
     """
     fused_attn: torch.jit.Final[bool]
@@ -49,13 +54,13 @@ 
         if pos_embed == 'abs':
             assert feat_size is not None
-            self.pos_embed = nn.Parameter(torch.zeros(feat_size, in_features, **dd))
+            self.pos_embed = ms.Parameter(mint.zeros(feat_size, in_features, **dd))
         else:
             self.pos_embed = None
 
         self.latent_dim = latent_dim or embed_dim
         self.latent_len = latent_len
-        self.latent = nn.Parameter(torch.zeros(1, self.latent_len, embed_dim, **dd))
+        self.latent = ms.Parameter(mint.zeros(1, self.latent_len, embed_dim, **dd))
 
         self.q = nn.Linear(embed_dim, embed_dim, bias=qkv_bias, **dd)
         self.kv = nn.Linear(embed_dim, embed_dim * 2, bias=qkv_bias, **dd)
@@ -64,12 +69,12 @@             self.q_norm = qk_norm_layer(self.head_dim, **dd)
             self.k_norm = qk_norm_layer(self.head_dim, **dd)
         else:
-            self.q_norm = nn.Identity()
-            self.k_norm = nn.Identity()
+            self.q_norm = msnn.Identity()
+            self.k_norm = msnn.Identity()
         self.proj = nn.Linear(embed_dim, embed_dim, **dd)
         self.proj_drop = nn.Dropout(drop)
 
-        self.norm = norm_layer(out_features, **dd) if norm_layer is not None else nn.Identity()
+        self.norm = norm_layer(out_features, **dd) if norm_layer is not None else msnn.Identity()
         self.mlp = Mlp(embed_dim, int(embed_dim * mlp_ratio), act_layer=act_layer, **dd)
 
         self.init_weights()
@@ -79,7 +84,7 @@             trunc_normal_tf_(self.pos_embed, std=self.pos_embed.shape[1] ** -0.5)
         trunc_normal_tf_(self.latent, std=self.latent_dim ** -0.5)
 
-    def forward(self, x, attn_mask: Optional[torch.Tensor] = None):
+    def construct(self, x, attn_mask: Optional[torch.Tensor] = None):
         B, N, C = x.shape
 
         if self.pos_embed is not None:
@@ -95,7 +100,7 @@         q, k = self.q_norm(q), self.k_norm(k)
 
         if self.fused_attn:
-            x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)
+            x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
