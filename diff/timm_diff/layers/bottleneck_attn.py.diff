--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Bottleneck Self Attention (Bottleneck Transformers)
 
 Paper: `Bottleneck Transformers for Visual Recognition` - https://arxiv.org/abs/2101.11605
@@ -16,9 +21,8 @@ """
 from typing import List, Optional, Tuple
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from .helpers import to_2tuple, make_divisible
 from .weight_init import trunc_normal_
@@ -41,8 +45,8 @@     x = x.reshape(-1, W, 2 * W -1)
 
     # pad to shift from relative to absolute indexing
-    x_pad = F.pad(x, [0, 1]).flatten(1)
-    x_pad = F.pad(x_pad, [0, W - 1])
+    x_pad = ms.Tensor.flatten(1)
+    x_pad = nn.functional.pad(x_pad, [0, W - 1])
 
     # reshape and slice out the padded elements
     x_pad = x_pad.reshape(-1, W + 1, 2 * W - 1)
@@ -53,7 +57,7 @@     return x.permute(permute_mask)
 
 
-class PosEmbedRel(nn.Module):
+class PosEmbedRel(msnn.Cell):
     """ Relative Position Embedding
     As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2
     Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925
@@ -72,16 +76,16 @@         self.dim_head = dim_head
         self.scale = scale
 
-        self.height_rel = nn.Parameter(torch.empty(self.height * 2 - 1, dim_head, **dd))
-        self.width_rel = nn.Parameter(torch.empty(self.width * 2 - 1, dim_head, **dd))
+        self.height_rel = ms.Parameter(mint.empty(self.height * 2 - 1, dim_head, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.width_rel = ms.Parameter(mint.empty(self.width * 2 - 1, dim_head, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.reset_parameters()
 
     def reset_parameters(self):
-        torch.nn.init.normal_(self.height_rel, std=self.scale)
-        torch.nn.init.normal_(self.width_rel, std=self.scale)
+        torch.nn.init.normal_(self.height_rel, std=self.scale)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch.nn.init.normal_(self.width_rel, std=self.scale)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-    def forward(self, q):
+    def construct(self, q):
         B, HW, _ = q.shape
 
         # relative logits in width dimension.
@@ -97,7 +101,7 @@         return rel_logits
 
 
-class BottleneckAttn(nn.Module):
+class BottleneckAttn(msnn.Cell):
     """ Bottleneck Attention
     Paper: `Bottleneck Transformers for Visual Recognition` - https://arxiv.org/abs/2101.11605
 
@@ -146,12 +150,12 @@         self.scale = self.dim_head_qk ** -0.5
         self.scale_pos_embed = scale_pos_embed
 
-        self.qkv = nn.Conv2d(dim, self.dim_out_qk * 2 + self.dim_out_v, 1, bias=qkv_bias, **dd)
+        self.qkv = nn.Conv2d(dim, self.dim_out_qk * 2 + self.dim_out_v, 1, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
 
         # NOTE I'm only supporting relative pos embedding for now
-        self.pos_embed = PosEmbedRel(feat_size, dim_head=self.dim_head_qk, scale=self.scale, **dd)
+        self.pos_embed = PosEmbedRel(feat_size, dim_head=self.dim_head_qk, scale=self.scale, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
-        self.pool = nn.AvgPool2d(2, 2) if stride == 2 else nn.Identity()
+        self.pool = nn.AvgPool2d(2, 2) if stride == 2 else msnn.Identity()
 
         self.reset_parameters()
 
@@ -160,7 +164,7 @@         trunc_normal_(self.pos_embed.height_rel, std=self.scale)
         trunc_normal_(self.pos_embed.width_rel, std=self.scale)
 
-    def forward(self, x):
+    def construct(self, x):
         B, C, H, W = x.shape
         _assert(H == self.pos_embed.height, '')
         _assert(W == self.pos_embed.width, '')
@@ -169,7 +173,7 @@ 
         # NOTE head vs channel split ordering in qkv projection was decided before I allowed qk to differ from v
         # So, this is more verbose than if heads were before qkv splits, but throughput is not impacted.
-        q, k, v = torch.split(x, [self.dim_out_qk, self.dim_out_qk, self.dim_out_v], dim=1)
+        q, k, v = mint.split(x, [self.dim_out_qk, self.dim_out_qk, self.dim_out_v], dim=1)
         q = q.reshape(B * self.num_heads, self.dim_head_qk, -1).transpose(-1, -2)
         k = k.reshape(B * self.num_heads, self.dim_head_qk, -1)  # no transpose, for q @ k
         v = v.reshape(B * self.num_heads, self.dim_head_v, -1).transpose(-1, -2)
