--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Classifier head and layer factory
 
 Hacked together by / Copyright 2020 Ross Wightman
@@ -6,9 +11,8 @@ from functools import partial
 from typing import Optional, Union, Callable
 
-import torch
-import torch.nn as nn
-from torch.nn import functional as F
+# import torch
+# import torch.nn as nn
 
 from .adaptive_avgmax_pool import SelectAdaptivePool2d
 from .create_act import get_act_layer
@@ -36,11 +40,11 @@ 
 def _create_fc(num_features, num_classes, use_conv=False, device=None, dtype=None):
     if num_classes <= 0:
-        fc = nn.Identity()  # pass-through (no classifier)
+        fc = msnn.Identity()  # pass-through (no classifier)
     elif use_conv:
-        fc = nn.Conv2d(num_features, num_classes, 1, bias=True, device=device, dtype=dtype)
+        fc = nn.Conv2d(num_features, num_classes, 1, bias = True, dtype = dtype)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device' (position 9);
     else:
-        fc = nn.Linear(num_features, num_classes, bias=True, device=device, dtype=dtype)
+        fc = nn.Linear(num_features, num_classes, bias = True, dtype = dtype)  # 'torch.nn.Linear':没有对应的mindspore参数 'device' (position 3);
     return fc
 
 
@@ -74,7 +78,7 @@     return global_pool, fc
 
 
-class ClassifierHead(nn.Module):
+class ClassifierHead(msnn.Cell):
     """Classifier head w/ configurable global pooling and dropout."""
 
     def __init__(
@@ -112,7 +116,7 @@         self.global_pool = global_pool
         self.drop = nn.Dropout(drop_rate)
         self.fc = fc
-        self.flatten = nn.Flatten(1) if use_conv and pool_type else nn.Identity()
+        self.flatten = mint.flatten(1) if use_conv and pool_type else msnn.Identity()
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None):
         # FIXME get current device/dtype for reset?
@@ -124,7 +128,7 @@                 use_conv=self.use_conv,
                 input_fmt=self.input_fmt,
             )
-            self.flatten = nn.Flatten(1) if self.use_conv and pool_type else nn.Identity()
+            self.flatten = mint.flatten(1) if self.use_conv and pool_type else msnn.Identity()
         else:
             num_pooled_features = self.in_features * self.global_pool.feat_mult()
             self.fc = _create_fc(
@@ -133,7 +137,7 @@                 use_conv=self.use_conv,
             )
 
-    def forward(self, x, pre_logits: bool = False):
+    def construct(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         x = self.drop(x)
         if pre_logits:
@@ -142,7 +146,7 @@         return self.flatten(x)
 
 
-class NormMlpClassifierHead(nn.Module):
+class NormMlpClassifierHead(msnn.Cell):
     """ A Pool -> Norm -> Mlp Classifier Head for '2D' NCHW tensors
     """
     def __init__(
@@ -178,24 +182,24 @@         linear_layer = partial(nn.Conv2d, kernel_size=1) if self.use_conv else nn.Linear
 
         self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)
-        self.norm = norm_layer(in_features, **dd)
-        self.flatten = nn.Flatten(1) if pool_type else nn.Identity()
+        self.norm = norm_layer(in_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.flatten = mint.flatten(1) if pool_type else msnn.Identity()
         if hidden_size:
-            self.pre_logits = nn.Sequential(OrderedDict([
+            self.pre_logits = msnn.SequentialCell(OrderedDict([
                 ('fc', linear_layer(in_features, hidden_size, **dd)),
                 ('act', act_layer()),
-            ]))
+            ]))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.num_features = hidden_size
         else:
-            self.pre_logits = nn.Identity()
+            self.pre_logits = msnn.Identity()
         self.drop = nn.Dropout(drop_rate)
-        self.fc = linear_layer(self.num_features, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.fc = linear_layer(self.num_features, num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None):
         # FIXME handle device/dtype on reset
         if pool_type is not None:
             self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)
-            self.flatten = nn.Flatten(1) if pool_type else nn.Identity()
+            self.flatten = mint.flatten(1) if pool_type else msnn.Identity()
         self.use_conv = self.global_pool.is_identity()
         linear_layer = partial(nn.Conv2d, kernel_size=1) if self.use_conv else nn.Linear
         if self.hidden_size:
@@ -206,9 +210,9 @@                     new_fc.weight.copy_(self.pre_logits.fc.weight.reshape(new_fc.weight.shape))
                     new_fc.bias.copy_(self.pre_logits.fc.bias)
                     self.pre_logits.fc = new_fc
-        self.fc = linear_layer(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
+        self.fc = linear_layer(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
+
+    def construct(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         x = self.norm(x)
         x = self.flatten(x)
@@ -220,7 +224,7 @@         return x
 
 
-class ClNormMlpClassifierHead(nn.Module):
+class ClNormMlpClassifierHead(msnn.Cell):
     """ A Pool -> Norm -> Mlp Classifier Head for n-D NxxC tensors
     """
     def __init__(
@@ -258,26 +262,26 @@         norm_layer = get_norm_layer(norm_layer)
         act_layer = get_act_layer(act_layer)
 
-        self.norm = norm_layer(in_features, **dd)
+        self.norm = norm_layer(in_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if hidden_size:
-            self.pre_logits = nn.Sequential(OrderedDict([
+            self.pre_logits = msnn.SequentialCell(OrderedDict([
                 ('fc', nn.Linear(in_features, hidden_size, **dd)),
                 ('act', act_layer()),
-            ]))
+            ]))  # 存在 *args/**kwargs，需手动确认参数映射;
             self.num_features = hidden_size
         else:
-            self.pre_logits = nn.Identity()
+            self.pre_logits = msnn.Identity()
         self.drop = nn.Dropout(drop_rate)
-        self.fc = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.fc = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，需手动确认参数映射;
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None, reset_other: bool = False):
         # FIXME extract dd on reset
         if pool_type is not None:
             self.pool_type = pool_type
         if reset_other:
-            self.pre_logits = nn.Identity()
-            self.norm = nn.Identity()
-        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+            self.pre_logits = msnn.Identity()
+            self.norm = msnn.Identity()
+        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
 
     def _global_pool(self, x):
         if self.pool_type:
@@ -289,7 +293,7 @@                 x = 0.5 * (x.amax(dim=self.pool_dim) + x.mean(dim=self.pool_dim))
         return x
 
-    def forward(self, x, pre_logits: bool = False):
+    def construct(self, x, pre_logits: bool = False):
         x = self._global_pool(x)
         x = self.norm(x)
         x = self.pre_logits(x)
