--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ PyTorch Conditionally Parameterized Convolution (CondConv)
 
 Paper: CondConv: Conditionally Parameterized Convolutions for Efficient Inference
@@ -9,10 +14,7 @@ import math
 from functools import partial
 from typing import Union, Tuple
-
-import torch
-from torch import nn as nn
-from torch.nn import functional as F
+# from torch import nn as nn
 
 from ._fx import register_notrace_module
 from .helpers import to_2tuple
@@ -34,7 +36,7 @@ 
 
 @register_notrace_module
-class CondConv2d(nn.Module):
+class CondConv2d(msnn.Cell):
     """ Conditionally Parameterized Convolution
     Inspired by: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/condconv/condconv_layers.py
 
@@ -76,11 +78,11 @@         weight_num_param = 1
         for wd in self.weight_shape:
             weight_num_param *= wd
-        self.weight = torch.nn.Parameter(torch.empty(self.num_experts, weight_num_param, **dd))
+        self.weight = ms.Parameter(mint.empty(self.num_experts, weight_num_param, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         if bias:
             self.bias_shape = (self.out_channels,)
-            self.bias = torch.nn.Parameter(torch.empty(self.num_experts, self.out_channels, **dd))
+            self.bias = ms.Parameter(mint.empty(self.num_experts, self.out_channels, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.register_parameter('bias', None)
 
@@ -97,14 +99,14 @@                 partial(nn.init.uniform_, a=-bound, b=bound), self.num_experts, self.bias_shape)
             init_bias(self.bias)
 
-    def forward(self, x, routing_weights):
+    def construct(self, x, routing_weights):
         B, C, H, W = x.shape
-        weight = torch.matmul(routing_weights, self.weight)
+        weight = mint.matmul(routing_weights, self.weight)
         new_weight_shape = (B * self.out_channels, self.in_channels // self.groups) + self.kernel_size
         weight = weight.view(new_weight_shape)
         bias = None
         if self.bias is not None:
-            bias = torch.matmul(routing_weights, self.bias)
+            bias = mint.matmul(routing_weights, self.bias)
             bias = bias.view(B * self.out_channels)
         # move batch elements with channels so each batch element can be efficiently convolved with separate kernel
         # reshape instead of view to work with channels_last input
@@ -114,9 +116,8 @@                 x, weight, bias, stride=self.stride, padding=self.padding,
                 dilation=self.dilation, groups=self.groups * B)
         else:
-            out = F.conv2d(
-                x, weight, bias, stride=self.stride, padding=self.padding,
-                dilation=self.dilation, groups=self.groups * B)
+            out = nn.functional.conv2d(
+                x, weight, bias, stride = self.stride, padding = self.padding, dilation = self.dilation, groups = self.groups * B)
         out = out.permute([1, 0, 2, 3]).view(B, self.out_channels, out.shape[-2], out.shape[-1])
 
         # Literal port (from TF definition)
