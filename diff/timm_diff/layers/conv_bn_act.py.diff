--- pytorch+++ mindspore@@ -1,10 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Conv2d + BN + Act
 
 Hacked together by / Copyright 2020 Ross Wightman
 """
 from typing import Any, Dict, Optional, Type
 
-from torch import nn as nn
+# from torch import nn as nn
 
 from .typing import LayerType, PadType
 from .blur_pool import create_aa
@@ -12,7 +17,7 @@ from .create_norm_act import get_norm_act_layer
 
 
-class ConvNormAct(nn.Module):
+class ConvNormAct(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -28,7 +33,7 @@             norm_layer: LayerType = nn.BatchNorm2d,
             act_layer: Optional[LayerType] = nn.ReLU,
             aa_layer: Optional[LayerType] = None,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             conv_kwargs: Optional[Dict[str, Any]] = None,
             norm_kwargs: Optional[Dict[str, Any]] = None,
             act_kwargs: Optional[Dict[str, Any]] = None,
@@ -52,7 +57,7 @@             groups=groups,
             bias=bias,
             **conv_kwargs,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         if apply_norm:
             # NOTE for backwards compatibility with models that use separate norm and act layer definitions
@@ -65,9 +70,9 @@                 apply_act=apply_act,
                 act_kwargs=act_kwargs,
                 **norm_kwargs,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.bn = nn.Sequential()
+            self.bn = msnn.SequentialCell()
             if drop_layer:
                 norm_kwargs['drop_layer'] = drop_layer
                 self.bn.add_module('drop', drop_layer())
@@ -79,7 +84,7 @@             enable=use_aa,
             noop=None,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     @property
     def in_channels(self):
@@ -89,7 +94,7 @@     def out_channels(self):
         return self.conv.out_channels
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv(x)
         x = self.bn(x)
         aa = getattr(self, 'aa', None)
