--- pytorch+++ mindspore@@ -1,8 +1,12 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Attention Factory
 
 Hacked together by / Copyright 2021 Ross Wightman
 """
-import torch
 from functools import partial
 
 from .bottleneck_attn import BottleneckAttn
@@ -20,7 +24,7 @@ 
 
 def get_attn(attn_type):
-    if isinstance(attn_type, torch.nn.Module):
+    if isinstance(attn_type, msnn.Cell):
         return attn_type
     module_cls = None
     if attn_type:
@@ -94,5 +98,5 @@     module_cls = get_attn(attn_type)
     if module_cls is not None:
         # NOTE: it's expected the first (positional) argument of all attention layers is the # input channels
-        return module_cls(channels, **kwargs)
+        return module_cls(channels, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return None
