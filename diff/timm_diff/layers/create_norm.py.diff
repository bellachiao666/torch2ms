--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Norm Layer Factory
 
 Create norm modules by string (to mirror create_act and creat_norm-act fns)
@@ -8,7 +13,7 @@ import types
 from typing import Type
 
-import torch.nn as nn
+# import torch.nn as nn
 
 from .norm import (
     GroupNorm,
@@ -26,7 +31,7 @@     SimpleNormFp32,
     SimpleNorm2dFp32,
 )
-from torchvision.ops.misc import FrozenBatchNorm2d
+# from torchvision.ops.misc import FrozenBatchNorm2d
 
 _NORM_MAP = dict(
     batchnorm=nn.BatchNorm2d,
@@ -53,7 +58,7 @@ 
 def create_norm_layer(layer_name, num_features, **kwargs):
     layer = get_norm_layer(layer_name)
-    layer_instance = layer(num_features, **kwargs)
+    layer_instance = layer(num_features, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return layer_instance
 
 
@@ -77,5 +82,5 @@         norm_layer = norm_layer
 
     if norm_kwargs:
-        norm_layer = functools.partial(norm_layer, **norm_kwargs)  # bind/rebind args
+        norm_layer = functools.partial(norm_layer, **norm_kwargs)  # bind/rebind args; 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return norm_layer
