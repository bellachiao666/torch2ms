--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ NormAct (Normalization + Activation Layer) Factory
 
 Create norm + act combo modules that attempt to be backwards compatible with separate norm + act
@@ -98,9 +103,9 @@         **kwargs,
 ):
     layer = get_norm_act_layer(layer_name, act_layer=act_layer)
-    layer_instance = layer(num_features, apply_act=apply_act, **kwargs)
+    layer_instance = layer(num_features, apply_act=apply_act, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     if jit:
-        layer_instance = torch.jit.script(layer_instance)
+        layer_instance = ms.jit(layer_instance)
     return layer_instance
 
 
@@ -140,6 +145,6 @@         # In the future, may force use of `apply_act` with `act_layer` arg bound to relevant NormAct types
         norm_act_kwargs.setdefault('act_layer', act_layer)
     if norm_act_kwargs:
-        norm_act_layer = functools.partial(norm_act_layer, **norm_act_kwargs)  # bind/rebind args
+        norm_act_layer = functools.partial(norm_act_layer, **norm_act_kwargs)  # bind/rebind args; 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     return norm_act_layer
