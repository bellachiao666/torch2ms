--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 ECA module from ECAnet
 
@@ -36,14 +41,13 @@ from typing import Optional, Tuple, Type, Union
 import math
 
-from torch import nn
-import torch.nn.functional as F
+# from torch import nn
 
 from .create_act import create_act_layer
 from .helpers import make_divisible
 
 
-class EcaModule(nn.Module):
+class EcaModule(msnn.Cell):
     """Constructs an ECA module.
 
     Args:
@@ -64,8 +68,8 @@             kernel_size: int = 3,
             gamma: float = 2,
             beta: float = 1,
-            act_layer: Optional[Type[nn.Module]] = None,
-            gate_layer: Union[str, Type[nn.Module]] = 'sigmoid',
+            act_layer: Optional[Type[msnn.Cell]] = None,
+            gate_layer: Union[str, Type[msnn.Cell]] = 'sigmoid',
             rd_ratio: float = 1/8,
             rd_channels: Optional[int] = None,
             rd_divisor: int = 8,
@@ -86,16 +90,16 @@             if rd_channels is None:
                 rd_channels = make_divisible(channels * rd_ratio, divisor=rd_divisor)
             act_layer = act_layer or nn.ReLU
-            self.conv = nn.Conv1d(1, rd_channels, kernel_size=1, padding=0, bias=True, **dd)
+            self.conv = nn.Conv1d(1, rd_channels, kernel_size=1, padding=0, bias=True, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
             self.act = create_act_layer(act_layer)
-            self.conv2 = nn.Conv1d(rd_channels, 1, kernel_size=kernel_size, padding=padding, bias=True, **dd)
+            self.conv2 = nn.Conv1d(rd_channels, 1, kernel_size=kernel_size, padding=padding, bias=True, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         else:
-            self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=padding, bias=False, **dd)
+            self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=padding, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
             self.act = None
             self.conv2 = None
         self.gate = create_act_layer(gate_layer)
 
-    def forward(self, x):
+    def construct(self, x):
         y = x.mean((2, 3)).view(x.shape[0], 1, -1)  # view for 1d conv
         y = self.conv(y)
         if self.conv2 is not None:
@@ -108,7 +112,7 @@ EfficientChannelAttn = EcaModule  # alias
 
 
-class CecaModule(nn.Module):
+class CecaModule(msnn.Cell):
     """Constructs a circular ECA module.
 
     ECA module where the conv uses circular padding rather than zero padding.
@@ -138,8 +142,8 @@             kernel_size: int = 3,
             gamma: float = 2,
             beta: float = 1,
-            act_layer: Optional[nn.Module] = None,
-            gate_layer: Union[str, Type[nn.Module]] = 'sigmoid',
+            act_layer: Optional[msnn.Cell] = None,
+            gate_layer: Union[str, Type[msnn.Cell]] = 'sigmoid',
             device=None,
             dtype=None,
     ):
@@ -155,13 +159,13 @@         # see https://github.com/pytorch/pytorch/pull/17240
         # implement manual circular padding
         self.padding = (kernel_size - 1) // 2
-        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=0, bias=has_act, **dd)
+        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=0, bias=has_act, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.gate = create_act_layer(gate_layer)
 
-    def forward(self, x):
+    def construct(self, x):
         y = x.mean((2, 3)).view(x.shape[0], 1, -1)
         # Manually implement circular padding, F.pad does not seemed to be bugged
-        y = F.pad(y, (self.padding, self.padding), mode='circular')
+        y = nn.functional.pad(y, (self.padding, self.padding), mode = 'circular')
         y = self.conv(y)
         y = self.gate(y).view(x.shape[0], -1, 1, 1)
         return x * y.expand_as(x)
