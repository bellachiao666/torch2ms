--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Halo Self Attention
 
 Paper: `Scaling Local Self-Attention for Parameter Efficient Visual Backbones`
@@ -18,9 +23,8 @@ """
 from typing import List, Optional, Tuple, Union
 
-import torch
-from torch import nn
-import torch.nn.functional as F
+# import torch
+# from torch import nn
 
 from .helpers import make_divisible
 from .weight_init import trunc_normal_
@@ -46,8 +50,8 @@     x = x.reshape(-1, W, rel_size)
 
     # pad to shift from relative to absolute indexing
-    x_pad = F.pad(x, [0, 1]).flatten(1)
-    x_pad = F.pad(x_pad, [0, rel_size - W])
+    x_pad = ms.Tensor.flatten(1)
+    x_pad = nn.functional.pad(x_pad, [0, rel_size - W])
 
     # reshape and slice out the padded elements
     x_pad = x_pad.reshape(-1, W + 1, rel_size)
@@ -58,7 +62,7 @@     return x.permute(permute_mask)
 
 
-class PosEmbedRel(nn.Module):
+class PosEmbedRel(msnn.Cell):
     """ Relative Position Embedding
     As per: https://gist.github.com/aravindsrinivas/56359b79f0ce4449bcb04ab4b56a57a2
     Originally from: `Attention Augmented Convolutional Networks` - https://arxiv.org/abs/1904.09925
@@ -86,16 +90,16 @@         self.dim_head = dim_head
         self.scale = scale
 
-        self.height_rel = nn.Parameter(torch.empty(win_size * 2 - 1, dim_head, **dd))
-        self.width_rel = nn.Parameter(torch.empty(win_size * 2 - 1, dim_head, **dd))
+        self.height_rel = ms.Parameter(mint.empty(win_size * 2 - 1, dim_head, **dd))
+        self.width_rel = ms.Parameter(mint.empty(win_size * 2 - 1, dim_head, **dd))
 
         self.reset_parameters()
 
     def reset_parameters(self):
-        torch.nn.init.normal_(self.height_rel, std=self.scale)
-        torch.nn.init.normal_(self.width_rel, std=self.scale)
-
-    def forward(self, q):
+        torch.nn.init.normal_(self.height_rel, std=self.scale)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch.nn.init.normal_(self.width_rel, std=self.scale)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, q):
         B, BB, HW, _ = q.shape
 
         # relative logits in width dimension.
@@ -111,7 +115,7 @@         return rel_logits
 
 
-class HaloAttn(nn.Module):
+class HaloAttn(msnn.Cell):
     """ Halo Attention
 
     Paper: `Scaling Local Self-Attention for Parameter Efficient Visual Backbones`
@@ -192,7 +196,7 @@             **dd,
         )
 
-        self.pool = nn.AvgPool2d(2, 2) if use_avg_pool else nn.Identity()
+        self.pool = nn.AvgPool2d(2, 2) if use_avg_pool else msnn.Identity()
 
         self.reset_parameters()
 
@@ -203,7 +207,7 @@         trunc_normal_(self.pos_embed.height_rel, std=self.scale)
         trunc_normal_(self.pos_embed.width_rel, std=self.scale)
 
-    def forward(self, x):
+    def construct(self, x):
         B, C, H, W = x.shape
         _assert(H % self.block_size == 0, '')
         _assert(W % self.block_size == 0, '')
@@ -224,10 +228,10 @@         # Generate overlapping windows for kv. This approach is good for GPU and CPU. However, unfold() is not
         # lowered for PyTorch XLA so it will be very slow. See code at bottom of file for XLA friendly approach.
         # FIXME figure out how to switch impl between this and conv2d if XLA being used.
-        kv = F.pad(kv, [self.halo_size, self.halo_size, self.halo_size, self.halo_size])
+        kv = nn.functional.pad(kv, [self.halo_size, self.halo_size, self.halo_size, self.halo_size])
         kv = kv.unfold(2, self.win_size, self.block_size).unfold(3, self.win_size, self.block_size).reshape(
             B * self.num_heads, self.dim_head_qk + self.dim_head_v, num_blocks, -1).permute(0, 2, 3, 1)
-        k, v = torch.split(kv, [self.dim_head_qk, self.dim_head_v], dim=-1)
+        k, v = mint.split(kv, [self.dim_head_qk, self.dim_head_v], dim = -1)
         # B * num_heads, num_blocks, win_size ** 2, dim_head_qk or dim_head_v
 
         if self.scale_pos_embed:
