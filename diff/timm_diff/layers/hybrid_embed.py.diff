--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Image to Patch Hybird Embedding Layer
 
 Hacked together by / Copyright 2020 Ross Wightman
@@ -6,9 +11,8 @@ import math
 from typing import List, Optional, Tuple, Union
 
-import torch
-from torch import nn as nn
-import torch.nn.functional as F
+# import torch
+# from torch import nn as nn
 
 from .format import Format, nchw_to
 from .helpers import to_2tuple
@@ -18,16 +22,16 @@ _logger = logging.getLogger(__name__)
 
 
-class HybridEmbed(nn.Module):
+class HybridEmbed(msnn.Cell):
     """ CNN Feature Map Embedding
     Extract feature map from CNN, flatten, project to embedding dim.
     """
     output_fmt: Format
-    dynamic_img_pad: torch.jit.Final[bool]
+    dynamic_img_pad: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
-            backbone: nn.Module,
+            backbone: msnn.Cell,
             img_size: Union[int, Tuple[int, int]] = 224,
             patch_size: Union[int, Tuple[int, int]] = 1,
             feature_size: Optional[Union[int, Tuple[int, int]]] = None,
@@ -45,7 +49,7 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        assert isinstance(backbone, nn.Module)
+        assert isinstance(backbone, msnn.Cell)
         self.backbone = backbone
         self.in_chans = in_chans
         (
@@ -62,7 +66,7 @@             feature_size=feature_size,
             feature_ratio=feature_ratio,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         if output_fmt is not None:
             self.flatten = False
@@ -84,11 +88,11 @@                 stride=patch_size,
                 bias=bias,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，需手动确认参数映射;
         else:
             assert self.feature_dim == embed_dim, \
                 f'The feature dim ({self.feature_dim} must match embed dim ({embed_dim}) when projection disabled.'
-            self.proj = nn.Identity()
+            self.proj = msnn.Identity()
 
     def _init_backbone(
             self,
@@ -109,7 +113,7 @@                 if training:
                     self.backbone.eval()
                 # FIXME whatif meta device?
-                o = self.backbone(torch.zeros(1, self.in_chans, img_size[0], img_size[1], device=device, dtype=dtype))
+                o = self.backbone(mint.zeros(1, self.in_chans, img_size[0], img_size[1], device=device, dtype=dtype))
                 if isinstance(o, (list, tuple)):
                     o = o[-1]  # last feature if backbone outputs list/tuple of features
                 feature_size = o.shape[-2:]
@@ -145,14 +149,7 @@             assert isinstance(self.proj, nn.Conv2d), 'HybridEmbed must have a projection layer to change patch size.'
             with torch.no_grad():
                 new_proj = nn.Conv2d(
-                    self.proj.in_channels,
-                    self.proj.out_channels,
-                    kernel_size=new_patch_size,
-                    stride=new_patch_size,
-                    bias=self.proj.bias is not None,
-                    device=self.proj.device,
-                    dtype=self.proj.dtype,
-                )
+                    self.proj.in_channels, self.proj.out_channels, kernel_size = new_patch_size, stride = new_patch_size, bias = self.proj.bias is not None, dtype = self.proj.dtype)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device' (position 9);
                 new_proj.weight.copy_(resample_patch_embed(self.proj.weight, new_patch_size, verbose=True))
                 if self.proj.bias is not None:
                     new_proj.bias.copy_(self.proj.bias)
@@ -197,14 +194,14 @@         else:
             return feat_size[0] // self.patch_size[0], feat_size[1] // self.patch_size[1]
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True):
         if hasattr(self.backbone, 'set_grad_checkpointing'):
             self.backbone.set_grad_checkpointing(enable=enable)
         elif hasattr(self.backbone, 'grad_checkpointing'):
             self.backbone.grad_checkpointing = enable
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.backbone(x)
         if isinstance(x, (list, tuple)):
             x = x[-1]  # last feature if backbone outputs list/tuple of features
@@ -212,7 +209,7 @@         if self.dynamic_img_pad:
             pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]
             pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]
-            x = F.pad(x, (0, pad_w, 0, pad_h))
+            x = nn.functional.pad(x, (0, pad_w, 0, pad_h))
         x = self.proj(x)
         if self.flatten:
             x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC
@@ -227,7 +224,7 @@     """
     def __init__(
             self,
-            backbone: nn.Module,
+            backbone: msnn.Cell,
             img_size: Union[int, Tuple[int, int]] = 224,
             patch_size: Union[int, Tuple[int, int]] = 1,
             feature_size: Optional[Union[int, Tuple[int, int]]] = None,
@@ -253,14 +250,14 @@             dtype=dtype,
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True):
         if hasattr(self.backbone, 'set_grad_checkpointing'):
             self.backbone.set_grad_checkpointing(enable=enable)
         elif hasattr(self.backbone, 'grad_checkpointing'):
             self.backbone.grad_checkpointing = enable
 
-    def forward(self, x) -> Tuple[torch.Tensor, List[int]]:
+    def forward(self, x) -> Tuple[ms.Tensor, List[int]]:
         x = self.backbone(x)
         if isinstance(x, (list, tuple)):
             x = x[-1]  # last feature if backbone outputs list/tuple of features
