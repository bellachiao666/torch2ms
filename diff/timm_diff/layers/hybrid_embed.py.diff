--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Image to Patch Hybird Embedding Layer
 
 Hacked together by / Copyright 2020 Ross Wightman
@@ -6,9 +11,8 @@ import math
 from typing import List, Optional, Tuple, Union
 
-import torch
-from torch import nn as nn
-import torch.nn.functional as F
+# import torch
+# from torch import nn as nn
 
 from .format import Format, nchw_to
 from .helpers import to_2tuple
@@ -18,13 +22,14 @@ _logger = logging.getLogger(__name__)
 
 
-class HybridEmbed(nn.Module):
+class HybridEmbed(msnn.Cell):
     """ CNN Feature Map Embedding
     Extract feature map from CNN, flatten, project to embedding dim.
     """
     output_fmt: Format
     dynamic_img_pad: torch.jit.Final[bool]
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             backbone: nn.Module,
@@ -88,7 +93,7 @@         else:
             assert self.feature_dim == embed_dim, \
                 f'The feature dim ({self.feature_dim} must match embed dim ({embed_dim}) when projection disabled.'
-            self.proj = nn.Identity()
+            self.proj = msnn.Identity()
 
     def _init_backbone(
             self,
@@ -109,7 +114,7 @@                 if training:
                     self.backbone.eval()
                 # FIXME whatif meta device?
-                o = self.backbone(torch.zeros(1, self.in_chans, img_size[0], img_size[1], device=device, dtype=dtype))
+                o = self.backbone(mint.zeros(size = (1, self.in_chans, img_size[0], img_size[1]), dtype = dtype))  # 'torch.zeros':没有对应的mindspore参数 'device' (position 4);
                 if isinstance(o, (list, tuple)):
                     o = o[-1]  # last feature if backbone outputs list/tuple of features
                 feature_size = o.shape[-2:]
@@ -145,14 +150,7 @@             assert isinstance(self.proj, nn.Conv2d), 'HybridEmbed must have a projection layer to change patch size.'
             with torch.no_grad():
                 new_proj = nn.Conv2d(
-                    self.proj.in_channels,
-                    self.proj.out_channels,
-                    kernel_size=new_patch_size,
-                    stride=new_patch_size,
-                    bias=self.proj.bias is not None,
-                    device=self.proj.device,
-                    dtype=self.proj.dtype,
-                )
+                    self.proj.in_channels, self.proj.out_channels, kernel_size = new_patch_size, stride = new_patch_size, bias = self.proj.bias is not None, dtype = self.proj.dtype)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device' (position 9);
                 new_proj.weight.copy_(resample_patch_embed(self.proj.weight, new_patch_size, verbose=True))
                 if self.proj.bias is not None:
                     new_proj.bias.copy_(self.proj.bias)
@@ -204,7 +202,7 @@         elif hasattr(self.backbone, 'grad_checkpointing'):
             self.backbone.grad_checkpointing = enable
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.backbone(x)
         if isinstance(x, (list, tuple)):
             x = x[-1]  # last feature if backbone outputs list/tuple of features
@@ -212,7 +210,7 @@         if self.dynamic_img_pad:
             pad_h = (self.patch_size[0] - H % self.patch_size[0]) % self.patch_size[0]
             pad_w = (self.patch_size[1] - W % self.patch_size[1]) % self.patch_size[1]
-            x = F.pad(x, (0, pad_w, 0, pad_h))
+            x = nn.functional.pad(x, (0, pad_w, 0, pad_h))
         x = self.proj(x)
         if self.flatten:
             x = x.flatten(2).transpose(1, 2)  # NCHW -> NLC
@@ -225,6 +223,7 @@     """ CNN Feature Map Embedding
     Extract feature map from CNN, flatten, project to embedding dim.
     """
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             backbone: nn.Module,
