--- pytorch+++ mindspore@@ -1,5 +1,9 @@-import torch
-from torch import nn as nn
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# from torch import nn as nn
 
 try:
     from inplace_abn.functions import inplace_abn, inplace_abn_sync
@@ -19,7 +23,7 @@ 
 
 @register_notrace_module
-class InplaceAbn(nn.Module):
+class InplaceAbn(msnn.Cell):
     """Activated Batch Normalization
 
     This gathers a BatchNorm and an activation function in a single module
@@ -66,7 +70,7 @@                     self.act_name = 'elu'
                 elif act_layer == nn.LeakyReLU:
                     self.act_name = 'leaky_relu'
-                elif act_layer is None or act_layer == nn.Identity:
+                elif act_layer is None or act_layer == msnn.Identity:
                     self.act_name = 'identity'
                 else:
                     assert False, f'Invalid act layer {act_layer.__name__} for IABN'
@@ -74,23 +78,23 @@             self.act_name = 'identity'
         self.act_param = act_param
         if self.affine:
-            self.weight = nn.Parameter(torch.ones(num_features))
-            self.bias = nn.Parameter(torch.zeros(num_features))
+            self.weight = ms.Parameter(mint.ones(num_features))
+            self.bias = ms.Parameter(mint.zeros(num_features))
         else:
             self.register_parameter('weight', None)
             self.register_parameter('bias', None)
-        self.register_buffer('running_mean', torch.zeros(num_features))
-        self.register_buffer('running_var', torch.ones(num_features))
+        self.register_buffer('running_mean', mint.zeros(num_features))
+        self.register_buffer('running_var', mint.ones(num_features))
         self.reset_parameters()
 
     def reset_parameters(self):
-        nn.init.constant_(self.running_mean, 0)
-        nn.init.constant_(self.running_var, 1)
+        nn.init.constant_(self.running_mean, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.constant_(self.running_var, 1)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.affine:
-            nn.init.constant_(self.weight, 1)
-            nn.init.constant_(self.bias, 0)
+            nn.init.constant_(self.weight, 1)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(self.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
-    def forward(self, x):
+    def construct(self, x):
         output = inplace_abn(
             x, self.weight, self.bias, self.running_mean, self.running_var,
             self.training, self.momentum, self.eps, self.act_name, self.act_param)
