--- pytorch+++ mindspore@@ -1,8 +1,12 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Linear layer (alternate definition)
 """
-import torch
-import torch.nn.functional as F
-from torch import nn as nn
+# import torch
+# from torch import nn as nn
 
 
 class Linear(nn.Linear):
@@ -11,9 +15,9 @@     Wraps torch.nn.Linear to support AMP + torchscript usage by manually casting
     weight & bias to input.dtype to work around an issue w/ torch.addmm in this use case.
     """
-    def forward(self, input: torch.Tensor) -> torch.Tensor:
+    def forward(self, input: ms.Tensor) -> ms.Tensor:
         if torch.jit.is_scripting():
             bias = self.bias.to(dtype=input.dtype) if self.bias is not None else None
-            return F.linear(input, self.weight.to(dtype=input.dtype), bias=bias)
+            return nn.functional.linear(input, self.weight.to(dtype=input.dtype), bias = bias)
         else:
-            return F.linear(input, self.weight, self.bias)
+            return nn.functional.linear(input, self.weight, self.bias)
