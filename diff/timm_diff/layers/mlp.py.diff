--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ MLP module w/ dropout and configurable activation layer
 
 Hacked together by / Copyright 2020 Ross Wightman
@@ -5,13 +10,13 @@ from functools import partial
 from typing import Optional, Type, Union, Tuple
 
-from torch import nn as nn
+# from torch import nn as nn
 
 from .grn import GlobalResponseNorm
 from .helpers import to_2tuple
 
 
-class Mlp(nn.Module):
+class Mlp(msnn.Cell):
     """ MLP as used in Vision Transformer, MLP-Mixer and related networks
 
     NOTE: When use_conv=True, expects 2D NCHW tensors, otherwise N*C expected.
@@ -21,8 +26,8 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             bias: Union[bool, Tuple[bool, bool]] = True,
             drop: Union[float, Tuple[float, float]] = 0.,
             use_conv: bool = False,
@@ -40,21 +45,21 @@         self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0], **dd)
         self.act = act_layer()
         self.drop1 = nn.Dropout(drop_probs[0])
-        self.norm = norm_layer(hidden_features, **dd) if norm_layer is not None else nn.Identity()
+        self.norm = norm_layer(hidden_features, **dd) if norm_layer is not None else msnn.Identity()
         self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1], **dd)
         self.drop2 = nn.Dropout(drop_probs[1])
 
-    def forward(self, x):
-        x = self.fc1(x)
-        x = self.act(x)
-        x = self.drop1(x)
-        x = self.norm(x)
-        x = self.fc2(x)
-        x = self.drop2(x)
-        return x
-
-
-class GluMlp(nn.Module):
+    def construct(self, x):
+        x = self.fc1(x)
+        x = self.act(x)
+        x = self.drop1(x)
+        x = self.norm(x)
+        x = self.fc2(x)
+        x = self.drop2(x)
+        return x
+
+
+class GluMlp(msnn.Cell):
     """ MLP w/ GLU style gating
     See: https://arxiv.org/abs/1612.08083, https://arxiv.org/abs/2002.05202
 
@@ -65,8 +70,8 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.Sigmoid,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.Sigmoid,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             bias: Union[bool, Tuple[bool, bool]] = True,
             drop: Union[float, Tuple[float, float]] = 0.,
             use_conv: bool = False,
@@ -88,17 +93,17 @@         self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0], **dd)
         self.act = act_layer()
         self.drop1 = nn.Dropout(drop_probs[0])
-        self.norm = norm_layer(hidden_features // 2, **dd) if norm_layer is not None else nn.Identity()
+        self.norm = norm_layer(hidden_features // 2, **dd) if norm_layer is not None else msnn.Identity()
         self.fc2 = linear_layer(hidden_features // 2, out_features, bias=bias[1], **dd)
         self.drop2 = nn.Dropout(drop_probs[1])
 
     def init_weights(self):
         # override init of fc1 w/ gate portion set to weight near zero, bias=1
         if self.fc1.bias is not None:
-            nn.init.ones_(self.fc1.bias[self.fc1.bias.shape[0] // 2:])
-        nn.init.normal_(self.fc1.weight[self.fc1.weight.shape[0] // 2:], std=1e-6)
-
-    def forward(self, x):
+            nn.init.ones_(self.fc1.bias[self.fc1.bias.shape[0] // 2:])  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.normal_(self.fc1.weight[self.fc1.weight.shape[0] // 2:], std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x):
         x = self.fc1(x)
         x1, x2 = x.chunk(2, dim=self.chunk_dim)
         x = x1 * self.act(x2) if self.gate_last else self.act(x1) * x2
@@ -112,7 +117,7 @@ SwiGLUPacked = partial(GluMlp, act_layer=nn.SiLU, gate_last=False)
 
 
-class SwiGLU(nn.Module):
+class SwiGLU(msnn.Cell):
     """ SwiGLU
     NOTE: GluMLP above can implement SwiGLU, but this impl has split fc1 and
     better matches some other common impl which makes mapping checkpoints simpler.
@@ -122,8 +127,8 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.SiLU,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.SiLU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             bias: Union[bool, Tuple[bool, bool]] = True,
             drop: Union[float, Tuple[float, float]] = 0.,
             align_to: int = 0,
@@ -144,17 +149,17 @@         self.fc1_x = nn.Linear(in_features, hidden_features, bias=bias[0], **dd)
         self.act = act_layer()
         self.drop1 = nn.Dropout(drop_probs[0])
-        self.norm = norm_layer(hidden_features, **dd) if norm_layer is not None else nn.Identity()
+        self.norm = norm_layer(hidden_features, **dd) if norm_layer is not None else msnn.Identity()
         self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1], **dd)
         self.drop2 = nn.Dropout(drop_probs[1])
 
     def init_weights(self):
         # override init of fc1 w/ gate portion set to weight near zero, bias=1
         if self.fc1_g.bias is not None:
-            nn.init.ones_(self.fc1_g.bias)
-        nn.init.normal_(self.fc1_g.weight, std=1e-6)
-
-    def forward(self, x):
+            nn.init.ones_(self.fc1_g.bias)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.normal_(self.fc1_g.weight, std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x):
         x_gate = self.fc1_g(x)
         x = self.fc1_x(x)
         x = self.act(x_gate) * x
@@ -165,7 +170,7 @@         return x
 
 
-class GatedMlp(nn.Module):
+class GatedMlp(msnn.Cell):
     """ MLP as used in gMLP
     """
     def __init__(
@@ -173,9 +178,9 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Optional[Type[nn.Module]] = None,
-            gate_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
+            gate_layer: Optional[Type[msnn.Cell]] = None,
             bias: Union[bool, Tuple[bool, bool]] = True,
             drop: Union[float, Tuple[float, float]] = 0.,
             device=None,
@@ -196,12 +201,12 @@             self.gate = gate_layer(hidden_features, **dd)
             hidden_features = hidden_features // 2  # FIXME base reduction on gate property?
         else:
-            self.gate = nn.Identity()
-        self.norm = norm_layer(hidden_features, **dd) if norm_layer is not None else nn.Identity()
+            self.gate = msnn.Identity()
+        self.norm = norm_layer(hidden_features, **dd) if norm_layer is not None else msnn.Identity()
         self.fc2 = nn.Linear(hidden_features, out_features, bias=bias[1], **dd)
         self.drop2 = nn.Dropout(drop_probs[1])
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.fc1(x)
         x = self.act(x)
         x = self.drop1(x)
@@ -212,7 +217,7 @@         return x
 
 
-class ConvMlp(nn.Module):
+class ConvMlp(msnn.Cell):
     """ MLP using 1x1 convs that keeps spatial dims (for 2D NCHW tensors)
     """
     def __init__(
@@ -220,8 +225,8 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             bias: Union[bool, Tuple[bool, bool]] = True,
             drop: float = 0.,
             device=None,
@@ -234,12 +239,12 @@         bias = to_2tuple(bias)
 
         self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=bias[0], **dd)
-        self.norm = norm_layer(hidden_features, **dd) if norm_layer else nn.Identity()
+        self.norm = norm_layer(hidden_features, **dd) if norm_layer else msnn.Identity()
         self.act = act_layer()
         self.drop = nn.Dropout(drop)
         self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=bias[1], **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.fc1(x)
         x = self.norm(x)
         x = self.act(x)
@@ -248,7 +253,7 @@         return x
 
 
-class GlobalResponseNormMlp(nn.Module):
+class GlobalResponseNormMlp(msnn.Cell):
     """ MLP w/ Global Response Norm (see grn.py), nn.Linear or 1x1 Conv2d
 
     NOTE: Intended for '2D' NCHW (use_conv=True) or NHWC (use_conv=False, channels-last) tensor layouts
@@ -258,7 +263,7 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             bias: Union[bool, Tuple[bool, bool]] = True,
             drop: Union[float, Tuple[float, float]] = 0.,
             use_conv: bool = False,
@@ -280,7 +285,7 @@         self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1], **dd)
         self.drop2 = nn.Dropout(drop_probs[1])
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.fc1(x)
         x = self.act(x)
         x = self.drop1(x)
