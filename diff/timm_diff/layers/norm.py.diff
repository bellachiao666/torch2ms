--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Normalization layers and wrappers
 
 Norm layer definitions that support fast norm and consistent channel arg order (always first arg).
@@ -7,9 +12,9 @@ import numbers
 from typing import Tuple
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from .fast_norm import (
     is_fast_norm,
@@ -23,7 +28,7 @@ )
 
 try:
-    from torch.nn.functional import rms_norm
+    # from torch.nn.functional import rms_norm
 except ImportError:
     from .fast_norm import rms_norm
 
@@ -47,7 +52,7 @@         if self._fast_norm:
             return fast_group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
         else:
-            return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
+            return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)  # 'torch.nn.functional.group_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 class GroupNorm1(nn.GroupNorm):
@@ -60,11 +65,11 @@         super().__init__(1, num_channels, **kwargs)
         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         if self._fast_norm:
             return fast_group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
         else:
-            return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
+            return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)  # 'torch.nn.functional.group_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 class LayerNorm(nn.LayerNorm):
@@ -82,11 +87,11 @@         super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)
         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         if self._fast_norm:
             x = fast_layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
         else:
-            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
+            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return x
 
 
@@ -103,10 +108,10 @@     ):
         super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         weight = self.weight.float() if self.weight is not None else None
         bias = self.bias.float() if self.bias is not None else None
-        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)
+        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.functional.layer_norm.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return x
 
 
@@ -124,12 +129,12 @@         super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)
         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         x = x.permute(0, 2, 3, 1)
         if self._fast_norm:
             x = fast_layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
         else:
-            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
+            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = x.permute(0, 3, 1, 2)
         return x
 
@@ -146,16 +151,16 @@     ):
         super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         x = x.permute(0, 2, 3, 1)
         weight = self.weight.float() if self.weight is not None else None
         bias = self.bias.float() if self.bias is not None else None
-        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)
+        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.functional.layer_norm.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = x.permute(0, 3, 1, 2)
         return x
 
 
-def _is_contiguous(tensor: torch.Tensor) -> bool:
+def _is_contiguous(tensor: ms.Tensor) -> bool:
     # jit is oh so lovely :/
     if torch.jit.is_scripting():
         return tensor.is_contiguous()
@@ -163,17 +168,17 @@         return tensor.is_contiguous(memory_format=torch.contiguous_format)
 
 
-def _layer_norm_cf(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float):
-    s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)
-    x = (x - u) * torch.rsqrt(s + eps)
+def _layer_norm_cf(x: ms.Tensor, weight: ms.Tensor, bias: ms.Tensor, eps: float):
+    s, u = mint.var_mean(x, dim=1, unbiased=False, keepdim=True)
+    x = (x - u) * mint.rsqrt(s + eps)
     x = x * weight[:, None, None] + bias[:, None, None]
     return x
 
 
-def _layer_norm_cf_sqm(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float):
+def _layer_norm_cf_sqm(x: ms.Tensor, weight: ms.Tensor, bias: ms.Tensor, eps: float):
     u = x.mean(dim=1, keepdim=True)
     s = ((x * x).mean(dim=1, keepdim=True) - (u * u)).clamp(0)
-    x = (x - u) * torch.rsqrt(s + eps)
+    x = (x - u) * mint.rsqrt(s + eps)
     x = x * weight.view(1, -1, 1, 1) + bias.view(1, -1, 1, 1)
     return x
 
@@ -190,16 +195,15 @@     def __init__(self, num_channels: int, eps: float = 1e-6):
         super().__init__(num_channels, eps=eps)
 
-    def forward(self, x) -> torch.Tensor:
+    def forward(self, x) -> ms.Tensor:
         if _is_contiguous(x):
-            x = F.layer_norm(
-                x.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)
+            x = mint.permute(0, 3, 1, 2)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             x = _layer_norm_cf(x, self.weight, self.bias, self.eps)
         return x
 
 
-class RmsNorm(nn.Module):
+class RmsNorm(msnn.Cell):
     """ RmsNorm w/ fast (apex) norm if available
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine', '_fast_norm']
@@ -228,27 +232,27 @@         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         # NOTE fast norm fallback needs our rms norm impl, so both paths through here.
         # Since there is no built-in PyTorch impl, always uses APEX RmsNorm if installed.
         if self._fast_norm:
             x = fast_rms_norm(x, self.normalized_shape, self.weight, self.eps)
         else:
-            x = rms_norm(x, self.normalized_shape, self.weight, self.eps)
-        return x
-
-
-class RmsNormFp32(nn.Module):
+            x = rms_norm(x, self.normalized_shape, self.weight, self.eps)  # 'torch.nn.functional.rms_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        return x
+
+
+class RmsNormFp32(msnn.Cell):
     """ RmsNorm w/ fast (apex) norm if available
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']
@@ -275,23 +279,23 @@         self.elementwise_affine = affine
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         weight = self.weight.float() if self.weight is not None else None
-        x = rms_norm(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)
-        return x
-
-
-class RmsNorm2d(nn.Module):
+        x = rms_norm(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)  # 'torch.nn.functional.rms_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.functional.rms_norm.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        return x
+
+
+class RmsNorm2d(msnn.Cell):
     """ RmsNorm2D for NCHW tensors, w/ fast apex or cast norm if available
 
     NOTE: It's currently (2025-05-10) faster to use an eager 2d kernel that does reduction
@@ -324,17 +328,17 @@         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         # NOTE fast norm fallback needs our rms norm impl, so both paths through here.
         # Since there is no built-in PyTorch impl, always use APEX RmsNorm if is installed.
         if self._fast_norm:
@@ -344,7 +348,7 @@         return x
 
 
-class RmsNorm2dFp32(nn.Module):
+class RmsNorm2dFp32(msnn.Cell):
     """ RmsNorm2D for NCHW tensors, w/ fast apex or cast norm if available
 
     NOTE: It's currently (2025-05-10) faster to use an eager 2d kernel that does reduction
@@ -375,23 +379,23 @@         self.elementwise_affine = affine
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         weight = self.weight.float() if self.weight is not None else None
         x = rms_norm2d(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)
         return x
 
 
-class SimpleNorm(nn.Module):
+class SimpleNorm(msnn.Cell):
     """ SimpleNorm (x / std(x))
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine', '_fast_norm']
@@ -420,17 +424,17 @@         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         if self._fast_norm:
             x = fast_simple_norm(x, self.normalized_shape, self.weight, self.eps)
         else:
@@ -438,7 +442,7 @@         return x
 
 
-class SimpleNormFp32(nn.Module):
+class SimpleNormFp32(msnn.Cell):
     """ SimpleNorm (x / std(x))
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']
@@ -465,23 +469,23 @@         self.elementwise_affine = affine
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         weight = self.weight.float() if self.weight is not None else None
         x = simple_norm(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)
         return x
 
 
-class SimpleNorm2d(nn.Module):
+class SimpleNorm2d(msnn.Cell):
     """ SimpleNorm for NCHW tensors
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine', '_fast_norm']
@@ -510,17 +514,17 @@         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = x.permute(0, 2, 3, 1)
         if self._fast_norm:
             x = fast_simple_norm(x, self.normalized_shape, self.weight, self.eps)
@@ -530,7 +534,7 @@         return x
 
 
-class SimpleNorm2dFp32(nn.Module):
+class SimpleNorm2dFp32(msnn.Cell):
     """ SimpleNorm for NCHW tensors
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']
@@ -557,17 +561,17 @@         self.elementwise_affine = affine
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = x.permute(0, 2, 3, 1)
         weight = self.weight.float() if self.weight is not None else None
         x = simple_norm(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)
