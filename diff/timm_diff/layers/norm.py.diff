--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Normalization layers and wrappers
 
 Norm layer definitions that support fast norm and consistent channel arg order (always first arg).
@@ -7,9 +12,9 @@ import numbers
 from typing import Tuple
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from .fast_norm import (
     is_fast_norm,
@@ -23,13 +28,13 @@ )
 
 try:
-    from torch.nn.functional import rms_norm
+    # from torch.nn.functional import rms_norm
 except ImportError:
     from .fast_norm import rms_norm
 
 
 class GroupNorm(nn.GroupNorm):
-    _fast_norm: torch.jit.Final[bool]
+    _fast_norm: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -40,37 +45,37 @@             **kwargs,
     ):
         # NOTE num_channels is swapped to first arg for consistency in swapping norm layers with BN
-        super().__init__(num_groups, num_channels, eps=eps, affine=affine, **kwargs)
+        super().__init__(num_groups, num_channels, eps=eps, affine=affine, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
     def forward(self, x):
         if self._fast_norm:
             return fast_group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
         else:
-            return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
+            return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)  # 'torch.nn.functional.group_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 class GroupNorm1(nn.GroupNorm):
     """ Group Normalization with 1 group.
     Input: tensor in shape [B, C, *]
     """
-    _fast_norm: torch.jit.Final[bool]
+    _fast_norm: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(self, num_channels: int, **kwargs):
-        super().__init__(1, num_channels, **kwargs)
-        self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        super().__init__(1, num_channels, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
+
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         if self._fast_norm:
             return fast_group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
         else:
-            return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
+            return F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)  # 'torch.nn.functional.group_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 class LayerNorm(nn.LayerNorm):
     """ LayerNorm w/ fast norm option
     """
-    _fast_norm: torch.jit.Final[bool]
+    _fast_norm: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -79,14 +84,14 @@             affine: bool = True,
             **kwargs,
     ):
-        super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)
-        self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
+
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         if self._fast_norm:
             x = fast_layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
         else:
-            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
+            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return x
 
 
@@ -101,18 +106,18 @@             affine: bool = True,
             **kwargs,
     ):
-        super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         weight = self.weight.float() if self.weight is not None else None
         bias = self.bias.float() if self.bias is not None else None
-        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)
+        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.functional.layer_norm.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return x
 
 
 class LayerNorm2d(nn.LayerNorm):
     """ LayerNorm for channels of '2D' spatial NCHW tensors """
-    _fast_norm: torch.jit.Final[bool]
+    _fast_norm: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -121,15 +126,15 @@             affine: bool = True,
             **kwargs,
     ):
-        super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)
-        self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
+
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         x = x.permute(0, 2, 3, 1)
         if self._fast_norm:
             x = fast_layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
         else:
-            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
+            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = x.permute(0, 3, 1, 2)
         return x
 
@@ -144,36 +149,37 @@             affine: bool = True,
             **kwargs,
     ):
-        super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         x = x.permute(0, 2, 3, 1)
         weight = self.weight.float() if self.weight is not None else None
         bias = self.bias.float() if self.bias is not None else None
-        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)
+        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.functional.layer_norm.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = x.permute(0, 3, 1, 2)
         return x
 
 
-def _is_contiguous(tensor: torch.Tensor) -> bool:
+def _is_contiguous(tensor: ms.Tensor) -> bool:
     # jit is oh so lovely :/
+    # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     if torch.jit.is_scripting():
         return tensor.is_contiguous()
     else:
         return tensor.is_contiguous(memory_format=torch.contiguous_format)
 
 
-def _layer_norm_cf(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float):
-    s, u = torch.var_mean(x, dim=1, unbiased=False, keepdim=True)
-    x = (x - u) * torch.rsqrt(s + eps)
+def _layer_norm_cf(x: ms.Tensor, weight: ms.Tensor, bias: ms.Tensor, eps: float):
+    s, u = mint.var_mean(x, dim=1, unbiased=False, keepdim=True)
+    x = (x - u) * mint.rsqrt(s + eps)
     x = x * weight[:, None, None] + bias[:, None, None]
     return x
 
 
-def _layer_norm_cf_sqm(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float):
+def _layer_norm_cf_sqm(x: ms.Tensor, weight: ms.Tensor, bias: ms.Tensor, eps: float):
     u = x.mean(dim=1, keepdim=True)
     s = ((x * x).mean(dim=1, keepdim=True) - (u * u)).clamp(0)
-    x = (x - u) * torch.rsqrt(s + eps)
+    x = (x - u) * mint.rsqrt(s + eps)
     x = x * weight.view(1, -1, 1, 1) + bias.view(1, -1, 1, 1)
     return x
 
@@ -190,16 +196,15 @@     def __init__(self, num_channels: int, eps: float = 1e-6):
         super().__init__(num_channels, eps=eps)
 
-    def forward(self, x) -> torch.Tensor:
+    def forward(self, x) -> ms.Tensor:
         if _is_contiguous(x):
-            x = F.layer_norm(
-                x.permute(0, 2, 3, 1), self.normalized_shape, self.weight, self.bias, self.eps).permute(0, 3, 1, 2)
+            x = mint.permute(0, 3, 1, 2)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             x = _layer_norm_cf(x, self.weight, self.bias, self.eps)
         return x
 
 
-class RmsNorm(nn.Module):
+class RmsNorm(msnn.Cell):
     """ RmsNorm w/ fast (apex) norm if available
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine', '_fast_norm']
@@ -228,27 +233,27 @@         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         # NOTE fast norm fallback needs our rms norm impl, so both paths through here.
         # Since there is no built-in PyTorch impl, always uses APEX RmsNorm if installed.
         if self._fast_norm:
             x = fast_rms_norm(x, self.normalized_shape, self.weight, self.eps)
         else:
-            x = rms_norm(x, self.normalized_shape, self.weight, self.eps)
-        return x
-
-
-class RmsNormFp32(nn.Module):
+            x = rms_norm(x, self.normalized_shape, self.weight, self.eps)  # 'torch.nn.functional.rms_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        return x
+
+
+class RmsNormFp32(msnn.Cell):
     """ RmsNorm w/ fast (apex) norm if available
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']
@@ -275,23 +280,23 @@         self.elementwise_affine = affine
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         weight = self.weight.float() if self.weight is not None else None
-        x = rms_norm(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)
-        return x
-
-
-class RmsNorm2d(nn.Module):
+        x = rms_norm(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)  # 'torch.nn.functional.rms_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.functional.rms_norm.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        return x
+
+
+class RmsNorm2d(msnn.Cell):
     """ RmsNorm2D for NCHW tensors, w/ fast apex or cast norm if available
 
     NOTE: It's currently (2025-05-10) faster to use an eager 2d kernel that does reduction
@@ -324,17 +329,17 @@         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         # NOTE fast norm fallback needs our rms norm impl, so both paths through here.
         # Since there is no built-in PyTorch impl, always use APEX RmsNorm if is installed.
         if self._fast_norm:
@@ -344,7 +349,7 @@         return x
 
 
-class RmsNorm2dFp32(nn.Module):
+class RmsNorm2dFp32(msnn.Cell):
     """ RmsNorm2D for NCHW tensors, w/ fast apex or cast norm if available
 
     NOTE: It's currently (2025-05-10) faster to use an eager 2d kernel that does reduction
@@ -375,23 +380,23 @@         self.elementwise_affine = affine
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         weight = self.weight.float() if self.weight is not None else None
         x = rms_norm2d(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)
         return x
 
 
-class SimpleNorm(nn.Module):
+class SimpleNorm(msnn.Cell):
     """ SimpleNorm (x / std(x))
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine', '_fast_norm']
@@ -420,17 +425,17 @@         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         if self._fast_norm:
             x = fast_simple_norm(x, self.normalized_shape, self.weight, self.eps)
         else:
@@ -438,7 +443,7 @@         return x
 
 
-class SimpleNormFp32(nn.Module):
+class SimpleNormFp32(msnn.Cell):
     """ SimpleNorm (x / std(x))
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']
@@ -465,23 +470,23 @@         self.elementwise_affine = affine
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         weight = self.weight.float() if self.weight is not None else None
         x = simple_norm(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)
         return x
 
 
-class SimpleNorm2d(nn.Module):
+class SimpleNorm2d(msnn.Cell):
     """ SimpleNorm for NCHW tensors
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine', '_fast_norm']
@@ -510,17 +515,17 @@         self._fast_norm = is_fast_norm()  # can't script unless we have these flags here (no globals)
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = x.permute(0, 2, 3, 1)
         if self._fast_norm:
             x = fast_simple_norm(x, self.normalized_shape, self.weight, self.eps)
@@ -530,7 +535,7 @@         return x
 
 
-class SimpleNorm2dFp32(nn.Module):
+class SimpleNorm2dFp32(msnn.Cell):
     """ SimpleNorm for NCHW tensors
     """
     __constants__ = ['normalized_shape', 'eps', 'elementwise_affine']
@@ -557,17 +562,17 @@         self.elementwise_affine = affine
 
         if self.elementwise_affine:
-            self.weight = nn.Parameter(torch.empty(self.normalized_shape, **dd))
-        else:
-            self.register_parameter('weight', None)
-
-        self.reset_parameters()
-
-    def reset_parameters(self) -> None:
-        if self.elementwise_affine:
-            nn.init.ones_(self.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.weight = ms.Parameter(mint.empty(self.normalized_shape, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.register_parameter('weight', None)
+
+        self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        if self.elementwise_affine:
+            nn.init.ones_(self.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = x.permute(0, 2, 3, 1)
         weight = self.weight.float() if self.weight is not None else None
         x = simple_norm(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)
