--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Normalization + Activation Layers
 
 Provides Norm+Act fns for standard PyTorch norm layers such as
@@ -14,10 +19,10 @@ """
 from typing import Any, Dict, List, Optional, Type, Union
 
-import torch
-from torch import nn as nn
-from torch.nn import functional as F
-from torchvision.ops.misc import FrozenBatchNorm2d
+# import torch
+# from torch import nn as nn
+# from torch.nn import functional as F
+# from torchvision.ops.misc import FrozenBatchNorm2d
 
 from ._fx import register_notrace_module
 from .create_act import create_act_layer
@@ -34,7 +39,7 @@ from .typing import LayerType
 
 try:
-    from torch.nn.functional import rms_norm
+    # from torch.nn.functional import rms_norm
 except ImportError:
     from .fast_norm import rms_norm
 
@@ -44,13 +49,13 @@         act_kwargs: Dict[str, Any] = None,
         inplace: Optional[bool] = False,
         apply_act: bool = True,
-) -> nn.Module:
+) -> msnn.Cell:
     act_kwargs = act_kwargs or {}
     act_kwargs.setdefault('inplace', inplace)
     act = None
     if apply_act:
         act = create_act_layer(act_layer, **act_kwargs)
-    return nn.Identity() if act is None else act
+    return msnn.Identity() if act is None else act
 
 
 @register_notrace_module
@@ -72,7 +77,7 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -95,7 +100,7 @@                 affine=affine,
                 track_running_stats=track_running_stats,
             )
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
         self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
 
     def forward(self, x):
@@ -133,17 +138,8 @@         passed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are
         used for normalization (i.e. in eval mode when buffers are not None).
         """
-        x = F.batch_norm(
-            x,
-            # If buffers are not to be tracked, ensure that they won't be updated
-            self.running_mean if not self.training or self.track_running_stats else None,
-            self.running_var if not self.training or self.track_running_stats else None,
-            self.weight,
-            self.bias,
-            bn_training,
-            exponential_average_factor,
-            self.eps,
-        )
+        x = nn.functional.batch_norm(
+            x, self.running_mean if not self.training or self.track_running_stats else None, self.running_var if not self.training or self.track_running_stats else None, self.weight, self.bias, bn_training, exponential_average_factor, self.eps)
         x = self.drop(x)
         x = self.act(x)
         return x
@@ -155,7 +151,7 @@     # This is a quick workaround to support SyncBatchNorm for timm BatchNormAct2d layers
     # but ONLY when used in conjunction with the timm conversion function below.
     # Do not create this module directly or use the PyTorch conversion function.
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         x = super().forward(x)  # SyncBN doesn't work with torchscript anyways, so this is fine
         if hasattr(self, "drop"):
             x = self.drop(x)
@@ -183,14 +179,8 @@             module_output.drop = module.drop
         else:
             # convert standard BatchNorm layers
-            module_output = torch.nn.SyncBatchNorm(
-                module.num_features,
-                module.eps,
-                module.momentum,
-                module.affine,
-                module.track_running_stats,
-                process_group,
-            )
+            module_output = nn.SyncBatchNorm(
+                module.num_features, module.eps, module.momentum, module.affine, module.track_running_stats, process_group)
         if module.affine:
             with torch.no_grad():
                 module_output.weight = module.weight
@@ -208,7 +198,7 @@ 
 
 @register_notrace_module
-class FrozenBatchNormAct2d(torch.nn.Module):
+class FrozenBatchNormAct2d(msnn.Cell):
     """
     BatchNormAct2d where the batch statistics and the affine parameters are fixed
 
@@ -225,19 +215,19 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.eps = eps
-        self.register_buffer("weight", torch.ones(num_features, **dd))
-        self.register_buffer("bias", torch.zeros(num_features, **dd))
-        self.register_buffer("running_mean", torch.zeros(num_features, **dd))
-        self.register_buffer("running_var", torch.ones(num_features, **dd))
-
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
+        self.register_buffer("weight", mint.ones(num_features, **dd))
+        self.register_buffer("bias", mint.zeros(num_features, **dd))
+        self.register_buffer("running_mean", mint.zeros(num_features, **dd))
+        self.register_buffer("running_var", mint.ones(num_features, **dd))
+
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
         self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
 
     def _load_from_state_dict(
@@ -258,7 +248,7 @@             state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs
         )
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         # move reshapes to the beginning
         # to make it fuser-friendly
         w = self.weight.reshape(1, -1, 1, 1)
@@ -301,8 +291,8 @@         res.eps = module.eps
         res.drop = module.drop
         res.act = module.act
-    elif isinstance(module, (torch.nn.modules.batchnorm.BatchNorm2d, torch.nn.modules.batchnorm.SyncBatchNorm)):
-        res = FrozenBatchNorm2d(module.num_features)
+    elif isinstance(module, (nn.BatchNorm2d, nn.SyncBatchNorm)):
+        res = FrozenBatchNorm2d(module.num_features)  # 'torchvision.ops.misc.FrozenBatchNorm2d' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         res.num_features = module.num_features
         res.affine = module.affine
         if module.affine:
@@ -345,7 +335,7 @@         res.drop = module.drop
         res.act = module.act
     elif isinstance(module, FrozenBatchNorm2d):
-        res = torch.nn.BatchNorm2d(module.num_features)
+        res = nn.BatchNorm2d(module.num_features)
         if module.affine:
             res.weight.data = module.weight.data.clone().detach()
             res.bias.data = module.bias.data.clone().detach()
@@ -382,7 +372,7 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -394,7 +384,7 @@             device=device,
             dtype=dtype,
         )
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
         self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
 
         self._fast_norm = is_fast_norm()
@@ -403,7 +393,7 @@         if self._fast_norm:
             x = fast_group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
         else:
-            x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
+            x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)  # 'torch.nn.functional.group_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = self.drop(x)
         x = self.act(x)
         return x
@@ -421,12 +411,12 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
         super().__init__(1, num_channels, eps=eps, affine=affine, device=device, dtype=dtype)
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
         self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
 
         self._fast_norm = is_fast_norm()
@@ -435,7 +425,7 @@         if self._fast_norm:
             x = fast_group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
         else:
-            x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)
+            x = F.group_norm(x, self.num_groups, self.weight, self.bias, self.eps)  # 'torch.nn.functional.group_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = self.drop(x)
         x = self.act(x)
         return x
@@ -444,6 +434,8 @@ class LayerNormAct(nn.LayerNorm):
     _fast_norm: torch.jit.Final[bool]
 
+    # 'torch.Size' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             normalization_shape: Union[int, List[int], torch.Size],
@@ -453,11 +445,11 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             **kwargs,
     ):
         super().__init__(normalization_shape, eps=eps, elementwise_affine=affine, **kwargs)
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
         self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
 
         self._fast_norm = is_fast_norm()
@@ -466,7 +458,7 @@         if self._fast_norm:
             x = fast_layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
         else:
-            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
+            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = self.drop(x)
         x = self.act(x)
         return x
@@ -474,6 +466,8 @@ 
 class LayerNormActFp32(nn.LayerNorm):
 
+    # 'torch.Size' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             normalization_shape: Union[int, List[int], torch.Size],
@@ -483,17 +477,17 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             **kwargs,
     ):
         super().__init__(normalization_shape, eps=eps, elementwise_affine=affine, **kwargs)
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
         self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
 
     def forward(self, x):
         weight = self.weight.float() if self.weight is not None else None
         bias = self.bias.float() if self.bias is not None else None
-        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)
+        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.functional.layer_norm.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = self.drop(x)
         x = self.act(x)
         return x
@@ -511,11 +505,11 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             **kwargs,
     ):
         super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
         self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
         self._fast_norm = is_fast_norm()
 
@@ -524,7 +518,7 @@         if self._fast_norm:
             x = fast_layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
         else:
-            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
+            x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = x.permute(0, 3, 1, 2)
         x = self.drop(x)
         x = self.act(x)
@@ -542,18 +536,18 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             **kwargs,
     ):
         super().__init__(num_channels, eps=eps, elementwise_affine=affine, **kwargs)
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
         self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
 
     def forward(self, x):
         x = x.permute(0, 2, 3, 1)
         weight = self.weight.float() if self.weight is not None else None
         bias = self.bias.float() if self.bias is not None else None
-        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)
+        x = F.layer_norm(x.float(), self.normalized_shape, weight, bias, self.eps).to(x.dtype)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.functional.layer_norm.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = x.permute(0, 3, 1, 2)
         x = self.drop(x)
         x = self.act(x)
@@ -576,19 +570,19 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             **kwargs,
     ):
         super().__init__(channels=num_channels, eps=eps, affine=affine, **kwargs)
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
         self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
         self._fast_norm = is_fast_norm()
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         if self._fast_norm:
             x = fast_rms_norm(x, self.normalized_shape, self.weight, self.eps)
         else:
-            x = rms_norm(x, self.normalized_shape, self.weight, self.eps)
+            x = rms_norm(x, self.normalized_shape, self.weight, self.eps)  # 'torch.nn.functional.rms_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = self.drop(x)
         x = self.act(x)
         return x
@@ -610,16 +604,16 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             **kwargs,
     ):
         super().__init__(channels=num_channels, eps=eps, affine=affine, **kwargs)
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
-        self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
+        self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
+
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         weight = self.weight.float() if self.weight is not None else None
-        x = rms_norm(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)
+        x = rms_norm(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)  # 'torch.nn.functional.rms_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.functional.rms_norm.to' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         x = self.drop(x)
         x = self.act(x)
         return x
@@ -641,15 +635,15 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             **kwargs,
     ):
         super().__init__(channels=num_channels, eps=eps, affine=affine, **kwargs)
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
         self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
         self._fast_norm = is_fast_norm()
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         if self._fast_norm:
             x = fast_rms_norm2d(x, self.normalized_shape, self.weight, self.eps)
         else:
@@ -675,14 +669,14 @@             act_layer: LayerType = nn.ReLU,
             act_kwargs: Dict[str, Any] = None,
             inplace: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             **kwargs,
     ):
         super().__init__(channels=num_channels, eps=eps, affine=affine, **kwargs)
-        self.drop = drop_layer() if drop_layer is not None else nn.Identity()
-        self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.drop = drop_layer() if drop_layer is not None else msnn.Identity()
+        self.act = _create_act(act_layer, act_kwargs=act_kwargs, inplace=inplace, apply_act=apply_act)
+
+    def forward(self, x: ms.Tensor) -> ms.Tensor:
         weight = self.weight.float() if self.weight is not None else None
         x = rms_norm2d(x.float(), self.normalized_shape, weight, self.eps).to(x.dtype)
         x = self.drop(x)
