--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Non-Local Attention Pooling Layers
 
 A collection of global pooling layers that go beyond simple avg/max pooling.
@@ -14,14 +19,14 @@ """
 from typing import Optional, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from .config import use_fused_attn
 
 
-class LsePlus2d(nn.Module):
+class LsePlus2d(msnn.Cell):
     """LogSumExp (LSE) Pooling for 2D inputs.
 
     A smooth approximation to max pooling that provides a learnable interpolation between
@@ -49,22 +54,22 @@         """
         super().__init__()
         if r_learnable:
-            self.r = nn.Parameter(torch.tensor(r, device=device, dtype=dtype))
-        else:
-            self.register_buffer('r', torch.tensor(r, device=device, dtype=dtype))
+            self.r = ms.Parameter(ms.Tensor(r, device=device, dtype=dtype))
+        else:
+            self.register_buffer('r', ms.Tensor(r, device=device, dtype=dtype))
         self.flatten = flatten
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        x_max = F.adaptive_max_pool2d(x, 1)
-        exp_x = torch.exp(self.r * (x - x_max))
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
+        x_max = F.adaptive_max_pool2d(x, 1)  # 'torch.nn.functional.adaptive_max_pool2d' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        exp_x = mint.exp(self.r * (x - x_max))
         sum_exp = exp_x.mean(dim=(2, 3), keepdim=True)
-        out = x_max + (1.0 / self.r) * torch.log(sum_exp)
+        out = x_max + (1.0 / self.r) * mint.log(sum_exp)
         if self.flatten:
             out = out.flatten(1)
         return out
 
 
-class LsePlus1d(nn.Module):
+class LsePlus1d(msnn.Cell):
     """LogSumExp (LSE) Pooling for sequence (NLC) inputs.
 
     A smooth approximation to max pooling that provides a learnable interpolation between
@@ -86,20 +91,20 @@         """
         super().__init__()
         if r_learnable:
-            self.r = nn.Parameter(torch.tensor(r, device=device, dtype=dtype))
-        else:
-            self.register_buffer('r', torch.tensor(r, device=device, dtype=dtype))
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.r = ms.Parameter(ms.Tensor(r, device=device, dtype=dtype))
+        else:
+            self.register_buffer('r', ms.Tensor(r, device=device, dtype=dtype))
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         # x: (B, N, C)
         x_max = x.max(dim=1, keepdim=True).values
-        exp_x = torch.exp(self.r * (x - x_max))
+        exp_x = mint.exp(self.r * (x - x_max))
         sum_exp = exp_x.mean(dim=1, keepdim=True)
-        out = x_max + (1.0 / self.r) * torch.log(sum_exp)
+        out = x_max + (1.0 / self.r) * mint.log(sum_exp)
         return out.squeeze(1)  # (B, C)
 
 
-class SimPool2d(nn.Module):
+class SimPool2d(msnn.Cell):
     """SimPool: Simple Attention-Based Pooling for 2D (NCHW) inputs.
 
     From 'Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?'
@@ -108,7 +113,7 @@     Uses GAP as query initialization and applies cross-attention between the GAP query
     and spatial features to produce a weighted pooled representation.
     """
-    fused_attn: torch.jit.Final[bool]
+    fused_attn: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -117,7 +122,7 @@             qkv_bias: bool = False,
             qk_norm: bool = False,
             gamma: Optional[float] = None,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -141,17 +146,17 @@         self.fused_attn = use_fused_attn()
 
         norm_layer = norm_layer or nn.LayerNorm
-        self.norm = norm_layer(dim, **dd)
-        self.q = nn.Linear(dim, dim, bias=qkv_bias, **dd)
-        self.k = nn.Linear(dim, dim, bias=qkv_bias, **dd)
+        self.norm = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.q = nn.Linear(dim, dim, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.k = nn.Linear(dim, dim, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         if qk_norm:
-            self.q_norm = norm_layer(self.head_dim, **dd)
-            self.k_norm = norm_layer(self.head_dim, **dd)
-        else:
-            self.q_norm = nn.Identity()
-            self.k_norm = nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.q_norm = norm_layer(self.head_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.k_norm = norm_layer(self.head_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.q_norm = msnn.Identity()
+            self.k_norm = msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         B, C, H, W = x.shape
         N = H * W
 
@@ -176,7 +181,7 @@             v_min = v.amin(dim=-2, keepdim=True)
             v_shifted = v - v_min + 1e-6
             if self.fused_attn:
-                attn_out = F.scaled_dot_product_attention(q, k, v_shifted.pow(self.gamma))
+                attn_out = F.scaled_dot_product_attention(q, k, v_shifted.pow(self.gamma))  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
                 attn = (q * self.scale) @ k.transpose(-2, -1)
                 attn = attn.softmax(dim=-1)
@@ -184,7 +189,7 @@             out = attn_out.pow(1.0 / self.gamma)
         else:
             if self.fused_attn:
-                out = F.scaled_dot_product_attention(q, k, v)
+                out = F.scaled_dot_product_attention(q, k, v)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
                 attn = (q * self.scale) @ k.transpose(-2, -1)
                 attn = attn.softmax(dim=-1)
@@ -195,7 +200,7 @@         return out
 
 
-class SimPool1d(nn.Module):
+class SimPool1d(msnn.Cell):
     """SimPool: Simple Attention-Based Pooling for sequence (NLC) inputs.
 
     From 'Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?'
@@ -204,7 +209,7 @@     Uses GAP as query initialization and applies cross-attention between the GAP query
     and sequence tokens to produce a weighted pooled representation.
     """
-    fused_attn: torch.jit.Final[bool]
+    fused_attn: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -213,7 +218,7 @@             qkv_bias: bool = False,
             qk_norm: bool = False,
             gamma: Optional[float] = None,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -236,17 +241,17 @@         self.fused_attn = use_fused_attn()
 
         norm_layer = norm_layer or nn.LayerNorm
-        self.norm = norm_layer(dim, **dd)
-        self.q = nn.Linear(dim, dim, bias=qkv_bias, **dd)
-        self.k = nn.Linear(dim, dim, bias=qkv_bias, **dd)
+        self.norm = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.q = nn.Linear(dim, dim, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.k = nn.Linear(dim, dim, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         if qk_norm:
-            self.q_norm = norm_layer(self.head_dim, **dd)
-            self.k_norm = norm_layer(self.head_dim, **dd)
-        else:
-            self.q_norm = nn.Identity()
-            self.k_norm = nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.q_norm = norm_layer(self.head_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.k_norm = norm_layer(self.head_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.q_norm = msnn.Identity()
+            self.k_norm = msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         B, N, C = x.shape
 
         # GAP as query initialization
@@ -267,7 +272,7 @@             v_min = v.amin(dim=-2, keepdim=True)
             v_shifted = v - v_min + 1e-6
             if self.fused_attn:
-                attn_out = F.scaled_dot_product_attention(q, k, v_shifted.pow(self.gamma))
+                attn_out = F.scaled_dot_product_attention(q, k, v_shifted.pow(self.gamma))  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
                 attn = (q * self.scale) @ k.transpose(-2, -1)
                 attn = attn.softmax(dim=-1)
@@ -275,7 +280,7 @@             out = attn_out.pow(1.0 / self.gamma)
         else:
             if self.fused_attn:
-                out = F.scaled_dot_product_attention(q, k, v)
+                out = F.scaled_dot_product_attention(q, k, v)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
                 attn = (q * self.scale) @ k.transpose(-2, -1)
                 attn = attn.softmax(dim=-1)
