--- pytorch+++ mindspore@@ -1,12 +1,14 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Padding Helpers
 
 Hacked together by / Copyright 2020 Ross Wightman
 """
 import math
 from typing import List, Tuple, Union
-
-import torch
-import torch.nn.functional as F
 
 from .helpers import to_2tuple
 
@@ -15,15 +17,15 @@ def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> Union[int, List[int]]:
     if any([isinstance(v, (tuple, list)) for v in [kernel_size, stride, dilation]]):
         kernel_size, stride, dilation = to_2tuple(kernel_size), to_2tuple(stride), to_2tuple(dilation)
-        return [get_padding(*a) for a in zip(kernel_size, stride, dilation)]
+        return [get_padding(*a) for a in zip(kernel_size, stride, dilation)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2
     return padding
 
 
 # Calculate asymmetric TensorFlow-like 'SAME' padding for a convolution
 def get_same_padding(x: int, kernel_size: int, stride: int, dilation: int):
-    if isinstance(x, torch.Tensor):
-        return torch.clamp(((x / stride).ceil() - 1) * stride + (kernel_size - 1) * dilation + 1 - x, min=0)
+    if isinstance(x, ms.Tensor):
+        return mint.clamp(((x / stride).ceil() - 1) * stride + (kernel_size - 1) * dilation + 1 - x, min=0)
     else:
         return max((math.ceil(x / stride) - 1) * stride + (kernel_size - 1) * dilation + 1 - x, 0)
 
@@ -32,7 +34,7 @@ def is_static_pad(kernel_size: int, stride: int = 1, dilation: int = 1, **_):
     if any([isinstance(v, (tuple, list)) for v in [kernel_size, stride, dilation]]):
         kernel_size, stride, dilation = to_2tuple(kernel_size), to_2tuple(stride), to_2tuple(dilation)
-        return all([is_static_pad(*a) for a in zip(kernel_size, stride, dilation)])
+        return all([is_static_pad(*a) for a in zip(kernel_size, stride, dilation)])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return stride == 1 and (dilation * (kernel_size - 1)) % 2 == 0
 
 
@@ -60,7 +62,7 @@     ih, iw = x.size()[-2:]
     pad_h = get_same_padding(ih, kernel_size[0], stride[0], dilation[0])
     pad_w = get_same_padding(iw, kernel_size[1], stride[1], dilation[1])
-    x = F.pad(x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2), value=value)
+    x = nn.functional.pad(x, (pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2), value=value)
     return x
 
 
@@ -73,7 +75,7 @@             # TF compatible 'SAME' padding, has a performance and GPU memory allocation impact
             if is_static_pad(kernel_size, **kwargs):
                 # static case, no extra overhead
-                padding = get_padding(kernel_size, **kwargs)
+                padding = get_padding(kernel_size, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
                 # dynamic 'SAME' padding, has runtime/GPU memory overhead
                 padding = 0
@@ -83,5 +85,5 @@             padding = 0
         else:
             # Default to PyTorch style 'same'-ish symmetric padding
-            padding = get_padding(kernel_size, **kwargs)
+            padding = get_padding(kernel_size, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return padding, dynamic
