--- pytorch+++ mindspore@@ -1,16 +1,18 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from typing import Optional, Tuple, Union
-
-import torch
-import torch.nn as nn
 
 
 def patch_dropout_forward(
-        x: torch.Tensor,
+        x: ms.Tensor,
         prob: float,
         num_prefix_tokens: int,
         ordered: bool,
         training: bool,
-) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
+) -> Tuple[ms.Tensor, Optional[ms.Tensor]]:
     """
     Common forward logic for patch dropout.
 
@@ -35,7 +37,7 @@     B = x.shape[0]
     L = x.shape[1]
     num_keep = max(1, int(L * (1. - prob)))
-    keep_indices = torch.argsort(torch.randn(B, L, device=x.device), dim=-1)[:, :num_keep]
+    keep_indices = mint.argsort(mint.randn(size = (B, L)), dim = -1)[:, :num_keep]  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
 
     if ordered:
         # NOTE does not need to maintain patch order in typical transformer use,
@@ -45,12 +47,12 @@     x = x.gather(1, keep_indices.unsqueeze(-1).expand((-1, -1) + x.shape[2:]))
 
     if prefix_tokens is not None:
-        x = torch.cat((prefix_tokens, x), dim=1)
+        x = mint.cat((prefix_tokens, x), dim = 1)
 
     return x, keep_indices
 
 
-class PatchDropout(nn.Module):
+class PatchDropout(msnn.Cell):
     """
     Patch Dropout without returning indices.
     https://arxiv.org/abs/2212.00794 and https://arxiv.org/pdf/2208.07220
@@ -68,7 +70,7 @@         self.num_prefix_tokens = num_prefix_tokens  # exclude CLS token (or other prefix tokens)
         self.ordered = ordered
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         output, _ = patch_dropout_forward(
             x,
             self.prob,
@@ -79,7 +81,7 @@         return output
 
 
-class PatchDropoutWithIndices(nn.Module):
+class PatchDropoutWithIndices(msnn.Cell):
     """
     Patch Dropout that returns both output and keep indices.
     https://arxiv.org/abs/2212.00794 and https://arxiv.org/pdf/2208.07220
@@ -97,7 +99,7 @@         self.num_prefix_tokens = num_prefix_tokens  # exclude CLS token (or other prefix tokens)
         self.ordered = ordered
 
-    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
+    def construct(self, x: ms.Tensor) -> Tuple[ms.Tensor, Optional[ms.Tensor]]:
         return patch_dropout_forward(
             x,
             self.prob,
