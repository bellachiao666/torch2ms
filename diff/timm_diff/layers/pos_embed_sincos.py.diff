--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Sin-cos, fourier, rotary position embedding modules and functions
 
 Hacked together by / Copyright 2022 Ross Wightman
@@ -5,14 +10,15 @@ import math
 from typing import List, Tuple, Optional, Union
 
-import torch
-from torch import nn as nn
+# import torch
 
 from ._fx import register_notrace_function
 from .grid import ndgrid
 from .trace_utils import _assert
 
 
+# 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def pixel_freq_bands(
         num_bands: int,
         max_freq: float = 224.,
@@ -20,23 +26,28 @@         device: Optional[torch.device] = None,
 ):
     if linear_bands:
-        bands = torch.linspace(1.0, max_freq / 2, num_bands, dtype=torch.float32, device=device)
+        bands = mint.linspace(1.0, max_freq / 2, num_bands, dtype=ms.float32, device=device)
     else:
-        bands = 2 ** torch.linspace(0, math.log(max_freq, 2) - 1, num_bands, dtype=torch.float32, device=device)
+        bands = 2 ** mint.linspace(0, math.log(max_freq, 2) - 1, num_bands, dtype=ms.float32, device=device)
     return bands * torch.pi
 
 
+# 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def freq_bands(
         num_bands: int,
         temperature: float = 10000.,
         step: int = 2,
         device: Optional[torch.device] = None,
-) -> torch.Tensor:
-    exp = torch.arange(0, num_bands, step, dtype=torch.int64, device=device).to(torch.float32) / num_bands
+) -> ms.Tensor:
+    exp = mint.arange(0, num_bands, step, dtype=ms.int64, device=device).to(ms.float32) / num_bands
     bands = 1. / (temperature ** exp)
     return bands
 
 
+# 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def build_sincos2d_pos_embed(
         feat_shape: List[int],
         dim: int = 64,
@@ -44,8 +55,8 @@         reverse_coord: bool = False,
         interleave_sin_cos: bool = False,
         device: Optional[torch.device] = None,
-        dtype: torch.dtype = torch.float32,
-) -> torch.Tensor:
+        dtype: torch.dtype = ms.float32,
+) -> ms.Tensor:
     """
 
     Args:
@@ -66,15 +77,15 @@ 
     if reverse_coord:
         feat_shape = feat_shape[::-1]  # stack W, H instead of H, W
-    grid = torch.stack(ndgrid([
-        torch.arange(s, device=device, dtype=torch.int64).to(torch.float32)
+    grid = mint.stack(ndgrid([
+        mint.arange(s, device=device, dtype=ms.int64).to(ms.float32)
         for s in feat_shape
     ])).flatten(1).transpose(0, 1)
     pos2 = grid.unsqueeze(-1) * bands.unsqueeze(0)
     # FIXME add support for unflattened spatial dim?
 
     stack_dim = 2 if interleave_sin_cos else 1  # stack sin, cos, sin, cos  instead of sin sin cos cos
-    pos_emb = torch.stack([torch.sin(pos2), torch.cos(pos2)], dim=stack_dim).flatten(1)
+    pos_emb = mint.stack([mint.sin(pos2), mint.cos(pos2)], dim=stack_dim).flatten(1)
     return pos_emb.to(dtype=dtype)
 
 
@@ -84,9 +95,12 @@     return [seq[1], seq[0]] + list(seq[2:])
 
 
+# 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def build_fourier_pos_embed(
         feat_shape: List[int],
-        bands: Optional[torch.Tensor] = None,
+        bands: Optional[ms.Tensor] = None,
         num_bands: int = 64,
         max_res: int = 224,
         temperature: float = 10000.,
@@ -97,8 +111,8 @@         grid_offset: float = 0.,
         grid_indexing: str = 'ij',
         device: Optional[torch.device] = None,
-        dtype: torch.dtype = torch.float32,
-) -> List[torch.Tensor]:
+        dtype: torch.dtype = ms.float32,
+) -> List[ms.Tensor]:
     """
 
     Args:
@@ -147,12 +161,12 @@ 
     if in_pixels:
         t = [
-            torch.linspace(-1., 1., steps=s, device=device, dtype=torch.float32)
+            mint.linspace(-1., 1., steps=s, device=device, dtype=ms.float32)
             for s in feat_shape
         ]
     else:
         t = [
-            torch.arange(s, device=device, dtype=torch.int64).to(torch.float32) + grid_offset
+            mint.arange(s, device=device, dtype=ms.int64).to(ms.float32) + grid_offset
             for s in feat_shape
         ]
 
@@ -160,7 +174,7 @@         # eva's scheme for resizing rope embeddings (ref shape = pretrain)
         t = [x / f * r for x, f, r in zip(t, feat_shape, ref_feat_shape)]
 
-    grid = torch.stack(torch.meshgrid(t, indexing=grid_indexing), dim=-1)
+    grid = mint.stack(mint.meshgrid(t, indexing=grid_indexing), dim=-1)
     grid = grid.unsqueeze(-1)
     pos = grid * bands
 
@@ -169,7 +183,7 @@     return out
 
 
-class FourierEmbed(nn.Module):
+class FourierEmbed(msnn.Cell):
 
     def __init__(
             self,
@@ -191,7 +205,7 @@             persistent=False,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         B, C = x.shape[:2]
         feat_shape = x.shape[2:]
         emb = build_fourier_pos_embed(
@@ -201,15 +215,15 @@             dtype=x.dtype,
             device=x.device,
         )
-        emb = torch.cat(emb, dim=-1)
+        emb = mint.cat(emb, dim=-1)
         emb = emb.transpose(-1, -2).flatten(len(feat_shape))
         batch_expand = (B,) + (-1,) * (x.ndim - 1)
 
         # FIXME support nD
         if self.keep_spatial:
-            x = torch.cat([x, emb.unsqueeze(0).expand(batch_expand).permute(0, 3, 1, 2)], dim=1)
-        else:
-            x = torch.cat([x.permute(0, 2, 3, 1), emb.unsqueeze(0).expand(batch_expand)], dim=-1)
+            x = mint.cat([x, emb.unsqueeze(0).expand(batch_expand).permute(0, 3, 1, 2)], dim=1)
+        else:
+            x = mint.cat([x.permute(0, 2, 3, 1), emb.unsqueeze(0).expand(batch_expand)], dim=-1)
             x = x.reshape(B, feat_shape.numel(), -1)
 
         return x
@@ -218,22 +232,22 @@ def rot(x):
     # x:   [ x0  x1  x2  x3  x4  x5]
     # out: [-x1  x0 -x3  x2 -x5  x4]
-    return torch.stack([-x[..., 1::2], x[..., ::2]], -1).reshape(x.shape)
-
-
-def rope_rotate_half(x: torch.Tensor) -> torch.Tensor:
+    return mint.stack([-x[..., 1::2], x[..., ::2]], -1).reshape(x.shape)
+
+
+def rope_rotate_half(x: ms.Tensor) -> ms.Tensor:
     # x:   [ x0  x1  x2  x3  x4  x5]
     # out: [-x3 -x4 -x5  x0  x1  x2]
     x1, x2 = x.chunk(2, dim=-1)
-    return torch.cat([-x2, x1], dim=-1)
+    return mint.cat([-x2, x1], dim=-1)
 
 
 def apply_rot_embed(
-        x: torch.Tensor,
-        sin_emb: torch.Tensor,
-        cos_emb: torch.Tensor,
+        x: ms.Tensor,
+        sin_emb: ms.Tensor,
+        cos_emb: ms.Tensor,
         half: bool = False,
-) -> torch.Tensor:
+) -> ms.Tensor:
     # x: [..., D], eg [x0, x1, x2, x3, x4, x5]
     if half:
         # sin: [..., D], eg [sin0, sin1, sin2, sin0, sin1, sin2]
@@ -248,12 +262,12 @@ 
 
 def apply_rot_embed_list(
-        x: List[torch.Tensor],
-        sin_emb: torch.Tensor,
-        cos_emb: torch.Tensor,
+        x: List[ms.Tensor],
+        sin_emb: ms.Tensor,
+        cos_emb: ms.Tensor,
         half: bool = False
-) -> List[torch.Tensor]:
-    if isinstance(x, torch.Tensor):
+) -> List[ms.Tensor]:
+    if isinstance(x, ms.Tensor):
         x = [x]
     # x: [..., D], eg [x0, x1, x2, x3, x4, x5]
     if half:
@@ -269,10 +283,10 @@ 
 
 def apply_rot_embed_cat(
-        x: torch.Tensor,
-        emb: torch.Tensor,
+        x: ms.Tensor,
+        emb: ms.Tensor,
         half: bool = False
-) -> torch.Tensor:
+) -> ms.Tensor:
     sin_emb, cos_emb = emb.chunk(2, -1)
     # x: [..., D], eg [x0, x1, x2, x3, x4, x5]
     if half:
@@ -288,11 +302,11 @@ 
 
 def apply_keep_indices_nlc(
-        x: torch.Tensor,
-        pos_embed: torch.Tensor,
-        keep_indices: torch.Tensor,
+        x: ms.Tensor,
+        pos_embed: ms.Tensor,
+        keep_indices: ms.Tensor,
         pos_embed_has_batch: bool = False,
-) -> torch.Tensor:
+) -> ms.Tensor:
     """ Apply keep indices to different ROPE shapes
 
     Expected pos_embed shapes:
@@ -324,9 +338,12 @@     return pos_embed.gather(-2, keep_indices)
 
 
+# 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def build_rotary_pos_embed(
         feat_shape: List[int],
-        bands: Optional[torch.Tensor] = None,
+        bands: Optional[ms.Tensor] = None,
         dim: int = 64,
         max_res: int = 224,
         temperature: float = 10000.,
@@ -336,7 +353,7 @@         grid_offset: float = 0.,
         grid_indexing: str = 'ij',
         device: Optional[torch.device] = None,
-        dtype: torch.dtype = torch.float32,
+        dtype: torch.dtype = ms.float32,
 ):
     """
 
@@ -380,7 +397,7 @@     return sin_emb, cos_emb
 
 
-class RotaryEmbedding(nn.Module):
+class RotaryEmbedding(msnn.Cell):
     """ Rotary position embedding
 
     NOTE: This is my initial attempt at impl rotary embedding for spatial use, it has not
@@ -452,7 +469,7 @@                 persistent=False,
             )
 
-    def _get_pos_embed_values(self, feat_shape: List[int], device=None, dtype=torch.float32):
+    def _get_pos_embed_values(self, feat_shape: List[int], device=None, dtype=ms.float32):
         emb_sin, emb_cos = build_rotary_pos_embed(
             feat_shape=feat_shape,
             dim=self.dim,
@@ -496,13 +513,13 @@         else:
             assert False, "get_embed() requires pre-computed pos embeds or valid shape w/ pre-computed bands"
 
-    def forward(self, x):
+    def construct(self, x):
         # assuming channel-first tensor where spatial dim are >= 2
         sin_emb, cos_emb = self.get_embed(x.shape[2:])
         return apply_rot_embed(x, sin_emb, cos_emb)
 
 
-class RotaryEmbeddingCat(nn.Module):
+class RotaryEmbeddingCat(msnn.Cell):
     """ Rotary position embedding w/ concatenatd sin & cos
 
     The following impl/resources were referenced for this impl:
@@ -564,7 +581,7 @@                 persistent=False,
             )
 
-    def _get_pos_embed_values(self, feat_shape: List[int], device=None, dtype=torch.float32):
+    def _get_pos_embed_values(self, feat_shape: List[int], device=None, dtype=ms.float32):
         embeds = build_rotary_pos_embed(
             feat_shape=feat_shape,
             dim=self.dim,
@@ -578,7 +595,7 @@             device=device,
             dtype=dtype,
         )
-        return torch.cat(embeds, -1)
+        return mint.cat(embeds, -1)
 
     def update_feat_shape(self, feat_shape: List[int]):
         if self.feat_shape is not None and feat_shape != self.feat_shape:
@@ -602,7 +619,7 @@                 grid_offset=self.grid_offset,
                 grid_indexing=self.grid_indexing,
             )
-            return torch.cat(embeds, -1)
+            return mint.cat(embeds, -1)
         elif self.pos_embed is not None:
             return self.pos_embed
         else:
@@ -612,7 +629,7 @@             self,
             shapes: List[Tuple[int, int]],
             seq_len: Optional[int] = None,
-    ) -> Union[torch.Tensor, List[torch.Tensor]]:
+    ) -> Union[ms.Tensor, List[ms.Tensor]]:
         """Generate ROPE embeddings for multiple grid shapes efficiently.
 
         Computes embeddings for the maximum grid size once, then extracts
@@ -649,10 +666,10 @@ 
         # sin_emb and cos_emb are (max_h * max_w, dim//2)
         # concat and reshape to 2D for slicing
-        rope_embed_2d = torch.cat([sin_emb, cos_emb], dim=-1).view(max_h, max_w, -1)
+        rope_embed_2d = mint.cat([sin_emb, cos_emb], dim=-1).view(max_h, max_w, -1)
 
         if seq_len is not None:
-            flat_embeds = torch.zeros(len(shapes), seq_len, rope_embed_2d.shape[-1]).type_as(sin_emb)
+            flat_embeds = mint.zeros(len(shapes), seq_len, rope_embed_2d.shape[-1]).type_as(sin_emb)
             for i, (h, w) in enumerate(shapes):
                 src_len = h * w
                 flat_embeds[i, :src_len] = rope_embed_2d[:h, :w].reshape(src_len, -1)
@@ -661,7 +678,7 @@             flat_embeds_list = [rope_embed_2d[:h, :w].reshape(h * w, -1) for h, w in shapes]
             return flat_embeds_list
 
-    def forward(self, x):
+    def construct(self, x):
         # assuming channel-first tensor where spatial dim are >= 2
         pos_embed = self.get_embed(x.shape[2:])
         return apply_rot_embed_cat(x, pos_embed)
@@ -675,45 +692,48 @@         rotate: bool = True,
         *,
         device=None,
-        dtype=torch.float32,
-) -> torch.Tensor:
+        dtype=ms.float32,
+) -> ms.Tensor:
     """ Vectorised 2D ROPE frequencies with random rotation for mixed mode ROPE.
     Returns:
          Tensor (2, depth, num_heads, head_dim//2)
     """
     # base magnitudes, shape: (head_dim//4,)
-    mag = 1.0 / (temperature ** (torch.arange(0, head_dim, 4, device=device, dtype=dtype) / head_dim))
+    mag = 1.0 / (temperature ** (mint.arange(0, head_dim, 4, device=device, dtype=dtype) / head_dim))
 
     # (1,1,L) so it broadcasts over both depth and heads
     mag = mag.unsqueeze(0).unsqueeze(0)  # (1,1,L)
 
     # random (or zero) rotation per head *and* per block
     if rotate:
-        angles = torch.rand(depth, num_heads, 1, device=device, dtype=dtype) * 2 * torch.pi
+        angles = mint.rand(depth, num_heads, 1, device=device, dtype=dtype) * 2 * torch.pi
     else:
-        angles = torch.zeros(depth, num_heads, 1, device=device, dtype=dtype)
+        angles = mint.zeros(depth, num_heads, 1, device=device, dtype=dtype)
 
     # build (depth, num_heads, 2·L) == head_dim//2 on the last axis
-    fx = torch.cat([mag * torch.cos(angles), mag * torch.cos(angles + torch.pi / 2)], dim=-1)
-    fy = torch.cat([mag * torch.sin(angles), mag * torch.sin(angles + torch.pi / 2)], dim=-1)
+    fx = mint.cat([mag * mint.cos(angles), mag * mint.cos(angles + torch.pi / 2)], dim=-1)
+    fy = mint.cat([mag * mint.sin(angles), mag * mint.sin(angles + torch.pi / 2)], dim=-1)
 
     # (2, depth, num_heads, head_dim//2)
-    return torch.stack([fx, fy], dim=0)
-
-
+    return mint.stack([fx, fy], dim=0)
+
+
+# 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 @torch.fx.wrap
 @register_notrace_function
 def get_mixed_grid(
         shape: List[int],
         grid_indexing: str = 'ij',
         device: Optional[torch.device] = None,
-        dtype: torch.dtype = torch.float32,
-) -> Tuple[torch.Tensor, torch.Tensor]:
+        dtype: torch.dtype = ms.float32,
+) -> Tuple[ms.Tensor, ms.Tensor]:
     if grid_indexing == 'xy':
         shape = swap_shape_xy(shape)
-    x_pos, y_pos = torch.meshgrid(
-        torch.arange(shape[0], device=device, dtype=torch.float32),
-        torch.arange(shape[1], device=device, dtype=torch.float32),
+    x_pos, y_pos = mint.meshgrid(
+        mint.arange(shape[0], device=device, dtype=ms.float32),
+        mint.arange(shape[1], device=device, dtype=ms.float32),
         indexing=grid_indexing,
     )
     t_x = x_pos.to(dtype).flatten()
@@ -722,10 +742,10 @@ 
 
 def get_mixed_freqs(
-        freqs: torch.Tensor,
-        t_x: torch.Tensor,
-        t_y: torch.Tensor,
-) -> torch.Tensor:
+        freqs: ms.Tensor,
+        t_x: ms.Tensor,
+        t_y: ms.Tensor,
+) -> ms.Tensor:
     """Compute mixed (learnable) frequencies."""
     # Create position indices
     dtype = freqs.dtype
@@ -733,13 +753,13 @@     freqs_x = (t_x.unsqueeze(-1) @ freqs[0].unsqueeze(-2))
     freqs_y = (t_y.unsqueeze(-1) @ freqs[1].unsqueeze(-2))
     combined = freqs_x + freqs_y  # shape: (num_heads, N, dim//4)
-    sin_emb = torch.sin(combined).repeat_interleave(2, -1)  # (N, dim//2)
-    cos_emb = torch.cos(combined).repeat_interleave(2, -1)  # (N, dim//2)
-    rope_embeds = torch.cat([sin_emb, cos_emb], dim=-1)  # (num_heads, H*W, head_dim)
+    sin_emb = mint.sin(combined).repeat_interleave(2, -1)  # (N, dim//2)
+    cos_emb = mint.cos(combined).repeat_interleave(2, -1)  # (N, dim//2)
+    rope_embeds = mint.cat([sin_emb, cos_emb], dim=-1)  # (num_heads, H*W, head_dim)
     return rope_embeds.to(dtype)
 
 
-class RotaryEmbeddingMixed(nn.Module):
+class RotaryEmbeddingMixed(msnn.Cell):
     """Rotary position embedding with depth-dependent learnable frequencies.
 
     This implementation supports mixed (learnable) ROPE. In mixed mode,
@@ -789,7 +809,7 @@             device=device,
             dtype=dtype,
         )  # (2, depth, num_heads, head_dim//2)
-        self.freqs = nn.Parameter(freqs)
+        self.freqs = ms.Parameter(freqs)
 
         if feat_shape is not None:
             # cache pre-computed grid
@@ -816,7 +836,7 @@             self.t_y = t_y.to(self.t_y.device, self.t_y.dtype)
             self.feat_shape = feat_shape
 
-    def get_embed(self, shape: Optional[List[int]] = None) -> torch.Tensor:
+    def get_embed(self, shape: Optional[List[int]] = None) -> ms.Tensor:
         """Generate rotary embeddings for the given spatial shape.
 
         Args:
@@ -842,7 +862,7 @@             self,
             shapes: List[Tuple[int, int]],
             seq_len: Optional[int] = None,
-    ) -> Union[torch.Tensor, List[torch.Tensor]]:
+    ) -> Union[ms.Tensor, List[ms.Tensor]]:
         """Generate ROPE embeddings for multiple grid shapes efficiently.
 
         Computes embeddings for the maximum grid size once, then extracts
@@ -878,7 +898,7 @@         if seq_len is not None:
             # Return padded tensor
             B = len(shapes)
-            padded = torch.zeros(B, depth, num_heads, seq_len, dim, device=self.freqs.device, dtype=self.freqs.dtype)
+            padded = mint.zeros(B, depth, num_heads, seq_len, dim, device=self.freqs.device, dtype=self.freqs.dtype)
             for i, (h, w) in enumerate(shapes):
                 # Slice and flatten
                 embed_slice = max_embed_2d[:, :, :h, :w].reshape(depth, num_heads, h * w, dim)
@@ -894,7 +914,7 @@                 results.append(embed_slice)
             return results
 
-    def forward(self, x):
+    def construct(self, x):
         # assuming channel-first tensor where spatial dim are >= 2
         pos_embed = self.get_embed(x.shape[2:])
         return apply_rot_embed_cat(x, pos_embed)
@@ -904,6 +924,9 @@         return {'freqs'}
 
 
+# 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 @torch.fx.wrap
 @register_notrace_function
 def make_coords_dinov3(
@@ -913,14 +936,14 @@         grid_indexing: str = 'ij',
         grid_offset: float = 0.,
         device: torch.device = 'cpu',
-        dtype: torch.dtype = torch.float32,
-) -> torch.Tensor:
+        dtype: torch.dtype = ms.float32,
+) -> ms.Tensor:
     """Make coordinate grid matching offset and normalization of original.
     Returns: coords with shape (HW, 2) in [-1, 1].
     """
     # 0.5-centered indices with optional offset
-    coords_h = torch.arange(0.5, height, device=device, dtype=torch.float32) + grid_offset
-    coords_w = torch.arange(0.5, width, device=device, dtype=torch.float32) + grid_offset
+    coords_h = mint.arange(0.5, height, device=device, dtype=ms.float32) + grid_offset
+    coords_w = mint.arange(0.5, width, device=device, dtype=ms.float32) + grid_offset
 
     # Normalization denominators
     if normalize_coords == "max":
@@ -945,16 +968,16 @@ 
     # Create grid then map to [-1, 1]
     if grid_indexing == "xy":
-        grid_w, grid_h = torch.meshgrid(coords_w, coords_h, indexing="xy")
-        coords = torch.stack([grid_h, grid_w], dim=-1)  # (H, W, 2) -> (h, w order)
+        grid_w, grid_h = mint.meshgrid(coords_w, coords_h, indexing="xy")
+        coords = mint.stack([grid_h, grid_w], dim=-1)  # (H, W, 2) -> (h, w order)
     else:
-        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing="ij"), dim=-1)  # (H, W, 2)
+        coords = mint.stack(mint.meshgrid(coords_h, coords_w, indexing="ij"), dim=-1)  # (H, W, 2)
     coords = coords.flatten(0, 1)  # (HW, 2)
     coords = 2.0 * coords - 1.0  # (H, W, 2) in [-1, 1]
     return coords
 
 
-class RotaryEmbeddingDinoV3(nn.Module):
+class RotaryEmbeddingDinoV3(msnn.Cell):
     """RoPE for timm DinoV3 port, numerically matching original.
 
     Math is aligned to original DinoV3 RopePositionEmbedding at https://github.com/facebookresearch/dinov3:
@@ -1013,24 +1036,27 @@             self.register_buffer("pos_embed_cached", None, persistent=False)
             self.feat_shape = None
 
-    def _compute_periods(self, device: torch.device = 'cpu', dtype: torch.dtype = torch.float32) -> torch.Tensor:
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def _compute_periods(self, device: torch.device = 'cpu', dtype: torch.dtype = ms.float32) -> ms.Tensor:
         """Construct periods from either min/max or temperature."""
         dim = self.dim // 4
 
         if self.min_period is not None and self.max_period is not None:
-            exponents = torch.linspace(0, 1, dim, device='cpu', dtype=torch.float32)
+            exponents = mint.linspace(0, 1, dim, device='cpu', dtype=ms.float32)
             periods = self.min_period * ((self.max_period / self.min_period) ** exponents)
         else:
             if self.temperature is None:
                 raise ValueError("Provide either min/max periods or `temperature`.")
-            exponents = 2.0 * torch.arange(dim, device='cpu', dtype=torch.float32) / (self.dim // 2)
+            exponents = 2.0 * mint.arange(dim, device='cpu', dtype=ms.float32) / (self.dim // 2)
             periods = self.temperature ** exponents
 
         # NOTE: The original dinv3 model weights have periods downcast to bfloat16 in persistent buffers,
         # loaded models will differ a bit vs timm as periods is not persistent and generated in float32 by default
         return periods.to(device=device, dtype=dtype)
 
-    def _apply_coord_augs(self, coords: torch.Tensor) -> torch.Tensor:
+    def _apply_coord_augs(self, coords: ms.Tensor) -> ms.Tensor:
         """Apply shift/jitter/rescale train time augmentations."""
         if not self.training or not self.aug_active:
             return coords
@@ -1041,7 +1067,7 @@         # Shift per-axis in [-s, +s]
         if self.shift_coords is not None:
             shift = float(self.shift_coords)
-            shift_hw = torch.empty(2, device=device, dtype=dtype).uniform_(-shift, shift)
+            shift_hw = mint.empty(2, device=device, dtype=dtype).uniform_(-shift, shift)
             coords = coords + shift_hw[None, :]
 
         # Jitter: per-axis log-uniform factor in [1/J, J]
@@ -1050,7 +1076,7 @@             if jitter_factor <= 0:
                 raise ValueError("jitter_coords must be > 0 (interpreted as multiplicative factor).")
             jitter_max = math.log(jitter_factor)
-            jitter_hw = torch.empty(2, device=device, dtype=dtype).uniform_(-jitter_max, jitter_max).exp()
+            jitter_hw = mint.empty(2, device=device, dtype=dtype).uniform_(-jitter_max, jitter_max).exp()
             coords = coords * jitter_hw[None, :]
 
         # Rescale: shared scalar log-uniform factor in [1/R, R]
@@ -1059,12 +1085,12 @@             if rescale_factor <= 0:
                 raise ValueError("rescale_coords must be > 0 (interpreted as multiplicative factor).")
             rescale_max = math.log(rescale_factor)
-            rescale = torch.empty(1, device=device, dtype=dtype).uniform_(-rescale_max, rescale_max).exp()
+            rescale = mint.empty(1, device=device, dtype=dtype).uniform_(-rescale_max, rescale_max).exp()
             coords = coords * rescale
 
         return coords
 
-    def _get_pos_embed_from_coords(self, coords: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
+    def _get_pos_embed_from_coords(self, coords: ms.Tensor) -> Tuple[ms.Tensor, ms.Tensor]:
         """Return sin/cos embeddings with either 'half' or 'interleaved' layout."""
         # coords: (HW, 2); periods: (dim)
         dim = self.dim // 4
@@ -1084,15 +1110,15 @@             # Interleaved layout (HW, dim // 2) -> (HW, dim)
             angles = angles.repeat_interleave(2, dim=-1)
 
-        sin = torch.sin(angles)
-        cos = torch.cos(angles)
+        sin = mint.sin(angles)
+        cos = mint.cos(angles)
         return sin, cos
 
     def _create_embed(
             self,
             feat_shape: List[int],
             no_aug: bool = False,
-    ) -> torch.Tensor:
+    ) -> ms.Tensor:
         H, W = feat_shape
         coords = make_coords_dinov3(
             H, W,
@@ -1103,7 +1129,7 @@         if not no_aug:
             coords = self._apply_coord_augs(coords)
         sin, cos = self._get_pos_embed_from_coords(coords)  # 2 * (HW, dim)
-        rope_embed = torch.cat([sin, cos], dim=-1)  # (HW, 2*dim)
+        rope_embed = mint.cat([sin, cos], dim=-1)  # (HW, 2*dim)
         return rope_embed
 
     def _cache_embed(self, feat_shape: List[int]):
@@ -1117,7 +1143,7 @@             # only update if feat_shape was set (valid cache) and different from previous value
             self._cache_embed(feat_shape)
 
-    def get_embed(self, shape: Optional[List[int]] = None) -> torch.Tensor:
+    def get_embed(self, shape: Optional[List[int]] = None) -> ms.Tensor:
         """Generate rope_embed matching DINOv3 RopePositionEmbedding numerics.
 
         Returns: (HW, num_heads, 2 * head_dim) with last dim = [sin, cos] cat.
@@ -1135,7 +1161,7 @@ 
         return rope_embed
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Get and apply rotary embeddings to x"""
         # assuming channel-first tensor where spatial dim are >= 2
         pos_embed = self.get_embed(x.shape[2:])
@@ -1147,7 +1173,7 @@         dim: int = 768,
         num_heads: int = 12,
         **kwargs
-) -> nn.Module:
+) -> msnn.Cell:
     """Factory function for creating rotary position embeddings.
 
     Args:
