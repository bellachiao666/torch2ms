--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Selective Kernel Convolution/Attention
 
 Paper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)
@@ -5,9 +10,7 @@ Hacked together by / Copyright 2020 Ross Wightman
 """
 from typing import List, Optional, Tuple, Type, Union
-
-import torch
-from torch import nn as nn
+# from torch import nn as nn
 
 from .conv_bn_act import ConvNormAct
 from .helpers import make_divisible
@@ -21,14 +24,14 @@     assert k >= 3 and k % 2
 
 
-class SelectiveKernelAttn(nn.Module):
+class SelectiveKernelAttn(msnn.Cell):
     def __init__(
             self,
             channels: int,
             num_paths: int = 2,
             attn_channels: int = 32,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -45,7 +48,7 @@         self.act = act_layer(inplace=True)
         self.fc_select = nn.Conv2d(attn_channels, channels * num_paths, kernel_size=1, bias=False, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         _assert(x.shape[1] == self.num_paths, '')
         x = x.sum(1).mean((2, 3), keepdim=True)
         x = self.fc_reduce(x)
@@ -54,11 +57,11 @@         x = self.fc_select(x)
         B, C, H, W = x.shape
         x = x.view(B, self.num_paths, C // self.num_paths, H, W)
-        x = torch.softmax(x, dim=1)
+        x = nn.functional.softmax(x, dim = 1)
         return x
 
 
-class SelectiveKernel(nn.Module):
+class SelectiveKernel(msnn.Cell):
 
     def __init__(
             self,
@@ -73,10 +76,10 @@             rd_divisor: int = 8,
             keep_3x3: bool = True,
             split_input: bool = True,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module]= nn.BatchNorm2d,
-            aa_layer: Optional[Type[nn.Module]] = None,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell]= nn.BatchNorm2d,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -129,21 +132,21 @@         conv_kwargs = dict(
             stride=stride, groups=groups, act_layer=act_layer, norm_layer=norm_layer,
             aa_layer=aa_layer, drop_layer=drop_layer, **dd)
-        self.paths = nn.ModuleList([
+        self.paths = msnn.CellList([
             ConvNormAct(in_channels, out_channels, kernel_size=k, dilation=d, **conv_kwargs)
             for k, d in zip(kernel_size, dilation)])
 
         attn_channels = rd_channels or make_divisible(out_channels * rd_ratio, divisor=rd_divisor)
         self.attn = SelectiveKernelAttn(out_channels, self.num_paths, attn_channels, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         if self.split_input:
-            x_split = torch.split(x, self.in_channels // self.num_paths, 1)
+            x_split = mint.split(x, self.in_channels // self.num_paths, 1)
             x_paths = [op(x_split[i]) for i, op in enumerate(self.paths)]
         else:
             x_paths = [op(x) for op in self.paths]
-        x = torch.stack(x_paths, dim=1)
+        x = mint.stack(x_paths, dim = 1)
         x_attn = self.attn(x)
         x = x * x_attn
-        x = torch.sum(x, dim=1)
+        x = mint.sum(x)
         return x
