--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Depthwise Separable Conv Modules
 
 Basic DWS convs. Other variations of DWS exist with batch norm or activations between the
@@ -7,13 +12,13 @@ """
 from typing import Optional, Type, Union
 
-from torch import nn as nn
+# from torch import nn as nn
 
 from .create_conv2d import create_conv2d
 from .create_norm_act import get_norm_act_layer
 
 
-class SeparableConvNormAct(nn.Module):
+class SeparableConvNormAct(msnn.Cell):
     """ Separable Conv w/ trailing Norm and Activation
     """
     def __init__(
@@ -27,10 +32,10 @@             bias: bool = False,
             channel_multiplier: float = 1.0,
             pw_kernel_size: int = 1,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            act_layer: Type[nn.Module] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
             apply_act: bool = True,
-            drop_layer: Optional[Type[nn.Module]] = None,
+            drop_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -69,7 +74,7 @@     def out_channels(self):
         return self.conv_pw.out_channels
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv_dw(x)
         x = self.conv_pw(x)
         x = self.bn(x)
@@ -79,7 +84,7 @@ SeparableConvBnAct = SeparableConvNormAct
 
 
-class SeparableConv2d(nn.Module):
+class SeparableConv2d(msnn.Cell):
     """ Separable Conv
     """
     def __init__(
@@ -127,7 +132,7 @@     def out_channels(self):
         return self.conv_pw.out_channels
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv_dw(x)
         x = self.conv_pw(x)
         return x
