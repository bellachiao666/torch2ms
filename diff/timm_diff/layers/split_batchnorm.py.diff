--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Split BatchNorm
 
 A PyTorch BatchNorm layer that splits input batch into N equal parts and passes each through
@@ -11,11 +16,11 @@ 
 Hacked together by / Copyright 2020 Ross Wightman
 """
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 
-class SplitBatchNorm2d(torch.nn.BatchNorm2d):
+class SplitBatchNorm2d(nn.BatchNorm2d):
 
     def __init__(
             self,
@@ -32,12 +37,12 @@         super().__init__(num_features, eps, momentum, affine, track_running_stats)
         assert num_splits > 1, 'Should have at least one aux BN layer (num_splits at least 2)'
         self.num_splits = num_splits
-        self.aux_bn = nn.ModuleList([
+        self.aux_bn = msnn.CellList([
             nn.BatchNorm2d(num_features, eps, momentum, affine, track_running_stats, **dd)
             for _ in range(num_splits - 1)
         ])
 
-    def forward(self, input: torch.Tensor):
+    def forward(self, input: ms.Tensor):
         if self.training:  # aux BN only relevant while training
             split_size = input.shape[0] // self.num_splits
             assert input.shape[0] == split_size * self.num_splits, "batch size must be evenly divisible by num_splits"
@@ -45,7 +50,7 @@             x = [super().forward(split_input[0])]
             for i, a in enumerate(self.aux_bn):
                 x.append(a(split_input[i + 1]))
-            return torch.cat(x, dim=0)
+            return mint.cat(x, dim=0)
         else:
             return super().forward(input)
 
