--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Convolution with Weight Standardization (StdConv and ScaledStdConv)
 
 StdConv:
@@ -18,9 +23,8 @@ """
 from typing import Optional, Tuple, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from ._fx import register_notrace_module
 from .padding import get_padding, get_padding_value, pad_same
@@ -54,15 +58,9 @@         self.eps = eps
 
     def forward(self, x):
-        weight = F.batch_norm(
-            self.weight.reshape(1, self.out_channels, -1),
-            None,  # running_mean
-            None,  # running_var
-            training=True,
-            momentum=0.,
-            eps=self.eps,
-        ).reshape_as(self.weight)
-        x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
+        weight = nn.functional.batch_norm(
+            self.weight.reshape(1, self.out_channels, -1), None, None, training = True, momentum = 0., eps = self.eps).reshape_as(self.weight)  # 'torch.nn.functional.batch_norm.reshape_as' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        x = nn.functional.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
         return x
 
 
@@ -97,15 +95,9 @@     def forward(self, x):
         if self.same_pad:
             x = pad_same(x, self.kernel_size, self.stride, self.dilation)
-        weight = F.batch_norm(
-            self.weight.reshape(1, self.out_channels, -1),
-            None,  # running_mean
-            None,  # running_var
-            training=True,
-            momentum=0.,
-            eps=self.eps,
-        ).reshape_as(self.weight)
-        x = F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
+        weight = nn.functional.batch_norm(
+            self.weight.reshape(1, self.out_channels, -1), None, None, training = True, momentum = 0., eps = self.eps).reshape_as(self.weight)  # 'torch.nn.functional.batch_norm.reshape_as' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        x = nn.functional.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
         return x
 
 
@@ -144,29 +136,22 @@         self.eps = eps
         self.gain_init = gain_init
 
-        self.gain = nn.Parameter(torch.empty((self.out_channels, 1, 1, 1), **dd))
+        self.gain = ms.Parameter(mint.empty((self.out_channels, 1, 1, 1), **dd))
 
         self.reset_parameters()
 
     def reset_parameters(self) -> None:
         # Only initialize gain if it exists (for the second call)
         if hasattr(self, 'gain'):
-            torch.nn.init.constant_(self.gain, self.gain_init)
+            torch.nn.init.constant_(self.gain, self.gain_init)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             # Also reset parent parameters if needed
             super().reset_parameters()
         # On first call (from super().__init__), do nothing
 
     def forward(self, x):
-        weight = F.batch_norm(
-            self.weight.reshape(1, self.out_channels, -1),
-            None,  # running_mean
-            None,  # running_var
-            weight=(self.gain * self.scale).view(-1),
-            training=True,
-            momentum=0.,
-            eps=self.eps,
-        ).reshape_as(self.weight)
-        return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
+        weight = nn.functional.batch_norm(
+            self.weight.reshape(1, self.out_channels, -1), None, None, weight = (self.gain * self.scale).view(-1), training = True, momentum = 0., eps = self.eps).reshape_as(self.weight)  # 'torch.nn.functional.batch_norm.reshape_as' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        return nn.functional.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
 
 
 @register_notrace_module
@@ -205,14 +190,14 @@         self.eps = eps
         self.gain_init = gain_init
 
-        self.gain = nn.Parameter(torch.empty((self.out_channels, 1, 1, 1), **dd))
+        self.gain = ms.Parameter(mint.empty((self.out_channels, 1, 1, 1), **dd))
 
         self.reset_parameters()
 
     def reset_parameters(self) -> None:
         # Only initialize gain if it exists (for the second call)
         if hasattr(self, 'gain'):
-            torch.nn.init.constant_(self.gain, self.gain_init)
+            torch.nn.init.constant_(self.gain, self.gain_init)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             # Also reset parent parameters if needed
             super().reset_parameters()
         # On first call (from super().__init__), do nothing
@@ -220,13 +205,6 @@     def forward(self, x):
         if self.same_pad:
             x = pad_same(x, self.kernel_size, self.stride, self.dilation)
-        weight = F.batch_norm(
-            self.weight.reshape(1, self.out_channels, -1),
-            None,  # running_mean
-            None,  # running_var
-            weight=(self.gain * self.scale).view(-1),
-            training=True,
-            momentum=0.,
-            eps=self.eps,
-        ).reshape_as(self.weight)
-        return F.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
+        weight = nn.functional.batch_norm(
+            self.weight.reshape(1, self.out_channels, -1), None, None, weight = (self.gain * self.scale).view(-1), training = True, momentum = 0., eps = self.eps).reshape_as(self.weight)  # 'torch.nn.functional.batch_norm.reshape_as' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        return nn.functional.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)
