--- pytorch+++ mindspore@@ -1,14 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Cross Entropy w/ smoothing or soft targets
 
 Hacked together by / Copyright 2021 Ross Wightman
 """
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
 
-
-class LabelSmoothingCrossEntropy(nn.Module):
+class LabelSmoothingCrossEntropy(msnn.Cell):
     """ NLL loss with label smoothing.
     """
     def __init__(self, smoothing=0.1):
@@ -17,8 +18,8 @@         self.smoothing = smoothing
         self.confidence = 1. - smoothing
 
-    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
-        logprobs = F.log_softmax(x, dim=-1)
+    def construct(self, x: ms.Tensor, target: ms.Tensor) -> ms.Tensor:
+        logprobs = mint.special.log_softmax(x, dim = -1)
         nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))
         nll_loss = nll_loss.squeeze(1)
         smooth_loss = -logprobs.mean(dim=-1)
@@ -26,11 +27,11 @@         return loss.mean()
 
 
-class SoftTargetCrossEntropy(nn.Module):
+class SoftTargetCrossEntropy(msnn.Cell):
 
     def __init__(self):
         super(SoftTargetCrossEntropy, self).__init__()
 
-    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
-        loss = torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1)
+    def construct(self, x: ms.Tensor, target: ms.Tensor) -> ms.Tensor:
+        loss = mint.sum(-target * mint.special.log_softmax(x, dim = -1))
         return loss.mean()
