--- pytorch+++ mindspore@@ -1,11 +1,15 @@-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from .cross_entropy import LabelSmoothingCrossEntropy
 
 
-class JsdCrossEntropy(nn.Module):
+class JsdCrossEntropy(msnn.Cell):
     """ Jensen-Shannon Divergence + Cross-Entropy Loss
 
     Based on impl here: https://github.com/google-research/augmix/blob/master/imagenet.py
@@ -21,19 +25,19 @@         if smoothing is not None and smoothing > 0:
             self.cross_entropy_loss = LabelSmoothingCrossEntropy(smoothing)
         else:
-            self.cross_entropy_loss = torch.nn.CrossEntropyLoss()
+            self.cross_entropy_loss = nn.CrossEntropyLoss()
 
     def __call__(self, output, target):
         split_size = output.shape[0] // self.num_splits
         assert split_size * self.num_splits == output.shape[0]
-        logits_split = torch.split(output, split_size)
+        logits_split = mint.split(output, split_size)
 
         # Cross-entropy is only computed on clean images
         loss = self.cross_entropy_loss(logits_split[0], target[:split_size])
-        probs = [F.softmax(logits, dim=1) for logits in logits_split]
+        probs = [nn.functional.softmax(logits, dim = 1) for logits in logits_split]
 
         # Clamp mixture distribution to avoid exploding KL divergence
-        logp_mixture = torch.clamp(torch.stack(probs).mean(axis=0), 1e-7, 1).log()
+        logp_mixture = mint.clamp(mint.stack(probs).mean(axis=0), 1e-7, 1).log()
         loss += self.alpha * sum([F.kl_div(
-            logp_mixture, p_split, reduction='batchmean') for p_split in probs]) / len(probs)
+            logp_mixture, p_split, reduction='batchmean') for p_split in probs]) / len(probs)  # 'torch.nn.functional.kl_div' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return loss
