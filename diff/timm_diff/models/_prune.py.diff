--- pytorch+++ mindspore@@ -1,8 +1,13 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import os
 import pkgutil
 from copy import deepcopy
 
-from torch import nn as nn
+# from torch import nn as nn
 
 from timm.layers import Conv2dSame, BatchNormAct2d, Linear
 
@@ -82,7 +87,7 @@             if old_module.groups > 1:
                 in_channels = out_channels
                 g = in_channels
-            new_conv = conv(
+            new_conv = nn.Conv2d(
                 in_channels=in_channels,
                 out_channels=out_channels,
                 kernel_size=old_module.kernel_size,
@@ -92,7 +97,7 @@                 groups=g,
                 stride=old_module.stride,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，需手动确认参数映射;
             set_layer(new_module, n, new_conv)
         elif isinstance(old_module, BatchNormAct2d):
             new_bn = BatchNormAct2d(
@@ -102,7 +107,7 @@                 affine=old_module.affine,
                 track_running_stats=True,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             new_bn.drop = old_module.drop
             new_bn.act = old_module.act
             set_layer(new_module, n, new_bn)
@@ -114,7 +119,7 @@                 affine=old_module.affine,
                 track_running_stats=True,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，需手动确认参数映射;
             set_layer(new_module, n, new_bn)
         elif isinstance(old_module, nn.Linear):
             # FIXME extra checks to ensure this is actually the FC classifier layer and not a diff Linear layer?
@@ -124,7 +129,7 @@                 out_features=old_module.out_features,
                 bias=old_module.bias is not None,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             set_layer(new_module, n, new_fc)
             if hasattr(new_module, 'num_features'):
                 if getattr(new_module, 'head_hidden_size', 0) == new_module.num_features:
