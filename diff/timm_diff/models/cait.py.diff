--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Class-Attention in Image Transformers (CaiT)
 
 Paper: 'Going deeper with Image Transformers' - https://arxiv.org/abs/2103.17239
@@ -11,8 +16,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Type, Any
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import PatchEmbed, Mlp, DropPath, trunc_normal_, use_fused_attn
@@ -24,10 +29,10 @@ __all__ = ['Cait', 'ClassAttn', 'LayerScaleBlockClassAttn', 'LayerScaleBlock', 'TalkingHeadAttn']
 
 
-class ClassAttn(nn.Module):
+class ClassAttn(msnn.Cell):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to do CA
-    fused_attn: torch.jit.Final[bool]
+    fused_attn: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -46,14 +51,14 @@         self.scale = head_dim ** -0.5
         self.fused_attn = use_fused_attn()
 
-        self.q = nn.Linear(dim, dim, bias=qkv_bias, **dd)
-        self.k = nn.Linear(dim, dim, bias=qkv_bias, **dd)
-        self.v = nn.Linear(dim, dim, bias=qkv_bias, **dd)
+        self.q = nn.Linear(dim, dim, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.k = nn.Linear(dim, dim, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.v = nn.Linear(dim, dim, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn_drop = nn.Dropout(attn_drop)
-        self.proj = nn.Linear(dim, dim, **dd)
+        self.proj = nn.Linear(dim, dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x):
+    def construct(self, x):
         B, N, C = x.shape
         q = self.q(x[:, 0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
         k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
@@ -63,7 +68,7 @@             x_cls = torch.nn.functional.scaled_dot_product_attention(
                 q, k, v,
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
@@ -78,7 +83,7 @@         return x_cls
 
 
-class LayerScaleBlockClassAttn(nn.Module):
+class LayerScaleBlockClassAttn(msnn.Cell):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to add CA and LayerScale
     def __init__(
@@ -90,17 +95,17 @@             proj_drop: float = 0.,
             attn_drop: float = 0.,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
-            attn_block: Type[nn.Module] = ClassAttn,
-            mlp_block: Type[nn.Module] = Mlp,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
+            attn_block: Type[msnn.Cell] = ClassAttn,
+            mlp_block: Type[msnn.Cell] = Mlp,
             init_values: float = 1e-4,
             device=None,
             dtype=None,
     ):
         super().__init__()
         dd = {'device': device, 'dtype': dtype}
-        self.norm1 = norm_layer(dim, **dd)
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.attn = attn_block(
             dim,
             num_heads=num_heads,
@@ -108,9 +113,9 @@             attn_drop=attn_drop,
             proj_drop=proj_drop,
             **dd,
-        )
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.norm2 = norm_layer(dim, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         mlp_hidden_dim = int(dim * mlp_ratio)
         self.mlp = mlp_block(
             in_features=dim,
@@ -118,18 +123,18 @@             act_layer=act_layer,
             drop=proj_drop,
             **dd,
-        )
-        self.gamma_1 = nn.Parameter(init_values * torch.ones(dim, **dd))
-        self.gamma_2 = nn.Parameter(init_values * torch.ones(dim, **dd))
-
-    def forward(self, x, x_cls):
-        u = torch.cat((x_cls, x), dim=1)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.gamma_1 = ms.Parameter(init_values * mint.ones(dim, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.gamma_2 = ms.Parameter(init_values * mint.ones(dim, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x, x_cls):
+        u = mint.cat((x_cls, x), dim=1)
         x_cls = x_cls + self.drop_path(self.gamma_1 * self.attn(self.norm1(u)))
         x_cls = x_cls + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x_cls)))
         return x_cls
 
 
-class TalkingHeadAttn(nn.Module):
+class TalkingHeadAttn(msnn.Cell):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to add Talking Heads Attention (https://arxiv.org/pdf/2003.02436v1.pdf)
     def __init__(
@@ -151,17 +156,17 @@ 
         self.scale = head_dim ** -0.5
 
-        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)
+        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn_drop = nn.Dropout(attn_drop)
 
-        self.proj = nn.Linear(dim, dim, **dd)
-
-        self.proj_l = nn.Linear(num_heads, num_heads, **dd)
-        self.proj_w = nn.Linear(num_heads, num_heads, **dd)
+        self.proj = nn.Linear(dim, dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+        self.proj_l = nn.Linear(num_heads, num_heads, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.proj_w = nn.Linear(num_heads, num_heads, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
 
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x):
+    def construct(self, x):
         B, N, C = x.shape
         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
         q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]
@@ -181,7 +186,7 @@         return x
 
 
-class LayerScaleBlock(nn.Module):
+class LayerScaleBlock(msnn.Cell):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to add layerScale
     def __init__(
@@ -193,17 +198,17 @@             proj_drop: float = 0.,
             attn_drop: float = 0.,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
-            attn_block: Type[nn.Module] = TalkingHeadAttn,
-            mlp_block: Type[nn.Module] = Mlp,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
+            attn_block: Type[msnn.Cell] = TalkingHeadAttn,
+            mlp_block: Type[msnn.Cell] = Mlp,
             init_values: float = 1e-4,
             device=None,
             dtype=None,
     ):
         super().__init__()
         dd = {'device': device, 'dtype': dtype}
-        self.norm1 = norm_layer(dim, **dd)
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.attn = attn_block(
             dim,
             num_heads=num_heads,
@@ -211,9 +216,9 @@             attn_drop=attn_drop,
             proj_drop=proj_drop,
             **dd,
-        )
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.norm2 = norm_layer(dim, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         mlp_hidden_dim = int(dim * mlp_ratio)
         self.mlp = mlp_block(
             in_features=dim,
@@ -221,17 +226,17 @@             act_layer=act_layer,
             drop=proj_drop,
             **dd,
-        )
-        self.gamma_1 = nn.Parameter(init_values * torch.ones(dim, **dd))
-        self.gamma_2 = nn.Parameter(init_values * torch.ones(dim, **dd))
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.gamma_1 = ms.Parameter(init_values * mint.ones(dim, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.gamma_2 = ms.Parameter(init_values * mint.ones(dim, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))
         x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
         return x
 
 
-class Cait(nn.Module):
+class Cait(msnn.Cell):
     # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
     # with slight modifications to adapt to our cait models
     def __init__(
@@ -251,16 +256,16 @@             proj_drop_rate: float = 0.,
             attn_drop_rate: float = 0.,
             drop_path_rate: float = 0.,
-            block_layers: Type[nn.Module] = LayerScaleBlock,
-            block_layers_token: Type[nn.Module] = LayerScaleBlockClassAttn,
-            patch_layer: Type[nn.Module] = PatchEmbed,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
-            attn_block: Type[nn.Module] = TalkingHeadAttn,
-            mlp_block: Type[nn.Module] = Mlp,
+            block_layers: Type[msnn.Cell] = LayerScaleBlock,
+            block_layers_token: Type[msnn.Cell] = LayerScaleBlockClassAttn,
+            patch_layer: Type[msnn.Cell] = PatchEmbed,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            attn_block: Type[msnn.Cell] = TalkingHeadAttn,
+            mlp_block: Type[msnn.Cell] = Mlp,
             init_values: float = 1e-4,
-            attn_block_token_only: Type[nn.Module] = ClassAttn,
-            mlp_block_token_only: Type[nn.Module] = Mlp,
+            attn_block_token_only: Type[msnn.Cell] = ClassAttn,
+            mlp_block_token_only: Type[msnn.Cell] = Mlp,
             depth_token_only: int = 2,
             mlp_ratio_token_only: float = 4.0,
             device=None,
@@ -281,16 +286,16 @@             in_chans=in_chans,
             embed_dim=embed_dim,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         num_patches = self.patch_embed.num_patches
         r = self.patch_embed.feat_ratio() if hasattr(self.patch_embed, 'feat_ratio') else patch_size
 
-        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim, **dd))
-        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim, **dd))
-        self.pos_drop = nn.Dropout(p=pos_drop_rate)
+        self.cls_token = ms.Parameter(mint.zeros(1, 1, embed_dim, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.pos_embed = ms.Parameter(mint.zeros(1, num_patches, embed_dim, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.pos_drop = nn.Dropout(p = pos_drop_rate)
 
         dpr = [drop_path_rate for i in range(depth)]
-        self.blocks = nn.Sequential(*[block_layers(
+        self.blocks = msnn.SequentialCell(*[block_layers(
             dim=embed_dim,
             num_heads=num_heads,
             mlp_ratio=mlp_ratio,
@@ -304,10 +309,10 @@             mlp_block=mlp_block,
             init_values=init_values,
             **dd,
-        ) for i in range(depth)])
+        ) for i in range(depth)])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info = [dict(num_chs=embed_dim, reduction=r, module=f'blocks.{i}') for i in range(depth)]
 
-        self.blocks_token_only = nn.ModuleList([block_layers_token(
+        self.blocks_token_only = msnn.CellList([block_layers_token(
             dim=embed_dim,
             num_heads=num_heads,
             mlp_ratio=mlp_ratio_token_only,
@@ -318,12 +323,12 @@             mlp_block=mlp_block_token_only,
             init_values=init_values,
             **dd,
-        ) for _ in range(depth_token_only)])
-
-        self.norm = norm_layer(embed_dim, **dd)
+        ) for _ in range(depth_token_only)])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.norm = norm_layer(embed_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(embed_dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(embed_dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，需手动确认参数映射;
 
         trunc_normal_(self.pos_embed, std=.02)
         trunc_normal_(self.cls_token, std=.02)
@@ -333,20 +338,20 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif isinstance(m, nn.LayerNorm):
-            nn.init.constant_(m.bias, 0)
-            nn.init.constant_(m.weight, 1.0)
-
-    @torch.jit.ignore
+            nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(m.weight, 1.0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def no_weight_decay(self):
         return {'pos_embed', 'cls_token'}
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         def _matcher(name):
             if any([name.startswith(n) for n in ('cls_token', 'pos_embed', 'patch_embed')]):
@@ -363,8 +368,8 @@                 return float('inf')
         return _matcher
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -372,17 +377,17 @@         if global_pool is not None:
             assert global_pool in ('', 'token', 'avg')
             self.global_pool = global_pool
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -404,11 +409,13 @@         x = x + self.pos_embed
         x = self.pos_drop(x)
 
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             blocks = self.blocks
         else:
             blocks = self.blocks[:max_index + 1]
         for i, blk in enumerate(blocks):
+            # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if self.grad_checkpointing and not torch.jit.is_scripting():
                 x = checkpoint(blk, x)
             else:
@@ -430,7 +437,7 @@         cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
         for i, blk in enumerate(self.blocks_token_only):
             cls_tokens = blk(x, cls_tokens)
-        x = torch.cat((cls_tokens, x), dim=1)
+        x = mint.cat((cls_tokens, x), dim=1)
         x = self.norm(x)
 
         return x, intermediates
@@ -446,9 +453,9 @@         take_indices, max_index = feature_take_indices(len(self.blocks), indices)
         self.blocks = self.blocks[:max_index + 1]  # truncate blocks
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
-            self.blocks_token_only = nn.ModuleList()  # prune token blocks with head
+            self.blocks_token_only = msnn.CellList()  # prune token blocks with head
             self.reset_classifier(0, '')
         return take_indices
 
@@ -456,6 +463,7 @@         x = self.patch_embed(x)
         x = x + self.pos_embed
         x = self.pos_drop(x)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
@@ -463,7 +471,7 @@         cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
         for i, blk in enumerate(self.blocks_token_only):
             cls_tokens = blk(x, cls_tokens)
-        x = torch.cat((cls_tokens, x), dim=1)
+        x = mint.cat((cls_tokens, x), dim=1)
         x = self.norm(x)
         return x
 
@@ -473,7 +481,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -497,7 +505,7 @@         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(out_indices=out_indices, feature_cls='getter'),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -564,68 +572,68 @@ @register_model
 def cait_xxs24_224(pretrained=False, **kwargs) -> Cait:
     model_args = dict(patch_size=16, embed_dim=192, depth=24, num_heads=4, init_values=1e-5)
-    model = _create_cait('cait_xxs24_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_cait('cait_xxs24_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def cait_xxs24_384(pretrained=False, **kwargs) -> Cait:
     model_args = dict(patch_size=16, embed_dim=192, depth=24, num_heads=4, init_values=1e-5)
-    model = _create_cait('cait_xxs24_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_cait('cait_xxs24_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def cait_xxs36_224(pretrained=False, **kwargs) -> Cait:
     model_args = dict(patch_size=16, embed_dim=192, depth=36, num_heads=4, init_values=1e-5)
-    model = _create_cait('cait_xxs36_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_cait('cait_xxs36_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def cait_xxs36_384(pretrained=False, **kwargs) -> Cait:
     model_args = dict(patch_size=16, embed_dim=192, depth=36, num_heads=4, init_values=1e-5)
-    model = _create_cait('cait_xxs36_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_cait('cait_xxs36_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def cait_xs24_384(pretrained=False, **kwargs) -> Cait:
     model_args = dict(patch_size=16, embed_dim=288, depth=24, num_heads=6, init_values=1e-5)
-    model = _create_cait('cait_xs24_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_cait('cait_xs24_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def cait_s24_224(pretrained=False, **kwargs) -> Cait:
     model_args = dict(patch_size=16, embed_dim=384, depth=24, num_heads=8, init_values=1e-5)
-    model = _create_cait('cait_s24_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_cait('cait_s24_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def cait_s24_384(pretrained=False, **kwargs) -> Cait:
     model_args = dict(patch_size=16, embed_dim=384, depth=24, num_heads=8, init_values=1e-5)
-    model = _create_cait('cait_s24_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_cait('cait_s24_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def cait_s36_384(pretrained=False, **kwargs) -> Cait:
     model_args = dict(patch_size=16, embed_dim=384, depth=36, num_heads=8, init_values=1e-6)
-    model = _create_cait('cait_s36_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_cait('cait_s36_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def cait_m36_384(pretrained=False, **kwargs) -> Cait:
     model_args = dict(patch_size=16, embed_dim=768, depth=36, num_heads=16, init_values=1e-6)
-    model = _create_cait('cait_m36_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_cait('cait_m36_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def cait_m48_448(pretrained=False, **kwargs) -> Cait:
     model_args = dict(patch_size=16, embed_dim=768, depth=48, num_heads=16, init_values=1e-6)
-    model = _create_cait('cait_m48_448', pretrained=pretrained, **dict(model_args, **kwargs))
-    return model
+    model = _create_cait('cait_m48_448', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return model
