--- pytorch+++ mindspore@@ -1,10 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ ConvMixer
 
 """
 from typing import Optional, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectAdaptivePool2d
@@ -15,16 +20,16 @@ __all__ = ['ConvMixer']
 
 
-class Residual(nn.Module):
-    def __init__(self, fn: nn.Module):
+class Residual(msnn.Cell):
+    def __init__(self, fn: msnn.Cell):
         super().__init__()
         self.fn = fn
 
-    def forward(self, x):
+    def construct(self, x):
         return self.fn(x) + x
 
 
-class ConvMixer(nn.Module):
+class ConvMixer(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -35,7 +40,7 @@             num_classes: int = 1000,
             global_pool: str = 'avg',
             drop_rate: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
             **kwargs,
@@ -46,14 +51,14 @@         self.num_features = self.head_hidden_size = dim
         self.grad_checkpointing = False
 
-        self.stem = nn.Sequential(
+        self.stem = msnn.SequentialCell(
             nn.Conv2d(in_chans, dim, kernel_size=patch_size, stride=patch_size, **dd),
             act_layer(),
             nn.BatchNorm2d(dim, **dd)
-        )
-        self.blocks = nn.Sequential(
-            *[nn.Sequential(
-                    Residual(nn.Sequential(
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.blocks = msnn.SequentialCell(
+            *[msnn.SequentialCell(
+                    Residual(msnn.SequentialCell(
                         nn.Conv2d(dim, dim, kernel_size, groups=dim, padding="same", **dd),
                         act_layer(),
                         nn.BatchNorm2d(dim, **dd)
@@ -62,29 +67,29 @@                     act_layer(),
                     nn.BatchNorm2d(dim, **dd)
             ) for i in range(depth)]
-        )
+        )  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，需手动确认参数映射;
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(stem=r'^stem', blocks=r'^blocks\.(\d+)')
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
         self.num_classes = num_classes
         if global_pool is not None:
             self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
 
     def forward_features(self, x):
         x = self.stem(x)
@@ -99,7 +104,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -109,7 +114,7 @@     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for ConvMixer models.')
 
-    return build_model_with_cfg(ConvMixer, variant, pretrained, **kwargs)
+    return build_model_with_cfg(ConvMixer, variant, pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url='', **kwargs):
@@ -133,17 +138,17 @@ 
 @register_model
 def convmixer_1536_20(pretrained=False, **kwargs) -> ConvMixer:
-    model_args = dict(dim=1536, depth=20, kernel_size=9, patch_size=7, **kwargs)
-    return _create_convmixer('convmixer_1536_20', pretrained, **model_args)
+    model_args = dict(dim=1536, depth=20, kernel_size=9, patch_size=7, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_convmixer('convmixer_1536_20', pretrained, **model_args)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def convmixer_768_32(pretrained=False, **kwargs) -> ConvMixer:
-    model_args = dict(dim=768, depth=32, kernel_size=7, patch_size=7, act_layer=nn.ReLU, **kwargs)
-    return _create_convmixer('convmixer_768_32', pretrained, **model_args)
+    model_args = dict(dim=768, depth=32, kernel_size=7, patch_size=7, act_layer=nn.ReLU, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_convmixer('convmixer_768_32', pretrained, **model_args)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def convmixer_1024_20_ks9_p14(pretrained=False, **kwargs) -> ConvMixer:
-    model_args = dict(dim=1024, depth=20, kernel_size=9, patch_size=14, **kwargs)
-    return _create_convmixer('convmixer_1024_20_ks9_p14', pretrained, **model_args)
+    model_args = dict(dim=1024, depth=20, kernel_size=9, patch_size=14, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_convmixer('convmixer_1024_20_ks9_p14', pretrained, **model_args)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
