--- pytorch+++ mindspore@@ -1,10 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ ConvMixer
 
 """
 from typing import Optional, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectAdaptivePool2d
@@ -15,16 +20,17 @@ __all__ = ['ConvMixer']
 
 
-class Residual(nn.Module):
+class Residual(msnn.Cell):
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(self, fn: nn.Module):
         super().__init__()
         self.fn = fn
 
-    def forward(self, x):
+    def construct(self, x):
         return self.fn(x) + x
 
 
-class ConvMixer(nn.Module):
+class ConvMixer(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -46,26 +52,30 @@         self.num_features = self.head_hidden_size = dim
         self.grad_checkpointing = False
 
-        self.stem = nn.Sequential(
+        self.stem = msnn.SequentialCell(
+            [
             nn.Conv2d(in_chans, dim, kernel_size=patch_size, stride=patch_size, **dd),
             act_layer(),
             nn.BatchNorm2d(dim, **dd)
-        )
-        self.blocks = nn.Sequential(
-            *[nn.Sequential(
-                    Residual(nn.Sequential(
-                        nn.Conv2d(dim, dim, kernel_size, groups=dim, padding="same", **dd),
-                        act_layer(),
-                        nn.BatchNorm2d(dim, **dd)
-                    )),
-                    nn.Conv2d(dim, dim, kernel_size=1, **dd),
-                    act_layer(),
-                    nn.BatchNorm2d(dim, **dd)
-            ) for i in range(depth)]
-        )
+        ])
+        self.blocks = msnn.SequentialCell(
+            [
+            [msnn.SequentialCell(
+                    [
+            Residual(msnn.SequentialCell(
+                        [
+            nn.Conv2d(dim, dim, kernel_size, groups=dim, padding="same", **dd),
+            act_layer(),
+            nn.BatchNorm2d(dim, **dd)
+        ])),
+            nn.Conv2d(dim, dim, kernel_size=1, **dd),
+            act_layer(),
+            nn.BatchNorm2d(dim, **dd)
+        ]) for i in range(depth)]
+        ])
         self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -76,6 +86,7 @@     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         return self.head
@@ -84,7 +95,7 @@         self.num_classes = num_classes
         if global_pool is not None:
             self.pooling = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
 
     def forward_features(self, x):
         x = self.stem(x)
@@ -99,7 +110,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
