--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ ConvNeXt
 
 Papers:
@@ -40,8 +45,8 @@ from functools import partial
 from typing import Callable, Dict, List, Optional, Tuple, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, OPENAI_CLIP_MEAN, OPENAI_CLIP_STD
 from timm.layers import (
@@ -73,7 +78,7 @@ __all__ = ['ConvNeXt']  # model_registry will add each entrypoint fn to this
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
     """Downsample module for ConvNeXt."""
 
     def __init__(
@@ -100,21 +105,21 @@             avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
             self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
         else:
-            self.pool = nn.Identity()
+            self.pool = msnn.Identity()
 
         if in_chs != out_chs:
             self.conv = create_conv2d(in_chs, out_chs, 1, stride=1, **dd)
         else:
-            self.conv = nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.conv = msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = self.pool(x)
         x = self.conv(x)
         return x
 
 
-class ConvNeXtBlock(nn.Module):
+class ConvNeXtBlock(msnn.Cell):
     """ConvNeXt Block.
 
     There are two equivalent implementations:
@@ -187,14 +192,14 @@             act_layer=act_layer,
             **dd,
         )
-        self.gamma = nn.Parameter(ls_init_value * torch.ones(out_chs, **dd)) if ls_init_value is not None else None
+        self.gamma = ms.Parameter(ls_init_value * mint.ones(out_chs, **dd)) if ls_init_value is not None else None
         if in_chs != out_chs or stride != 1 or dilation[0] != dilation[1]:
             self.shortcut = Downsample(in_chs, out_chs, stride=stride, dilation=dilation[0], **dd)
         else:
-            self.shortcut = nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.shortcut = msnn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         shortcut = x
         x = self.conv_dw(x)
@@ -213,7 +218,7 @@         return x
 
 
-class ConvNeXtStage(nn.Module):
+class ConvNeXtStage(msnn.Cell):
     """ConvNeXt stage (multiple blocks)."""
 
     def __init__(
@@ -260,7 +265,8 @@         if in_chs != out_chs or stride > 1 or dilation[0] != dilation[1]:
             ds_ks = 2 if stride > 1 or dilation[0] != dilation[1] else 1
             pad = 'same' if dilation[1] > 1 else 0  # same padding needed if dilation used
-            self.downsample = nn.Sequential(
+            self.downsample = msnn.SequentialCell(
+                [
                 norm_layer(in_chs, **dd),
                 create_conv2d(
                     in_chs,
@@ -271,11 +277,11 @@                     padding=pad,
                     bias=conv_bias,
                     **dd,
-                ),
-            )
+                )
+            ])
             in_chs = out_chs
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         drop_path_rates = drop_path_rates or [0.] * depth
         stage_blocks = []
@@ -295,9 +301,11 @@                 **dd,
             ))
             in_chs = out_chs
-        self.blocks = nn.Sequential(*stage_blocks)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.blocks = msnn.SequentialCell([
+            stage_blocks
+        ])
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
@@ -335,7 +343,7 @@     return norm_layer, norm_layer_cl
 
 
-class ConvNeXt(nn.Module):
+class ConvNeXt(msnn.Cell):
     """ConvNeXt model architecture.
 
     A PyTorch impl of : `A ConvNet for the 2020s`  - https://arxiv.org/pdf/2201.03545.pdf
@@ -404,22 +412,25 @@         assert stem_type in ('patch', 'overlap', 'overlap_tiered', 'overlap_act')
         if stem_type == 'patch':
             # NOTE: this stem is a minimal form of ViT PatchEmbed, as used in SwinTransformer w/ patch_size = 4
-            self.stem = nn.Sequential(
+            self.stem = msnn.SequentialCell(
+                [
                 nn.Conv2d(in_chans, dims[0], kernel_size=patch_size, stride=patch_size, bias=conv_bias, **dd),
-                norm_layer(dims[0], **dd),
-            )
+                norm_layer(dims[0], **dd)
+            ])
             stem_stride = patch_size
         else:
             mid_chs = make_divisible(dims[0] // 2) if 'tiered' in stem_type else dims[0]
-            self.stem = nn.Sequential(*filter(None, [
+            self.stem = msnn.SequentialCell([
+                filter(None, [
                 nn.Conv2d(in_chans, mid_chs, kernel_size=3, stride=2, padding=1, bias=conv_bias, **dd),
                 act_layer() if 'act' in stem_type else None,
                 nn.Conv2d(mid_chs, dims[0], kernel_size=3, stride=2, padding=1, bias=conv_bias, **dd),
                 norm_layer(dims[0], **dd),
-            ]))
+            ])
+            ])
             stem_stride = 4
 
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         dp_rates = calculate_drop_path_rates(drop_path_rate, depths, stagewise=True)
         stages = []
         prev_chs = dims[0]
@@ -454,7 +465,9 @@             prev_chs = out_chs
             # NOTE feature_info use currently assumes stage 0 == stride 1, rest are stride 2
             self.feature_info += [dict(num_chs=prev_chs, reduction=curr_stride, module=f'stages.{i}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell([
+            stages
+        ])
         self.num_features = self.head_hidden_size = prev_chs
 
         # if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets
@@ -470,7 +483,7 @@                 **dd,
             )
         else:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
             self.head = NormMlpClassifierHead(
                 self.num_features,
                 num_classes,
@@ -513,6 +526,7 @@         for s in self.stages:
             s.grad_checkpointing = enable
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         """Get the classifier module."""
@@ -530,7 +544,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
@@ -597,19 +611,19 @@         take_indices, max_index = feature_take_indices(len(self.stages), indices)
         self.stages = self.stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers."""
         x = self.stem(x)
         x = self.stages(x)
         x = self.norm_pre(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -621,13 +635,14 @@         """
         return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def _init_weights(module: nn.Module, name: Optional[str] = None, head_init_scale: float = 1.0) -> None:
     """Initialize model weights.
 
@@ -639,10 +654,10 @@     if isinstance(module, nn.Conv2d):
         trunc_normal_(module.weight, std=.02)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Linear):
         trunc_normal_(module.weight, std=.02)
-        nn.init.zeros_(module.bias)
+        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if name and 'head.' in name:
             module.weight.data.mul_(head_init_scale)
             module.bias.data.mul_(head_init_scale)
@@ -660,12 +675,12 @@         out_dict = {k.replace('visual.trunk.', ''): v for k, v in state_dict.items() if k.startswith('visual.trunk.')}
         if 'visual.head.proj.weight' in state_dict:
             out_dict['head.fc.weight'] = state_dict['visual.head.proj.weight']
-            out_dict['head.fc.bias'] = torch.zeros(state_dict['visual.head.proj.weight'].shape[0])
+            out_dict['head.fc.bias'] = mint.zeros(state_dict['visual.head.proj.weight'].shape[0])
         elif 'visual.head.mlp.fc1.weight' in state_dict:
             out_dict['head.pre_logits.fc.weight'] = state_dict['visual.head.mlp.fc1.weight']
             out_dict['head.pre_logits.fc.bias'] = state_dict['visual.head.mlp.fc1.bias']
             out_dict['head.fc.weight'] = state_dict['visual.head.mlp.fc2.weight']
-            out_dict['head.fc.bias'] = torch.zeros(state_dict['visual.head.mlp.fc2.weight'].shape[0])
+            out_dict['head.fc.bias'] = mint.zeros(state_dict['visual.head.mlp.fc2.weight'].shape[0])
         return out_dict
 
     import re
