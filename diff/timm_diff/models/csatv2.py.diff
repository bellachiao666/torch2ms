--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """CSATv2
 
 A frequency-domain vision model using DCT transforms with spatial attention.
@@ -16,9 +21,8 @@ from typing import List, Optional, Tuple, Union
 
 import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.layers import trunc_normal_, DropPath, Mlp, LayerNorm2d, Attention, NormMlpClassifierHead
 from timm.layers.grn import GlobalResponseNorm
@@ -107,24 +111,24 @@         orthonormal: bool,
         device=None,
         dtype=None,
-) -> torch.Tensor:
+) -> ms.Tensor:
     """Generate Type-II DCT kernel matrix."""
     dd = dict(device=device, dtype=dtype)
-    x = torch.eye(kernel_size, **dd)
+    x = mint.eye(kernel_size, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     v = x.clone().contiguous().view(-1, kernel_size)
-    v = torch.cat([v, v.flip([1])], dim=-1)
-    v = torch.fft.fft(v, dim=-1)[:, :kernel_size]
+    v = mint.cat([v, v.flip([1])], dim=-1)
+    v = torch.fft.fft(v, dim=-1)[:, :kernel_size]  # 'torch.fft.fft' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     try:
-        k = torch.tensor(-1j, **dd) * torch.pi * torch.arange(kernel_size, **dd)[None, :]
+        k = ms.Tensor(-1j, **dd) * torch.pi * mint.arange(kernel_size, **dd)[None, :]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     except AttributeError:
-        k = torch.tensor(-1j, **dd) * math.pi * torch.arange(kernel_size, **dd)[None, :]
-    k = torch.exp(k / (kernel_size * 2))
+        k = ms.Tensor(-1j, **dd) * math.pi * mint.arange(kernel_size, **dd)[None, :]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    k = mint.exp(k / (kernel_size * 2))
     v = v * k
     v = v.real
     if orthonormal:
-        v[:, 0] = v[:, 0] * torch.sqrt(torch.tensor(1 / (kernel_size * 4), **dd))
-        v[:, 1:] = v[:, 1:] * torch.sqrt(torch.tensor(1 / (kernel_size * 2), **dd))
-    v = v.contiguous().view(*x.shape)
+        v[:, 0] = v[:, 0] * mint.sqrt(ms.Tensor(1 / (kernel_size * 4), **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        v[:, 1:] = v[:, 1:] * mint.sqrt(ms.Tensor(1 / (kernel_size * 2), **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    v = v.contiguous().view(*x.shape)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return v
 
 
@@ -133,12 +137,12 @@         orthonormal: bool,
         device=None,
         dtype=None,
-) -> torch.Tensor:
+) -> ms.Tensor:
     """Generate Type-III DCT kernel matrix (inverse of Type-II)."""
-    return torch.linalg.inv(_dct_kernel_type_2(kernel_size, orthonormal, device, dtype))
-
-
-class Dct1d(nn.Module):
+    return mint.linalg.inv(_dct_kernel_type_2(kernel_size, orthonormal, device, dtype))
+
+
+class Dct1d(msnn.Cell):
     """1D Discrete Cosine Transform layer."""
 
     def __init__(
@@ -152,15 +156,15 @@         dd = dict(device=device, dtype=dtype)
         super().__init__()
         kernel = {'2': _dct_kernel_type_2, '3': _dct_kernel_type_3}
-        dct_weights = kernel[f'{kernel_type}'](kernel_size, orthonormal, **dd).T
+        dct_weights = kernel[f'{kernel_type}'](kernel_size, orthonormal, **dd).T  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.register_buffer('weights', dct_weights.contiguous())
         self.register_parameter('bias', None)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        return F.linear(x, self.weights, self.bias)
-
-
-class Dct2d(nn.Module):
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
+        return nn.functional.linear(x, self.weights, self.bias)
+
+
+class Dct2d(msnn.Cell):
     """2D Discrete Cosine Transform layer."""
 
     def __init__(
@@ -173,9 +177,9 @@     ) -> None:
         dd = dict(device=device, dtype=dtype)
         super().__init__()
-        self.transform = Dct1d(kernel_size, kernel_type, orthonormal, **dd)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.transform = Dct1d(kernel_size, kernel_type, orthonormal, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         return self.transform(self.transform(x).transpose(-1, -2)).transpose(-1, -2)
 
 
@@ -196,7 +200,7 @@     return y, cb, cr
 
 
-class LearnableDct2d(nn.Module):
+class LearnableDct2d(msnn.Cell):
     """Learnable 2D DCT stem with RGB to YCbCr conversion and frequency selection."""
 
     def __init__(
@@ -211,38 +215,38 @@         dd = dict(device=device, dtype=dtype)
         super().__init__()
         self.k = kernel_size
-        self.transform = Dct2d(kernel_size, kernel_type, orthonormal, **dd)
+        self.transform = Dct2d(kernel_size, kernel_type, orthonormal, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.permutation = _zigzag_permutation(kernel_size, kernel_size)
 
         y_ch, cb_ch, cr_ch = _split_out_chs(out_chs, ratio=(24, 4, 4))
-        self.conv_y  = nn.Conv2d(kernel_size ** 2, y_ch,  kernel_size=1, padding=0, **dd)
-        self.conv_cb = nn.Conv2d(kernel_size ** 2, cb_ch, kernel_size=1, padding=0, **dd)
-        self.conv_cr = nn.Conv2d(kernel_size ** 2, cr_ch, kernel_size=1, padding=0, **dd)
-
-        self.register_buffer('mean', torch.tensor(_DCT_MEAN, device=device), persistent=False)
-        self.register_buffer('var', torch.tensor(_DCT_VAR, device=device), persistent=False)
+        self.conv_y  = nn.Conv2d(kernel_size ** 2, y_ch,  kernel_size=1, padding=0, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv_cb = nn.Conv2d(kernel_size ** 2, cb_ch, kernel_size=1, padding=0, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv_cr = nn.Conv2d(kernel_size ** 2, cr_ch, kernel_size=1, padding=0, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+        self.register_buffer('mean', ms.Tensor(_DCT_MEAN, device=device), persistent=False)
+        self.register_buffer('var', ms.Tensor(_DCT_VAR, device=device), persistent=False)
         # Shape (3, 1, 1) for BCHW broadcasting
-        self.register_buffer('imagenet_mean', torch.tensor([0.485, 0.456, 0.406], device=device).view(3, 1, 1), persistent=False)
-        self.register_buffer('imagenet_std', torch.tensor([0.229, 0.224, 0.225], device=device).view(3, 1, 1), persistent=False)
-
-    def _denormalize(self, x: torch.Tensor) -> torch.Tensor:
+        self.register_buffer('imagenet_mean', ms.Tensor([0.485, 0.456, 0.406], device=device).view(3, 1, 1), persistent=False)
+        self.register_buffer('imagenet_std', ms.Tensor([0.229, 0.224, 0.225], device=device).view(3, 1, 1), persistent=False)
+
+    def _denormalize(self, x: ms.Tensor) -> ms.Tensor:
         """Convert from ImageNet normalized to [0, 255] range."""
         return x.mul(self.imagenet_std).add_(self.imagenet_mean) * 255
 
-    def _rgb_to_ycbcr(self, x: torch.Tensor) -> torch.Tensor:
+    def _rgb_to_ycbcr(self, x: ms.Tensor) -> ms.Tensor:
         """Convert RGB to YCbCr color space (BCHW input/output)."""
         r, g, b = x[:, 0], x[:, 1], x[:, 2]
         y = r * 0.299 + g * 0.587 + b * 0.114
         cb = 0.564 * (b - y) + 128
         cr = 0.713 * (r - y) + 128
-        return torch.stack([y, cb, cr], dim=1)
-
-    def _frequency_normalize(self, x: torch.Tensor) -> torch.Tensor:
+        return mint.stack([y, cb, cr], dim=1)
+
+    def _frequency_normalize(self, x: ms.Tensor) -> ms.Tensor:
         """Normalize DCT coefficients using precomputed statistics."""
         std = self.var ** 0.5 + 1e-8
         return (x - self.mean) / std
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         b, c, h, w = x.shape
         x = self._denormalize(x)
         x = self._rgb_to_ycbcr(x)
@@ -258,10 +262,10 @@         x_y = self.conv_y(x[:, 0])
         x_cb = self.conv_cb(x[:, 1])
         x_cr = self.conv_cr(x[:, 2])
-        return torch.cat([x_y, x_cb, x_cr], dim=1)
-
-
-class Dct2dStats(nn.Module):
+        return mint.cat([x_y, x_cb, x_cr], dim=1)
+
+
+class Dct2dStats(msnn.Cell):
     """Utility module to compute DCT coefficient statistics."""
 
     def __init__(
@@ -275,10 +279,10 @@         dd = dict(device=device, dtype=dtype)
         super().__init__()
         self.k = kernel_size
-        self.transform = Dct2d(kernel_size, kernel_type, orthonormal, **dd)
+        self.transform = Dct2d(kernel_size, kernel_type, orthonormal, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.permutation = _zigzag_permutation(kernel_size, kernel_size)
 
-    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
+    def construct(self, x: ms.Tensor) -> Tuple[ms.Tensor, ms.Tensor]:
         b, c, h, w = x.shape
         # Extract non-overlapping k x k patches
         x = x.reshape(b, c, h // self.k, self.k, w // self.k, self.k)  # (B, C, H//k, k, W//k, k)
@@ -288,15 +292,15 @@         x = x[:, :, self.permutation]
         x = x.reshape(b * (h // self.k) * (w // self.k), c, -1)
 
-        mean_list = torch.zeros([3, 64])
-        var_list = torch.zeros([3, 64])
+        mean_list = mint.zeros([3, 64])
+        var_list = mint.zeros([3, 64])
         for i in range(3):
-            mean_list[i] = torch.mean(x[:, i], dim=0)
-            var_list[i] = torch.var(x[:, i], dim=0)
+            mean_list[i] = mint.mean(x[:, i], dim=0)
+            var_list[i] = mint.var(x[:, i], dim=0)
         return mean_list, var_list
 
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     """ConvNeXt-style block with spatial attention."""
 
     def __init__(
@@ -308,16 +312,16 @@     ) -> None:
         dd = dict(device=device, dtype=dtype)
         super().__init__()
-        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim, **dd)
-        self.norm = nn.LayerNorm(dim, eps=1e-6, **dd)
-        self.pwconv1 = nn.Linear(dim, 4 * dim, **dd)
+        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm = nn.LayerNorm(dim, eps=1e-6, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.pwconv1 = nn.Linear(dim, 4 * dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.act = nn.GELU()
-        self.grn = GlobalResponseNorm(4 * dim, channels_last=True, **dd)
-        self.pwconv2 = nn.Linear(4 * dim, dim, **dd)
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.attn = SpatialAttention(**dd)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.grn = GlobalResponseNorm(4 * dim, channels_last=True, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.pwconv2 = nn.Linear(4 * dim, dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+        self.attn = SpatialAttention(**dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = x
         x = self.dwconv(x)
         x = x.permute(0, 2, 3, 1)
@@ -329,13 +333,13 @@         x = x.permute(0, 3, 1, 2)
 
         attn = self.attn(x)
-        attn = F.interpolate(attn, size=x.shape[2:], mode='bilinear', align_corners=True)
+        attn = nn.functional.interpolate(attn, size = x.shape[2:], mode = 'bilinear', align_corners = True)
         x = x * attn
 
         return shortcut + self.drop_path(x)
 
 
-class SpatialTransformerBlock(nn.Module):
+class SpatialTransformerBlock(msnn.Cell):
     """Lightweight transformer block for spatial attention (1-channel, 7x7 grid).
 
     This is a simplified transformer with single-head, 1-dim attention over spatial
@@ -350,15 +354,15 @@         dd = dict(device=device, dtype=dtype)
         super().__init__()
         # Single-head attention with 1-dim q/k/v (no output projection needed)
-        self.pos_embed = PosConv(in_chans=1, **dd)
-        self.norm1 = nn.LayerNorm(1, **dd)
-        self.qkv = nn.Linear(1, 3, bias=False, **dd)
+        self.pos_embed = PosConv(in_chans=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm1 = nn.LayerNorm(1, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.qkv = nn.Linear(1, 3, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
 
         # Feedforward: 1 -> 4 -> 1
-        self.norm2 = nn.LayerNorm(1, **dd)
-        self.mlp = Mlp(1, 4, 1, act_layer=nn.GELU, **dd)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.norm2 = nn.LayerNorm(1, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.mlp = Mlp(1, 4, 1, act_layer=nn.GELU, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         B, C, H, W = x.shape
 
         # Attention block
@@ -386,7 +390,7 @@         return x
 
 
-class SpatialAttention(nn.Module):
+class SpatialAttention(msnn.Cell):
     """Spatial attention module using channel statistics and transformer."""
 
     def __init__(
@@ -397,20 +401,20 @@         dd = dict(device=device, dtype=dtype)
         super().__init__()
         self.avgpool = nn.AdaptiveAvgPool2d((7, 7))
-        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, **dd)
-        self.attn = SpatialTransformerBlock(**dd)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.attn = SpatialTransformerBlock(**dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x_avg = x.mean(dim=1, keepdim=True)
         x_max = x.amax(dim=1, keepdim=True)
-        x = torch.cat([x_avg, x_max], dim=1)
+        x = mint.cat([x_avg, x_max], dim=1)
         x = self.avgpool(x)
         x = self.conv(x)
         x = self.attn(x)
         return x
 
 
-class TransformerBlock(nn.Module):
+class TransformerBlock(msnn.Cell):
     """Transformer block with optional downsampling and convolutional position encoding."""
 
     def __init__(
@@ -434,14 +438,14 @@         if self.downsample:
             self.pool1 = nn.MaxPool2d(3, 2, 1)
             self.pool2 = nn.MaxPool2d(3, 2, 1)
-            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False, **dd)
+            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         else:
-            self.pool1 = nn.Identity()
-            self.pool2 = nn.Identity()
-            self.proj = nn.Identity()
-
-        self.pos_embed = PosConv(in_chans=inp, **dd)
-        self.norm1 = nn.LayerNorm(inp, **dd)
+            self.pool1 = msnn.Identity()
+            self.pool2 = msnn.Identity()
+            self.proj = msnn.Identity()
+
+        self.pos_embed = PosConv(in_chans=inp, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm1 = nn.LayerNorm(inp, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn = Attention(
             dim=inp,
             num_heads=num_heads,
@@ -450,14 +454,14 @@             attn_drop=attn_drop,
             proj_drop=proj_drop,
             **dd,
-        )
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        self.norm2 = nn.LayerNorm(oup, **dd)
-        self.mlp = Mlp(oup, hidden_dim, oup, act_layer=nn.GELU, drop=proj_drop, **dd)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+        self.norm2 = nn.LayerNorm(oup, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.mlp = Mlp(oup, hidden_dim, oup, act_layer=nn.GELU, drop=proj_drop, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         if self.downsample:
             shortcut = self.proj(self.pool1(x))
             x_t = self.pool2(x)
@@ -489,7 +493,7 @@         return x
 
 
-class PosConv(nn.Module):
+class PosConv(msnn.Cell):
     """Convolutional position encoding."""
 
     def __init__(
@@ -500,9 +504,9 @@     ) -> None:
         dd = dict(device=device, dtype=dtype)
         super().__init__()
-        self.proj = nn.Conv2d(in_chans, in_chans, kernel_size=3, stride=1, padding=1, bias=True, groups=in_chans, **dd)
-
-    def forward(self, x: torch.Tensor, size: Tuple[int, int]) -> torch.Tensor:
+        self.proj = nn.Conv2d(in_chans, in_chans, kernel_size=3, stride=1, padding=1, bias=True, groups=in_chans, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor, size: Tuple[int, int]) -> ms.Tensor:
         B, N, C = x.shape
         H, W = size
         cnn_feat = x.transpose(1, 2).view(B, C, H, W)
@@ -510,7 +514,7 @@         return x.flatten(2).transpose(1, 2)
 
 
-class CSATv2(nn.Module):
+class CSATv2(msnn.Cell):
     """CSATv2: Frequency-domain vision model with spatial attention.
 
     A hybrid architecture that processes images in the DCT frequency domain
@@ -555,13 +559,13 @@ 
         # Build drop path rates for all blocks (0 for transformer blocks when transformer_drop_path=False)
         total_blocks = sum(depths) if transformer_drop_path else sum(d - t for d, t in zip(depths, transformer_depths))
-        dp_iter = iter(torch.linspace(0, drop_path_rate, total_blocks).tolist())
+        dp_iter = iter(mint.linspace(0, drop_path_rate, total_blocks).tolist())
         dp_rates = []
         for depth, t_depth in zip(depths, transformer_depths):
             dp_rates += [next(dp_iter) for _ in range(depth - t_depth)]
             dp_rates += [next(dp_iter) if transformer_drop_path else 0. for _ in range(t_depth)]
 
-        self.stem_dct = LearnableDct2d(8, out_chs=dims[0], **dd)
+        self.stem_dct = LearnableDct2d(8, out_chs=dims[0], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # Build stages dynamically
         dp_iter = iter(dp_rates)
@@ -576,22 +580,22 @@                 [TransformerBlock(inp=dim, oup=dim, drop_path=next(dp_iter), **dd) for _ in range(t_depth)] +
                 # Trailing LayerNorm (except last stage)
                 ([LayerNorm2d(dim, eps=1e-6, **dd)] if i < len(depths) - 1 else [])
-            )
-            stages.append(nn.Sequential(*layers))
-        self.stages = nn.Sequential(*stages)
-
-        self.head = NormMlpClassifierHead(dims[-1], num_classes, pool_type=global_pool, **dd)
+            )  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            stages.append(msnn.SequentialCell(*layers))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.head = NormMlpClassifierHead(dims[-1], num_classes, pool_type=global_pool, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.apply(self._init_weights)
 
-    def _init_weights(self, m: nn.Module) -> None:
+    def _init_weights(self, m: msnn.Cell) -> None:
         if isinstance(m, (nn.Conv2d, nn.Linear)):
             trunc_normal_(m.weight, std=0.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None) -> None:
@@ -600,11 +604,11 @@             self.global_pool = global_pool
         self.head.reset(num_classes, pool_type=global_pool)
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True) -> None:
         self.grad_checkpointing = enable
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         x = self.stem_dct(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stages, x)
@@ -614,13 +618,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """Forward pass returning intermediate features.
 
         Args:
@@ -681,19 +685,19 @@         # 5 feature levels: stem_dct (0) + stages 0-3 (1-4)
         take_indices, max_index = feature_take_indices(len(self.stages) + 1, indices)
         # max_index is 0-4, stages are 1-4, so we keep max_index stages
-        self.stages = self.stages[:max_index] if max_index > 0 else nn.Sequential()
+        self.stages = self.stages[:max_index] if max_index > 0 else msnn.SequentialCell()
 
         if prune_norm:
-            self.head.norm = nn.Identity()
+            self.head.norm = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
 
         return take_indices
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         return self.head(x, pre_logits=pre_logits)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.forward_features(x)
         return self.forward_head(x)
 
@@ -717,7 +721,7 @@ })
 
 
-def checkpoint_filter_fn(state_dict: dict, model: nn.Module) -> dict:
+def checkpoint_filter_fn(state_dict: dict, model: msnn.Cell) -> dict:
     """Remap original CSATv2 checkpoint to timm format.
 
     Handles two key structural changes:
@@ -809,12 +813,12 @@         feature_cfg=dict(out_indices=out_indices, flatten_sequential=True),
         default_cfg=default_cfgs[variant],
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def csatv2(pretrained: bool = False, **kwargs) -> CSATv2:
-    return _create_csatv2('csatv2', pretrained, **kwargs)
+    return _create_csatv2('csatv2', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -826,4 +830,4 @@         transformer_depths = (0, 0, 4, 3)
 
     )
-    return _create_csatv2('csatv2_21m', pretrained, **dict(model_args, **kwargs))+    return _create_csatv2('csatv2_21m', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;