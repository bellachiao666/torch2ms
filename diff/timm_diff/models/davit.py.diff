--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ DaViT: Dual Attention Vision Transformers
 
 As described in https://arxiv.org/abs/2204.03645
@@ -14,10 +19,10 @@ from functools import partial
 from typing import List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch import Tensor
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
+# from torch import Tensor
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, calculate_drop_path_rates, to_2tuple, trunc_normal_, Mlp, LayerNorm2d, get_norm_layer, use_fused_attn
@@ -31,7 +36,7 @@ __all__ = ['DaVit']
 
 
-class ConvPosEnc(nn.Module):
+class ConvPosEnc(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -51,16 +56,16 @@             padding=k // 2,
             groups=dim,
             **dd,
-        )
-        self.act = nn.GELU() if act else nn.Identity()
-
-    def forward(self, x: Tensor):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.act = nn.GELU() if act else msnn.Identity()
+
+    def construct(self, x: ms.Tensor):
         feat = self.proj(x)
         x = x + self.act(feat)
         return x
 
 
-class Stem(nn.Module):
+class Stem(msnn.Cell):
     """ Size-agnostic implementation of 2D image to patch embedding,
         allowing input size to be adjusted during model forward operation
     """
@@ -70,7 +75,7 @@             in_chs: int = 3,
             out_chs: int = 96,
             stride: int = 4,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             device=None,
             dtype=None,
     ):
@@ -88,26 +93,26 @@             stride=stride,
             padding=3,
             **dd,
-        )
-        self.norm = norm_layer(out_chs, **dd)
-
-    def forward(self, x: Tensor):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm = norm_layer(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor):
         B, C, H, W = x.shape
         pad_r = (self.stride[1] - W % self.stride[1]) % self.stride[1]
         pad_b = (self.stride[0] - H % self.stride[0]) % self.stride[0]
-        x = F.pad(x, (0, pad_r, 0, pad_b))
+        x = nn.functional.pad(x, (0, pad_r, 0, pad_b))
         x = self.conv(x)
         x = self.norm(x)
         return x
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             kernel_size: int = 3,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             device=None,
             dtype=None,
     ):
@@ -116,7 +121,7 @@         self.in_chs = in_chs
         self.out_chs = out_chs
 
-        self.norm = norm_layer(in_chs, **dd)
+        self.norm = norm_layer(in_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.even_k = kernel_size % 2 == 0
         self.conv = nn.Conv2d(
             in_chs,
@@ -125,21 +130,21 @@             stride=2,
             padding=0 if self.even_k else kernel_size // 2,
             **dd,
-        )
-
-    def forward(self, x: Tensor):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor):
         B, C, H, W = x.shape
         x = self.norm(x)
         if self.even_k:
             k_h, k_w = self.conv.kernel_size
             pad_r = (k_w - W % k_w) % k_w
             pad_b = (k_h - H % k_h) % k_h
-            x = F.pad(x, (0, pad_r , 0, pad_b))
+            x = nn.functional.pad(x, (0, pad_r , 0, pad_b))
         x = self.conv(x)
         return x
 
 
-class ChannelAttentionV2(nn.Module):
+class ChannelAttentionV2(msnn.Cell):
 
     def __init__(
             self,
@@ -156,10 +161,10 @@         self.head_dim = dim // num_heads
         self.dynamic_scale = dynamic_scale
 
-        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)
-        self.proj = nn.Linear(dim, dim, **dd)
-
-    def forward(self, x):
+        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.proj = nn.Linear(dim, dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x):
         B, N, C = x.shape
 
         qkv = self.qkv(x).reshape(B, N, 3, self.groups, C // self.groups).permute(2, 0, 3, 1, 4)
@@ -179,7 +184,7 @@ 
 
 
-class ChannelAttention(nn.Module):
+class ChannelAttention(msnn.Cell):
 
     def __init__(
             self,
@@ -195,10 +200,10 @@         head_dim = dim // num_heads
         self.scale = head_dim ** -0.5
 
-        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)
-        self.proj = nn.Linear(dim, dim, **dd)
-
-    def forward(self, x: Tensor):
+        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.proj = nn.Linear(dim, dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor):
         B, N, C = x.shape
 
         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
@@ -213,7 +218,7 @@         return x
 
 
-class ChannelBlock(nn.Module):
+class ChannelBlock(msnn.Cell):
 
     def __init__(
             self,
@@ -222,8 +227,8 @@             mlp_ratio: float = 4.,
             qkv_bias: bool = False,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
             ffn: bool = True,
             cpe_act: bool = False,
             v2: bool = False,
@@ -233,34 +238,34 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
 
-        self.cpe1 = ConvPosEnc(dim=dim, k=3, act=cpe_act, **dd)
+        self.cpe1 = ConvPosEnc(dim=dim, k=3, act=cpe_act, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.ffn = ffn
-        self.norm1 = norm_layer(dim, **dd)
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         attn_layer = ChannelAttentionV2 if v2 else ChannelAttention
         self.attn = attn_layer(
             dim,
             num_heads=num_heads,
             qkv_bias=qkv_bias,
             **dd,
-        )
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.cpe2 = ConvPosEnc(dim=dim, k=3, act=cpe_act, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+        self.cpe2 = ConvPosEnc(dim=dim, k=3, act=cpe_act, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         if self.ffn:
-            self.norm2 = norm_layer(dim, **dd)
+            self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.mlp = Mlp(
                 in_features=dim,
                 hidden_features=int(dim * mlp_ratio),
                 act_layer=act_layer,
                 **dd,
-            )
-            self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         else:
             self.norm2 = None
             self.mlp = None
             self.drop_path2 = None
 
-    def forward(self, x: Tensor):
+    def construct(self, x: ms.Tensor):
         B, C, H, W = x.shape
 
         x = self.cpe1(x).flatten(2).transpose(1, 2)
@@ -279,7 +284,7 @@         return x
 
 
-def window_partition(x: Tensor, window_size: Tuple[int, int]):
+def window_partition(x: ms.Tensor, window_size: Tuple[int, int]):
     """
     Args:
         x: (B, H, W, C)
@@ -294,7 +299,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def window_reverse(windows: Tensor, window_size: Tuple[int, int], H: int, W: int):
+def window_reverse(windows: ms.Tensor, window_size: Tuple[int, int], H: int, W: int):
     """
     Args:
         windows: (num_windows*B, window_size, window_size, C)
@@ -310,7 +315,7 @@     return x
 
 
-class WindowAttention(nn.Module):
+class WindowAttention(msnn.Cell):
     r""" Window based multi-head self attention (W-MSA) module with relative position bias.
     It supports both of shifted and non-shifted window.
     Args:
@@ -319,7 +324,7 @@         num_heads (int): Number of attention heads.
         qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
     """
-    fused_attn: torch.jit.Final[bool]
+    fused_attn: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -339,19 +344,19 @@         self.scale = head_dim ** -0.5
         self.fused_attn = use_fused_attn()
 
-        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)
-        self.proj = nn.Linear(dim, dim, **dd)
-
-        self.softmax = nn.Softmax(dim=-1)
-
-    def forward(self, x: Tensor):
+        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.proj = nn.Linear(dim, dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+        self.softmax = nn.Softmax(dim = -1)
+
+    def construct(self, x: ms.Tensor):
         B_, N, C = x.shape
 
         qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
         q, k, v = qkv.unbind(0)
 
         if self.fused_attn:
-            x = F.scaled_dot_product_attention(q, k, v)
+            x = F.scaled_dot_product_attention(q, k, v)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = (q @ k.transpose(-2, -1))
@@ -363,7 +368,7 @@         return x
 
 
-class SpatialBlock(nn.Module):
+class SpatialBlock(msnn.Cell):
     r""" Windows Block.
     Args:
         dim (int): Number of input channels.
@@ -384,8 +389,8 @@             mlp_ratio: float = 4.,
             qkv_bias: bool = True,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
             ffn: bool = True,
             cpe_act: bool = False,
             device=None,
@@ -399,34 +404,34 @@         self.window_size = to_2tuple(window_size)
         self.mlp_ratio = mlp_ratio
 
-        self.cpe1 = ConvPosEnc(dim=dim, k=3, act=cpe_act, **dd)
-        self.norm1 = norm_layer(dim, **dd)
+        self.cpe1 = ConvPosEnc(dim=dim, k=3, act=cpe_act, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.attn = WindowAttention(
             dim,
             self.window_size,
             num_heads=num_heads,
             qkv_bias=qkv_bias,
             **dd,
-        )
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        self.cpe2 = ConvPosEnc(dim=dim, k=3, act=cpe_act, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+        self.cpe2 = ConvPosEnc(dim=dim, k=3, act=cpe_act, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if self.ffn:
-            self.norm2 = norm_layer(dim, **dd)
+            self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             mlp_hidden_dim = int(dim * mlp_ratio)
             self.mlp = Mlp(
                 in_features=dim,
                 hidden_features=mlp_hidden_dim,
                 act_layer=act_layer,
                 **dd,
-            )
-            self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         else:
             self.norm2 = None
             self.mlp = None
             self.drop_path1 = None
 
-    def forward(self, x: Tensor):
+    def construct(self, x: ms.Tensor):
         B, C, H, W = x.shape
 
         shortcut = self.cpe1(x).flatten(2).transpose(1, 2)
@@ -437,7 +442,7 @@         pad_l = pad_t = 0
         pad_r = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]
         pad_b = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]
-        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))
+        x = nn.functional.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))
         _, Hp, Wp, _ = x.shape
 
         x_windows = window_partition(x, self.window_size)
@@ -466,7 +471,7 @@         return x
 
 
-class DaVitStage(nn.Module):
+class DaVitStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -479,8 +484,8 @@             mlp_ratio: float = 4.,
             qkv_bias: bool = True,
             drop_path_rates: Tuple[float, ...] = (0, 0),
-            norm_layer: Type[nn.Module] = LayerNorm2d,
-            norm_layer_cl: Type[nn.Module] = nn.LayerNorm,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
+            norm_layer_cl: Type[msnn.Cell] = nn.LayerNorm,
             ffn: bool = True,
             cpe_act: bool = False,
             down_kernel_size: int = 2,
@@ -496,9 +501,9 @@ 
         # downsample embedding layer at the beginning of each stage
         if downsample:
-            self.downsample = Downsample(in_chs, out_chs, kernel_size=down_kernel_size, norm_layer=norm_layer, **dd)
+            self.downsample = Downsample(in_chs, out_chs, kernel_size=down_kernel_size, norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         '''
          repeating alternating attention blocks in each stage
@@ -524,7 +529,7 @@                         cpe_act=cpe_act,
                         window_size=window_size,
                         **dd,
-                    )))
+                    )))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
                 elif attn_type == 'channel':
                     dual_attention_block.append(('channel_block', ChannelBlock(
                         dim=out_chs,
@@ -537,18 +542,18 @@                         cpe_act=cpe_act,
                         v2=channel_attn_v2,
                         **dd,
-                    )))
+                    )))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             if named_blocks:
-                stage_blocks.append(nn.Sequential(OrderedDict(dual_attention_block)))
+                stage_blocks.append(msnn.SequentialCell(OrderedDict(dual_attention_block)))
             else:
-                stage_blocks.append(nn.Sequential(*[b[1] for b in dual_attention_block]))
-        self.blocks = nn.Sequential(*stage_blocks)
-
-    @torch.jit.ignore
+                stage_blocks.append(msnn.SequentialCell(*[b[1] for b in dual_attention_block]))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.blocks = msnn.SequentialCell(*stage_blocks)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    def forward(self, x: Tensor):
+    def construct(self, x: ms.Tensor):
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
@@ -557,7 +562,7 @@         return x
 
 
-class DaVit(nn.Module):
+class DaVit(msnn.Cell):
     r""" DaViT
         A PyTorch implementation of `DaViT: Dual Attention Vision Transformers`  - https://arxiv.org/abs/2204.03645
         Supports arbitrary input sizes and pyramid feature extraction
@@ -613,7 +618,7 @@         self.grad_checkpointing = False
         self.feature_info = []
 
-        self.stem = Stem(in_chans, embed_dims[0], norm_layer=norm_layer, **dd)
+        self.stem = Stem(in_chans, embed_dims[0], norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         in_chs = embed_dims[0]
 
         dpr = calculate_drop_path_rates(drop_path_rate, depths, stagewise=True)
@@ -639,27 +644,27 @@                 channel_attn_v2=channel_attn_v2,
                 named_blocks=named_blocks,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             in_chs = out_chs
             stages.append(stage)
             self.feature_info += [dict(num_chs=out_chs, reduction=2**(i+2), module=f'stages.{i}')]
 
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets
         # otherwise pool -> norm -> fc, the default DaViT order, similar to ConvNeXt
         # FIXME generalize this structure to ClassifierHead
         if head_norm_first:
-            self.norm_pre = norm_layer(self.num_features, **dd)
+            self.norm_pre = norm_layer(self.num_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.head = ClassifierHead(
                 self.num_features,
                 num_classes,
                 pool_type=global_pool,
                 drop_rate=self.drop_rate,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
             self.head = NormMlpClassifierHead(
                 self.num_features,
                 num_classes,
@@ -667,16 +672,16 @@                 drop_rate=self.drop_rate,
                 norm_layer=norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.apply(self._init_weights)
 
     def _init_weights(self, m):
         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-    @torch.jit.ignore
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',  # stem and embed
@@ -687,14 +692,14 @@             ]
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
         for stage in self.stages:
             stage.set_grad_checkpointing(enable=enable)
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -703,13 +708,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -765,7 +770,7 @@         take_indices, max_index = feature_take_indices(len(self.stages), indices)
         self.stages = self.stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
@@ -782,7 +787,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -858,7 +863,7 @@         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         pretrained_strict=strict,
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     return model
 
@@ -899,37 +904,37 @@ @register_model
 def davit_tiny(pretrained=False, **kwargs) -> DaVit:
     model_args = dict(depths=(1, 1, 3, 1), embed_dims=(96, 192, 384, 768), num_heads=(3, 6, 12, 24))
-    return _create_davit('davit_tiny', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_davit('davit_tiny', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def davit_small(pretrained=False, **kwargs) -> DaVit:
     model_args = dict(depths=(1, 1, 9, 1), embed_dims=(96, 192, 384, 768), num_heads=(3, 6, 12, 24))
-    return _create_davit('davit_small', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_davit('davit_small', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def davit_base(pretrained=False, **kwargs) -> DaVit:
     model_args = dict(depths=(1, 1, 9, 1), embed_dims=(128, 256, 512, 1024), num_heads=(4, 8, 16, 32))
-    return _create_davit('davit_base', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_davit('davit_base', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def davit_large(pretrained=False, **kwargs) -> DaVit:
     model_args = dict(depths=(1, 1, 9, 1), embed_dims=(192, 384, 768, 1536), num_heads=(6, 12, 24, 48))
-    return _create_davit('davit_large', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_davit('davit_large', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def davit_huge(pretrained=False, **kwargs) -> DaVit:
     model_args = dict(depths=(1, 1, 9, 1), embed_dims=(256, 512, 1024, 2048), num_heads=(8, 16, 32, 64))
-    return _create_davit('davit_huge', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_davit('davit_huge', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def davit_giant(pretrained=False, **kwargs) -> DaVit:
     model_args = dict(depths=(1, 1, 12, 3), embed_dims=(384, 768, 1536, 3072), num_heads=(12, 24, 48, 96))
-    return _create_davit('davit_giant', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_davit('davit_giant', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 
@@ -939,7 +944,7 @@         depths=(1, 1, 9, 1), embed_dims=(128, 256, 512, 1024), num_heads=(4, 8, 16, 32),
         window_size=12, down_kernel_size=3, channel_attn_v2=True, named_blocks=True,
     )
-    return _create_davit('davit_base_fl', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_davit('davit_base_fl', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -949,4 +954,4 @@         depths=(1, 1, 9, 1), embed_dims=(256, 512, 1024, 2048), num_heads=(8, 16, 32, 64),
         window_size=12, down_kernel_size=3, channel_attn_v2=True, named_blocks=True,
     )
-    return _create_davit('davit_huge_fl', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_davit('davit_huge_fl', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
