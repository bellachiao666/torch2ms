--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Deep Layer Aggregation and DLA w/ Res2Net
 DLA original adapted from Official Pytorch impl at: https://github.com/ucbdrive/dla
 DLA Paper: `Deep Layer Aggregation` - https://arxiv.org/abs/1707.06484
@@ -7,9 +12,7 @@ """
 import math
 from typing import List, Optional, Tuple, Type
-
-import torch
-import torch.nn as nn
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
@@ -19,7 +22,7 @@ __all__ = ['DLA']
 
 
-class DlaBasic(nn.Module):
+class DlaBasic(msnn.Cell):
     """DLA Basic"""
 
     def __init__(
@@ -43,9 +46,9 @@             bias=False,
             dilation=dilation,
             **dd,
-        )
-        self.bn1 = nn.BatchNorm2d(planes, **dd)
-        self.relu = nn.ReLU(inplace=True)
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn1 = nn.BatchNorm2d(planes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.conv2 = nn.Conv2d(
             planes,
             planes,
@@ -55,11 +58,11 @@             bias=False,
             dilation=dilation,
             **dd,
-        )
-        self.bn2 = nn.BatchNorm2d(planes, **dd)
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn2 = nn.BatchNorm2d(planes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.stride = stride
 
-    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
+    def construct(self, x, shortcut: Optional[ms.Tensor] = None, children: Optional[List[ms.Tensor]] = None):
         if shortcut is None:
             shortcut = x
 
@@ -76,7 +79,7 @@         return out
 
 
-class DlaBottleneck(nn.Module):
+class DlaBottleneck(msnn.Cell):
     """DLA/DLA-X Bottleneck"""
     expansion = 2
 
@@ -97,8 +100,8 @@         mid_planes = int(math.floor(outplanes * (base_width / 64)) * cardinality)
         mid_planes = mid_planes // self.expansion
 
-        self.conv1 = nn.Conv2d(inplanes, mid_planes, kernel_size=1, bias=False, **dd)
-        self.bn1 = nn.BatchNorm2d(mid_planes, **dd)
+        self.conv1 = nn.Conv2d(inplanes, mid_planes, kernel_size=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn1 = nn.BatchNorm2d(mid_planes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.conv2 = nn.Conv2d(
             mid_planes,
             mid_planes,
@@ -109,13 +112,13 @@             dilation=dilation,
             groups=cardinality,
             **dd,
-        )
-        self.bn2 = nn.BatchNorm2d(mid_planes, **dd)
-        self.conv3 = nn.Conv2d(mid_planes, outplanes, kernel_size=1, bias=False, **dd)
-        self.bn3 = nn.BatchNorm2d(outplanes, **dd)
-        self.relu = nn.ReLU(inplace=True)
-
-    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn2 = nn.BatchNorm2d(mid_planes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.conv3 = nn.Conv2d(mid_planes, outplanes, kernel_size=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn3 = nn.BatchNorm2d(outplanes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+    def construct(self, x, shortcut: Optional[ms.Tensor] = None, children: Optional[List[ms.Tensor]] = None):
         if shortcut is None:
             shortcut = x
 
@@ -136,7 +139,7 @@         return out
 
 
-class DlaBottle2neck(nn.Module):
+class DlaBottle2neck(msnn.Cell):
     """ Res2Net/Res2NeXT DLA Bottleneck
     Adapted from https://github.com/gasvn/Res2Net/blob/master/dla.py
     """
@@ -162,8 +165,8 @@         mid_planes = mid_planes // self.expansion
         self.width = mid_planes
 
-        self.conv1 = nn.Conv2d(inplanes, mid_planes * scale, kernel_size=1, bias=False, **dd)
-        self.bn1 = nn.BatchNorm2d(mid_planes * scale, **dd)
+        self.conv1 = nn.Conv2d(inplanes, mid_planes * scale, kernel_size=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn1 = nn.BatchNorm2d(mid_planes * scale, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
 
         num_scale_convs = max(1, scale - 1)
         convs = []
@@ -179,17 +182,17 @@                 groups=cardinality,
                 bias=False,
                 **dd,
-            ))
-            bns.append(nn.BatchNorm2d(mid_planes, **dd))
-        self.convs = nn.ModuleList(convs)
-        self.bns = nn.ModuleList(bns)
-        self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1) if self.is_first else None
-
-        self.conv3 = nn.Conv2d(mid_planes * scale, outplanes, kernel_size=1, bias=False, **dd)
-        self.bn3 = nn.BatchNorm2d(outplanes, **dd)
-        self.relu = nn.ReLU(inplace=True)
-
-    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
+            ))  # 存在 *args/**kwargs，需手动确认参数映射;
+            bns.append(nn.BatchNorm2d(mid_planes, **dd))  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.convs = msnn.CellList(convs)
+        self.bns = msnn.CellList(bns)
+        self.pool = nn.AvgPool2d(kernel_size = 3, stride = stride, padding = 1) if self.is_first else None
+
+        self.conv3 = nn.Conv2d(mid_planes * scale, outplanes, kernel_size=1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn3 = nn.BatchNorm2d(outplanes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+    def construct(self, x, shortcut: Optional[ms.Tensor] = None, children: Optional[List[ms.Tensor]] = None):
         if shortcut is None:
             shortcut = x
 
@@ -197,7 +200,7 @@         out = self.bn1(out)
         out = self.relu(out)
 
-        spx = torch.split(out, self.width, 1)
+        spx = mint.split(out, self.width, 1)
         spo = []
         sp = spx[0]  # redundant, for torchscript
         for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):
@@ -214,7 +217,7 @@                 spo.append(self.pool(spx[-1]))
             else:
                 spo.append(spx[-1])
-        out = torch.cat(spo, 1)
+        out = mint.cat(spo, 1)
 
         out = self.conv3(out)
         out = self.bn3(out)
@@ -225,7 +228,7 @@         return out
 
 
-class DlaRoot(nn.Module):
+class DlaRoot(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -245,13 +248,13 @@             bias=False,
             padding=(kernel_size - 1) // 2,
             **dd,
-        )
-        self.bn = nn.BatchNorm2d(out_channels, **dd)
-        self.relu = nn.ReLU(inplace=True)
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn = nn.BatchNorm2d(out_channels, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.shortcut = shortcut
 
-    def forward(self, x_children: List[torch.Tensor]):
-        x = self.conv(torch.cat(x_children, 1))
+    def construct(self, x_children: List[ms.Tensor]):
+        x = self.conv(mint.cat(x_children, 1))
         x = self.bn(x)
         if self.shortcut:
             x += x_children[0]
@@ -260,11 +263,11 @@         return x
 
 
-class DlaTree(nn.Module):
+class DlaTree(msnn.Cell):
     def __init__(
             self,
             levels: int,
-            block: Type[nn.Module],
+            block: Type[msnn.Cell],
             in_channels: int,
             out_channels: int,
             stride: int = 1,
@@ -284,20 +287,22 @@             root_dim = 2 * out_channels
         if level_root:
             root_dim += in_channels
-        self.downsample = nn.MaxPool2d(stride, stride=stride) if stride > 1 else nn.Identity()
-        self.project = nn.Identity()
-        cargs = dict(dilation=dilation, cardinality=cardinality, base_width=base_width, **dd)
+        self.downsample = nn.MaxPool2d(stride, stride = stride) if stride > 1 else msnn.Identity()
+        self.project = msnn.Identity()
+        cargs = dict(dilation=dilation, cardinality=cardinality, base_width=base_width, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if levels == 1:
-            self.tree1 = block(in_channels, out_channels, stride, **cargs)
-            self.tree2 = block(out_channels, out_channels, 1, **cargs)
+            self.tree1 = block(in_channels, out_channels, stride, **cargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.tree2 = block(out_channels, out_channels, 1, **cargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             if in_channels != out_channels:
                 # NOTE the official impl/weights have  project layers in levels > 1 case that are never
                 # used, I've moved the project layer here to avoid wasted params but old checkpoints will
                 # need strict=False while loading.
-                self.project = nn.Sequential(
+                self.project = msnn.SequentialCell(
+                    [
                     nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False, **dd),
-                    nn.BatchNorm2d(out_channels, **dd))
-            self.root = DlaRoot(root_dim, out_channels, root_kernel_size, root_shortcut, **dd)
+                    nn.BatchNorm2d(out_channels, **dd)
+                ])  # 存在 *args/**kwargs，需手动确认参数映射;
+            self.root = DlaRoot(root_dim, out_channels, root_kernel_size, root_shortcut, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             cargs.update(dict(root_kernel_size=root_kernel_size, root_shortcut=root_shortcut))
             self.tree1 = DlaTree(
@@ -308,7 +313,7 @@                 stride,
                 root_dim=0,
                 **cargs,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.tree2 = DlaTree(
                 levels - 1,
                 block,
@@ -316,13 +321,13 @@                 out_channels,
                 root_dim=root_dim + out_channels,
                 **cargs,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.root = None
         self.level_root = level_root
         self.root_dim = root_dim
         self.levels = levels
 
-    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
+    def construct(self, x, shortcut: Optional[ms.Tensor] = None, children: Optional[List[ms.Tensor]] = None):
         if children is None:
             children = []
         bottom = self.downsample(x)
@@ -339,7 +344,7 @@         return x
 
 
-class DLA(nn.Module):
+class DLA(msnn.Cell):
     def __init__(
             self,
             levels: Tuple[int, ...],
@@ -350,7 +355,7 @@             global_pool: str = 'avg',
             cardinality: int = 1,
             base_width: int = 64,
-            block: Type[nn.Module] = DlaBottle2neck,
+            block: Type[msnn.Cell] = DlaBottle2neck,
             shortcut_root: bool = False,
             drop_rate: float = 0.0,
             device=None,
@@ -364,18 +369,19 @@         self.base_width = base_width
         assert output_stride == 32  # FIXME support dilation
 
-        self.base_layer = nn.Sequential(
+        self.base_layer = msnn.SequentialCell(
+            [
             nn.Conv2d(in_chans, channels[0], kernel_size=7, stride=1, padding=3, bias=False, **dd),
             nn.BatchNorm2d(channels[0], **dd),
-            nn.ReLU(inplace=True),
-        )
-        self.level0 = self._make_conv_level(channels[0], channels[0], levels[0], **dd)
-        self.level1 = self._make_conv_level(channels[0], channels[1], levels[1], stride=2, **dd)
-        cargs = dict(cardinality=cardinality, base_width=base_width, root_shortcut=shortcut_root, **dd)
-        self.level2 = DlaTree(levels[2], block, channels[1], channels[2], 2, level_root=False, **cargs)
-        self.level3 = DlaTree(levels[3], block, channels[2], channels[3], 2, level_root=True, **cargs)
-        self.level4 = DlaTree(levels[4], block, channels[3], channels[4], 2, level_root=True, **cargs)
-        self.level5 = DlaTree(levels[5], block, channels[4], channels[5], 2, level_root=True, **cargs)
+            nn.ReLU()
+        ])  # 存在 *args/**kwargs，需手动确认参数映射;; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.level0 = self._make_conv_level(channels[0], channels[0], levels[0], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.level1 = self._make_conv_level(channels[0], channels[1], levels[1], stride=2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        cargs = dict(cardinality=cardinality, base_width=base_width, root_shortcut=shortcut_root, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.level2 = DlaTree(levels[2], block, channels[1], channels[2], 2, level_root=False, **cargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.level3 = DlaTree(levels[3], block, channels[2], channels[3], 2, level_root=True, **cargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.level4 = DlaTree(levels[4], block, channels[3], channels[4], 2, level_root=True, **cargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.level5 = DlaTree(levels[5], block, channels[4], channels[5], 2, level_root=True, **cargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info = [
             dict(num_chs=channels[0], reduction=1, module='level0'),  # rare to have a meaningful stride 1 level
             dict(num_chs=channels[1], reduction=2, module='level1'),
@@ -393,8 +399,8 @@             use_conv=True,
             drop_rate=drop_rate,
             **dd,
-        )
-        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.flatten = mint.flatten(1) if global_pool else msnn.Identity()
 
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
@@ -420,11 +426,11 @@                     **dd,
                 ),
                 nn.BatchNorm2d(planes, **dd),
-                nn.ReLU(inplace=True)])
+                nn.ReLU()])  # 存在 *args/**kwargs，需手动确认参数映射;; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
             inplanes = planes
-        return nn.Sequential(*modules)
-
-    @torch.jit.ignore
+        return msnn.SequentialCell(*modules)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(
             stem=r'^base_layer',
@@ -437,19 +443,19 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         assert not enable, 'gradient checkpointing not supported'
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.fc
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
         self.num_classes = num_classes
         self.global_pool, self.fc = create_classifier(
             self.num_features, self.num_classes, pool_type=global_pool, use_conv=True)
-        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()
+        self.flatten = mint.flatten(1) if global_pool else msnn.Identity()
 
     def forward_features(self, x):
         x = self.base_layer(x)
@@ -469,7 +475,7 @@         x = self.fc(x)
         return self.flatten(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -483,7 +489,7 @@         pretrained_strict=False,
         feature_cfg=dict(out_indices=(1, 2, 3, 4, 5)),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url='', **kwargs):
@@ -518,7 +524,7 @@     model_args = dict(
         levels=(1, 1, 1, 2, 3, 1), channels=(16, 32, 128, 256, 512, 1024),
         block=DlaBottle2neck, cardinality=1, base_width=28)
-    return _create_dla('dla60_res2net', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla60_res2net', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -526,21 +532,21 @@     model_args = dict(
         levels=(1, 1, 1, 2, 3, 1), channels=(16, 32, 128, 256, 512, 1024),
         block=DlaBottle2neck, cardinality=8, base_width=4)
-    return _create_dla('dla60_res2next', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla60_res2next', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def dla34(pretrained=False, **kwargs) -> DLA:  # DLA-34
     model_args = dict(
         levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 128, 256, 512], block=DlaBasic)
-    return _create_dla('dla34', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla34', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def dla46_c(pretrained=False, **kwargs) -> DLA:  # DLA-46-C
     model_args = dict(
         levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 64, 128, 256], block=DlaBottleneck)
-    return _create_dla('dla46_c', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla46_c', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -548,7 +554,7 @@     model_args = dict(
         levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 64, 128, 256],
         block=DlaBottleneck, cardinality=32, base_width=4)
-    return _create_dla('dla46x_c', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla46x_c', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -556,7 +562,7 @@     model_args = dict(
         levels=[1, 1, 1, 2, 3, 1], channels=[16, 32, 64, 64, 128, 256],
         block=DlaBottleneck, cardinality=32, base_width=4)
-    return _create_dla('dla60x_c', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla60x_c', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -564,7 +570,7 @@     model_args = dict(
         levels=[1, 1, 1, 2, 3, 1], channels=[16, 32, 128, 256, 512, 1024],
         block=DlaBottleneck)
-    return _create_dla('dla60', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla60', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -572,7 +578,7 @@     model_args = dict(
         levels=[1, 1, 1, 2, 3, 1], channels=[16, 32, 128, 256, 512, 1024],
         block=DlaBottleneck, cardinality=32, base_width=4)
-    return _create_dla('dla60x', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla60x', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -580,7 +586,7 @@     model_args = dict(
         levels=[1, 1, 1, 3, 4, 1], channels=[16, 32, 128, 256, 512, 1024],
         block=DlaBottleneck, shortcut_root=True)
-    return _create_dla('dla102', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla102', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -588,7 +594,7 @@     model_args = dict(
         levels=[1, 1, 1, 3, 4, 1], channels=[16, 32, 128, 256, 512, 1024],
         block=DlaBottleneck, cardinality=32, base_width=4, shortcut_root=True)
-    return _create_dla('dla102x', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla102x', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -596,7 +602,7 @@     model_args = dict(
         levels=[1, 1, 1, 3, 4, 1], channels=[16, 32, 128, 256, 512, 1024],
         block=DlaBottleneck, cardinality=64, base_width=4, shortcut_root=True)
-    return _create_dla('dla102x2', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla102x2', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -604,4 +610,4 @@     model_args = dict(
         levels=[1, 1, 2, 3, 5, 1], channels=[16, 32, 128, 256, 512, 1024],
         block=DlaBottleneck, shortcut_root=True)
-    return _create_dla('dla169', pretrained, **dict(model_args, **kwargs))
+    return _create_dla('dla169', pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
