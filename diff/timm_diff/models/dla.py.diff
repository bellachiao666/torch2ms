--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Deep Layer Aggregation and DLA w/ Res2Net
 DLA original adapted from Official Pytorch impl at: https://github.com/ucbdrive/dla
 DLA Paper: `Deep Layer Aggregation` - https://arxiv.org/abs/1707.06484
@@ -8,8 +13,8 @@ import math
 from typing import List, Optional, Tuple, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
@@ -19,7 +24,7 @@ __all__ = ['DLA']
 
 
-class DlaBasic(nn.Module):
+class DlaBasic(msnn.Cell):
     """DLA Basic"""
 
     def __init__(
@@ -45,7 +50,7 @@             **dd,
         )
         self.bn1 = nn.BatchNorm2d(planes, **dd)
-        self.relu = nn.ReLU(inplace=True)
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.conv2 = nn.Conv2d(
             planes,
             planes,
@@ -59,7 +64,7 @@         self.bn2 = nn.BatchNorm2d(planes, **dd)
         self.stride = stride
 
-    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
+    def construct(self, x, shortcut: Optional[ms.Tensor] = None, children: Optional[List[ms.Tensor]] = None):
         if shortcut is None:
             shortcut = x
 
@@ -76,7 +81,7 @@         return out
 
 
-class DlaBottleneck(nn.Module):
+class DlaBottleneck(msnn.Cell):
     """DLA/DLA-X Bottleneck"""
     expansion = 2
 
@@ -113,9 +118,9 @@         self.bn2 = nn.BatchNorm2d(mid_planes, **dd)
         self.conv3 = nn.Conv2d(mid_planes, outplanes, kernel_size=1, bias=False, **dd)
         self.bn3 = nn.BatchNorm2d(outplanes, **dd)
-        self.relu = nn.ReLU(inplace=True)
-
-    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+    def construct(self, x, shortcut: Optional[ms.Tensor] = None, children: Optional[List[ms.Tensor]] = None):
         if shortcut is None:
             shortcut = x
 
@@ -136,7 +141,7 @@         return out
 
 
-class DlaBottle2neck(nn.Module):
+class DlaBottle2neck(msnn.Cell):
     """ Res2Net/Res2NeXT DLA Bottleneck
     Adapted from https://github.com/gasvn/Res2Net/blob/master/dla.py
     """
@@ -181,15 +186,15 @@                 **dd,
             ))
             bns.append(nn.BatchNorm2d(mid_planes, **dd))
-        self.convs = nn.ModuleList(convs)
-        self.bns = nn.ModuleList(bns)
-        self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1) if self.is_first else None
+        self.convs = msnn.CellList(convs)
+        self.bns = msnn.CellList(bns)
+        self.pool = nn.AvgPool2d(kernel_size = 3, stride = stride, padding = 1) if self.is_first else None
 
         self.conv3 = nn.Conv2d(mid_planes * scale, outplanes, kernel_size=1, bias=False, **dd)
         self.bn3 = nn.BatchNorm2d(outplanes, **dd)
-        self.relu = nn.ReLU(inplace=True)
-
-    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+    def construct(self, x, shortcut: Optional[ms.Tensor] = None, children: Optional[List[ms.Tensor]] = None):
         if shortcut is None:
             shortcut = x
 
@@ -197,7 +202,7 @@         out = self.bn1(out)
         out = self.relu(out)
 
-        spx = torch.split(out, self.width, 1)
+        spx = mint.split(out, self.width, 1)
         spo = []
         sp = spx[0]  # redundant, for torchscript
         for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):
@@ -214,7 +219,7 @@                 spo.append(self.pool(spx[-1]))
             else:
                 spo.append(spx[-1])
-        out = torch.cat(spo, 1)
+        out = mint.cat(spo, 1)
 
         out = self.conv3(out)
         out = self.bn3(out)
@@ -225,7 +230,7 @@         return out
 
 
-class DlaRoot(nn.Module):
+class DlaRoot(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -247,11 +252,11 @@             **dd,
         )
         self.bn = nn.BatchNorm2d(out_channels, **dd)
-        self.relu = nn.ReLU(inplace=True)
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.shortcut = shortcut
 
-    def forward(self, x_children: List[torch.Tensor]):
-        x = self.conv(torch.cat(x_children, 1))
+    def construct(self, x_children: List[ms.Tensor]):
+        x = self.conv(mint.cat(x_children, 1))
         x = self.bn(x)
         if self.shortcut:
             x += x_children[0]
@@ -260,11 +265,11 @@         return x
 
 
-class DlaTree(nn.Module):
+class DlaTree(msnn.Cell):
     def __init__(
             self,
             levels: int,
-            block: Type[nn.Module],
+            block: Type[msnn.Cell],
             in_channels: int,
             out_channels: int,
             stride: int = 1,
@@ -284,8 +289,8 @@             root_dim = 2 * out_channels
         if level_root:
             root_dim += in_channels
-        self.downsample = nn.MaxPool2d(stride, stride=stride) if stride > 1 else nn.Identity()
-        self.project = nn.Identity()
+        self.downsample = nn.MaxPool2d(stride, stride = stride) if stride > 1 else msnn.Identity()
+        self.project = msnn.Identity()
         cargs = dict(dilation=dilation, cardinality=cardinality, base_width=base_width, **dd)
         if levels == 1:
             self.tree1 = block(in_channels, out_channels, stride, **cargs)
@@ -294,9 +299,11 @@                 # NOTE the official impl/weights have  project layers in levels > 1 case that are never
                 # used, I've moved the project layer here to avoid wasted params but old checkpoints will
                 # need strict=False while loading.
-                self.project = nn.Sequential(
+                self.project = msnn.SequentialCell(
+                    [
                     nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False, **dd),
-                    nn.BatchNorm2d(out_channels, **dd))
+                    nn.BatchNorm2d(out_channels, **dd)
+                ])
             self.root = DlaRoot(root_dim, out_channels, root_kernel_size, root_shortcut, **dd)
         else:
             cargs.update(dict(root_kernel_size=root_kernel_size, root_shortcut=root_shortcut))
@@ -322,7 +329,7 @@         self.root_dim = root_dim
         self.levels = levels
 
-    def forward(self, x, shortcut: Optional[torch.Tensor] = None, children: Optional[List[torch.Tensor]] = None):
+    def construct(self, x, shortcut: Optional[ms.Tensor] = None, children: Optional[List[ms.Tensor]] = None):
         if children is None:
             children = []
         bottom = self.downsample(x)
@@ -339,7 +346,7 @@         return x
 
 
-class DLA(nn.Module):
+class DLA(msnn.Cell):
     def __init__(
             self,
             levels: Tuple[int, ...],
@@ -350,7 +357,7 @@             global_pool: str = 'avg',
             cardinality: int = 1,
             base_width: int = 64,
-            block: Type[nn.Module] = DlaBottle2neck,
+            block: Type[msnn.Cell] = DlaBottle2neck,
             shortcut_root: bool = False,
             drop_rate: float = 0.0,
             device=None,
@@ -364,11 +371,12 @@         self.base_width = base_width
         assert output_stride == 32  # FIXME support dilation
 
-        self.base_layer = nn.Sequential(
+        self.base_layer = msnn.SequentialCell(
+            [
             nn.Conv2d(in_chans, channels[0], kernel_size=7, stride=1, padding=3, bias=False, **dd),
             nn.BatchNorm2d(channels[0], **dd),
-            nn.ReLU(inplace=True),
-        )
+            nn.ReLU()
+        ])  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.level0 = self._make_conv_level(channels[0], channels[0], levels[0], **dd)
         self.level1 = self._make_conv_level(channels[0], channels[1], levels[1], stride=2, **dd)
         cargs = dict(cardinality=cardinality, base_width=base_width, root_shortcut=shortcut_root, **dd)
@@ -394,7 +402,7 @@             drop_rate=drop_rate,
             **dd,
         )
-        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()
+        self.flatten = mint.flatten(1) if global_pool else msnn.Identity()
 
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
@@ -420,9 +428,11 @@                     **dd,
                 ),
                 nn.BatchNorm2d(planes, **dd),
-                nn.ReLU(inplace=True)])
+                nn.ReLU()])  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
             inplanes = planes
-        return nn.Sequential(*modules)
+        return msnn.SequentialCell([
+            modules
+        ])
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -442,14 +452,14 @@         assert not enable, 'gradient checkpointing not supported'
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.fc
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
         self.num_classes = num_classes
         self.global_pool, self.fc = create_classifier(
             self.num_features, self.num_classes, pool_type=global_pool, use_conv=True)
-        self.flatten = nn.Flatten(1) if global_pool else nn.Identity()
+        self.flatten = mint.flatten(1) if global_pool else msnn.Identity()
 
     def forward_features(self, x):
         x = self.base_layer(x)
@@ -469,7 +479,7 @@         x = self.fc(x)
         return self.flatten(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
