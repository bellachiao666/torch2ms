--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ EdgeNeXt
 
 Paper: `EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications`
@@ -11,9 +16,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn.functional as F
-from torch import nn
+# import torch
+# from torch import nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -37,7 +41,7 @@ 
 
 @register_notrace_module  # reason: FX can't symbolically trace torch.arange in forward method
-class PositionalEncodingFourier(nn.Module):
+class PositionalEncodingFourier(msnn.Cell):
     def __init__(
             self,
             hidden_dim: int = 32,
@@ -48,40 +52,40 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.token_projection = nn.Conv2d(hidden_dim * 2, dim, kernel_size=1, **dd)
+        self.token_projection = nn.Conv2d(hidden_dim * 2, dim, kernel_size=1, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.scale = 2 * math.pi
         self.temperature = temperature
         self.hidden_dim = hidden_dim
         self.dim = dim
 
-    def forward(self, shape: Tuple[int, int, int]):
+    def construct(self, shape: Tuple[int, int, int]):
         device = self.token_projection.weight.device
         dtype = self.token_projection.weight.dtype
-        inv_mask = ~torch.zeros(shape).to(device=device, dtype=torch.bool)
-        y_embed = inv_mask.cumsum(1, dtype=torch.float32)
-        x_embed = inv_mask.cumsum(2, dtype=torch.float32)
+        inv_mask = ~mint.zeros(shape).to(device=device, dtype=ms.bool_)
+        y_embed = inv_mask.cumsum(1, dtype=ms.float32)
+        x_embed = inv_mask.cumsum(2, dtype=ms.float32)
         eps = 1e-6
         y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale
         x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale
 
-        dim_t = torch.arange(self.hidden_dim, dtype=torch.int64, device=device).to(torch.float32)
-        dim_t = self.temperature ** (2 * torch.div(dim_t, 2, rounding_mode='floor') / self.hidden_dim)
+        dim_t = mint.arange(self.hidden_dim, dtype=ms.int64, device=device).to(ms.float32)
+        dim_t = self.temperature ** (2 * mint.div(dim_t, 2, rounding_mode='floor') / self.hidden_dim)
 
         pos_x = x_embed[:, :, :, None] / dim_t
         pos_y = y_embed[:, :, :, None] / dim_t
-        pos_x = torch.stack(
+        pos_x = mint.stack(
             (pos_x[:, :, :, 0::2].sin(),
              pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)
-        pos_y = torch.stack(
+        pos_y = mint.stack(
             (pos_y[:, :, :, 0::2].sin(),
              pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)
-        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)
+        pos = mint.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)
         pos = self.token_projection(pos.to(dtype))
 
         return pos
 
 
-class ConvBlock(nn.Module):
+class ConvBlock(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -91,8 +95,8 @@             conv_bias: bool = True,
             expand_ratio: float = 4,
             ls_init_value: float = 1e-6,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop_path: float = 0.,
             device=None,
             dtype=None,
@@ -110,18 +114,18 @@             depthwise=True,
             bias=conv_bias,
             **dd,
-        )
-        self.norm = norm_layer(dim_out, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm = norm_layer(dim_out, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.mlp = Mlp(
             dim_out,
             int(expand_ratio * dim_out),
             act_layer=act_layer,
             **dd,
-        )
-        self.gamma = nn.Parameter(ls_init_value * torch.ones(dim_out, **dd)) if ls_init_value > 0 else None
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.gamma = ms.Parameter(ls_init_value * mint.ones(dim_out, **dd)) if ls_init_value > 0 else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x
         x = self.conv_dw(x)
         if self.shortcut_after_dw:
@@ -138,7 +142,7 @@         return x
 
 
-class CrossCovarianceAttn(nn.Module):
+class CrossCovarianceAttn(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -152,20 +156,20 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.num_heads = num_heads
-        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1, **dd))
-
-        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)
+        self.temperature = ms.Parameter(mint.ones(num_heads, 1, 1, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn_drop = nn.Dropout(attn_drop)
-        self.proj = nn.Linear(dim, dim, **dd)
+        self.proj = nn.Linear(dim, dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x):
+    def construct(self, x):
         B, N, C = x.shape
         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 4, 1)
         q, k, v = qkv.unbind(0)
 
         # NOTE, this is NOT spatial attn, q, k, v are B, num_heads, C, L -->  C x C attn map
-        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)) * self.temperature
+        attn = (nn.functional.normalize(q, dim = -1) @ mint.transpose(-2, -1)) * self.temperature
         attn = attn.softmax(dim=-1)
         attn = self.attn_drop(attn)
         x = (attn @ v)
@@ -175,12 +179,12 @@         x = self.proj_drop(x)
         return x
 
-    @torch.jit.ignore
+    @ms.jit
     def no_weight_decay(self):
         return {'temperature'}
 
 
-class SplitTransposeBlock(nn.Module):
+class SplitTransposeBlock(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -191,8 +195,8 @@             conv_bias: bool = True,
             qkv_bias: bool = True,
             ls_init_value: float = 1e-6,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop_path: float = 0.,
             attn_drop: float = 0.,
             proj_drop: float = 0.,
@@ -207,14 +211,14 @@ 
         convs = []
         for i in range(self.num_scales):
-            convs.append(create_conv2d(width, width, kernel_size=3, depthwise=True, bias=conv_bias, **dd))
-        self.convs = nn.ModuleList(convs)
+            convs.append(create_conv2d(width, width, kernel_size=3, depthwise=True, bias=conv_bias, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.convs = msnn.CellList(convs)
 
         self.pos_embd = None
         if use_pos_emb:
-            self.pos_embd = PositionalEncodingFourier(dim=dim, **dd)
-        self.norm_xca = norm_layer(dim, **dd)
-        self.gamma_xca = nn.Parameter(ls_init_value * torch.ones(dim, **dd)) if ls_init_value > 0 else None
+            self.pos_embd = PositionalEncodingFourier(dim=dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm_xca = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.gamma_xca = ms.Parameter(ls_init_value * mint.ones(dim, **dd)) if ls_init_value > 0 else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.xca = CrossCovarianceAttn(
             dim,
             num_heads=num_heads,
@@ -222,19 +226,19 @@             attn_drop=attn_drop,
             proj_drop=proj_drop,
             **dd,
-        )
-
-        self.norm = norm_layer(dim, eps=1e-6, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.norm = norm_layer(dim, eps=1e-6, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.mlp = Mlp(
             dim,
             int(expand_ratio * dim),
             act_layer=act_layer,
             **dd,
-        )
-        self.gamma = nn.Parameter(ls_init_value * torch.ones(dim, **dd)) if ls_init_value > 0 else None
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.gamma = ms.Parameter(ls_init_value * mint.ones(dim, **dd)) if ls_init_value > 0 else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x
 
         # scales code re-written for torchscript as per my res2net fixes -rw
@@ -248,7 +252,7 @@             sp = conv(sp)
             spo.append(sp)
         spo.append(spx[-1])
-        x = torch.cat(spo, 1)
+        x = mint.cat(spo, 1)
 
         # XCA
         B, C, H, W = x.shape
@@ -270,7 +274,7 @@         return x
 
 
-class EdgeNeXtStage(nn.Module):
+class EdgeNeXtStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -287,9 +291,9 @@             conv_bias: float = True,
             ls_init_value: float = 1.0,
             drop_path_rates: Optional[List[float]] = None,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
-            norm_layer_cl: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
+            norm_layer_cl: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
     ):
@@ -298,12 +302,13 @@         self.grad_checkpointing = False
 
         if downsample_block or stride == 1:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
         else:
-            self.downsample = nn.Sequential(
+            self.downsample = msnn.SequentialCell(
+                [
                 norm_layer(in_chs, **dd),
                 nn.Conv2d(in_chs, out_chs, kernel_size=2, stride=2, bias=conv_bias, **dd)
-            )
+            ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;; 存在 *args/**kwargs，需手动确认参数映射;
             in_chs = out_chs
 
         stage_blocks = []
@@ -323,7 +328,7 @@                         act_layer=act_layer,
                         **dd,
                     )
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
                 stage_blocks.append(
                     SplitTransposeBlock(
@@ -339,12 +344,13 @@                         act_layer=act_layer,
                         **dd,
                     )
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             in_chs = out_chs
-        self.blocks = nn.Sequential(*stage_blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell(*stage_blocks)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.downsample(x)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
@@ -352,7 +358,7 @@         return x
 
 
-class EdgeNeXt(nn.Module):
+class EdgeNeXt(msnn.Cell):
     def __init__(
             self,
             in_chans: int = 3,
@@ -372,7 +378,7 @@             conv_bias: bool = True,
             stem_type: str = 'patch',
             head_norm_first: bool = False,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop_path_rate: float = 0.,
             drop_rate: float = 0.,
             device=None,
@@ -389,15 +395,17 @@ 
         assert stem_type in ('patch', 'overlap')
         if stem_type == 'patch':
-            self.stem = nn.Sequential(
+            self.stem = msnn.SequentialCell(
+                [
                 nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4, bias=conv_bias, **dd,),
-                norm_layer(dims[0], **dd),
-            )
+                norm_layer(dims[0], **dd)
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.stem = nn.Sequential(
+            self.stem = msnn.SequentialCell(
+                [
                 nn.Conv2d(in_chans, dims[0], kernel_size=9, stride=4, padding=9 // 2, bias=conv_bias, **dd),
-                norm_layer(dims[0], **dd),
-            )
+                norm_layer(dims[0], **dd)
+            ])  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         curr_stride = 4
         stages = []
@@ -426,25 +434,25 @@                 norm_layer_cl=norm_layer_cl,
                 act_layer=act_layer,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             # NOTE feature_info use currently assumes stage 0 == stride 1, rest are stride 2
             in_chs = dims[i]
             self.feature_info += [dict(num_chs=in_chs, reduction=curr_stride, module=f'stages.{i}')]
 
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.num_features = self.head_hidden_size = dims[-1]
         if head_norm_first:
-            self.norm_pre = norm_layer(self.num_features, **dd)
+            self.norm_pre = norm_layer(self.num_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.head = ClassifierHead(
                 self.num_features,
                 num_classes,
                 pool_type=global_pool,
                 drop_rate=self.drop_rate,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
             self.head = NormMlpClassifierHead(
                 self.num_features,
                 num_classes,
@@ -452,11 +460,11 @@                 drop_rate=self.drop_rate,
                 norm_layer=norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',
@@ -467,13 +475,13 @@             ]
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         for s in self.stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -482,13 +490,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -508,6 +516,7 @@         # forward pass
         x = self.stem(x)
         last_idx = len(self.stages) - 1
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             stages = self.stages
         else:
@@ -541,7 +550,7 @@         take_indices, max_index = feature_take_indices(len(self.stages), indices)
         self.stages = self.stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
@@ -555,7 +564,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -565,10 +574,10 @@     if isinstance(module, nn.Conv2d):
         trunc_normal_tf_(module.weight, std=.02)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Linear):
         trunc_normal_tf_(module.weight, std=.02)
-        nn.init.zeros_(module.bias)
+        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if name and 'head.' in name:
             module.weight.data.mul_(head_init_scale)
             module.bias.data.mul_(head_init_scale)
@@ -610,7 +619,7 @@         EdgeNeXt, variant, pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(out_indices=(0, 1, 2, 3), flatten_sequential=True),
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -660,7 +669,7 @@     # Jetson FPS=51.66 versus 47.67 for MobileViT_XXS
     # For A100: FPS @ BS=1: 212.13 & @ BS=256: 7042.06 versus FPS @ BS=1: 96.68 & @ BS=256: 4624.71 for MobileViT_XXS
     model_args = dict(depths=(2, 2, 6, 2), dims=(24, 48, 88, 168), heads=(4, 4, 4, 4))
-    return _create_edgenext('edgenext_xx_small', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_edgenext('edgenext_xx_small', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -671,7 +680,7 @@     # Jetson FPS=31.61 versus 28.49 for MobileViT_XS
     # For A100: FPS @ BS=1: 179.55 & @ BS=256: 4404.95 versus FPS @ BS=1: 94.55 & @ BS=256: 2361.53 for MobileViT_XS
     model_args = dict(depths=(3, 3, 9, 3), dims=(32, 64, 100, 192), heads=(4, 4, 4, 4))
-    return _create_edgenext('edgenext_x_small', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_edgenext('edgenext_x_small', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -682,7 +691,7 @@     # Jetson FPS=20.47 versus 18.86 for MobileViT_S
     # For A100: FPS @ BS=1: 172.33 & @ BS=256: 3010.25 versus FPS @ BS=1: 93.84 & @ BS=256: 1785.92 for MobileViT_S
     model_args = dict(depths=(3, 3, 9, 3), dims=(48, 96, 160, 304))
-    return _create_edgenext('edgenext_small', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_edgenext('edgenext_small', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -693,7 +702,7 @@     # Jetson FPS=xx.xx versus xx.xx for MobileViT_S
     # For A100: FPS @ BS=1: xxx.xx & @ BS=256: xxxx.xx
     model_args = dict(depths=[3, 3, 9, 3], dims=[80, 160, 288, 584])
-    return _create_edgenext('edgenext_base', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_edgenext('edgenext_base', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -701,5 +710,5 @@     model_args = dict(
         depths=(3, 3, 9, 3), dims=(48, 96, 192, 384),
         downsample_block=True, conv_bias=False, stem_type='overlap')
-    return _create_edgenext('edgenext_small_rw', pretrained=pretrained, **dict(model_args, **kwargs))
-
+    return _create_edgenext('edgenext_small_rw', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
