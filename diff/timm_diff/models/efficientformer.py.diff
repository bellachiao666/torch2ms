--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ EfficientFormer
 
 @article{li2022efficientformer,
@@ -14,8 +19,8 @@ """
 from typing import Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -49,8 +54,8 @@ }
 
 
-class Attention(torch.nn.Module):
-    attention_bias_cache: Dict[str, torch.Tensor]
+class Attention(msnn.Cell):
+    attention_bias_cache: Dict[str, ms.Tensor]
 
     def __init__(
             self,
@@ -72,17 +77,17 @@         self.val_attn_dim = self.val_dim * num_heads
         self.attn_ratio = attn_ratio
 
-        self.qkv = nn.Linear(dim, self.key_attn_dim * 2 + self.val_attn_dim, **dd)
-        self.proj = nn.Linear(self.val_attn_dim, dim, **dd)
+        self.qkv = nn.Linear(dim, self.key_attn_dim * 2 + self.val_attn_dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.proj = nn.Linear(self.val_attn_dim, dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
 
         resolution = to_2tuple(resolution)
-        pos = torch.stack(ndgrid(
-            torch.arange(resolution[0], device=device, dtype=torch.long),
-            torch.arange(resolution[1], device=device, dtype=torch.long)
+        pos = mint.stack(ndgrid(
+            mint.arange(resolution[0], device=device, dtype=ms.int64),
+            mint.arange(resolution[1], device=device, dtype=ms.int64)
         )).flatten(1)
         rel_pos = (pos[..., :, None] - pos[..., None, :]).abs()
         rel_pos = (rel_pos[0] * resolution[1]) + rel_pos[1]
-        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, resolution[0] * resolution[1], **dd))
+        self.attention_biases = ms.Parameter(mint.zeros(num_heads, resolution[0] * resolution[1], **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.register_buffer('attention_bias_idxs', rel_pos)
         self.attention_bias_cache = {}  # per-device attention_biases cache (data-parallel compat)
 
@@ -92,7 +97,7 @@         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
+    def get_attention_biases(self, device: torch.device) -> ms.Tensor:
         if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
@@ -101,7 +106,7 @@                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
-    def forward(self, x):  # x (B,N,C)
+    def construct(self, x):  # x (B,N,C)
         B, N, C = x.shape
         qkv = self.qkv(x)
         qkv = qkv.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)
@@ -116,13 +121,13 @@         return x
 
 
-class Stem4(nn.Sequential):
+class Stem4(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -130,15 +135,15 @@         super().__init__()
         self.stride = 4
 
-        self.add_module('conv1', nn.Conv2d(in_chs, out_chs // 2, kernel_size=3, stride=2, padding=1, **dd))
-        self.add_module('norm1', norm_layer(out_chs // 2, **dd))
+        self.add_module('conv1', nn.Conv2d(in_chs, out_chs // 2, kernel_size=3, stride=2, padding=1, **dd))  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.add_module('norm1', norm_layer(out_chs // 2, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.add_module('act1', act_layer())
-        self.add_module('conv2', nn.Conv2d(out_chs // 2, out_chs, kernel_size=3, stride=2, padding=1, **dd))
-        self.add_module('norm2', norm_layer(out_chs, **dd))
+        self.add_module('conv2', nn.Conv2d(out_chs // 2, out_chs, kernel_size=3, stride=2, padding=1, **dd))  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.add_module('norm2', norm_layer(out_chs, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.add_module('act2', act_layer())
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
     """
     Downsampling via strided conv w/ norm
     Input: tensor in shape [B, C, H, W]
@@ -152,7 +157,7 @@             kernel_size: int = 3,
             stride: int = 2,
             padding: Optional[int] = None,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -160,26 +165,26 @@         super().__init__()
         if padding is None:
             padding = kernel_size // 2
-        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding, **dd)
-        self.norm = norm_layer(out_chs, **dd)
-
-    def forward(self, x):
+        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm = norm_layer(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.conv(x)
         x = self.norm(x)
         return x
 
 
-class Flat(nn.Module):
+class Flat(msnn.Cell):
 
     def __init__(self, ):
         super().__init__()
 
-    def forward(self, x):
+    def construct(self, x):
         x = x.flatten(2).transpose(1, 2)
         return x
 
 
-class Pooling(nn.Module):
+class Pooling(msnn.Cell):
     """
     Implementation of pooling for PoolFormer
     --pool_size: pooling size
@@ -187,13 +192,13 @@ 
     def __init__(self, pool_size: int = 3):
         super().__init__()
-        self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size // 2, count_include_pad=False)
-
-    def forward(self, x):
+        self.pool = nn.AvgPool2d(pool_size, stride = 1, padding = pool_size // 2, count_include_pad = False)
+
+    def construct(self, x):
         return self.pool(x) - x
 
 
-class ConvMlpWithNorm(nn.Module):
+class ConvMlpWithNorm(msnn.Cell):
     """
     Implementation of MLP with 1*1 convolutions.
     Input: tensor with shape [B, C, H, W]
@@ -204,8 +209,8 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             drop: float = 0.,
             device=None,
             dtype=None,
@@ -214,14 +219,14 @@         super().__init__()
         out_features = out_features or in_features
         hidden_features = hidden_features or in_features
-        self.fc1 = nn.Conv2d(in_features, hidden_features, 1, **dd)
-        self.norm1 = norm_layer(hidden_features, **dd) if norm_layer is not None else nn.Identity()
+        self.fc1 = nn.Conv2d(in_features, hidden_features, 1, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm1 = norm_layer(hidden_features, **dd) if norm_layer is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act = act_layer()
-        self.fc2 = nn.Conv2d(hidden_features, out_features, 1, **dd)
-        self.norm2 = norm_layer(out_features, **dd) if norm_layer is not None else nn.Identity()
+        self.fc2 = nn.Conv2d(hidden_features, out_features, 1, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm2 = norm_layer(out_features, **dd) if norm_layer is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.drop = nn.Dropout(drop)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.fc1(x)
         x = self.norm1(x)
         x = self.act(x)
@@ -232,14 +237,14 @@         return x
 
 
-class MetaBlock1d(nn.Module):
+class MetaBlock1d(msnn.Cell):
 
     def __init__(
             self,
             dim: int,
             mlp_ratio: float = 4.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
             proj_drop: float = 0.,
             drop_path: float = 0.,
             layer_scale_init_value: float = 1e-5,
@@ -248,37 +253,37 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.norm1 = norm_layer(dim, **dd)
-        self.token_mixer = Attention(dim, **dd)
-        self.ls1 = LayerScale(dim, layer_scale_init_value, **dd)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        self.norm2 = norm_layer(dim, **dd)
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.token_mixer = Attention(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls1 = LayerScale(dim, layer_scale_init_value, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.mlp = Mlp(
             in_features=dim,
             hidden_features=int(dim * mlp_ratio),
             act_layer=act_layer,
             drop=proj_drop,
             **dd,
-        )
-        self.ls2 = LayerScale(dim, layer_scale_init_value, **dd)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls2 = LayerScale(dim, layer_scale_init_value, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         x = x + self.drop_path1(self.ls1(self.token_mixer(self.norm1(x))))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-class MetaBlock2d(nn.Module):
+class MetaBlock2d(msnn.Cell):
 
     def __init__(
             self,
             dim: int,
             pool_size: int = 3,
             mlp_ratio: float = 4.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             proj_drop: float = 0.,
             drop_path: float = 0.,
             layer_scale_init_value: float = 1e-5,
@@ -288,8 +293,8 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.token_mixer = Pooling(pool_size=pool_size)
-        self.ls1 = LayerScale2d(dim, layer_scale_init_value, **dd)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.ls1 = LayerScale2d(dim, layer_scale_init_value, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.mlp = ConvMlpWithNorm(
             dim,
@@ -298,17 +303,17 @@             norm_layer=norm_layer,
             drop=proj_drop,
             **dd,
-        )
-        self.ls2 = LayerScale2d(dim, layer_scale_init_value, **dd)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls2 = LayerScale2d(dim, layer_scale_init_value, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         x = x + self.drop_path1(self.ls1(self.token_mixer(x)))
         x = x + self.drop_path2(self.ls2(self.mlp(x)))
         return x
 
 
-class EfficientFormerStage(nn.Module):
+class EfficientFormerStage(msnn.Cell):
 
     def __init__(
             self,
@@ -319,9 +324,9 @@             num_vit: int = 1,
             pool_size: int = 3,
             mlp_ratio: float = 4.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            norm_layer_cl: Type[nn.Module] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            norm_layer_cl: Type[msnn.Cell] = nn.LayerNorm,
             proj_drop: float = .0,
             drop_path: float = 0.,
             layer_scale_init_value: float = 1e-5,
@@ -333,11 +338,11 @@         self.grad_checkpointing = False
 
         if downsample:
-            self.downsample = Downsample(in_chs=dim, out_chs=dim_out, norm_layer=norm_layer, **dd)
+            self.downsample = Downsample(in_chs=dim, out_chs=dim_out, norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             dim = dim_out
         else:
             assert dim == dim_out
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         blocks = []
         if num_vit and num_vit >= depth:
@@ -356,7 +361,7 @@                         drop_path=drop_path[block_idx],
                         layer_scale_init_value=layer_scale_init_value,
                         **dd,
-                    ))
+                    ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
                 blocks.append(
                     MetaBlock2d(
@@ -369,13 +374,13 @@                         drop_path=drop_path[block_idx],
                         layer_scale_init_value=layer_scale_init_value,
                         **dd,
-                    ))
+                    ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
                 if num_vit and num_vit == remain_idx:
                     blocks.append(Flat())
 
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell(*blocks)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
@@ -384,7 +389,7 @@         return x
 
 
-class EfficientFormer(nn.Module):
+class EfficientFormer(msnn.Cell):
 
     def __init__(
             self,
@@ -398,9 +403,9 @@             mlp_ratios: float = 4,
             pool_size: int = 3,
             layer_scale_init_value: float = 1e-5,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            norm_layer_cl: Type[nn.Module] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            norm_layer_cl: Type[msnn.Cell] = nn.LayerNorm,
             drop_rate: float = 0.,
             proj_drop_rate: float = 0.,
             drop_path_rate: float = 0.,
@@ -413,7 +418,7 @@         self.num_classes = num_classes
         self.global_pool = global_pool
 
-        self.stem = Stem4(in_chans, embed_dims[0], norm_layer=norm_layer, **dd)
+        self.stem = Stem4(in_chans, embed_dims[0], norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         prev_dim = embed_dims[0]
 
         # stochastic depth decay rule
@@ -439,19 +444,19 @@                 drop_path=dpr[i],
                 layer_scale_init_value=layer_scale_init_value,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_dim = embed_dims[i]
             stages.append(stage)
             self.feature_info += [dict(num_chs=embed_dims[i], reduction=2**(i+2), module=f'stages.{i}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # Classifier head
         self.num_features = self.head_hidden_size = embed_dims[-1]
-        self.norm = norm_layer_cl(self.num_features, **dd)
+        self.norm = norm_layer_cl(self.num_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，需手动确认参数映射;
         # assuming model is always distilled (valid for current checkpoints, will split def if that changes)
-        self.head_dist = nn.Linear(embed_dims[-1], num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head_dist = nn.Linear(embed_dims[-1], num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，需手动确认参数映射;
         self.distilled_training = False  # must set this True to train w/ distillation token
 
         self.apply(self._init_weights)
@@ -461,13 +466,13 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-    @torch.jit.ignore
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def no_weight_decay(self):
         return {k for k, _ in self.named_parameters() if 'attention_biases' in k}
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(
             stem=r'^stem',  # stem and embed
@@ -475,35 +480,35 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         for s in self.stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head, self.head_dist
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
         self.num_classes = num_classes
         if global_pool is not None:
             self.global_pool = global_pool
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-
-    @torch.jit.ignore
+        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
+        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
+
+    @ms.jit
     def set_distilled_training(self, enable=True):
         self.distilled_training = enable
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -560,7 +565,7 @@         take_indices, max_index = feature_take_indices(len(self.stages), indices)
         self.stages = self.stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
@@ -585,7 +590,7 @@             # during standard train/finetune, inference average the classifier predictions
             return (x + x_dist) / 2
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -650,7 +655,7 @@         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(out_indices=out_indices, feature_cls='getter'),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -661,7 +666,7 @@         embed_dims=EfficientFormer_width['l1'],
         num_vit=1,
     )
-    return _create_efficientformer('efficientformer_l1', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_efficientformer('efficientformer_l1', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -671,7 +676,7 @@         embed_dims=EfficientFormer_width['l3'],
         num_vit=4,
     )
-    return _create_efficientformer('efficientformer_l3', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_efficientformer('efficientformer_l3', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -681,5 +686,5 @@         embed_dims=EfficientFormer_width['l7'],
         num_vit=8,
     )
-    return _create_efficientformer('efficientformer_l7', pretrained=pretrained, **dict(model_args, **kwargs))
-
+    return _create_efficientformer('efficientformer_l7', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
