--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ EfficientFormer
 
 @article{li2022efficientformer,
@@ -14,8 +19,8 @@ """
 from typing import Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -49,8 +54,8 @@ }
 
 
-class Attention(torch.nn.Module):
-    attention_bias_cache: Dict[str, torch.Tensor]
+class Attention(msnn.Cell):
+    attention_bias_cache: Dict[str, ms.Tensor]
 
     def __init__(
             self,
@@ -76,13 +81,13 @@         self.proj = nn.Linear(self.val_attn_dim, dim, **dd)
 
         resolution = to_2tuple(resolution)
-        pos = torch.stack(ndgrid(
-            torch.arange(resolution[0], device=device, dtype=torch.long),
-            torch.arange(resolution[1], device=device, dtype=torch.long)
-        )).flatten(1)
+        pos = mint.stack(ndgrid(
+            mint.arange(resolution[0], dtype = torch.long),
+            mint.arange(resolution[1], dtype = torch.long)
+        )).flatten(1)  # 'torch.arange':没有对应的mindspore参数 'device' (position 6);
         rel_pos = (pos[..., :, None] - pos[..., None, :]).abs()
         rel_pos = (rel_pos[0] * resolution[1]) + rel_pos[1]
-        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, resolution[0] * resolution[1], **dd))
+        self.attention_biases = ms.Parameter(mint.zeros(num_heads, resolution[0] * resolution[1], **dd))
         self.register_buffer('attention_bias_idxs', rel_pos)
         self.attention_bias_cache = {}  # per-device attention_biases cache (data-parallel compat)
 
@@ -92,7 +97,9 @@         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def get_attention_biases(self, device: torch.device) -> ms.Tensor:
         if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
@@ -101,7 +108,7 @@                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
-    def forward(self, x):  # x (B,N,C)
+    def construct(self, x):  # x (B,N,C)
         B, N, C = x.shape
         qkv = self.qkv(x)
         qkv = qkv.reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)
@@ -116,13 +123,13 @@         return x
 
 
-class Stem4(nn.Sequential):
+class Stem4(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -138,7 +145,7 @@         self.add_module('act2', act_layer())
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
     """
     Downsampling via strided conv w/ norm
     Input: tensor in shape [B, C, H, W]
@@ -152,7 +159,7 @@             kernel_size: int = 3,
             stride: int = 2,
             padding: Optional[int] = None,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -163,23 +170,23 @@         self.conv = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding, **dd)
         self.norm = norm_layer(out_chs, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv(x)
         x = self.norm(x)
         return x
 
 
-class Flat(nn.Module):
+class Flat(msnn.Cell):
 
     def __init__(self, ):
         super().__init__()
 
-    def forward(self, x):
+    def construct(self, x):
         x = x.flatten(2).transpose(1, 2)
         return x
 
 
-class Pooling(nn.Module):
+class Pooling(msnn.Cell):
     """
     Implementation of pooling for PoolFormer
     --pool_size: pooling size
@@ -187,13 +194,13 @@ 
     def __init__(self, pool_size: int = 3):
         super().__init__()
-        self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size // 2, count_include_pad=False)
-
-    def forward(self, x):
+        self.pool = nn.AvgPool2d(pool_size, stride = 1, padding = pool_size // 2, count_include_pad = False)
+
+    def construct(self, x):
         return self.pool(x) - x
 
 
-class ConvMlpWithNorm(nn.Module):
+class ConvMlpWithNorm(msnn.Cell):
     """
     Implementation of MLP with 1*1 convolutions.
     Input: tensor with shape [B, C, H, W]
@@ -204,8 +211,8 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             drop: float = 0.,
             device=None,
             dtype=None,
@@ -215,13 +222,13 @@         out_features = out_features or in_features
         hidden_features = hidden_features or in_features
         self.fc1 = nn.Conv2d(in_features, hidden_features, 1, **dd)
-        self.norm1 = norm_layer(hidden_features, **dd) if norm_layer is not None else nn.Identity()
+        self.norm1 = norm_layer(hidden_features, **dd) if norm_layer is not None else msnn.Identity()
         self.act = act_layer()
         self.fc2 = nn.Conv2d(hidden_features, out_features, 1, **dd)
-        self.norm2 = norm_layer(out_features, **dd) if norm_layer is not None else nn.Identity()
+        self.norm2 = norm_layer(out_features, **dd) if norm_layer is not None else msnn.Identity()
         self.drop = nn.Dropout(drop)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.fc1(x)
         x = self.norm1(x)
         x = self.act(x)
@@ -232,14 +239,14 @@         return x
 
 
-class MetaBlock1d(nn.Module):
+class MetaBlock1d(msnn.Cell):
 
     def __init__(
             self,
             dim: int,
             mlp_ratio: float = 4.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
             proj_drop: float = 0.,
             drop_path: float = 0.,
             layer_scale_init_value: float = 1e-5,
@@ -251,7 +258,7 @@         self.norm1 = norm_layer(dim, **dd)
         self.token_mixer = Attention(dim, **dd)
         self.ls1 = LayerScale(dim, layer_scale_init_value, **dd)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.norm2 = norm_layer(dim, **dd)
         self.mlp = Mlp(
@@ -262,23 +269,23 @@             **dd,
         )
         self.ls2 = LayerScale(dim, layer_scale_init_value, **dd)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         x = x + self.drop_path1(self.ls1(self.token_mixer(self.norm1(x))))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-class MetaBlock2d(nn.Module):
+class MetaBlock2d(msnn.Cell):
 
     def __init__(
             self,
             dim: int,
             pool_size: int = 3,
             mlp_ratio: float = 4.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             proj_drop: float = 0.,
             drop_path: float = 0.,
             layer_scale_init_value: float = 1e-5,
@@ -289,7 +296,7 @@         super().__init__()
         self.token_mixer = Pooling(pool_size=pool_size)
         self.ls1 = LayerScale2d(dim, layer_scale_init_value, **dd)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.mlp = ConvMlpWithNorm(
             dim,
@@ -300,15 +307,15 @@             **dd,
         )
         self.ls2 = LayerScale2d(dim, layer_scale_init_value, **dd)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         x = x + self.drop_path1(self.ls1(self.token_mixer(x)))
         x = x + self.drop_path2(self.ls2(self.mlp(x)))
         return x
 
 
-class EfficientFormerStage(nn.Module):
+class EfficientFormerStage(msnn.Cell):
 
     def __init__(
             self,
@@ -319,9 +326,9 @@             num_vit: int = 1,
             pool_size: int = 3,
             mlp_ratio: float = 4.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            norm_layer_cl: Type[nn.Module] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            norm_layer_cl: Type[msnn.Cell] = nn.LayerNorm,
             proj_drop: float = .0,
             drop_path: float = 0.,
             layer_scale_init_value: float = 1e-5,
@@ -337,7 +344,7 @@             dim = dim_out
         else:
             assert dim == dim_out
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         blocks = []
         if num_vit and num_vit >= depth:
@@ -373,9 +380,11 @@                 if num_vit and num_vit == remain_idx:
                     blocks.append(Flat())
 
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x):
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
@@ -384,7 +393,7 @@         return x
 
 
-class EfficientFormer(nn.Module):
+class EfficientFormer(msnn.Cell):
 
     def __init__(
             self,
@@ -398,9 +407,9 @@             mlp_ratios: float = 4,
             pool_size: int = 3,
             layer_scale_init_value: float = 1e-5,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            norm_layer_cl: Type[nn.Module] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            norm_layer_cl: Type[msnn.Cell] = nn.LayerNorm,
             drop_rate: float = 0.,
             proj_drop_rate: float = 0.,
             drop_path_rate: float = 0.,
@@ -443,15 +452,17 @@             prev_dim = embed_dims[i]
             stages.append(stage)
             self.feature_info += [dict(num_chs=embed_dims[i], reduction=2**(i+2), module=f'stages.{i}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell([
+            stages
+        ])
 
         # Classifier head
         self.num_features = self.head_hidden_size = embed_dims[-1]
         self.norm = norm_layer_cl(self.num_features, **dd)
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else msnn.Identity()
         # assuming model is always distilled (valid for current checkpoints, will split def if that changes)
-        self.head_dist = nn.Linear(embed_dims[-1], num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head_dist = nn.Linear(embed_dims[-1], num_classes, **dd) if num_classes > 0 else msnn.Identity()
         self.distilled_training = False  # must set this True to train w/ distillation token
 
         self.apply(self._init_weights)
@@ -461,7 +472,7 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def no_weight_decay(self):
@@ -481,15 +492,15 @@             s.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head, self.head_dist
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
         self.num_classes = num_classes
         if global_pool is not None:
             self.global_pool = global_pool
-        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
+        self.head_dist = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
 
     @torch.jit.ignore
     def set_distilled_training(self, enable=True):
@@ -497,13 +508,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -560,7 +571,7 @@         take_indices, max_index = feature_take_indices(len(self.stages), indices)
         self.stages = self.stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
@@ -585,7 +596,7 @@             # during standard train/finetune, inference average the classifier predictions
             return (x + x_dist) / 2
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
