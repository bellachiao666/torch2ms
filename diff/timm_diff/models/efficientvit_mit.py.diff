--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ EfficientViT (by MIT Song Han's Lab)
 
 Paper: `Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition`
@@ -10,9 +15,8 @@ from typing import List, Optional, Tuple, Type, Union
 from functools import partial
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectAdaptivePool2d, create_conv2d, GELUTanh
@@ -46,7 +50,7 @@         return kernel_size // 2
 
 
-class ConvNormAct(nn.Module):
+class ConvNormAct(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -57,14 +61,14 @@             groups: int = 1,
             bias: bool = False,
             dropout: float = 0.,
-            norm_layer: Optional[Type[nn.Module]] = nn.BatchNorm2d,
-            act_layer: Optional[Type[nn.Module]] = nn.ReLU,
+            norm_layer: Optional[Type[msnn.Cell]] = nn.BatchNorm2d,
+            act_layer: Optional[Type[msnn.Cell]] = nn.ReLU,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.dropout = nn.Dropout(dropout, inplace=False)
+        self.dropout = nn.Dropout(dropout, inplace = False)
         self.conv = create_conv2d(
             in_channels,
             out_channels,
@@ -74,11 +78,11 @@             groups=groups,
             bias=bias,
             **dd,
-        )
-        self.norm = norm_layer(num_features=out_channels, **dd) if norm_layer else nn.Identity()
-        self.act = act_layer(inplace=True) if act_layer is not None else nn.Identity()
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm = norm_layer(num_features=out_channels, **dd) if norm_layer else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.act = act_layer(inplace=True) if act_layer is not None else msnn.Identity()
+
+    def construct(self, x):
         x = self.dropout(x)
         x = self.conv(x)
         x = self.norm(x)
@@ -86,7 +90,7 @@         return x
 
 
-class DSConv(nn.Module):
+class DSConv(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -94,8 +98,8 @@             kernel_size: int = 3,
             stride: int = 1,
             use_bias: Union[bool, Tuple[bool, bool]] = False,
-            norm_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = nn.BatchNorm2d,
-            act_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (nn.ReLU6, None),
+            norm_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = nn.BatchNorm2d,
+            act_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (nn.ReLU6, None),
             device=None,
             dtype=None,
     ):
@@ -115,7 +119,7 @@             act_layer=act_layer[0],
             bias=use_bias[0],
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.point_conv = ConvNormAct(
             in_channels,
             out_channels,
@@ -124,15 +128,15 @@             act_layer=act_layer[1],
             bias=use_bias[1],
             **dd,
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.depth_conv(x)
         x = self.point_conv(x)
         return x
 
 
-class ConvBlock(nn.Module):
+class ConvBlock(msnn.Cell):
     def __init__(
         self,
             in_channels: int,
@@ -142,8 +146,8 @@             mid_channels: Optional[int] = None,
             expand_ratio: float = 1,
             use_bias: Union[bool, Tuple[bool, bool]] = False,
-            norm_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = nn.BatchNorm2d,
-            act_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (nn.ReLU6, None),
+            norm_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = nn.BatchNorm2d,
+            act_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (nn.ReLU6, None),
             device=None,
             dtype=None,
     ):
@@ -163,7 +167,7 @@             act_layer=act_layer[0],
             bias=use_bias[0],
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv2 = ConvNormAct(
             mid_channels,
             out_channels,
@@ -173,15 +177,15 @@             act_layer=act_layer[1],
             bias=use_bias[1],
             **dd,
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.conv1(x)
         x = self.conv2(x)
         return x
 
 
-class MBConv(nn.Module):
+class MBConv(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -191,8 +195,8 @@             mid_channels: Optional[int] = None,
             expand_ratio: float = 6,
             use_bias: Union[bool, Tuple[bool, ...]] = False,
-            norm_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = nn.BatchNorm2d,
-            act_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (nn.ReLU6, nn.ReLU6, None),
+            norm_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = nn.BatchNorm2d,
+            act_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (nn.ReLU6, nn.ReLU6, None),
             device=None,
             dtype=None,
     ):
@@ -212,7 +216,7 @@             act_layer=act_layer[0],
             bias=use_bias[0],
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.depth_conv = ConvNormAct(
             mid_channels,
             mid_channels,
@@ -223,7 +227,7 @@             act_layer=act_layer[1],
             bias=use_bias[1],
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.point_conv = ConvNormAct(
             mid_channels,
             out_channels,
@@ -232,16 +236,16 @@             act_layer=act_layer[2],
             bias=use_bias[2],
             **dd,
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.inverted_conv(x)
         x = self.depth_conv(x)
         x = self.point_conv(x)
         return x
 
 
-class FusedMBConv(nn.Module):
+class FusedMBConv(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -252,8 +256,8 @@             expand_ratio: float = 6,
             groups: int = 1,
             use_bias: Union[bool, Tuple[bool, ...]] = False,
-            norm_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = nn.BatchNorm2d,
-            act_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (nn.ReLU6, None),
+            norm_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = nn.BatchNorm2d,
+            act_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (nn.ReLU6, None),
             device=None,
             dtype=None,
     ):
@@ -274,7 +278,7 @@             act_layer=act_layer[0],
             bias=use_bias[0],
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.point_conv = ConvNormAct(
             mid_channels,
             out_channels,
@@ -283,15 +287,15 @@             act_layer=act_layer[1],
             bias=use_bias[1],
             **dd,
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.spatial_conv(x)
         x = self.point_conv(x)
         return x
 
 
-class LiteMLA(nn.Module):
+class LiteMLA(msnn.Cell):
     """Lightweight multi-scale linear attention"""
 
     def __init__(
@@ -302,9 +306,9 @@             heads_ratio: float = 1.0,
             dim: int = 8,
             use_bias: Union[bool, Tuple[bool, ...]] = False,
-            norm_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (None, nn.BatchNorm2d),
-            act_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (None, None),
-            kernel_func: Type[nn.Module] = nn.ReLU,
+            norm_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (None, nn.BatchNorm2d),
+            act_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (None, None),
+            kernel_func: Type[msnn.Cell] = nn.ReLU,
             scales: Tuple[int, ...] = (5,),
             eps: float = 1e-5,
             device=None,
@@ -328,9 +332,9 @@             norm_layer=norm_layer[0],
             act_layer=act_layer[0],
             **dd,
-        )
-        self.aggreg = nn.ModuleList([
-            nn.Sequential(
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.aggreg = msnn.CellList([
+            msnn.SequentialCell(
                 nn.Conv2d(
                     3 * total_dim,
                     3 * total_dim,
@@ -343,7 +347,7 @@                 nn.Conv2d(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0], **dd),
             )
             for scale in scales
-        ])
+        ])  # 存在 *args/**kwargs，需手动确认参数映射;
         self.kernel_func = kernel_func(inplace=False)
 
         self.proj = ConvNormAct(
@@ -354,7 +358,7 @@             norm_layer=norm_layer[1],
             act_layer=act_layer[1],
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def _attn(self, q, k, v):
         dtype = v.dtype
@@ -364,7 +368,7 @@         out = out[..., :-1] / (out[..., -1:] + self.eps)
         return out.to(dtype)
 
-    def forward(self, x):
+    def construct(self, x):
         B, _, H, W = x.shape
 
         # generate multi-scale q, k, v
@@ -372,14 +376,14 @@         multi_scale_qkv = [qkv]
         for op in self.aggreg:
             multi_scale_qkv.append(op(qkv))
-        multi_scale_qkv = torch.cat(multi_scale_qkv, dim=1)
+        multi_scale_qkv = mint.cat(multi_scale_qkv, dim=1)
         multi_scale_qkv = multi_scale_qkv.reshape(B, -1, 3 * self.dim, H * W).transpose(-1, -2)
         q, k, v = multi_scale_qkv.chunk(3, dim=-1)
 
         # lightweight global attention
         q = self.kernel_func(q)
         k = self.kernel_func(k)
-        v = F.pad(v, (0, 1), mode="constant", value=1.)
+        v = nn.functional.pad(v, (0, 1), mode = "constant", value = 1.)
 
         if not torch.jit.is_scripting():
             with torch.autocast(device_type=v.device.type, enabled=False):
@@ -396,15 +400,15 @@ register_notrace_module(LiteMLA)
 
 
-class EfficientVitBlock(nn.Module):
+class EfficientVitBlock(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
             heads_ratio: float = 1.0,
             head_dim: int = 32,
             expand_ratio: float = 4,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            act_layer: Type[nn.Module] = nn.Hardswish,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.Hardswish,
             device=None,
             dtype=None,
     ):
@@ -419,8 +423,8 @@                 norm_layer=(None, norm_layer),
                 **dd,
             ),
-            nn.Identity(),
-        )
+            msnn.Identity(),
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.local_module = ResidualBlock(
             MBConv(
                 in_channels=in_channels,
@@ -431,28 +435,28 @@                 act_layer=(act_layer, act_layer, None),
                 **dd,
             ),
-            nn.Identity(),
-        )
-
-    def forward(self, x):
+            msnn.Identity(),
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.context_module(x)
         x = self.local_module(x)
         return x
 
 
-class ResidualBlock(nn.Module):
-    def __init__(
-            self,
-            main: Optional[nn.Module],
-            shortcut: Optional[nn.Module] = None,
-            pre_norm: Optional[nn.Module] = None,
-    ):
-        super().__init__()
-        self.pre_norm = pre_norm if pre_norm is not None else nn.Identity()
+class ResidualBlock(msnn.Cell):
+    def __init__(
+            self,
+            main: Optional[msnn.Cell],
+            shortcut: Optional[msnn.Cell] = None,
+            pre_norm: Optional[msnn.Cell] = None,
+    ):
+        super().__init__()
+        self.pre_norm = pre_norm if pre_norm is not None else msnn.Identity()
         self.main = main
         self.shortcut = shortcut
 
-    def forward(self, x):
+    def construct(self, x):
         res = self.main(self.pre_norm(x))
         if self.shortcut is not None:
             res = res + self.shortcut(x)
@@ -483,7 +487,7 @@                 norm_layer=(None, norm_layer) if fewer_norm else norm_layer,
                 act_layer=(act_layer, None),
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             block = ConvBlock(
                 in_channels=in_channels,
@@ -493,7 +497,7 @@                 norm_layer=(None, norm_layer) if fewer_norm else norm_layer,
                 act_layer=(act_layer, None),
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     else:
         if block_type == "default":
             block = MBConv(
@@ -505,7 +509,7 @@                 norm_layer=(None, None, norm_layer) if fewer_norm else norm_layer,
                 act_layer=(act_layer, act_layer, None),
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             block = FusedMBConv(
                 in_channels=in_channels,
@@ -516,18 +520,18 @@                 norm_layer=(None, norm_layer) if fewer_norm else norm_layer,
                 act_layer=(act_layer, None),
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return block
 
 
-class Stem(nn.Sequential):
+class Stem(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             depth: int,
-            norm_layer: Type[nn.Module],
-            act_layer: Type[nn.Module],
+            norm_layer: Type[msnn.Cell],
+            act_layer: Type[msnn.Cell],
             block_type: str = 'default',
             device=None,
             dtype=None,
@@ -547,7 +551,7 @@                 act_layer=act_layer,
                 **dd,
             )
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stem_block = 0
         for _ in range(depth):
             self.add_module(f'res{stem_block}', ResidualBlock(
@@ -561,19 +565,19 @@                     block_type=block_type,
                     **dd,
                 ),
-                nn.Identity(),
-            ))
+                msnn.Identity(),
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             stem_block += 1
 
 
-class EfficientVitStage(nn.Module):
+class EfficientVitStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             depth: int,
-            norm_layer: Type[nn.Module],
-            act_layer: Type[nn.Module],
+            norm_layer: Type[msnn.Cell],
+            act_layer: Type[msnn.Cell],
             expand_ratio: float,
             head_dim: int,
             vit_stage: bool = False,
@@ -594,7 +598,7 @@                 **dd,
             ),
             None,
-        )]
+        )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         in_chs = out_chs
 
         if vit_stage:
@@ -609,7 +613,7 @@                         act_layer=act_layer,
                         **dd,
                     )
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             # for stage 1, 2
             for i in range(1, depth):
@@ -623,23 +627,23 @@                         act_layer=act_layer,
                         **dd,
                     ),
-                    nn.Identity(),
-                ))
-
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+                    msnn.Identity(),
+                ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.blocks = msnn.SequentialCell(*blocks)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         return self.blocks(x)
 
 
-class EfficientVitLargeStage(nn.Module):
+class EfficientVitLargeStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             depth: int,
-            norm_layer: Type[nn.Module],
-            act_layer: Type[nn.Module],
+            norm_layer: Type[msnn.Cell],
+            act_layer: Type[msnn.Cell],
             head_dim: int,
             vit_stage: bool = False,
             fewer_norm: bool = False,
@@ -661,7 +665,7 @@                 **dd,
             ),
             None,
-        )]
+        )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         in_chs = out_chs
 
         if vit_stage:
@@ -676,7 +680,7 @@                         act_layer=act_layer,
                         **dd,
                     )
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             # for stage 1, 2, 3
             for i in range(depth):
@@ -692,24 +696,24 @@                         block_type='default' if fewer_norm else 'fused',
                         **dd,
                     ),
-                    nn.Identity(),
-                ))
-
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+                    msnn.Identity(),
+                ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.blocks = msnn.SequentialCell(*blocks)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         return self.blocks(x)
 
 
-class ClassifierHead(nn.Module):
+class ClassifierHead(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
             widths: List[int],
             num_classes: int = 1000,
             dropout: float = 0.,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            act_layer: Optional[Type[nn.Module]] = nn.Hardswish,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            act_layer: Optional[Type[msnn.Cell]] = nn.Hardswish,
             pool_type: str = 'avg',
             norm_eps: float = 1e-5,
             device=None,
@@ -721,26 +725,26 @@         self.num_features = widths[-1]
 
         assert pool_type, 'Cannot disable pooling'
-        self.in_conv = ConvNormAct(in_channels, widths[0], 1, norm_layer=norm_layer, act_layer=act_layer, **dd)
+        self.in_conv = ConvNormAct(in_channels, widths[0], 1, norm_layer=norm_layer, act_layer=act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True)
-        self.classifier = nn.Sequential(
+        self.classifier = msnn.SequentialCell(
             nn.Linear(widths[0], widths[1], bias=False, **dd),
             nn.LayerNorm(widths[1], eps=norm_eps, **dd),
-            act_layer(inplace=True) if act_layer is not None else nn.Identity(),
-            nn.Dropout(dropout, inplace=False),
-            nn.Linear(widths[1], num_classes, bias=True, **dd) if num_classes > 0 else nn.Identity(),
-        )
+            act_layer(inplace=True) if act_layer is not None else msnn.Identity(),
+            nn.Dropout(dropout, inplace = False),
+            nn.Linear(widths[1], num_classes, bias=True, **dd) if num_classes > 0 else msnn.Identity(),
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None):
         if pool_type is not None:
             assert pool_type, 'Cannot disable pooling'
             self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True,)
         if num_classes > 0:
-            self.classifier[-1] = nn.Linear(self.num_features, num_classes, bias=True)
+            self.classifier[-1] = nn.Linear(self.num_features, num_classes, bias = True)
         else:
-            self.classifier[-1] = nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
+            self.classifier[-1] = msnn.Identity()
+
+    def construct(self, x, pre_logits: bool = False):
         x = self.in_conv(x)
         x = self.global_pool(x)
         if pre_logits:
@@ -754,7 +758,7 @@         return x
 
 
-class EfficientVit(nn.Module):
+class EfficientVit(msnn.Cell):
     def __init__(
             self,
             in_chans: int = 3,
@@ -762,8 +766,8 @@             depths: Tuple[int, ...] = (),
             head_dim: int = 32,
             expand_ratio: float = 4,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            act_layer: Type[nn.Module] = nn.Hardswish,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.Hardswish,
             global_pool: str = 'avg',
             head_widths: Tuple[int, ...] = (),
             drop_rate: float = 0.0,
@@ -778,12 +782,12 @@         self.num_classes = num_classes
 
         # input stem
-        self.stem = Stem(in_chans, widths[0], depths[0], norm_layer, act_layer, **dd)
+        self.stem = Stem(in_chans, widths[0], depths[0], norm_layer, act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stride = self.stem.stride
 
         # stages
         self.feature_info = []
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         in_channels = widths[0]
         for i, (w, d) in enumerate(zip(widths[1:], depths[1:])):
             self.stages.append(EfficientVitStage(
@@ -796,7 +800,7 @@                 head_dim=head_dim,
                 vit_stage=i >= 2,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             stride *= 2
             in_channels = w
             self.feature_info += [dict(num_chs=in_channels, reduction=stride, module=f'stages.{i}')]
@@ -809,10 +813,10 @@             dropout=drop_rate,
             pool_type=self.global_pool,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.head_hidden_size = self.head.num_features
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(
             stem=r'^stem',
@@ -823,12 +827,12 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.classifier[-1]
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -837,13 +841,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -906,21 +910,21 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-class EfficientVitLarge(nn.Module):
+class EfficientVitLarge(msnn.Cell):
     def __init__(
         self,
         in_chans: int = 3,
         widths: Tuple[int, ...] = (),
         depths: Tuple[int, ...] = (),
         head_dim: int = 32,
-        norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-        act_layer: Type[nn.Module] = GELUTanh,
+        norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+        act_layer: Type[msnn.Cell] = GELUTanh,
         global_pool: str = 'avg',
         head_widths: Tuple[int, ...] = (),
         drop_rate: float = 0.0,
@@ -938,12 +942,12 @@         norm_layer = partial(norm_layer, eps=self.norm_eps)
 
         # input stem
-        self.stem = Stem(in_chans, widths[0], depths[0], norm_layer, act_layer, block_type='large', **dd)
+        self.stem = Stem(in_chans, widths[0], depths[0], norm_layer, act_layer, block_type='large', **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stride = self.stem.stride
 
         # stages
         self.feature_info = []
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         in_channels = widths[0]
         for i, (w, d) in enumerate(zip(widths[1:], depths[1:])):
             self.stages.append(EfficientVitLargeStage(
@@ -956,7 +960,7 @@                 vit_stage=i >= 3,
                 fewer_norm=i >= 2,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             stride *= 2
             in_channels = w
             self.feature_info += [dict(num_chs=in_channels, reduction=stride, module=f'stages.{i}')]
@@ -971,10 +975,10 @@             act_layer=act_layer,
             norm_eps=self.norm_eps,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.head_hidden_size = self.head.num_features
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(
             stem=r'^stem',
@@ -985,12 +989,12 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.classifier[-1]
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -999,13 +1003,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -1068,7 +1072,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -1189,7 +1193,7 @@         pretrained,
         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         **kwargs
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1201,7 +1205,7 @@         pretrained,
         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         **kwargs
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -1209,49 +1213,49 @@ def efficientvit_b0(pretrained=False, **kwargs):
     model_args = dict(
         widths=(8, 16, 32, 64, 128), depths=(1, 2, 2, 2, 2), head_dim=16, head_widths=(1024, 1280))
-    return _create_efficientvit('efficientvit_b0', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_efficientvit('efficientvit_b0', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def efficientvit_b1(pretrained=False, **kwargs):
     model_args = dict(
         widths=(16, 32, 64, 128, 256), depths=(1, 2, 3, 3, 4), head_dim=16, head_widths=(1536, 1600))
-    return _create_efficientvit('efficientvit_b1', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_efficientvit('efficientvit_b1', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def efficientvit_b2(pretrained=False, **kwargs):
     model_args = dict(
         widths=(24, 48, 96, 192, 384), depths=(1, 3, 4, 4, 6), head_dim=32, head_widths=(2304, 2560))
-    return _create_efficientvit('efficientvit_b2', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_efficientvit('efficientvit_b2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def efficientvit_b3(pretrained=False, **kwargs):
     model_args = dict(
         widths=(32, 64, 128, 256, 512), depths=(1, 4, 6, 6, 9), head_dim=32, head_widths=(2304, 2560))
-    return _create_efficientvit('efficientvit_b3', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_efficientvit('efficientvit_b3', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def efficientvit_l1(pretrained=False, **kwargs):
     model_args = dict(
         widths=(32, 64, 128, 256, 512), depths=(1, 1, 1, 6, 6), head_dim=32, head_widths=(3072, 3200))
-    return _create_efficientvit_large('efficientvit_l1', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_efficientvit_large('efficientvit_l1', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def efficientvit_l2(pretrained=False, **kwargs):
     model_args = dict(
         widths=(32, 64, 128, 256, 512), depths=(1, 2, 2, 8, 8), head_dim=32, head_widths=(3072, 3200))
-    return _create_efficientvit_large('efficientvit_l2', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_efficientvit_large('efficientvit_l2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def efficientvit_l3(pretrained=False, **kwargs):
     model_args = dict(
         widths=(64, 128, 256, 512, 1024), depths=(1, 2, 2, 8, 8), head_dim=32, head_widths=(6144, 6400))
-    return _create_efficientvit_large('efficientvit_l3', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_efficientvit_large('efficientvit_l3', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 # FIXME will wait for v2 SAM models which are pending
