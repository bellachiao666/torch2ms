--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ EfficientViT (by MIT Song Han's Lab)
 
 Paper: `Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition`
@@ -10,9 +15,8 @@ from typing import List, Optional, Tuple, Type, Union
 from functools import partial
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectAdaptivePool2d, create_conv2d, GELUTanh
@@ -46,7 +50,7 @@         return kernel_size // 2
 
 
-class ConvNormAct(nn.Module):
+class ConvNormAct(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -57,14 +61,14 @@             groups: int = 1,
             bias: bool = False,
             dropout: float = 0.,
-            norm_layer: Optional[Type[nn.Module]] = nn.BatchNorm2d,
-            act_layer: Optional[Type[nn.Module]] = nn.ReLU,
+            norm_layer: Optional[Type[msnn.Cell]] = nn.BatchNorm2d,
+            act_layer: Optional[Type[msnn.Cell]] = nn.ReLU,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.dropout = nn.Dropout(dropout, inplace=False)
+        self.dropout = nn.Dropout(dropout, inplace = False)
         self.conv = create_conv2d(
             in_channels,
             out_channels,
@@ -75,10 +79,10 @@             bias=bias,
             **dd,
         )
-        self.norm = norm_layer(num_features=out_channels, **dd) if norm_layer else nn.Identity()
-        self.act = act_layer(inplace=True) if act_layer is not None else nn.Identity()
-
-    def forward(self, x):
+        self.norm = norm_layer(num_features=out_channels, **dd) if norm_layer else msnn.Identity()
+        self.act = act_layer(inplace=True) if act_layer is not None else msnn.Identity()
+
+    def construct(self, x):
         x = self.dropout(x)
         x = self.conv(x)
         x = self.norm(x)
@@ -86,7 +90,7 @@         return x
 
 
-class DSConv(nn.Module):
+class DSConv(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -94,8 +98,8 @@             kernel_size: int = 3,
             stride: int = 1,
             use_bias: Union[bool, Tuple[bool, bool]] = False,
-            norm_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = nn.BatchNorm2d,
-            act_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (nn.ReLU6, None),
+            norm_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = nn.BatchNorm2d,
+            act_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (nn.ReLU6, None),
             device=None,
             dtype=None,
     ):
@@ -126,13 +130,13 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.depth_conv(x)
         x = self.point_conv(x)
         return x
 
 
-class ConvBlock(nn.Module):
+class ConvBlock(msnn.Cell):
     def __init__(
         self,
             in_channels: int,
@@ -142,8 +146,8 @@             mid_channels: Optional[int] = None,
             expand_ratio: float = 1,
             use_bias: Union[bool, Tuple[bool, bool]] = False,
-            norm_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = nn.BatchNorm2d,
-            act_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (nn.ReLU6, None),
+            norm_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = nn.BatchNorm2d,
+            act_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (nn.ReLU6, None),
             device=None,
             dtype=None,
     ):
@@ -175,13 +179,13 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         x = self.conv2(x)
         return x
 
 
-class MBConv(nn.Module):
+class MBConv(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -191,8 +195,8 @@             mid_channels: Optional[int] = None,
             expand_ratio: float = 6,
             use_bias: Union[bool, Tuple[bool, ...]] = False,
-            norm_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = nn.BatchNorm2d,
-            act_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (nn.ReLU6, nn.ReLU6, None),
+            norm_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = nn.BatchNorm2d,
+            act_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (nn.ReLU6, nn.ReLU6, None),
             device=None,
             dtype=None,
     ):
@@ -234,14 +238,14 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.inverted_conv(x)
         x = self.depth_conv(x)
         x = self.point_conv(x)
         return x
 
 
-class FusedMBConv(nn.Module):
+class FusedMBConv(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -252,8 +256,8 @@             expand_ratio: float = 6,
             groups: int = 1,
             use_bias: Union[bool, Tuple[bool, ...]] = False,
-            norm_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = nn.BatchNorm2d,
-            act_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (nn.ReLU6, None),
+            norm_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = nn.BatchNorm2d,
+            act_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (nn.ReLU6, None),
             device=None,
             dtype=None,
     ):
@@ -285,13 +289,13 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.spatial_conv(x)
         x = self.point_conv(x)
         return x
 
 
-class LiteMLA(nn.Module):
+class LiteMLA(msnn.Cell):
     """Lightweight multi-scale linear attention"""
 
     def __init__(
@@ -302,9 +306,9 @@             heads_ratio: float = 1.0,
             dim: int = 8,
             use_bias: Union[bool, Tuple[bool, ...]] = False,
-            norm_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (None, nn.BatchNorm2d),
-            act_layer: Union[Type[nn.Module], Tuple[Optional[Type[nn.Module]], ...]] = (None, None),
-            kernel_func: Type[nn.Module] = nn.ReLU,
+            norm_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (None, nn.BatchNorm2d),
+            act_layer: Union[Type[msnn.Cell], Tuple[Optional[Type[msnn.Cell]], ...]] = (None, None),
+            kernel_func: Type[msnn.Cell] = nn.ReLU,
             scales: Tuple[int, ...] = (5,),
             eps: float = 1e-5,
             device=None,
@@ -329,9 +333,10 @@             act_layer=act_layer[0],
             **dd,
         )
-        self.aggreg = nn.ModuleList([
-            nn.Sequential(
-                nn.Conv2d(
+        self.aggreg = msnn.CellList([
+            msnn.SequentialCell(
+                [
+            nn.Conv2d(
                     3 * total_dim,
                     3 * total_dim,
                     scale,
@@ -340,8 +345,8 @@                     bias=use_bias[0],
                     **dd,
                 ),
-                nn.Conv2d(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0], **dd),
-            )
+            nn.Conv2d(3 * total_dim, 3 * total_dim, 1, groups=3 * heads, bias=use_bias[0], **dd)
+        ])
             for scale in scales
         ])
         self.kernel_func = kernel_func(inplace=False)
@@ -364,7 +369,7 @@         out = out[..., :-1] / (out[..., -1:] + self.eps)
         return out.to(dtype)
 
-    def forward(self, x):
+    def construct(self, x):
         B, _, H, W = x.shape
 
         # generate multi-scale q, k, v
@@ -372,14 +377,14 @@         multi_scale_qkv = [qkv]
         for op in self.aggreg:
             multi_scale_qkv.append(op(qkv))
-        multi_scale_qkv = torch.cat(multi_scale_qkv, dim=1)
+        multi_scale_qkv = mint.cat(multi_scale_qkv, dim = 1)
         multi_scale_qkv = multi_scale_qkv.reshape(B, -1, 3 * self.dim, H * W).transpose(-1, -2)
         q, k, v = multi_scale_qkv.chunk(3, dim=-1)
 
         # lightweight global attention
         q = self.kernel_func(q)
         k = self.kernel_func(k)
-        v = F.pad(v, (0, 1), mode="constant", value=1.)
+        v = nn.functional.pad(v, (0, 1), mode = "constant", value = 1.)
 
         if not torch.jit.is_scripting():
             with torch.autocast(device_type=v.device.type, enabled=False):
@@ -396,15 +401,15 @@ register_notrace_module(LiteMLA)
 
 
-class EfficientVitBlock(nn.Module):
+class EfficientVitBlock(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
             heads_ratio: float = 1.0,
             head_dim: int = 32,
             expand_ratio: float = 4,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            act_layer: Type[nn.Module] = nn.Hardswish,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.Hardswish,
             device=None,
             dtype=None,
     ):
@@ -419,7 +424,7 @@                 norm_layer=(None, norm_layer),
                 **dd,
             ),
-            nn.Identity(),
+            msnn.Identity(),
         )
         self.local_module = ResidualBlock(
             MBConv(
@@ -431,28 +436,28 @@                 act_layer=(act_layer, act_layer, None),
                 **dd,
             ),
-            nn.Identity(),
-        )
-
-    def forward(self, x):
+            msnn.Identity(),
+        )
+
+    def construct(self, x):
         x = self.context_module(x)
         x = self.local_module(x)
         return x
 
 
-class ResidualBlock(nn.Module):
-    def __init__(
-            self,
-            main: Optional[nn.Module],
-            shortcut: Optional[nn.Module] = None,
-            pre_norm: Optional[nn.Module] = None,
-    ):
-        super().__init__()
-        self.pre_norm = pre_norm if pre_norm is not None else nn.Identity()
+class ResidualBlock(msnn.Cell):
+    def __init__(
+            self,
+            main: Optional[msnn.Cell],
+            shortcut: Optional[msnn.Cell] = None,
+            pre_norm: Optional[msnn.Cell] = None,
+    ):
+        super().__init__()
+        self.pre_norm = pre_norm if pre_norm is not None else msnn.Identity()
         self.main = main
         self.shortcut = shortcut
 
-    def forward(self, x):
+    def construct(self, x):
         res = self.main(self.pre_norm(x))
         if self.shortcut is not None:
             res = res + self.shortcut(x)
@@ -520,14 +525,14 @@     return block
 
 
-class Stem(nn.Sequential):
+class Stem(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             depth: int,
-            norm_layer: Type[nn.Module],
-            act_layer: Type[nn.Module],
+            norm_layer: Type[msnn.Cell],
+            act_layer: Type[msnn.Cell],
             block_type: str = 'default',
             device=None,
             dtype=None,
@@ -561,19 +566,19 @@                     block_type=block_type,
                     **dd,
                 ),
-                nn.Identity(),
+                msnn.Identity(),
             ))
             stem_block += 1
 
 
-class EfficientVitStage(nn.Module):
+class EfficientVitStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             depth: int,
-            norm_layer: Type[nn.Module],
-            act_layer: Type[nn.Module],
+            norm_layer: Type[msnn.Cell],
+            act_layer: Type[msnn.Cell],
             expand_ratio: float,
             head_dim: int,
             vit_stage: bool = False,
@@ -623,23 +628,25 @@                         act_layer=act_layer,
                         **dd,
                     ),
-                    nn.Identity(),
+                    msnn.Identity(),
                 ))
 
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x):
         return self.blocks(x)
 
 
-class EfficientVitLargeStage(nn.Module):
+class EfficientVitLargeStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             depth: int,
-            norm_layer: Type[nn.Module],
-            act_layer: Type[nn.Module],
+            norm_layer: Type[msnn.Cell],
+            act_layer: Type[msnn.Cell],
             head_dim: int,
             vit_stage: bool = False,
             fewer_norm: bool = False,
@@ -692,24 +699,26 @@                         block_type='default' if fewer_norm else 'fused',
                         **dd,
                     ),
-                    nn.Identity(),
+                    msnn.Identity(),
                 ))
 
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x):
         return self.blocks(x)
 
 
-class ClassifierHead(nn.Module):
+class ClassifierHead(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
             widths: List[int],
             num_classes: int = 1000,
             dropout: float = 0.,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            act_layer: Optional[Type[nn.Module]] = nn.Hardswish,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            act_layer: Optional[Type[msnn.Cell]] = nn.Hardswish,
             pool_type: str = 'avg',
             norm_eps: float = 1e-5,
             device=None,
@@ -723,24 +732,25 @@         assert pool_type, 'Cannot disable pooling'
         self.in_conv = ConvNormAct(in_channels, widths[0], 1, norm_layer=norm_layer, act_layer=act_layer, **dd)
         self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True)
-        self.classifier = nn.Sequential(
+        self.classifier = msnn.SequentialCell(
+            [
             nn.Linear(widths[0], widths[1], bias=False, **dd),
             nn.LayerNorm(widths[1], eps=norm_eps, **dd),
-            act_layer(inplace=True) if act_layer is not None else nn.Identity(),
-            nn.Dropout(dropout, inplace=False),
-            nn.Linear(widths[1], num_classes, bias=True, **dd) if num_classes > 0 else nn.Identity(),
-        )
+            act_layer(inplace=True) if act_layer is not None else msnn.Identity(),
+            nn.Dropout(dropout, inplace = False),
+            nn.Linear(widths[1], num_classes, bias=True, **dd) if num_classes > 0 else msnn.Identity()
+        ])
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None):
         if pool_type is not None:
             assert pool_type, 'Cannot disable pooling'
             self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True,)
         if num_classes > 0:
-            self.classifier[-1] = nn.Linear(self.num_features, num_classes, bias=True)
+            self.classifier[-1] = nn.Linear(self.num_features, num_classes, bias = True)
         else:
-            self.classifier[-1] = nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
+            self.classifier[-1] = msnn.Identity()
+
+    def construct(self, x, pre_logits: bool = False):
         x = self.in_conv(x)
         x = self.global_pool(x)
         if pre_logits:
@@ -754,7 +764,7 @@         return x
 
 
-class EfficientVit(nn.Module):
+class EfficientVit(msnn.Cell):
     def __init__(
             self,
             in_chans: int = 3,
@@ -762,8 +772,8 @@             depths: Tuple[int, ...] = (),
             head_dim: int = 32,
             expand_ratio: float = 4,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            act_layer: Type[nn.Module] = nn.Hardswish,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.Hardswish,
             global_pool: str = 'avg',
             head_widths: Tuple[int, ...] = (),
             drop_rate: float = 0.0,
@@ -783,7 +793,7 @@ 
         # stages
         self.feature_info = []
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         in_channels = widths[0]
         for i, (w, d) in enumerate(zip(widths[1:], depths[1:])):
             self.stages.append(EfficientVitStage(
@@ -828,7 +838,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.classifier[-1]
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -837,13 +847,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -906,21 +916,21 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-class EfficientVitLarge(nn.Module):
+class EfficientVitLarge(msnn.Cell):
     def __init__(
         self,
         in_chans: int = 3,
         widths: Tuple[int, ...] = (),
         depths: Tuple[int, ...] = (),
         head_dim: int = 32,
-        norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-        act_layer: Type[nn.Module] = GELUTanh,
+        norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+        act_layer: Type[msnn.Cell] = GELUTanh,
         global_pool: str = 'avg',
         head_widths: Tuple[int, ...] = (),
         drop_rate: float = 0.0,
@@ -943,7 +953,7 @@ 
         # stages
         self.feature_info = []
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         in_channels = widths[0]
         for i, (w, d) in enumerate(zip(widths[1:], depths[1:])):
             self.stages.append(EfficientVitLargeStage(
@@ -990,7 +1000,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.classifier[-1]
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -999,13 +1009,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -1068,7 +1078,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
