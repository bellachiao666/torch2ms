--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ EfficientViT (by MSRA)
 
 Paper: `EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention`
@@ -11,8 +16,8 @@ from collections import OrderedDict
 from typing import Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SqueezeExcite, SelectAdaptivePool2d, trunc_normal_, _assert
@@ -22,7 +27,7 @@ from ._registry import register_model, generate_default_cfgs
 
 
-class ConvNorm(torch.nn.Sequential):
+class ConvNorm(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
@@ -40,24 +45,22 @@         super().__init__()
         self.conv = nn.Conv2d(in_chs, out_chs, ks, stride, pad, dilation, groups, bias=False, **dd)
         self.bn = nn.BatchNorm2d(out_chs, **dd)
-        torch.nn.init.constant_(self.bn.weight, bn_weight_init)
-
-    @torch.no_grad()
+        torch.nn.init.constant_(self.bn.weight, bn_weight_init)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
     def fuse(self):
         c, bn = self.conv, self.bn
         w = bn.weight / (bn.running_var + bn.eps)**0.5
         w = c.weight * w[:, None, None, None]
         b = bn.bias - bn.running_mean * bn.weight / \
             (bn.running_var + bn.eps)**0.5
-        m = torch.nn.Conv2d(
-            w.size(1) * self.conv.groups, w.size(0), w.shape[2:],
-            stride=self.conv.stride, padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups)
+        m = nn.Conv2d(
+            w.size(1) * self.conv.groups, w.size(0), w.shape[2:], stride = self.conv.stride, padding = self.conv.padding, dilation = self.conv.dilation, groups = self.conv.groups)
         m.weight.data.copy_(w)
         m.bias.data.copy_(b)
         return m
 
 
-class NormLinear(torch.nn.Sequential):
+class NormLinear(msnn.SequentialCell):
     def __init__(
             self,
             in_features: int,
@@ -76,9 +79,8 @@ 
         trunc_normal_(self.linear.weight, std=std)
         if self.linear.bias is not None:
-            nn.init.constant_(self.linear.bias, 0)
-
-    @torch.no_grad()
+            nn.init.constant_(self.linear.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
     def fuse(self):
         bn, linear = self.bn, self.linear
         w = bn.weight / (bn.running_var + bn.eps)**0.5
@@ -89,13 +91,13 @@             b = b @ self.linear.weight.T
         else:
             b = (linear.weight @ b[:, None]).view(-1) + self.linear.bias
-        m = torch.nn.Linear(w.size(1), w.size(0))
+        m = nn.Linear(w.size(1), w.size(0))
         m.weight.data.copy_(w)
         m.bias.data.copy_(b)
         return m
 
 
-class PatchMerging(torch.nn.Module):
+class PatchMerging(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -107,31 +109,31 @@         super().__init__()
         hid_dim = int(dim * 4)
         self.conv1 = ConvNorm(dim, hid_dim, 1, 1, 0, **dd)
-        self.act = torch.nn.ReLU()
+        self.act = nn.ReLU()
         self.conv2 = ConvNorm(hid_dim, hid_dim, 3, 2, 1, groups=hid_dim, **dd)
         self.se = SqueezeExcite(hid_dim, .25, **dd)
         self.conv3 = ConvNorm(hid_dim, out_dim, 1, 1, 0, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv3(self.se(self.act(self.conv2(self.act(self.conv1(x))))))
         return x
 
 
-class ResidualDrop(torch.nn.Module):
-    def __init__(self, m: nn.Module, drop: float = 0.):
+class ResidualDrop(msnn.Cell):
+    def __init__(self, m: msnn.Cell, drop: float = 0.):
         super().__init__()
         self.m = m
         self.drop = drop
 
-    def forward(self, x):
+    def construct(self, x):
         if self.training and self.drop > 0:
-            return x + self.m(x) * torch.rand(
-                x.size(0), 1, 1, 1, device=x.device).ge_(self.drop).div(1 - self.drop).detach()
+            return x + self.m(x) * mint.rand(
+                size = (x.size(0), 1, 1, 1)).ge_(self.drop).div(1 - self.drop).detach()  # 'torch.rand':没有对应的mindspore参数 'device' (position 5);
         else:
             return x + self.m(x)
 
 
-class ConvMlp(torch.nn.Module):
+class ConvMlp(msnn.Cell):
     def __init__(
         self,
         ed: int,
@@ -142,16 +144,16 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.pw1 = ConvNorm(ed, h, **dd)
-        self.act = torch.nn.ReLU()
+        self.act = nn.ReLU()
         self.pw2 = ConvNorm(h, ed, bn_weight_init=0, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.pw2(self.act(self.pw1(x)))
         return x
 
 
-class CascadedGroupAttention(torch.nn.Module):
-    attention_bias_cache: Dict[str, torch.Tensor]
+class CascadedGroupAttention(msnn.Cell):
+    attention_bias_cache: Dict[str, ms.Tensor]
 
     r""" Cascaded Group Attention.
 
@@ -187,12 +189,13 @@         for i in range(num_heads):
             qkvs.append(ConvNorm(dim // num_heads, self.key_dim * 2 + self.val_dim, **dd))
             dws.append(ConvNorm(self.key_dim, self.key_dim, kernels[i], 1, kernels[i] // 2, groups=self.key_dim, **dd))
-        self.qkvs = torch.nn.ModuleList(qkvs)
-        self.dws = torch.nn.ModuleList(dws)
-        self.proj = torch.nn.Sequential(
-            torch.nn.ReLU(),
+        self.qkvs = msnn.CellList(qkvs)
+        self.dws = msnn.CellList(dws)
+        self.proj = msnn.SequentialCell(
+            [
+            nn.ReLU(),
             ConvNorm(self.val_dim * num_heads, dim, bn_weight_init=0, **dd)
-        )
+        ])
 
         points = list(itertools.product(range(resolution), range(resolution)))
         N = len(points)
@@ -204,21 +207,22 @@                 if offset not in attention_offsets:
                     attention_offsets[offset] = len(attention_offsets)
                 idxs.append(attention_offsets[offset])
-        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, len(attention_offsets), **dd))
+        self.attention_biases = ms.Parameter(mint.zeros(num_heads, len(attention_offsets), **dd))
         self.register_buffer(
             'attention_bias_idxs',
-            torch.tensor(idxs, device=device, dtype=torch.long).view(N, N),
+            ms.Tensor(idxs, dtype = torch.long).view(N, N),
             persistent=False,
-        )
+        )  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;; 'torch.tensor':没有对应的mindspore参数 'device' (position 2);
         self.attention_bias_cache = {}
 
-    @torch.no_grad()
     def train(self, mode=True):
         super().train(mode)
         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def get_attention_biases(self, device: torch.device) -> ms.Tensor:
         if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
@@ -227,7 +231,7 @@                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
-    def forward(self, x):
+    def construct(self, x):
         B, C, H, W = x.shape
         feats_in = x.chunk(len(self.qkvs), dim=1)
         feats_out = []
@@ -247,11 +251,11 @@             feat = v @ attn.transpose(-2, -1)
             feat = feat.view(B, self.val_dim, H, W)
             feats_out.append(feat)
-        x = self.proj(torch.cat(feats_out, 1))
+        x = self.proj(mint.cat(feats_out, 1))
         return x
 
 
-class LocalWindowAttention(torch.nn.Module):
+class LocalWindowAttention(msnn.Cell):
     r""" Local Window Attention.
 
     Args:
@@ -291,7 +295,7 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         H = W = self.resolution
         B, C, H_, W_ = x.shape
         # Only check this for classification models
@@ -303,7 +307,7 @@             x = x.permute(0, 2, 3, 1)
             pad_b = (self.window_resolution - H % self.window_resolution) % self.window_resolution
             pad_r = (self.window_resolution - W % self.window_resolution) % self.window_resolution
-            x = torch.nn.functional.pad(x, (0, 0, 0, pad_r, 0, pad_b))
+            x = nn.functional.pad(x, (0, 0, 0, pad_r, 0, pad_b))
 
             pH, pW = H + pad_b, W + pad_r
             nH = pH // self.window_resolution
@@ -320,7 +324,7 @@         return x
 
 
-class EfficientVitBlock(torch.nn.Module):
+class EfficientVitBlock(msnn.Cell):
     """ A basic EfficientVit building block.
 
     Args:
@@ -364,11 +368,11 @@         self.dw1 = ResidualDrop(ConvNorm(dim, dim, 3, 1, 1, groups=dim, bn_weight_init=0., **dd))
         self.ffn1 = ResidualDrop(ConvMlp(dim, int(dim * 2), **dd))
 
-    def forward(self, x):
+    def construct(self, x):
         return self.ffn1(self.dw1(self.mixer(self.ffn0(self.dw0(x)))))
 
 
-class EfficientVitStage(torch.nn.Module):
+class EfficientVitStage(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
@@ -391,23 +395,27 @@             down_blocks = []
             down_blocks.append((
                 'res1',
-                torch.nn.Sequential(
-                    ResidualDrop(ConvNorm(in_dim, in_dim, 3, 1, 1, groups=in_dim, **dd)),
-                    ResidualDrop(ConvMlp(in_dim, int(in_dim * 2), **dd)),
-                )
+                msnn.SequentialCell(
+                    [
+                ResidualDrop(ConvNorm(in_dim, in_dim, 3, 1, 1, groups=in_dim, **dd)),
+                ResidualDrop(ConvMlp(in_dim, int(in_dim * 2), **dd))
+            ])
             ))
             down_blocks.append(('patchmerge', PatchMerging(in_dim, out_dim, **dd)))
             down_blocks.append((
                 'res2',
-                torch.nn.Sequential(
-                    ResidualDrop(ConvNorm(out_dim, out_dim, 3, 1, 1, groups=out_dim, **dd)),
-                    ResidualDrop(ConvMlp(out_dim, int(out_dim * 2), **dd)),
-                )
+                msnn.SequentialCell(
+                    [
+                ResidualDrop(ConvNorm(out_dim, out_dim, 3, 1, 1, groups=out_dim, **dd)),
+                ResidualDrop(ConvMlp(out_dim, int(out_dim * 2), **dd))
+            ])
             ))
-            self.downsample = nn.Sequential(OrderedDict(down_blocks))
+            self.downsample = msnn.SequentialCell([
+                OrderedDict(down_blocks)
+            ])
         else:
             assert in_dim == out_dim
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
             self.resolution = resolution
 
         blocks = []
@@ -422,15 +430,17 @@                 kernels,
                 **dd,
             ))
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x):
         x = self.downsample(x)
         x = self.blocks(x)
         return x
 
 
-class PatchEmbedding(torch.nn.Sequential):
+class PatchEmbedding(msnn.SequentialCell):
     def __init__(
             self,
             in_chans: int,
@@ -441,16 +451,16 @@         super().__init__()
         dd = {'device': device, 'dtype': dtype}
         self.add_module('conv1', ConvNorm(in_chans, dim // 8, 3, 2, 1, **dd))
-        self.add_module('relu1', torch.nn.ReLU())
+        self.add_module('relu1', nn.ReLU())
         self.add_module('conv2', ConvNorm(dim // 8, dim // 4, 3, 2, 1, **dd))
-        self.add_module('relu2', torch.nn.ReLU())
+        self.add_module('relu2', nn.ReLU())
         self.add_module('conv3', ConvNorm(dim // 4, dim // 2, 3, 2, 1, **dd))
-        self.add_module('relu3', torch.nn.ReLU())
+        self.add_module('relu3', nn.ReLU())
         self.add_module('conv4', ConvNorm(dim // 2, dim, 3, 2, 1, **dd))
         self.patch_size = 16
 
 
-class EfficientVitMsra(nn.Module):
+class EfficientVitMsra(msnn.Cell):
     def __init__(
             self,
             img_size: int = 224,
@@ -505,16 +515,18 @@             resolution = stage.resolution
             stages.append(stage)
             self.feature_info += [dict(num_chs=ed, reduction=stride, module=f'stages.{i}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell([
+            stages
+        ])
 
         if global_pool == 'avg':
             self.global_pool = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)
         else:
             assert num_classes == 0
-            self.global_pool = nn.Identity()
+            self.global_pool = msnn.Identity()
         self.num_features = self.head_hidden_size = embed_dim[-1]
         self.head = NormLinear(
-            self.num_features, num_classes, drop=self.drop_rate, **dd) if num_classes > 0 else torch.nn.Identity()
+            self.num_features, num_classes, drop=self.drop_rate, **dd) if num_classes > 0 else msnn.Identity()
 
     @torch.jit.ignore
     def no_weight_decay(self):
@@ -536,7 +548,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.linear
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -546,19 +558,19 @@                 self.global_pool = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)
             else:
                 assert num_classes == 0
-                self.global_pool = nn.Identity()
+                self.global_pool = msnn.Identity()
         self.head = NormLinear(
-            self.num_features, num_classes, drop=self.drop_rate) if num_classes > 0 else torch.nn.Identity()
+            self.num_features, num_classes, drop=self.drop_rate) if num_classes > 0 else msnn.Identity()
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -622,7 +634,7 @@         x = self.global_pool(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
