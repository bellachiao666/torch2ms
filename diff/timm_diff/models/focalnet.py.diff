--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ FocalNet
 
 As described in `Focal Modulation Networks` - https://arxiv.org/abs/2203.11926
@@ -20,8 +25,8 @@ from functools import partial
 from typing import Callable, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -42,7 +47,7 @@ __all__ = ['FocalNet']
 
 
-class FocalModulation(nn.Module):
+class FocalModulation(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -53,7 +58,7 @@             use_post_norm: bool = False,
             normalize_modulator: bool = False,
             proj_drop: float = 0.,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             device=None,
             dtype=None,
     ):
@@ -68,28 +73,28 @@         self.normalize_modulator = normalize_modulator
         self.input_split = [dim, dim, self.focal_level + 1]
 
-        self.f = nn.Conv2d(dim, 2 * dim + (self.focal_level + 1), kernel_size=1, bias=bias, **dd)
-        self.h = nn.Conv2d(dim, dim, kernel_size=1, bias=bias, **dd)
+        self.f = nn.Conv2d(dim, 2 * dim + (self.focal_level + 1), kernel_size=1, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.h = nn.Conv2d(dim, dim, kernel_size=1, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
 
         self.act = nn.GELU()
-        self.proj = nn.Conv2d(dim, dim, kernel_size=1, **dd)
+        self.proj = nn.Conv2d(dim, dim, kernel_size=1, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.proj_drop = nn.Dropout(proj_drop)
-        self.focal_layers = nn.ModuleList()
+        self.focal_layers = msnn.CellList()
 
         self.kernel_sizes = []
         for k in range(self.focal_level):
             kernel_size = self.focal_factor * k + self.focal_window
-            self.focal_layers.append(nn.Sequential(
+            self.focal_layers.append(msnn.SequentialCell(
                 nn.Conv2d(dim, dim, kernel_size=kernel_size, groups=dim, padding=kernel_size // 2, bias=False, **dd),
                 nn.GELU(),
-            ))
+            ))  # 存在 *args/**kwargs，需手动确认参数映射;
             self.kernel_sizes.append(kernel_size)
-        self.norm = norm_layer(dim, **dd) if self.use_post_norm else nn.Identity()
-
-    def forward(self, x):
+        self.norm = norm_layer(dim, **dd) if self.use_post_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         # pre linear projection
         x = self.f(x)
-        q, ctx, gates = torch.split(x, self.input_split, 1)
+        q, ctx, gates = mint.split(x, self.input_split, 1)
 
         # context aggregation
         ctx_all = 0
@@ -113,7 +118,7 @@         return x_out
 
 
-class FocalNetBlock(nn.Module):
+class FocalNetBlock(msnn.Cell):
     """ Focal Modulation Network Block.
     """
 
@@ -129,8 +134,8 @@             layerscale_value: Optional[float] = 1e-4,
             proj_drop: float = 0.,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             device=None,
             dtype=None,
     ):
@@ -157,7 +162,7 @@         self.focal_level = focal_level
         self.use_post_norm = use_post_norm
 
-        self.norm1 = norm_layer(dim, **dd) if not use_post_norm else nn.Identity()
+        self.norm1 = norm_layer(dim, **dd) if not use_post_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.modulation = FocalModulation(
             dim,
             focal_window=focal_window,
@@ -167,12 +172,12 @@             proj_drop=proj_drop,
             norm_layer=norm_layer,
             **dd,
-        )
-        self.norm1_post = norm_layer(dim, **dd) if use_post_norm else nn.Identity()
-        self.ls1 = LayerScale2d(dim, layerscale_value, **dd) if layerscale_value is not None else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        self.norm2 = norm_layer(dim, **dd) if not use_post_norm else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm1_post = norm_layer(dim, **dd) if use_post_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls1 = LayerScale2d(dim, layerscale_value, **dd) if layerscale_value is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+        self.norm2 = norm_layer(dim, **dd) if not use_post_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.mlp = Mlp(
             in_features=dim,
             hidden_features=int(dim * mlp_ratio),
@@ -180,12 +185,12 @@             drop=proj_drop,
             use_conv=True,
             **dd,
-        )
-        self.norm2_post = norm_layer(dim, **dd) if use_post_norm else nn.Identity()
-        self.ls2 = LayerScale2d(dim, layerscale_value, **dd) if layerscale_value is not None else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm2_post = norm_layer(dim, **dd) if use_post_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls2 = LayerScale2d(dim, layerscale_value, **dd) if layerscale_value is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x
 
         # Focal Modulation
@@ -200,7 +205,7 @@         return x
 
 
-class FocalNetStage(nn.Module):
+class FocalNetStage(msnn.Cell):
     """ A basic Focal Transformer layer for one stage.
     """
 
@@ -220,7 +225,7 @@             layerscale_value: Optional[float] = 1e-4,
             proj_drop: float = 0.,
             drop_path: Union[float, List[float]] = 0.,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             device=None,
             dtype=None,
     ):
@@ -255,12 +260,12 @@                 overlap=use_overlap_down,
                 norm_layer=norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         # build blocks
-        self.blocks = nn.ModuleList([
+        self.blocks = msnn.CellList([
             FocalNetBlock(
                 dim=out_dim,
                 mlp_ratio=mlp_ratio,
@@ -275,13 +280,13 @@                 norm_layer=norm_layer,
                 **dd,
             )
-            for i in range(depth)])
-
-    @torch.jit.ignore
+            for i in range(depth)])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.downsample(x)
         for blk in self.blocks:
             if self.grad_checkpointing and not torch.jit.is_scripting():
@@ -291,7 +296,7 @@         return x
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
 
     def __init__(
             self,
@@ -299,7 +304,7 @@             out_chs: int,
             stride: int = 4,
             overlap: bool = False,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -323,16 +328,16 @@                 kernel_size, padding = 7, 2
             elif stride == 2:
                 kernel_size, padding = 3, 1
-        self.proj = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding, **dd)
-        self.norm = norm_layer(out_chs, **dd) if norm_layer is not None else nn.Identity()
-
-    def forward(self, x):
+        self.proj = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm = norm_layer(out_chs, **dd) if norm_layer is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.proj(x)
         x = self.norm(x)
         return x
 
 
-class FocalNet(nn.Module):
+class FocalNet(msnn.Cell):
     """" Focal Modulation Networks (FocalNets)
     """
 
@@ -356,7 +361,7 @@             drop_rate: float = 0.,
             proj_drop_rate: float = 0.,
             drop_path_rate: float = 0.1,
-            norm_layer: Type[nn.Module] = partial(LayerNorm2d, eps=1e-5),
+            norm_layer: Type[msnn.Cell] = partial(LayerNorm2d, eps=1e-5),
             device=None,
             dtype=None,
     ):
@@ -392,7 +397,7 @@             overlap=use_overlap_down,
             norm_layer=norm_layer,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         in_dim = embed_dim[0]
 
         dpr = calculate_drop_path_rates(drop_path_rate, sum(depths))  # stochastic depth decay rule
@@ -416,15 +421,15 @@                 drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                 norm_layer=norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             in_dim = out_dim
             layers += [layer]
             self.feature_info += [dict(num_chs=out_dim, reduction=4 * 2 ** i_layer, module=f'layers.{i_layer}')]
 
-        self.layers = nn.Sequential(*layers)
+        self.layers = msnn.SequentialCell(*layers)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         if head_hidden_size:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
             self.head_hidden_size = head_hidden_size
             self.head = NormMlpClassifierHead(
                 self.num_features,
@@ -434,24 +439,24 @@                 drop_rate=drop_rate,
                 norm_layer=norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.norm = norm_layer(self.num_features, **dd)
+            self.norm = norm_layer(self.num_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.head = ClassifierHead(
                 self.num_features,
                 num_classes,
                 pool_type=global_pool,
                 drop_rate=drop_rate,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         named_apply(partial(_init_weights, head_init_scale=head_init_scale), self)
 
-    @torch.jit.ignore
+    @ms.jit
     def no_weight_decay(self):
         return {''}
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',
@@ -465,14 +470,14 @@             ]
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
         for l in self.layers:
             l.set_grad_checkpointing(enable=enable)
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -481,13 +486,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -540,7 +545,7 @@         take_indices, max_index = feature_take_indices(len(self.layers), indices)
         self.layers = self.layers[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
@@ -554,7 +559,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -564,11 +569,11 @@     if isinstance(module, nn.Conv2d):
         trunc_normal_(module.weight, std=.02)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Linear):
         trunc_normal_(module.weight, std=.02)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if name and 'head.fc' in name:
             module.weight.data.mul_(head_init_scale)
             module.bias.data.mul_(head_init_scale)
@@ -649,44 +654,44 @@         FocalNet, variant, pretrained,
         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
 @register_model
 def focalnet_tiny_srf(pretrained=False, **kwargs) -> FocalNet:
-    model_kwargs = dict(depths=[2, 2, 6, 2], embed_dim=96, **kwargs)
-    return _create_focalnet('focalnet_tiny_srf', pretrained=pretrained, **model_kwargs)
+    model_kwargs = dict(depths=[2, 2, 6, 2], embed_dim=96, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_tiny_srf', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def focalnet_small_srf(pretrained=False, **kwargs) -> FocalNet:
-    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=96, **kwargs)
-    return _create_focalnet('focalnet_small_srf', pretrained=pretrained, **model_kwargs)
+    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=96, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_small_srf', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def focalnet_base_srf(pretrained=False, **kwargs) -> FocalNet:
-    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=128, **kwargs)
-    return _create_focalnet('focalnet_base_srf', pretrained=pretrained, **model_kwargs)
+    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=128, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_base_srf', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def focalnet_tiny_lrf(pretrained=False, **kwargs) -> FocalNet:
-    model_kwargs = dict(depths=[2, 2, 6, 2], embed_dim=96, focal_levels=[3, 3, 3, 3], **kwargs)
-    return _create_focalnet('focalnet_tiny_lrf', pretrained=pretrained, **model_kwargs)
+    model_kwargs = dict(depths=[2, 2, 6, 2], embed_dim=96, focal_levels=[3, 3, 3, 3], **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_tiny_lrf', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def focalnet_small_lrf(pretrained=False, **kwargs) -> FocalNet:
-    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=96, focal_levels=[3, 3, 3, 3], **kwargs)
-    return _create_focalnet('focalnet_small_lrf', pretrained=pretrained, **model_kwargs)
+    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=96, focal_levels=[3, 3, 3, 3], **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_small_lrf', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def focalnet_base_lrf(pretrained=False, **kwargs) -> FocalNet:
-    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=128, focal_levels=[3, 3, 3, 3], **kwargs)
-    return _create_focalnet('focalnet_base_lrf', pretrained=pretrained, **model_kwargs)
+    model_kwargs = dict(depths=[2, 2, 18, 2], embed_dim=128, focal_levels=[3, 3, 3, 3], **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_base_lrf', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 # FocalNet large+ models
@@ -694,46 +699,46 @@ def focalnet_large_fl3(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
         depths=[2, 2, 18, 2], embed_dim=192, focal_levels=[3, 3, 3, 3], focal_windows=[5] * 4,
-        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
-    return _create_focalnet('focalnet_large_fl3', pretrained=pretrained, **model_kwargs)
+        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_large_fl3', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def focalnet_large_fl4(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
         depths=[2, 2, 18, 2], embed_dim=192, focal_levels=[4, 4, 4, 4],
-        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
-    return _create_focalnet('focalnet_large_fl4', pretrained=pretrained, **model_kwargs)
+        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_large_fl4', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def focalnet_xlarge_fl3(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
         depths=[2, 2, 18, 2], embed_dim=256, focal_levels=[3, 3, 3, 3], focal_windows=[5] * 4,
-        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
-    return _create_focalnet('focalnet_xlarge_fl3', pretrained=pretrained, **model_kwargs)
+        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_xlarge_fl3', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def focalnet_xlarge_fl4(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
         depths=[2, 2, 18, 2], embed_dim=256, focal_levels=[4, 4, 4, 4],
-        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
-    return _create_focalnet('focalnet_xlarge_fl4', pretrained=pretrained, **model_kwargs)
+        use_post_norm=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_xlarge_fl4', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def focalnet_huge_fl3(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
         depths=[2, 2, 18, 2], embed_dim=352, focal_levels=[3, 3, 3, 3], focal_windows=[3] * 4,
-        use_post_norm=True, use_post_norm_in_modulation=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
-    return _create_focalnet('focalnet_huge_fl3', pretrained=pretrained, **model_kwargs)
+        use_post_norm=True, use_post_norm_in_modulation=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_huge_fl3', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def focalnet_huge_fl4(pretrained=False, **kwargs) -> FocalNet:
     model_kwargs = dict(
         depths=[2, 2, 18, 2], embed_dim=352, focal_levels=[4, 4, 4, 4],
-        use_post_norm=True, use_post_norm_in_modulation=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)
-    return _create_focalnet('focalnet_huge_fl4', pretrained=pretrained, **model_kwargs)
-
+        use_post_norm=True, use_post_norm_in_modulation=True, use_overlap_down=True, layerscale_value=1e-4, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_focalnet('focalnet_huge_fl4', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
