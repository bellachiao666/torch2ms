--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ FocalNet
 
 As described in `Focal Modulation Networks` - https://arxiv.org/abs/2203.11926
@@ -20,8 +25,8 @@ from functools import partial
 from typing import Callable, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -42,7 +47,7 @@ __all__ = ['FocalNet']
 
 
-class FocalModulation(nn.Module):
+class FocalModulation(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -74,22 +79,23 @@         self.act = nn.GELU()
         self.proj = nn.Conv2d(dim, dim, kernel_size=1, **dd)
         self.proj_drop = nn.Dropout(proj_drop)
-        self.focal_layers = nn.ModuleList()
+        self.focal_layers = msnn.CellList()
 
         self.kernel_sizes = []
         for k in range(self.focal_level):
             kernel_size = self.focal_factor * k + self.focal_window
-            self.focal_layers.append(nn.Sequential(
+            self.focal_layers.append(msnn.SequentialCell(
+                [
                 nn.Conv2d(dim, dim, kernel_size=kernel_size, groups=dim, padding=kernel_size // 2, bias=False, **dd),
-                nn.GELU(),
-            ))
+                nn.GELU()
+            ]))
             self.kernel_sizes.append(kernel_size)
-        self.norm = norm_layer(dim, **dd) if self.use_post_norm else nn.Identity()
-
-    def forward(self, x):
+        self.norm = norm_layer(dim, **dd) if self.use_post_norm else msnn.Identity()
+
+    def construct(self, x):
         # pre linear projection
         x = self.f(x)
-        q, ctx, gates = torch.split(x, self.input_split, 1)
+        q, ctx, gates = mint.split(x, self.input_split, 1)
 
         # context aggregation
         ctx_all = 0
@@ -113,7 +119,7 @@         return x_out
 
 
-class FocalNetBlock(nn.Module):
+class FocalNetBlock(msnn.Cell):
     """ Focal Modulation Network Block.
     """
 
@@ -157,7 +163,7 @@         self.focal_level = focal_level
         self.use_post_norm = use_post_norm
 
-        self.norm1 = norm_layer(dim, **dd) if not use_post_norm else nn.Identity()
+        self.norm1 = norm_layer(dim, **dd) if not use_post_norm else msnn.Identity()
         self.modulation = FocalModulation(
             dim,
             focal_window=focal_window,
@@ -168,11 +174,11 @@             norm_layer=norm_layer,
             **dd,
         )
-        self.norm1_post = norm_layer(dim, **dd) if use_post_norm else nn.Identity()
-        self.ls1 = LayerScale2d(dim, layerscale_value, **dd) if layerscale_value is not None else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        self.norm2 = norm_layer(dim, **dd) if not use_post_norm else nn.Identity()
+        self.norm1_post = norm_layer(dim, **dd) if use_post_norm else msnn.Identity()
+        self.ls1 = LayerScale2d(dim, layerscale_value, **dd) if layerscale_value is not None else msnn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+        self.norm2 = norm_layer(dim, **dd) if not use_post_norm else msnn.Identity()
         self.mlp = Mlp(
             in_features=dim,
             hidden_features=int(dim * mlp_ratio),
@@ -181,11 +187,11 @@             use_conv=True,
             **dd,
         )
-        self.norm2_post = norm_layer(dim, **dd) if use_post_norm else nn.Identity()
-        self.ls2 = LayerScale2d(dim, layerscale_value, **dd) if layerscale_value is not None else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.norm2_post = norm_layer(dim, **dd) if use_post_norm else msnn.Identity()
+        self.ls2 = LayerScale2d(dim, layerscale_value, **dd) if layerscale_value is not None else msnn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x
 
         # Focal Modulation
@@ -200,7 +206,7 @@         return x
 
 
-class FocalNetStage(nn.Module):
+class FocalNetStage(msnn.Cell):
     """ A basic Focal Transformer layer for one stage.
     """
 
@@ -257,10 +263,10 @@                 **dd,
             )
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         # build blocks
-        self.blocks = nn.ModuleList([
+        self.blocks = msnn.CellList([
             FocalNetBlock(
                 dim=out_dim,
                 mlp_ratio=mlp_ratio,
@@ -281,7 +287,7 @@     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.downsample(x)
         for blk in self.blocks:
             if self.grad_checkpointing and not torch.jit.is_scripting():
@@ -291,7 +297,7 @@         return x
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
 
     def __init__(
             self,
@@ -324,15 +330,15 @@             elif stride == 2:
                 kernel_size, padding = 3, 1
         self.proj = nn.Conv2d(in_chs, out_chs, kernel_size=kernel_size, stride=stride, padding=padding, **dd)
-        self.norm = norm_layer(out_chs, **dd) if norm_layer is not None else nn.Identity()
-
-    def forward(self, x):
+        self.norm = norm_layer(out_chs, **dd) if norm_layer is not None else msnn.Identity()
+
+    def construct(self, x):
         x = self.proj(x)
         x = self.norm(x)
         return x
 
 
-class FocalNet(nn.Module):
+class FocalNet(msnn.Cell):
     """" Focal Modulation Networks (FocalNets)
     """
 
@@ -421,10 +427,12 @@             layers += [layer]
             self.feature_info += [dict(num_chs=out_dim, reduction=4 * 2 ** i_layer, module=f'layers.{i_layer}')]
 
-        self.layers = nn.Sequential(*layers)
+        self.layers = msnn.SequentialCell([
+            layers
+        ])
 
         if head_hidden_size:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
             self.head_hidden_size = head_hidden_size
             self.head = NormMlpClassifierHead(
                 self.num_features,
@@ -471,6 +479,7 @@         for l in self.layers:
             l.set_grad_checkpointing(enable=enable)
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         return self.head.fc
@@ -481,7 +490,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
@@ -540,7 +549,7 @@         take_indices, max_index = feature_take_indices(len(self.layers), indices)
         self.layers = self.layers[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
@@ -554,7 +563,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -564,11 +573,11 @@     if isinstance(module, nn.Conv2d):
         trunc_normal_(module.weight, std=.02)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Linear):
         trunc_normal_(module.weight, std=.02)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if name and 'head.fc' in name:
             module.weight.data.mul_(head_init_scale)
             module.bias.data.mul_(head_init_scale)
