--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ PP-HGNet (V1 & V2)
 
 Reference:
@@ -8,9 +13,8 @@ """
 from typing import Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectAdaptivePool2d, DropPath, calculate_drop_path_rates, create_conv2d
@@ -22,7 +26,7 @@ __all__ = ['HighPerfGpuNet']
 
 
-class LearnableAffineBlock(nn.Module):
+class LearnableAffineBlock(msnn.Cell):
     def __init__(
             self,
             scale_value: float = 1.0,
@@ -32,14 +36,14 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.scale = nn.Parameter(torch.tensor([scale_value], **dd), requires_grad=True)
-        self.bias = nn.Parameter(torch.tensor([bias_value], **dd), requires_grad=True)
-
-    def forward(self, x):
+        self.scale = ms.Parameter(ms.Tensor([scale_value], **dd), requires_grad=True)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.bias = ms.Parameter(ms.Tensor([bias_value], **dd), requires_grad=True)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         return self.scale * x + self.bias
 
 
-class ConvBNAct(nn.Module):
+class ConvBNAct(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -65,18 +69,18 @@             padding=padding,
             groups=groups,
             **dd,
-        )
-        self.bn = nn.BatchNorm2d(out_chs, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.bn = nn.BatchNorm2d(out_chs, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         if self.use_act:
             self.act = nn.ReLU()
         else:
-            self.act = nn.Identity()
+            self.act = msnn.Identity()
         if self.use_act and self.use_lab:
-            self.lab = LearnableAffineBlock(**dd)
+            self.lab = LearnableAffineBlock(**dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.lab = nn.Identity()
-
-    def forward(self, x):
+            self.lab = msnn.Identity()
+
+    def construct(self, x):
         x = self.conv(x)
         x = self.bn(x)
         x = self.act(x)
@@ -84,7 +88,7 @@         return x
 
 
-class LightConvBNAct(nn.Module):
+class LightConvBNAct(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -104,7 +108,7 @@             use_act=False,
             use_lab=use_lab,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv2 = ConvBNAct(
             out_chs,
             out_chs,
@@ -113,15 +117,15 @@             use_act=True,
             use_lab=use_lab,
             **dd,
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.conv1(x)
         x = self.conv2(x)
         return x
 
 
-class EseModule(nn.Module):
+class EseModule(msnn.Cell):
     def __init__(self, chs: int, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
@@ -132,23 +136,23 @@             stride=1,
             padding=0,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
         self.sigmoid = nn.Sigmoid()
 
-    def forward(self, x):
+    def construct(self, x):
         identity = x
         x = x.mean((2, 3), keepdim=True)
         x = self.conv(x)
         x = self.sigmoid(x)
-        return torch.mul(identity, x)
-
-
-class StemV1(nn.Module):
+        return mint.mul(identity, x)
+
+
+class StemV1(msnn.Cell):
     # for PP-HGNet
     def __init__(self, stem_chs: List[int], device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.stem = nn.Sequential(*[
+        self.stem = msnn.SequentialCell(*[
             ConvBNAct(
                 stem_chs[i],
                 stem_chs[i + 1],
@@ -156,16 +160,16 @@                 stride=2 if i == 0 else 1,
                 **dd) for i in range(
                 len(stem_chs) - 1)
-        ])
-        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
-
-    def forward(self, x):
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.pool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
+
+    def construct(self, x):
         x = self.stem(x)
         x = self.pool(x)
         return x
 
 
-class StemV2(nn.Module):
+class StemV2(msnn.Cell):
     # for PP-HGNetv2
     def __init__(
             self,
@@ -185,7 +189,7 @@             stride=2,
             use_lab=use_lab,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.stem2a = ConvBNAct(
             mid_chs,
             mid_chs // 2,
@@ -193,7 +197,7 @@             stride=1,
             use_lab=use_lab,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.stem2b = ConvBNAct(
             mid_chs // 2,
             mid_chs,
@@ -201,7 +205,7 @@             stride=1,
             use_lab=use_lab,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.stem3 = ConvBNAct(
             mid_chs * 2,
             mid_chs,
@@ -209,7 +213,7 @@             stride=2,
             use_lab=use_lab,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.stem4 = ConvBNAct(
             mid_chs,
             out_chs,
@@ -217,23 +221,23 @@             stride=1,
             use_lab=use_lab,
             **dd,
-        )
-        self.pool = nn.MaxPool2d(kernel_size=2, stride=1, ceil_mode=True)
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 1, ceil_mode = True)
+
+    def construct(self, x):
         x = self.stem1(x)
-        x = F.pad(x, (0, 1, 0, 1))
+        x = nn.functional.pad(x, (0, 1, 0, 1))
         x2 = self.stem2a(x)
-        x2 = F.pad(x2, (0, 1, 0, 1))
+        x2 = nn.functional.pad(x2, (0, 1, 0, 1))
         x2 = self.stem2b(x2)
         x1 = self.pool(x)
-        x = torch.cat([x1, x2], dim=1)
+        x = mint.cat([x1, x2], dim=1)
         x = self.stem3(x)
         x = self.stem4(x)
         return x
 
 
-class HighPerfGpuBlock(nn.Module):
+class HighPerfGpuBlock(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -253,7 +257,7 @@         super().__init__()
         self.residual = residual
 
-        self.layers = nn.ModuleList()
+        self.layers = msnn.CellList()
         for i in range(layer_num):
             if light_block:
                 self.layers.append(
@@ -264,7 +268,7 @@                         use_lab=use_lab,
                         **dd,
                     )
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
                 self.layers.append(
                     ConvBNAct(
@@ -275,7 +279,7 @@                         use_lab=use_lab,
                         **dd,
                     )
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # feature aggregation
         total_chs = in_chs + layer_num * mid_chs
@@ -287,7 +291,7 @@                 stride=1,
                 use_lab=use_lab,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             aggregation_excitation_conv = ConvBNAct(
                 out_chs // 2,
                 out_chs,
@@ -295,11 +299,12 @@                 stride=1,
                 use_lab=use_lab,
                 **dd,
-            )
-            self.aggregation = nn.Sequential(
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.aggregation = msnn.SequentialCell(
+                [
                 aggregation_squeeze_conv,
-                aggregation_excitation_conv,
-            )
+                aggregation_excitation_conv
+            ])
         else:
             aggregation_conv = ConvBNAct(
                 total_chs,
@@ -308,29 +313,30 @@                 stride=1,
                 use_lab=use_lab,
                 **dd,
-            )
-            att = EseModule(out_chs, **dd)
-            self.aggregation = nn.Sequential(
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            att = EseModule(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.aggregation = msnn.SequentialCell(
+                [
                 aggregation_conv,
-                att,
-            )
-
-        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()
-
-    def forward(self, x):
+                att
+            ])
+
+        self.drop_path = DropPath(drop_path) if drop_path else msnn.Identity()
+
+    def construct(self, x):
         identity = x
         output = [x]
         for layer in self.layers:
             x = layer(x)
             output.append(x)
-        x = torch.cat(output, dim=1)
+        x = mint.cat(output, dim=1)
         x = self.aggregation(x)
         if self.residual:
             x = self.drop_path(x) + identity
         return x
 
 
-class HighPerfGpuStage(nn.Module):
+class HighPerfGpuStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -361,9 +367,9 @@                 use_act=False,
                 use_lab=use_lab,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         blocks_list = []
         for i in range(block_num):
@@ -381,12 +387,13 @@                     drop_path=drop_path[i] if isinstance(drop_path, (list, tuple)) else drop_path,
                     **dd,
                 )
-            )
-        self.blocks = nn.Sequential(*blocks_list)
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.blocks = msnn.SequentialCell(*blocks_list)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.grad_checkpointing= False
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.downsample(x)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
@@ -394,7 +401,7 @@         return x
 
 
-class ClassifierHead(nn.Module):
+class ClassifierHead(msnn.Cell):
     def __init__(
             self,
             in_features: int,
@@ -424,19 +431,26 @@                 padding=0,
                 bias=False,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，需手动确认参数映射;
             act = nn.ReLU()
             if use_lab:
-                lab = LearnableAffineBlock(**dd)
-                self.last_conv = nn.Sequential(last_conv, act, lab)
+                lab = LearnableAffineBlock(**dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+                self.last_conv = msnn.SequentialCell([
+                    last_conv,
+                    act,
+                    lab
+                ])
             else:
-                self.last_conv = nn.Sequential(last_conv, act)
+                self.last_conv = msnn.SequentialCell([
+                    last_conv,
+                    act
+                ])
         else:
-            self.last_conv = nn.Identity()
+            self.last_conv = msnn.Identity()
 
         self.dropout = nn.Dropout(drop_rate)
-        self.flatten = nn.Flatten(1) if pool_type else nn.Identity()  # don't flatten if pooling disabled
-        self.fc = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.flatten = mint.flatten(1) if pool_type else msnn.Identity()  # don't flatten if pooling disabled
+        self.fc = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，需手动确认参数映射;
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
@@ -444,11 +458,11 @@             if not pool_type:
                 assert num_classes == 0, 'Classifier head must be removed if pooling is disabled'
             self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)
-            self.flatten = nn.Flatten(1) if pool_type else nn.Identity()  # don't flatten if pooling disabled
-
-        self.fc = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
+            self.flatten = mint.flatten(1) if pool_type else msnn.Identity()  # don't flatten if pooling disabled
+
+        self.fc = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         x = self.last_conv(x)
         x = self.dropout(x)
@@ -459,7 +473,7 @@         return x
 
 
-class HighPerfGpuNet(nn.Module):
+class HighPerfGpuNet(msnn.Cell):
 
     def __init__(
             self,
@@ -492,9 +506,9 @@                 out_chs=stem_chs[1],
                 use_lab=use_lab,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.stem = StemV1([in_chans] + stem_chs, **dd)
+            self.stem = StemV1([in_chans] + stem_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         current_stride = 4
 
@@ -517,12 +531,12 @@                 agg='ese' if stem_type == 'v1' else 'se',
                 drop_path=dpr[i],
                 **dd,
-            )]
+            )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.num_features = out_chs
             if downsample:
                 current_stride *= 2
             self.feature_info += [dict(num_chs=self.num_features, reduction=current_stride, module=f'stages.{i}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.head = ClassifierHead(
             self.num_features,
@@ -532,32 +546,32 @@             hidden_size=head_hidden_size,
             use_lab=use_lab,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.head_hidden_size = self.head.num_features
 
         for n, m in self.named_modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.BatchNorm2d):
-                nn.init.ones_(m.weight)
-                nn.init.zeros_(m.bias)
+                nn.init.ones_(m.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                nn.init.zeros_(m.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.Linear):
-                nn.init.zeros_(m.bias)
-
-    @torch.jit.ignore
+                nn.init.zeros_(m.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',
             blocks=r'^stages\.(\d+)' if coarse else r'^stages\.(\d+).blocks\.(\d+)',
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         for s in self.stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None, device=None, dtype=None):
@@ -566,13 +580,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -591,6 +605,7 @@ 
         # forward pass
         x = self.stem(x)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             stages = self.stages
         else:
@@ -627,7 +642,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -738,7 +753,7 @@         model_cfg=model_cfgs[variant],
         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url='', **kwargs):
@@ -803,49 +818,49 @@ 
 @register_model
 def hgnet_tiny(pretrained=False, **kwargs) -> HighPerfGpuNet:
-    return _create_hgnet('hgnet_tiny', pretrained=pretrained, **kwargs)
+    return _create_hgnet('hgnet_tiny', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hgnet_small(pretrained=False, **kwargs) -> HighPerfGpuNet:
-    return _create_hgnet('hgnet_small', pretrained=pretrained, **kwargs)
+    return _create_hgnet('hgnet_small', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hgnet_base(pretrained=False, **kwargs) -> HighPerfGpuNet:
-    return _create_hgnet('hgnet_base', pretrained=pretrained, **kwargs)
+    return _create_hgnet('hgnet_base', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hgnetv2_b0(pretrained=False, **kwargs) -> HighPerfGpuNet:
-    return _create_hgnet('hgnetv2_b0', pretrained=pretrained, use_lab=True, **kwargs)
+    return _create_hgnet('hgnetv2_b0', pretrained=pretrained, use_lab=True, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hgnetv2_b1(pretrained=False, **kwargs) -> HighPerfGpuNet:
-    return _create_hgnet('hgnetv2_b1', pretrained=pretrained, use_lab=True, **kwargs)
+    return _create_hgnet('hgnetv2_b1', pretrained=pretrained, use_lab=True, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hgnetv2_b2(pretrained=False, **kwargs) -> HighPerfGpuNet:
-    return _create_hgnet('hgnetv2_b2', pretrained=pretrained, use_lab=True, **kwargs)
+    return _create_hgnet('hgnetv2_b2', pretrained=pretrained, use_lab=True, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hgnetv2_b3(pretrained=False, **kwargs) -> HighPerfGpuNet:
-    return _create_hgnet('hgnetv2_b3', pretrained=pretrained, use_lab=True, **kwargs)
+    return _create_hgnet('hgnetv2_b3', pretrained=pretrained, use_lab=True, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hgnetv2_b4(pretrained=False, **kwargs) -> HighPerfGpuNet:
-    return _create_hgnet('hgnetv2_b4', pretrained=pretrained, **kwargs)
+    return _create_hgnet('hgnetv2_b4', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hgnetv2_b5(pretrained=False, **kwargs) -> HighPerfGpuNet:
-    return _create_hgnet('hgnetv2_b5', pretrained=pretrained, **kwargs)
+    return _create_hgnet('hgnetv2_b5', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hgnetv2_b6(pretrained=False, **kwargs) -> HighPerfGpuNet:
-    return _create_hgnet('hgnetv2_b6', pretrained=pretrained, **kwargs)
+    return _create_hgnet('hgnetv2_b6', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
