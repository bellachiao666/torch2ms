--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ An PyTorch implementation of Hiera
 
 Adapted for timm from originals at https://github.com/facebookresearch/hiera
@@ -26,9 +31,9 @@ from functools import partial
 from typing import Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -55,16 +60,16 @@ __all__ = ['Hiera']
 
 
-def conv_nd(n: int) -> Type[nn.Module]:
+def conv_nd(n: int) -> Type[msnn.Cell]:
     """
     Returns a conv with nd (e.g., Conv2d for n=2). Work up to n=3.
     If you wanted a 4d Hiera, you could probably just implement this for n=4. (no promises)
     """
-    return [nn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n]
+    return [msnn.Identity, nn.Conv1d, nn.Conv2d, nn.Conv3d][n]
 
 
 @register_notrace_function
-def get_resized_mask(target_size: List[int], mask: torch.Tensor) -> torch.Tensor:
+def get_resized_mask(target_size: List[int], mask: ms.Tensor) -> ms.Tensor:
     # target_size: [(T), (H), W]
     # (spatial) mask: [B, C, (t), (h), w]
     if mask is None:
@@ -72,15 +77,15 @@ 
     _assert(len(mask.shape[2:]) == len(target_size), "mask spatial shape and target_size must match.")
     if mask.shape[2:] != target_size:
-        return F.interpolate(mask.float(), size=target_size)
+        return nn.functional.interpolate(mask.float(), size = target_size)
     return mask
 
 
 def undo_windowing(
-        x: torch.Tensor,
+        x: ms.Tensor,
         shape: List[int],
         mu_shape: List[int],
-) -> torch.Tensor:
+) -> ms.Tensor:
     """
     Restore spatial organization by undoing windowed organization of mask units.
 
@@ -96,7 +101,7 @@     B, C = x.shape[0], x.shape[-1]
     # [B, #MUy*#MUx, MUy, MUx, C] -> [B, #MUy, #MUx, MUy, MUx, C]
     num_MUs = [s // mu for s, mu in zip(shape, mu_shape)]
-    x = x.view(B, *num_MUs, *mu_shape, C)
+    x = x.view(B, *num_MUs, *mu_shape, C)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     # [B, #MUy, #MUx, MUy, MUx, C] -> [B, #MUy*MUy, #MUx*MUx, C]
     permute = (
@@ -104,12 +109,12 @@         + sum([list(p) for p in zip(range(1, 1 + D), range(1 + D, 1 + 2 * D))], [])
         + [len(x.shape) - 1]
     )
-    x = x.permute(permute).reshape(B, *shape, C)
+    x = x.permute(permute).reshape(B, *shape, C)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     return x
 
 
-class Unroll(nn.Module):
+class Unroll(msnn.Cell):
     """
     Reorders the tokens such that patches are contiguous in memory.
     E.g., given [B, (H, W), C] and stride of (Sy, Sx), this will re-order the tokens as
@@ -139,14 +144,14 @@         self.size = [i // s for i, s in zip(input_size, patch_stride)]
         self.schedule = unroll_schedule
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """
         Input: Flattened patch embeddings [B, N, C]
         Output: Patch embeddings [B, N, C] permuted such that [B, 4, N//4, C].max(1) etc. performs MaxPoolNd
         """
         B, _, C = x.shape
         cur_size = self.size
-        x = x.view(*([B] + cur_size + [C]))
+        x = x.view(*([B] + cur_size + [C]))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         for strides in self.schedule:
             # Move patches with the given strides to the batch dimension
@@ -171,7 +176,7 @@         return x
 
 
-class Reroll(nn.Module):
+class Reroll(msnn.Cell):
     """
     Undos the "unroll" operation so that you can use intermediate features.
     """
@@ -199,12 +204,12 @@                     size = [n // s for n, s in zip(size, unroll_schedule[0])]
                 unroll_schedule = unroll_schedule[1:]
 
-    def forward(
-            self,
-            x: torch.Tensor,
+    def construct(
+            self,
+            x: ms.Tensor,
             block_idx: int,
-            mask: torch.Tensor = None
-    ) -> torch.Tensor:
+            mask: ms.Tensor = None
+    ) -> ms.Tensor:
         """
         Roll the given tensor back up to spatial order assuming it's from the given block.
 
@@ -221,7 +226,7 @@ 
         for strides in schedule:
             # Extract the current patch from N
-            x = x.view(B, *strides, N // math.prod(strides), *cur_mu_shape, C)
+            x = x.view(B, *strides, N // math.prod(strides), *cur_mu_shape, C)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
             # Move that patch into the current MU
             # Example in 2d: [B, Sy, Sx, N//(Sy*Sx), MUy, MUx, C] -> [B, N//(Sy*Sx), Sy, MUy, Sx, MUx, C]
@@ -236,11 +241,11 @@             # Reshape to [B, N//(Sy*Sx), *MU, C]
             for i in range(D):
                 cur_mu_shape[i] *= strides[i]
-            x = x.reshape(B, -1, *cur_mu_shape, C)
+            x = x.reshape(B, -1, *cur_mu_shape, C)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             N = x.shape[1]
 
         # Current shape (e.g., 2d: [B, #MUy*#MUx, MUy, MUx, C])
-        x = x.view(B, N, *cur_mu_shape, C)
+        x = x.view(B, N, *cur_mu_shape, C)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # If masked, return [B, #MUs, MUy, MUx, C]
         if mask is not None:
@@ -252,14 +257,14 @@         return x
 
 
-class MaskUnitAttention(nn.Module):
+class MaskUnitAttention(msnn.Cell):
     """
     Computes either Mask Unit or Global Attention. Also is able to perform q pooling.
 
     Note: this assumes the tokens have already been flattened and unrolled into mask units.
     See `Unroll` for more details.
     """
-    fused_attn: torch.jit.Final[bool]
+    fused_attn: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -290,13 +295,13 @@         self.scale = self.head_dim ** -0.5
         self.fused_attn = use_fused_attn()
 
-        self.qkv = nn.Linear(dim, 3 * dim_out, **dd)
-        self.proj = nn.Linear(dim_out, dim_out, **dd)
+        self.qkv = nn.Linear(dim, 3 * dim_out, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.proj = nn.Linear(dim_out, dim_out, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
 
         self.window_size = window_size
         self.use_mask_unit_attn = use_mask_unit_attn
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """ Input should be of shape [batch, tokens, channels]. """
         B, N, _ = x.shape
         num_windows = (N // (self.q_stride * self.window_size)) if self.use_mask_unit_attn else 1
@@ -309,7 +314,7 @@ 
         if self.fused_attn:
             # Note: the original paper did *not* use SDPA, it's a free boost!
-            x = F.scaled_dot_product_attention(q, k, v)
+            x = F.scaled_dot_product_attention(q, k, v)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             attn = (q * self.scale) @ k.transpose(-1, -2)
             attn = attn.softmax(dim=-1)
@@ -320,7 +325,7 @@         return x
 
 
-class HieraBlock(nn.Module):
+class HieraBlock(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -329,8 +334,8 @@             mlp_ratio: float = 4.0,
             drop_path: float = 0.0,
             init_values: Optional[float] = None,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             q_stride: int = 1,
             window_size: int = 0,
             use_expand_proj: bool = True,
@@ -343,11 +348,11 @@         self.dim = dim
         self.dim_out = dim_out
 
-        self.norm1 = norm_layer(dim, **dd)
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if dim != dim_out:
             self.do_expand = True
             if use_expand_proj:
-                self.proj = nn.Linear(dim, dim_out, **dd)
+                self.proj = nn.Linear(dim, dim_out, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
             else:
                 assert dim_out == dim * 2
                 self.proj = None
@@ -362,16 +367,16 @@             window_size,
             use_mask_unit_attn,
             **dd
-        )
-        self.ls1 = LayerScale(dim_out, init_values=init_values, **dd) if init_values is not None else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0 else nn.Identity()
-
-        self.norm2 = norm_layer(dim_out, **dd)
-        self.mlp = Mlp(dim_out, int(dim_out * mlp_ratio), act_layer=act_layer, **dd)
-        self.ls2 = LayerScale(dim_out, init_values=init_values, **dd) if init_values is not None else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0 else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls1 = LayerScale(dim_out, init_values=init_values, **dd) if init_values is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0 else msnn.Identity()
+
+        self.norm2 = norm_layer(dim_out, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.mlp = Mlp(dim_out, int(dim_out * mlp_ratio), act_layer=act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls2 = LayerScale(dim_out, init_values=init_values, **dd) if init_values is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0 else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         # Attention + Q Pooling
         x_norm = self.norm1(x)
         if self.do_expand:
@@ -379,7 +384,7 @@                 x = self.proj(x_norm)
                 x = x.view(x.shape[0], self.attn.q_stride, -1, x.shape[-1]).amax(dim=1)  # max-pool
             else:
-                x = torch.cat([
+                x = mint.cat([
                     x.view(x.shape[0], self.attn.q_stride, -1, x.shape[-1]).amax(dim=1),  # max-pool
                     x.view(x.shape[0], self.attn.q_stride, -1, x.shape[-1]).mean(dim=1),  # avg-pool
                     ],
@@ -392,7 +397,7 @@         return x
 
 
-class PatchEmbed(nn.Module):
+class PatchEmbed(msnn.Cell):
     """Patch embed that supports any number of spatial dimensions (1d, 2d, 3d)."""
 
     def __init__(
@@ -418,16 +423,16 @@             stride=stride,
             padding=padding,
             **dd,
-        )
-
-    def forward(
-            self,
-            x: torch.Tensor,
-            mask: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(
+            self,
+            x: ms.Tensor,
+            mask: Optional[ms.Tensor] = None,
+    ) -> ms.Tensor:
         if mask is not None:
             mask = get_resized_mask(target_size=x.shape[2:], mask=mask)
-            x = self.proj(x * mask.to(torch.bool))
+            x = self.proj(x * mask.to(ms.bool_))
         else:
             x = self.proj(x)
         if self.reshape:
@@ -435,7 +440,7 @@         return x
 
 
-class Hiera(nn.Module):
+class Hiera(msnn.Cell):
 
     def __init__(
             self,
@@ -462,7 +467,7 @@             init_values: Optional[float] = None,
             fix_init: bool = True,
             weight_init: str = '',
-            norm_layer: Union[str, Type[nn.Module]] = "LayerNorm",
+            norm_layer: Union[str, Type[msnn.Cell]] = "LayerNorm",
             drop_rate: float = 0.0,
             patch_drop_rate: float = 0.0,
             head_init_scale: float = 0.001,
@@ -499,26 +504,26 @@             patch_stride,
             patch_padding,
             **dd,
-        )
-
-        self.pos_embed: Optional[nn.Parameter] = None
-        self.pos_embed_win: Optional[nn.Parameter] = None
-        self.pos_embed_spatial: Optional[nn.Parameter] = None
-        self.pos_embed_temporal: Optional[nn.Parameter] = None
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.pos_embed: Optional[ms.Parameter] = None
+        self.pos_embed_win: Optional[ms.Parameter] = None
+        self.pos_embed_spatial: Optional[ms.Parameter] = None
+        self.pos_embed_temporal: Optional[ms.Parameter] = None
         if sep_pos_embed:
-            self.pos_embed_spatial = nn.Parameter(
-                torch.zeros(1, self.tokens_spatial_shape[1] * self.tokens_spatial_shape[2], embed_dim, **dd)
-            )
-            self.pos_embed_temporal = nn.Parameter(
-                torch.zeros(1, self.tokens_spatial_shape[0], embed_dim, **dd)
-            )
+            self.pos_embed_spatial = ms.Parameter(
+                mint.zeros(1, self.tokens_spatial_shape[1] * self.tokens_spatial_shape[2], embed_dim, **dd)
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.pos_embed_temporal = ms.Parameter(
+                mint.zeros(1, self.tokens_spatial_shape[0], embed_dim, **dd)
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             if abs_win_pos_embed:
                 # absolute win, params NCHW to make tile & interpolate more natural before add & reshape
-                self.pos_embed = nn.Parameter(torch.zeros(1, embed_dim, *global_pos_size, **dd))
-                self.pos_embed_win = nn.Parameter(torch.zeros(1, embed_dim, *mask_unit_size, **dd))
+                self.pos_embed = ms.Parameter(mint.zeros(1, embed_dim, *global_pos_size, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+                self.pos_embed_win = ms.Parameter(mint.zeros(1, embed_dim, *mask_unit_size, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
-                self.pos_embed = nn.Parameter(torch.zeros(1, num_tokens, embed_dim, **dd))
+                self.pos_embed = ms.Parameter(mint.zeros(1, num_tokens, embed_dim, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # Setup roll and reroll modules
         self.unroll = Unroll(
@@ -540,7 +545,7 @@         cur_stage = 0
         depth = sum(stages)
         dpr = calculate_drop_path_rates(drop_path_rate, depth)  # stochastic depth decay rule
-        self.blocks = nn.ModuleList()
+        self.blocks = msnn.CellList()
         self.feature_info = []
         for i in range(depth):
             dim_out = embed_dim
@@ -569,7 +574,7 @@                 use_expand_proj=use_expand_proj,
                 use_mask_unit_attn=use_mask_unit_attn,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             embed_dim = dim_out
             if i in self.stage_ends:
                 self.feature_info += [
@@ -585,17 +590,17 @@             norm_layer=norm_layer,
             input_fmt='NLC',
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # Initialize everything
         if sep_pos_embed:
-            nn.init.trunc_normal_(self.pos_embed_spatial, std=0.02)
-            nn.init.trunc_normal_(self.pos_embed_temporal, std=0.02)
+            nn.init.trunc_normal_(self.pos_embed_spatial, std=0.02)  # 'torch.nn.init.trunc_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.trunc_normal_(self.pos_embed_temporal, std=0.02)  # 'torch.nn.init.trunc_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             if self.pos_embed is not None:
-                nn.init.trunc_normal_(self.pos_embed, std=0.02)
+                nn.init.trunc_normal_(self.pos_embed, std=0.02)  # 'torch.nn.init.trunc_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if self.pos_embed_win is not None:
-                nn.init.trunc_normal_(self.pos_embed_win, std=0.02)
+                nn.init.trunc_normal_(self.pos_embed_win, std=0.02)  # 'torch.nn.init.trunc_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         if weight_init != 'skip':
             init_fn = init_weight_jax if weight_init == 'jax' else init_weight_vit
@@ -615,7 +620,7 @@             rescale(layer.attn.proj.weight.data, layer_id + 1)
             rescale(layer.mlp.fc2.weight.data, layer_id + 1)
 
-    @torch.jit.ignore
+    @ms.jit
     def no_weight_decay(self):
         if self.pos_embed is not None:
             return ["pos_embed"]
@@ -624,18 +629,18 @@         else:
             return ["pos_embed_spatial", "pos_embed_temporal"]
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse: bool = False) -> Dict:
         return dict(
             stem=r'^pos_embed|pos_embed_spatial|pos_embed_temporal|pos_embed_abs|pos_embed_win|patch_embed',
             blocks=[(r'^blocks\.(\d+)', None), (r'^norm', (99999,))]
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True) -> None:
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
+    @ms.jit
     def get_classifier(self):
         return self.head.fc
 
@@ -643,7 +648,7 @@         self.num_classes = num_classes
         self.head.reset(num_classes, global_pool, reset_other=reset_other)
 
-    def get_random_mask(self, x: torch.Tensor, mask_ratio: float) -> torch.Tensor:
+    def get_random_mask(self, x: ms.Tensor, mask_ratio: float) -> ms.Tensor:
         """
         Generates a random mask, mask_ratio fraction are dropped.
         1 is *keep*, 0 is *remove*. Useful for MAE, FLIP, etc.
@@ -652,32 +657,28 @@         # Tokens selected for masking at mask unit level
         num_windows = math.prod(self.mask_spatial_shape)  # num_mask_units
         len_keep = int(num_windows * (1 - mask_ratio))
-        noise = torch.rand(B, num_windows, device=x.device)
+        noise = mint.rand(B, num_windows, device=x.device)
 
         # Sort noise for each sample
-        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove
-        ids_restore = torch.argsort(ids_shuffle, dim=1)
+        ids_shuffle = mint.argsort(noise, dim=1)  # ascend: small is keep, large is remove
+        ids_restore = mint.argsort(ids_shuffle, dim=1)
 
         # Generate the binary mask: 1 is *keep*, 0 is *remove*
         # Note this is opposite to original MAE
-        mask = torch.zeros([B, num_windows], device=x.device)
+        mask = mint.zeros([B, num_windows], device=x.device)
         mask[:, :len_keep] = 1
         # Unshuffle to get the binary mask
-        mask = torch.gather(mask, dim=1, index=ids_restore)
+        mask = ms.Tensor.gather(mask, dim=1, index=ids_restore)
 
         return mask.bool()
 
-    def _pos_embed(self, x) -> torch.Tensor:
+    def _pos_embed(self, x) -> ms.Tensor:
         if self.pos_embed_win is not None:
             # absolute win position embedding, from
             # Window Attention is Bugged: How not to Interpolate Position Embeddings (https://arxiv.org/abs/2311.05613)
             pos_embed_win = self.pos_embed_win.tile(self.mask_spatial_shape)
-            pos_embed = F.interpolate(
-                self.pos_embed,
-                size=pos_embed_win.shape[-2:],
-                mode='bicubic',
-                antialias=True,
-            )
+            pos_embed = nn.functional.interpolate(
+                self.pos_embed, size = pos_embed_win.shape[-2:], mode = 'bicubic')  # 'torch.nn.functional.interpolate':没有对应的mindspore参数 'antialias' (position 6);
             pos_embed = pos_embed + pos_embed_win
             pos_embed = pos_embed.flatten(2).transpose(1, 2)
         elif self.pos_embed is not None:
@@ -686,7 +687,7 @@             pos_embed = (
                 self.pos_embed_spatial.repeat(1, self.tokens_spatial_shape[0], 1)
                 +
-                torch.repeat_interleave(
+                mint.repeat_interleave(
                     self.pos_embed_temporal,
                     self.tokens_spatial_shape[1] * self.tokens_spatial_shape[2],
                     dim=1,
@@ -697,15 +698,15 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
-            mask: Optional[torch.Tensor] = None,
+            x: ms.Tensor,
+            mask: Optional[ms.Tensor] = None,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = True,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
             coarse: bool = True,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -728,7 +729,7 @@             take_indices, max_index = feature_take_indices(len(self.blocks), indices)
 
         if mask is not None:
-            patch_mask = mask.view(x.shape[0], 1, *self.mask_spatial_shape)  # B, C, *mask_spatial_shape
+            patch_mask = mask.view(x.shape[0], 1, *self.mask_spatial_shape)  # B, C, *mask_spatial_shape; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             patch_mask = None
         x = self.patch_embed(x, mask=patch_mask)
@@ -740,11 +741,13 @@             x = x[mask[..., None].tile(1, self.mu_size, x.shape[2])].view(x.shape[0], -1, x.shape[-1])
 
         intermediates = []
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             blocks = self.blocks
         else:
             blocks = self.blocks[:max_index + 1]
         for i, blk in enumerate(blocks):
+            # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if self.grad_checkpointing and not torch.jit.is_scripting():
                 x = checkpoint(blk, x)
             else:
@@ -779,10 +782,10 @@ 
     def forward_features(
             self,
-            x: torch.Tensor,
-            mask: Optional[torch.Tensor] = None,
+            x: ms.Tensor,
+            mask: Optional[ms.Tensor] = None,
             return_intermediates: bool = False,
-    ) -> torch.Tensor:
+    ) -> ms.Tensor:
         """
         mask should be a boolean tensor of shape [B, #MUt*#MUy*#MUx] where #MU are the number of mask units in that dim.
         Note: 1 in mask is *keep*, 0 is *remove*; mask.sum(dim=-1) should be the same across the batch.
@@ -793,7 +796,7 @@             mask = self.get_random_mask(x, mask_ratio=self.patch_drop_rate)
 
         if mask is not None:
-            patch_mask = mask.view(x.shape[0], 1, *self.mask_spatial_shape)  # B, C, *mask_spatial_shape
+            patch_mask = mask.view(x.shape[0], 1, *self.mask_spatial_shape)  # B, C, *mask_spatial_shape; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             patch_mask = None
         x = self.patch_embed(x, mask=patch_mask)
@@ -806,6 +809,7 @@ 
         intermediates = []
         for i, blk in enumerate(self.blocks):
+            # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if self.grad_checkpointing and not torch.jit.is_scripting():
                 x = checkpoint(blk, x)
             else:
@@ -822,15 +826,15 @@ 
         return x
 
-    def forward_head(self, x, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x, pre_logits: bool = False) -> ms.Tensor:
         x = self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
         return x
 
-    def forward(
-            self,
-            x: torch.Tensor,
-            mask: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
+    def construct(
+            self,
+            x: ms.Tensor,
+            mask: Optional[ms.Tensor] = None,
+    ) -> ms.Tensor:
         x = self.forward_features(x, mask=mask)
         if mask is None:
             x = self.forward_head(x)
@@ -970,43 +974,43 @@         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(out_indices=out_indices, feature_cls='getter'),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hiera_tiny_224(pretrained=False, **kwargs):
     model_args = dict(embed_dim=96, num_heads=1, stages=(1, 2, 7, 2))
-    return _create_hiera('hiera_tiny_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_hiera('hiera_tiny_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hiera_small_224(pretrained=False, **kwargs):
     model_args = dict(embed_dim=96, num_heads=1, stages=(1, 2, 11, 2))
-    return _create_hiera('hiera_small_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_hiera('hiera_small_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hiera_base_224(pretrained=False, **kwargs):
     model_args = dict(embed_dim=96, num_heads=1, stages=(2, 3, 16, 3))
-    return _create_hiera('hiera_base_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_hiera('hiera_base_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hiera_base_plus_224(pretrained=False, **kwargs):
     model_args = dict(embed_dim=112, num_heads=2, stages=(2, 3, 16, 3))
-    return _create_hiera('hiera_base_plus_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_hiera('hiera_base_plus_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hiera_large_224(pretrained=False, **kwargs):
     model_args = dict(embed_dim=144, num_heads=2, stages=(2, 6, 36, 4))
-    return _create_hiera('hiera_large_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_hiera('hiera_large_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hiera_huge_224(pretrained=False, **kwargs):
     model_args = dict(embed_dim=256, num_heads=4, stages=(2, 6, 36, 4))
-    return _create_hiera('hiera_huge_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_hiera('hiera_huge_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1015,11 +1019,11 @@         embed_dim=96, num_heads=1, stages=(1, 2, 11, 2), abs_win_pos_embed=True, global_pos_size=(16, 16),
         init_values=1e-5, weight_init='jax', use_expand_proj=False,
     )
-    return _create_hiera('hiera_small_abswin_256', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_hiera('hiera_small_abswin_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def hiera_base_abswin_256(pretrained=False, **kwargs):
     model_args = dict(
         embed_dim=96, num_heads=1, stages=(2, 3, 16, 3), abs_win_pos_embed=True, init_values=1e-5, weight_init='jax')
-    return _create_hiera('hiera_base_abswin_256', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_hiera('hiera_base_abswin_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
