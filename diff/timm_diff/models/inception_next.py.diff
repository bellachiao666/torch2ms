--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 InceptionNeXt paper: https://arxiv.org/abs/2303.16900
 Original implementation & weights from: https://github.com/sail-sg/inceptionnext
@@ -6,8 +11,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import trunc_normal_, DropPath, calculate_drop_path_rates, to_2tuple, get_padding, SelectAdaptivePool2d
@@ -19,7 +24,7 @@ __all__ = ['MetaNeXt']
 
 
-class InceptionDWConv2d(nn.Module):
+class InceptionDWConv2d(msnn.Cell):
     """ Inception depthwise convolution
     """
 
@@ -40,18 +45,18 @@         band_padding = get_padding(band_kernel_size, dilation=dilation)
         self.dwconv_hw = nn.Conv2d(
             gc, gc, square_kernel_size,
-            padding=square_padding, dilation=dilation, groups=gc, **dd)
+            padding=square_padding, dilation=dilation, groups=gc, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.dwconv_w = nn.Conv2d(
             gc, gc, (1, band_kernel_size),
-            padding=(0, band_padding), dilation=(1, dilation), groups=gc, **dd)
+            padding=(0, band_padding), dilation=(1, dilation), groups=gc, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.dwconv_h = nn.Conv2d(
             gc, gc, (band_kernel_size, 1),
-            padding=(band_padding, 0), dilation=(dilation, 1), groups=gc, **dd)
+            padding=(band_padding, 0), dilation=(dilation, 1), groups=gc, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.split_indexes = (in_chs - 3 * gc, gc, gc, gc)
 
-    def forward(self, x):
-        x_id, x_hw, x_w, x_h = torch.split(x, self.split_indexes, dim=1)
-        return torch.cat((
+    def construct(self, x):
+        x_id, x_hw, x_w, x_h = mint.split(x, self.split_indexes, dim=1)
+        return mint.cat((
             x_id,
             self.dwconv_hw(x_hw),
             self.dwconv_w(x_w),
@@ -60,7 +65,7 @@         )
 
 
-class ConvMlp(nn.Module):
+class ConvMlp(msnn.Cell):
     """ MLP using 1x1 convs that keeps spatial dims
     copied from timm: https://github.com/huggingface/pytorch-image-models/blob/v0.6.11/timm/models/layers/mlp.py
     """
@@ -70,8 +75,8 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             bias: bool = True,
             drop: float = 0.,
             device=None,
@@ -83,13 +88,13 @@         hidden_features = hidden_features or in_features
         bias = to_2tuple(bias)
 
-        self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=bias[0], **dd)
-        self.norm = norm_layer(hidden_features, **dd) if norm_layer else nn.Identity()
+        self.fc1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, bias=bias[0], **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm = norm_layer(hidden_features, **dd) if norm_layer else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act = act_layer()
         self.drop = nn.Dropout(drop)
-        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=bias[1], **dd)
-
-    def forward(self, x):
+        self.fc2 = nn.Conv2d(hidden_features, out_features, kernel_size=1, bias=bias[1], **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.fc1(x)
         x = self.norm(x)
         x = self.act(x)
@@ -98,7 +103,7 @@         return x
 
 
-class MlpClassifierHead(nn.Module):
+class MlpClassifierHead(msnn.Cell):
     """ MLP classification head
     """
 
@@ -108,8 +113,8 @@             num_classes: int = 1000,
             pool_type: str = 'avg',
             mlp_ratio: float = 3,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
             drop: float = 0.,
             bias: bool = True,
             device=None,
@@ -124,10 +129,10 @@         assert pool_type, 'Cannot disable pooling'
         self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True)
 
-        self.fc1 = nn.Linear(in_features * self.global_pool.feat_mult(), hidden_features, bias=bias, **dd)
+        self.fc1 = nn.Linear(in_features * self.global_pool.feat_mult(), hidden_features, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.act = act_layer()
-        self.norm = norm_layer(hidden_features, **dd)
-        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias, **dd)
+        self.norm = norm_layer(hidden_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.drop = nn.Dropout(drop)
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None):
@@ -135,9 +140,9 @@             assert pool_type, 'Cannot disable pooling'
             self.global_pool = SelectAdaptivePool2d(pool_type=pool_type, flatten=True)
 
-        self.fc2 = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
+        self.fc2 = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
+
+    def construct(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         x = self.fc1(x)
         x = self.act(x)
@@ -146,7 +151,7 @@         return x if pre_logits else self.fc2(x)
 
 
-class MetaNeXtBlock(nn.Module):
+class MetaNeXtBlock(msnn.Cell):
     """ MetaNeXtBlock Block
     Args:
         dim (int): Number of input channels.
@@ -158,11 +163,11 @@             self,
             dim: int,
             dilation: int = 1,
-            token_mixer: Type[nn.Module] = InceptionDWConv2d,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            mlp_layer: Type[nn.Module] = ConvMlp,
+            token_mixer: Type[msnn.Cell] = InceptionDWConv2d,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            mlp_layer: Type[msnn.Cell] = ConvMlp,
             mlp_ratio: float = 4,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             ls_init_value: float = 1e-6,
             drop_path: float = 0.,
             device=None,
@@ -170,13 +175,13 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.token_mixer = token_mixer(dim, dilation=dilation, **dd)
-        self.norm = norm_layer(dim, **dd)
-        self.mlp = mlp_layer(dim, int(mlp_ratio * dim), act_layer=act_layer, **dd)
-        self.gamma = nn.Parameter(ls_init_value * torch.ones(dim, **dd)) if ls_init_value else None
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.token_mixer = token_mixer(dim, dilation=dilation, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.mlp = mlp_layer(dim, int(mlp_ratio * dim), act_layer=act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.gamma = ms.Parameter(ls_init_value * mint.ones(dim, **dd)) if ls_init_value else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x
         x = self.token_mixer(x)
         x = self.norm(x)
@@ -187,7 +192,7 @@         return x
 
 
-class MetaNeXtStage(nn.Module):
+class MetaNeXtStage(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -197,9 +202,9 @@             dilation: Tuple[int, int] = (1, 1),
             drop_path_rates: Optional[List[float]] = None,
             ls_init_value: float = 1.0,
-            token_mixer: Type[nn.Module] = InceptionDWConv2d,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            token_mixer: Type[msnn.Cell] = InceptionDWConv2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             mlp_ratio: float = 4,
             device=None,
             dtype=None,
@@ -208,7 +213,7 @@         super().__init__()
         self.grad_checkpointing = False
         if stride > 1 or dilation[0] != dilation[1]:
-            self.downsample = nn.Sequential(
+            self.downsample = msnn.SequentialCell(
                 norm_layer(in_chs, **dd),
                 nn.Conv2d(
                     in_chs,
@@ -218,9 +223,9 @@                     dilation=dilation[0],
                     **dd,
                 ),
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;; 存在 *args/**kwargs，需手动确认参数映射;
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         drop_path_rates = drop_path_rates or [0.] * depth
         stage_blocks = []
@@ -235,10 +240,10 @@                 norm_layer=norm_layer,
                 mlp_ratio=mlp_ratio,
                 **dd,
-            ))
-        self.blocks = nn.Sequential(*stage_blocks)
-
-    def forward(self, x):
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.blocks = msnn.SequentialCell(*stage_blocks)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
@@ -247,7 +252,7 @@         return x
 
 
-class MetaNeXt(nn.Module):
+class MetaNeXt(msnn.Cell):
     r""" MetaNeXt
         A PyTorch impl of : `InceptionNeXt: When Inception Meets ConvNeXt` - https://arxiv.org/abs/2303.16900
 
@@ -273,9 +278,9 @@             output_stride: int = 32,
             depths: Tuple[int, ...] = (3, 3, 9, 3),
             dims: Tuple[int, ...] = (96, 192, 384, 768),
-            token_mixers: Union[Type[nn.Module], List[Type[nn.Module]]] = InceptionDWConv2d,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            act_layer: Type[nn.Module] = nn.GELU,
+            token_mixers: Union[Type[msnn.Cell], List[Type[msnn.Cell]]] = InceptionDWConv2d,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             mlp_ratios: Union[int, Tuple[int, ...]] = (4, 4, 4, 3),
             drop_rate: float = 0.,
             drop_path_rate: float = 0.,
@@ -295,17 +300,17 @@         self.drop_rate = drop_rate
         self.feature_info = []
 
-        self.stem = nn.Sequential(
+        self.stem = msnn.SequentialCell(
             nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4, **dd),
             norm_layer(dims[0], **dd)
-        )
+        )  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         dp_rates = calculate_drop_path_rates(drop_path_rate, depths, stagewise=True)
         prev_chs = dims[0]
         curr_stride = 4
         dilation = 1
         # feature resolution stages, each consisting of multiple residual blocks
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         for i in range(num_stage):
             stride = 2 if curr_stride == 2 or i > 0 else 1
             if curr_stride >= output_stride and stride > 1:
@@ -327,11 +332,11 @@                 norm_layer=norm_layer,
                 mlp_ratio=mlp_ratios[i],
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_chs = out_chs
             self.feature_info += [dict(num_chs=prev_chs, reduction=curr_stride, module=f'stages.{i}')]
         self.num_features = prev_chs
-        self.head = MlpClassifierHead(self.num_features, num_classes, pool_type=self.global_pool, drop=drop_rate, **dd)
+        self.head = MlpClassifierHead(self.num_features, num_classes, pool_type=self.global_pool, drop=drop_rate, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.head_hidden_size = self.head.num_features
         self.apply(self._init_weights)
 
@@ -339,9 +344,9 @@         if isinstance(m, (nn.Conv2d, nn.Linear)):
             trunc_normal_(m.weight, std=.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-    @torch.jit.ignore
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',
@@ -351,32 +356,32 @@             ]
         )
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc2
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
         self.num_classes = num_classes
         self.head.reset(num_classes, global_pool)
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         for s in self.stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
+    @ms.jit
     def no_weight_decay(self):
         return set()
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -432,7 +437,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -481,7 +486,7 @@         MetaNeXt, variant, pretrained,
         feature_cfg=dict(out_indices=(0, 1, 2, 3), flatten_sequential=True),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -491,7 +496,7 @@         depths=(2, 2, 6, 2), dims=(40, 80, 160, 320),
         token_mixers=partial(InceptionDWConv2d, band_kernel_size=9, branch_ratio=0.25)
     )
-    return _create_inception_next('inception_next_atto', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_inception_next('inception_next_atto', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -500,7 +505,7 @@         depths=(3, 3, 9, 3), dims=(96, 192, 384, 768),
         token_mixers=InceptionDWConv2d,
     )
-    return _create_inception_next('inception_next_tiny', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_inception_next('inception_next_tiny', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -509,7 +514,7 @@         depths=(3, 3, 27, 3), dims=(96, 192, 384, 768),
         token_mixers=InceptionDWConv2d,
     )
-    return _create_inception_next('inception_next_small', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_inception_next('inception_next_small', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -518,4 +523,4 @@         depths=(3, 3, 27, 3), dims=(128, 256, 512, 1024),
         token_mixers=InceptionDWConv2d,
     )
-    return _create_inception_next('inception_next_base', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_inception_next('inception_next_base', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
