--- pytorch+++ mindspore@@ -1,11 +1,16 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Pytorch Inception-Resnet-V2 implementation
 Sourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is
 based upon Google's Tensorflow implementation and pretrained weights (Apache 2.0 License)
 """
 from functools import partial
 from typing import Type, Optional
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
 from timm.layers import create_classifier, ConvNormAct
@@ -16,10 +21,10 @@ __all__ = ['InceptionResnetV2']
 
 
-class Mixed_5b(nn.Module):
-    def __init__(
-            self,
-            conv_block: Optional[Type[nn.Module]] = None,
+class Mixed_5b(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -29,36 +34,39 @@ 
         self.branch0 = conv_block(192, 96, kernel_size=1, stride=1, **dd)
 
-        self.branch1 = nn.Sequential(
+        self.branch1 = msnn.SequentialCell(
+            [
             conv_block(192, 48, kernel_size=1, stride=1, **dd),
             conv_block(48, 64, kernel_size=5, stride=1, padding=2, **dd)
-        )
-
-        self.branch2 = nn.Sequential(
+        ])
+
+        self.branch2 = msnn.SequentialCell(
+            [
             conv_block(192, 64, kernel_size=1, stride=1, **dd),
             conv_block(64, 96, kernel_size=3, stride=1, padding=1, **dd),
             conv_block(96, 96, kernel_size=3, stride=1, padding=1, **dd)
-        )
-
-        self.branch3 = nn.Sequential(
-            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
+        ])
+
+        self.branch3 = msnn.SequentialCell(
+            [
+            nn.AvgPool2d(3, stride = 1, padding = 1, count_include_pad = False),
             conv_block(192, 64, kernel_size=1, stride=1, **dd)
-        )
-
-    def forward(self, x):
+        ])
+
+    def construct(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         x3 = self.branch3(x)
-        out = torch.cat((x0, x1, x2, x3), 1)
-        return out
-
-
-class Block35(nn.Module):
+        out = mint.cat((x0, x1, x2, x3), 1)
+        return out
+
+
+class Block35(msnn.Cell):
     def __init__(
             self,
             scale: float = 1.0,
-            conv_block: Optional[Type[nn.Module]] = None,
+            conv_block: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -69,35 +77,37 @@ 
         self.branch0 = conv_block(320, 32, kernel_size=1, stride=1, **dd)
 
-        self.branch1 = nn.Sequential(
+        self.branch1 = msnn.SequentialCell(
+            [
             conv_block(320, 32, kernel_size=1, stride=1, **dd),
             conv_block(32, 32, kernel_size=3, stride=1, padding=1, **dd)
-        )
-
-        self.branch2 = nn.Sequential(
+        ])
+
+        self.branch2 = msnn.SequentialCell(
+            [
             conv_block(320, 32, kernel_size=1, stride=1, **dd),
             conv_block(32, 48, kernel_size=3, stride=1, padding=1, **dd),
             conv_block(48, 64, kernel_size=3, stride=1, padding=1, **dd)
-        )
+        ])
 
         self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1, **dd)
         self.act = nn.ReLU()
 
-    def forward(self, x):
+    def construct(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
-        out = torch.cat((x0, x1, x2), 1)
+        out = mint.cat((x0, x1, x2), 1)
         out = self.conv2d(out)
         out = out * self.scale + x
         out = self.act(out)
         return out
 
 
-class Mixed_6a(nn.Module):
-    def __init__(
-            self,
-            conv_block: Optional[Type[nn.Module]] = None,
+class Mixed_6a(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -107,27 +117,28 @@ 
         self.branch0 = conv_block(320, 384, kernel_size=3, stride=2, **dd)
 
-        self.branch1 = nn.Sequential(
+        self.branch1 = msnn.SequentialCell(
+            [
             conv_block(320, 256, kernel_size=1, stride=1, **dd),
             conv_block(256, 256, kernel_size=3, stride=1, padding=1, **dd),
             conv_block(256, 384, kernel_size=3, stride=2, **dd)
-        )
-
-        self.branch2 = nn.MaxPool2d(3, stride=2)
-
-    def forward(self, x):
+        ])
+
+        self.branch2 = nn.MaxPool2d(3, stride = 2)
+
+    def construct(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
-        out = torch.cat((x0, x1, x2), 1)
-        return out
-
-
-class Block17(nn.Module):
+        out = mint.cat((x0, x1, x2), 1)
+        return out
+
+
+class Block17(msnn.Cell):
     def __init__(
             self,
             scale: float = 1.0,
-            conv_block: Optional[Type[nn.Module]] = None,
+            conv_block: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -138,70 +149,74 @@ 
         self.branch0 = conv_block(1088, 192, kernel_size=1, stride=1, **dd)
 
-        self.branch1 = nn.Sequential(
+        self.branch1 = msnn.SequentialCell(
+            [
             conv_block(1088, 128, kernel_size=1, stride=1, **dd),
             conv_block(128, 160, kernel_size=(1, 7), stride=1, padding=(0, 3), **dd),
             conv_block(160, 192, kernel_size=(7, 1), stride=1, padding=(3, 0), **dd)
-        )
+        ])
 
         self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1, **dd)
         self.act = nn.ReLU()
 
-    def forward(self, x):
-        x0 = self.branch0(x)
-        x1 = self.branch1(x)
-        out = torch.cat((x0, x1), 1)
+    def construct(self, x):
+        x0 = self.branch0(x)
+        x1 = self.branch1(x)
+        out = mint.cat((x0, x1), 1)
         out = self.conv2d(out)
         out = out * self.scale + x
         out = self.act(out)
         return out
 
 
-class Mixed_7a(nn.Module):
-    def __init__(
-            self,
-            conv_block: Optional[Type[nn.Module]] = None,
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-        conv_block = conv_block or ConvNormAct
-
-        self.branch0 = nn.Sequential(
+class Mixed_7a(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Optional[Type[msnn.Cell]] = None,
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+        conv_block = conv_block or ConvNormAct
+
+        self.branch0 = msnn.SequentialCell(
+            [
             conv_block(1088, 256, kernel_size=1, stride=1, **dd),
             conv_block(256, 384, kernel_size=3, stride=2, **dd)
-        )
-
-        self.branch1 = nn.Sequential(
+        ])
+
+        self.branch1 = msnn.SequentialCell(
+            [
             conv_block(1088, 256, kernel_size=1, stride=1, **dd),
             conv_block(256, 288, kernel_size=3, stride=2, **dd)
-        )
-
-        self.branch2 = nn.Sequential(
+        ])
+
+        self.branch2 = msnn.SequentialCell(
+            [
             conv_block(1088, 256, kernel_size=1, stride=1, **dd),
             conv_block(256, 288, kernel_size=3, stride=1, padding=1, **dd),
             conv_block(288, 320, kernel_size=3, stride=2, **dd)
-        )
-
-        self.branch3 = nn.MaxPool2d(3, stride=2)
-
-    def forward(self, x):
+        ])
+
+        self.branch3 = nn.MaxPool2d(3, stride = 2)
+
+    def construct(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         x3 = self.branch3(x)
-        out = torch.cat((x0, x1, x2, x3), 1)
-        return out
-
-
-class Block8(nn.Module):
+        out = mint.cat((x0, x1, x2, x3), 1)
+        return out
+
+
+class Block8(msnn.Cell):
 
     def __init__(
             self,
             scale: float = 1.0,
             no_relu: bool = False,
-            conv_block: Optional[Type[nn.Module]] = None,
+            conv_block: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -212,19 +227,20 @@ 
         self.branch0 = conv_block(2080, 192, kernel_size=1, stride=1, **dd)
 
-        self.branch1 = nn.Sequential(
+        self.branch1 = msnn.SequentialCell(
+            [
             conv_block(2080, 192, kernel_size=1, stride=1, **dd),
             conv_block(192, 224, kernel_size=(1, 3), stride=1, padding=(0, 1), **dd),
             conv_block(224, 256, kernel_size=(3, 1), stride=1, padding=(1, 0), **dd)
-        )
+        ])
 
         self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1, **dd)
         self.relu = None if no_relu else nn.ReLU()
 
-    def forward(self, x):
-        x0 = self.branch0(x)
-        x1 = self.branch1(x)
-        out = torch.cat((x0, x1), 1)
+    def construct(self, x):
+        x0 = self.branch0(x)
+        x1 = self.branch1(x)
+        out = mint.cat((x0, x1), 1)
         out = self.conv2d(out)
         out = out * self.scale + x
         if self.relu is not None:
@@ -232,7 +248,7 @@         return out
 
 
-class InceptionResnetV2(nn.Module):
+class InceptionResnetV2(msnn.Cell):
     def __init__(
             self,
             num_classes: int = 1000,
@@ -265,22 +281,28 @@         self.conv2d_2b = conv_block(32, 64, kernel_size=3, stride=1, padding=1, **dd)
         self.feature_info = [dict(num_chs=64, reduction=2, module='conv2d_2b')]
 
-        self.maxpool_3a = nn.MaxPool2d(3, stride=2)
+        self.maxpool_3a = nn.MaxPool2d(3, stride = 2)
         self.conv2d_3b = conv_block(64, 80, kernel_size=1, stride=1, **dd)
         self.conv2d_4a = conv_block(80, 192, kernel_size=3, stride=1, **dd)
         self.feature_info += [dict(num_chs=192, reduction=4, module='conv2d_4a')]
 
-        self.maxpool_5a = nn.MaxPool2d(3, stride=2)
+        self.maxpool_5a = nn.MaxPool2d(3, stride = 2)
         self.mixed_5b = Mixed_5b(conv_block=conv_block, **dd)
-        self.repeat = nn.Sequential(*[Block35(scale=0.17, conv_block=conv_block, **dd) for _ in range(10)])
+        self.repeat = msnn.SequentialCell([
+            [Block35(scale=0.17, conv_block=conv_block, **dd) for _ in range(10)]
+        ])
         self.feature_info += [dict(num_chs=320, reduction=8, module='repeat')]
 
         self.mixed_6a = Mixed_6a(conv_block=conv_block, **dd)
-        self.repeat_1 = nn.Sequential(*[Block17(scale=0.10, conv_block=conv_block, **dd) for _ in range(20)])
+        self.repeat_1 = msnn.SequentialCell([
+            [Block17(scale=0.10, conv_block=conv_block, **dd) for _ in range(20)]
+        ])
         self.feature_info += [dict(num_chs=1088, reduction=16, module='repeat_1')]
 
         self.mixed_7a = Mixed_7a(conv_block=conv_block, **dd)
-        self.repeat_2 = nn.Sequential(*[Block8(scale=0.20, conv_block=conv_block, **dd) for _ in range(9)])
+        self.repeat_2 = msnn.SequentialCell([
+            [Block8(scale=0.20, conv_block=conv_block, **dd) for _ in range(9)]
+        ])
 
         self.block8 = Block8(no_relu=True, conv_block=conv_block, **dd)
         self.conv2d_7b = conv_block(2080, self.num_features, kernel_size=1, stride=1, **dd)
@@ -318,7 +340,7 @@         assert not enable, "checkpointing not supported"
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.classif
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -348,7 +370,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.classif(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
