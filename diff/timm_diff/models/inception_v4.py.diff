--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Pytorch Inception-V4 implementation
 Sourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is
 based upon Google's Tensorflow implementation and pretrained weights (Apache 2.0 License)
@@ -5,8 +10,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
 from timm.layers import create_classifier, ConvNormAct
@@ -17,256 +22,256 @@ __all__ = ['InceptionV4']
 
 
-class Mixed3a(nn.Module):
-    def __init__(
-            self,
-            conv_block: Type[nn.Module] = ConvNormAct,
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-        self.maxpool = nn.MaxPool2d(3, stride=2)
-        self.conv = conv_block(64, 96, kernel_size=3, stride=2, **dd)
-
-    def forward(self, x):
+class Mixed3a(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Type[msnn.Cell] = ConvNormAct,
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+        self.maxpool = nn.MaxPool2d(3, stride = 2)
+        self.conv = conv_block(64, 96, kernel_size=3, stride=2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x0 = self.maxpool(x)
         x1 = self.conv(x)
-        out = torch.cat((x0, x1), 1)
-        return out
-
-
-class Mixed4a(nn.Module):
-    def __init__(
-            self,
-            conv_block: Type[nn.Module] = ConvNormAct,
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-
-        self.branch0 = nn.Sequential(
+        out = mint.cat((x0, x1), 1)
+        return out
+
+
+class Mixed4a(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Type[msnn.Cell] = ConvNormAct,
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+
+        self.branch0 = msnn.SequentialCell(
             conv_block(160, 64, kernel_size=1, stride=1, **dd),
             conv_block(64, 96, kernel_size=3, stride=1, **dd)
-        )
-
-        self.branch1 = nn.Sequential(
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch1 = msnn.SequentialCell(
             conv_block(160, 64, kernel_size=1, stride=1, **dd),
             conv_block(64, 64, kernel_size=(1, 7), stride=1, padding=(0, 3), **dd),
             conv_block(64, 64, kernel_size=(7, 1), stride=1, padding=(3, 0), **dd),
             conv_block(64, 96, kernel_size=(3, 3), stride=1, **dd)
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
-        out = torch.cat((x0, x1), 1)
-        return out
-
-
-class Mixed5a(nn.Module):
-    def __init__(
-            self,
-            conv_block: Type[nn.Module] = ConvNormAct,
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-        self.conv = conv_block(192, 192, kernel_size=3, stride=2, **dd)
-        self.maxpool = nn.MaxPool2d(3, stride=2)
-
-    def forward(self, x):
+        out = mint.cat((x0, x1), 1)
+        return out
+
+
+class Mixed5a(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Type[msnn.Cell] = ConvNormAct,
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+        self.conv = conv_block(192, 192, kernel_size=3, stride=2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.maxpool = nn.MaxPool2d(3, stride = 2)
+
+    def construct(self, x):
         x0 = self.conv(x)
         x1 = self.maxpool(x)
-        out = torch.cat((x0, x1), 1)
-        return out
-
-
-class InceptionA(nn.Module):
-    def __init__(
-            self,
-            conv_block: Type[nn.Module] = ConvNormAct,
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-        self.branch0 = conv_block(384, 96, kernel_size=1, stride=1, **dd)
-
-        self.branch1 = nn.Sequential(
+        out = mint.cat((x0, x1), 1)
+        return out
+
+
+class InceptionA(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Type[msnn.Cell] = ConvNormAct,
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+        self.branch0 = conv_block(384, 96, kernel_size=1, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch1 = msnn.SequentialCell(
             conv_block(384, 64, kernel_size=1, stride=1, **dd),
             conv_block(64, 96, kernel_size=3, stride=1, padding=1, **dd)
-        )
-
-        self.branch2 = nn.Sequential(
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch2 = msnn.SequentialCell(
             conv_block(384, 64, kernel_size=1, stride=1, **dd),
             conv_block(64, 96, kernel_size=3, stride=1, padding=1, **dd),
             conv_block(96, 96, kernel_size=3, stride=1, padding=1, **dd)
-        )
-
-        self.branch3 = nn.Sequential(
-            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch3 = msnn.SequentialCell(
+            nn.AvgPool2d(3, stride = 1, padding = 1, count_include_pad = False),
             conv_block(384, 96, kernel_size=1, stride=1, **dd)
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         x3 = self.branch3(x)
-        out = torch.cat((x0, x1, x2, x3), 1)
-        return out
-
-
-class ReductionA(nn.Module):
-    def __init__(
-            self,
-            conv_block: Type[nn.Module] = ConvNormAct,
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-        self.branch0 = conv_block(384, 384, kernel_size=3, stride=2, **dd)
-
-        self.branch1 = nn.Sequential(
+        out = mint.cat((x0, x1, x2, x3), 1)
+        return out
+
+
+class ReductionA(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Type[msnn.Cell] = ConvNormAct,
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+        self.branch0 = conv_block(384, 384, kernel_size=3, stride=2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch1 = msnn.SequentialCell(
             conv_block(384, 192, kernel_size=1, stride=1, **dd),
             conv_block(192, 224, kernel_size=3, stride=1, padding=1, **dd),
             conv_block(224, 256, kernel_size=3, stride=2, **dd)
-        )
-
-        self.branch2 = nn.MaxPool2d(3, stride=2)
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch2 = nn.MaxPool2d(3, stride = 2)
+
+    def construct(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
-        out = torch.cat((x0, x1, x2), 1)
-        return out
-
-
-class InceptionB(nn.Module):
-    def __init__(
-            self,
-            conv_block: Type[nn.Module] = ConvNormAct,
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-        self.branch0 = conv_block(1024, 384, kernel_size=1, stride=1, **dd)
-
-        self.branch1 = nn.Sequential(
+        out = mint.cat((x0, x1, x2), 1)
+        return out
+
+
+class InceptionB(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Type[msnn.Cell] = ConvNormAct,
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+        self.branch0 = conv_block(1024, 384, kernel_size=1, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch1 = msnn.SequentialCell(
             conv_block(1024, 192, kernel_size=1, stride=1, **dd),
             conv_block(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3), **dd),
             conv_block(224, 256, kernel_size=(7, 1), stride=1, padding=(3, 0), **dd)
-        )
-
-        self.branch2 = nn.Sequential(
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch2 = msnn.SequentialCell(
             conv_block(1024, 192, kernel_size=1, stride=1, **dd),
             conv_block(192, 192, kernel_size=(7, 1), stride=1, padding=(3, 0), **dd),
             conv_block(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3), **dd),
             conv_block(224, 224, kernel_size=(7, 1), stride=1, padding=(3, 0), **dd),
             conv_block(224, 256, kernel_size=(1, 7), stride=1, padding=(0, 3), **dd)
-        )
-
-        self.branch3 = nn.Sequential(
-            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch3 = msnn.SequentialCell(
+            nn.AvgPool2d(3, stride = 1, padding = 1, count_include_pad = False),
             conv_block(1024, 128, kernel_size=1, stride=1, **dd)
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
         x3 = self.branch3(x)
-        out = torch.cat((x0, x1, x2, x3), 1)
-        return out
-
-
-class ReductionB(nn.Module):
-    def __init__(
-            self,
-            conv_block: Type[nn.Module] = ConvNormAct,
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-
-        self.branch0 = nn.Sequential(
+        out = mint.cat((x0, x1, x2, x3), 1)
+        return out
+
+
+class ReductionB(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Type[msnn.Cell] = ConvNormAct,
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+
+        self.branch0 = msnn.SequentialCell(
             conv_block(1024, 192, kernel_size=1, stride=1, **dd),
             conv_block(192, 192, kernel_size=3, stride=2, **dd)
-        )
-
-        self.branch1 = nn.Sequential(
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch1 = msnn.SequentialCell(
             conv_block(1024, 256, kernel_size=1, stride=1, **dd),
             conv_block(256, 256, kernel_size=(1, 7), stride=1, padding=(0, 3), **dd),
             conv_block(256, 320, kernel_size=(7, 1), stride=1, padding=(3, 0), **dd),
             conv_block(320, 320, kernel_size=3, stride=2, **dd)
-        )
-
-        self.branch2 = nn.MaxPool2d(3, stride=2)
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch2 = nn.MaxPool2d(3, stride = 2)
+
+    def construct(self, x):
         x0 = self.branch0(x)
         x1 = self.branch1(x)
         x2 = self.branch2(x)
-        out = torch.cat((x0, x1, x2), 1)
-        return out
-
-
-class InceptionC(nn.Module):
-    def __init__(
-            self,
-            conv_block: Type[nn.Module] = ConvNormAct,
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-
-        self.branch0 = conv_block(1536, 256, kernel_size=1, stride=1, **dd)
-
-        self.branch1_0 = conv_block(1536, 384, kernel_size=1, stride=1, **dd)
-        self.branch1_1a = conv_block(384, 256, kernel_size=(1, 3), stride=1, padding=(0, 1), **dd)
-        self.branch1_1b = conv_block(384, 256, kernel_size=(3, 1), stride=1, padding=(1, 0), **dd)
-
-        self.branch2_0 = conv_block(1536, 384, kernel_size=1, stride=1, **dd)
-        self.branch2_1 = conv_block(384, 448, kernel_size=(3, 1), stride=1, padding=(1, 0), **dd)
-        self.branch2_2 = conv_block(448, 512, kernel_size=(1, 3), stride=1, padding=(0, 1), **dd)
-        self.branch2_3a = conv_block(512, 256, kernel_size=(1, 3), stride=1, padding=(0, 1), **dd)
-        self.branch2_3b = conv_block(512, 256, kernel_size=(3, 1), stride=1, padding=(1, 0), **dd)
-
-        self.branch3 = nn.Sequential(
-            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),
+        out = mint.cat((x0, x1, x2), 1)
+        return out
+
+
+class InceptionC(msnn.Cell):
+    def __init__(
+            self,
+            conv_block: Type[msnn.Cell] = ConvNormAct,
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+
+        self.branch0 = conv_block(1536, 256, kernel_size=1, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch1_0 = conv_block(1536, 384, kernel_size=1, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.branch1_1a = conv_block(384, 256, kernel_size=(1, 3), stride=1, padding=(0, 1), **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.branch1_1b = conv_block(384, 256, kernel_size=(3, 1), stride=1, padding=(1, 0), **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch2_0 = conv_block(1536, 384, kernel_size=1, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.branch2_1 = conv_block(384, 448, kernel_size=(3, 1), stride=1, padding=(1, 0), **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.branch2_2 = conv_block(448, 512, kernel_size=(1, 3), stride=1, padding=(0, 1), **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.branch2_3a = conv_block(512, 256, kernel_size=(1, 3), stride=1, padding=(0, 1), **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.branch2_3b = conv_block(512, 256, kernel_size=(3, 1), stride=1, padding=(1, 0), **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.branch3 = msnn.SequentialCell(
+            nn.AvgPool2d(3, stride = 1, padding = 1, count_include_pad = False),
             conv_block(1536, 256, kernel_size=1, stride=1, **dd)
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x0 = self.branch0(x)
 
         x1_0 = self.branch1_0(x)
         x1_1a = self.branch1_1a(x1_0)
         x1_1b = self.branch1_1b(x1_0)
-        x1 = torch.cat((x1_1a, x1_1b), 1)
+        x1 = mint.cat((x1_1a, x1_1b), 1)
 
         x2_0 = self.branch2_0(x)
         x2_1 = self.branch2_1(x2_0)
         x2_2 = self.branch2_2(x2_1)
         x2_3a = self.branch2_3a(x2_2)
         x2_3b = self.branch2_3b(x2_2)
-        x2 = torch.cat((x2_3a, x2_3b), 1)
+        x2 = mint.cat((x2_3a, x2_3b), 1)
 
         x3 = self.branch3(x)
 
-        out = torch.cat((x0, x1, x2, x3), 1)
-        return out
-
-
-class InceptionV4(nn.Module):
+        out = mint.cat((x0, x1, x2, x3), 1)
+        return out
+
+
+class InceptionV4(msnn.Cell):
     def __init__(
             self,
             num_classes: int = 1000,
@@ -301,13 +306,13 @@             Mixed3a(conv_block, **dd),
             Mixed4a(conv_block, **dd),
             Mixed5a(conv_block, **dd),
-        ]
-        features += [InceptionA(conv_block, **dd) for _ in range(4)]
-        features += [ReductionA(conv_block, **dd)]  # Mixed6a
-        features += [InceptionB(conv_block, **dd) for _ in range(7)]
-        features += [ReductionB(conv_block, **dd)]  # Mixed7a
-        features += [InceptionC(conv_block, **dd) for _ in range(3)]
-        self.features = nn.Sequential(*features)
+        ]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        features += [InceptionA(conv_block, **dd) for _ in range(4)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        features += [ReductionA(conv_block, **dd)]  # Mixed6a; 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        features += [InceptionB(conv_block, **dd) for _ in range(7)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        features += [ReductionB(conv_block, **dd)]  # Mixed7a; 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        features += [InceptionC(conv_block, **dd) for _ in range(3)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.features = msnn.SequentialCell(*features)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info = [
             dict(num_chs=64, reduction=2, module='features.2'),
             dict(num_chs=160, reduction=4, module='features.3'),
@@ -321,21 +326,21 @@             pool_type=global_pool,
             drop_rate=drop_rate,
             **dd,
-        )
-
-    @torch.jit.ignore
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^features\.[012]\.',
             blocks=r'^features\.(\d+)'
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         assert not enable, 'gradient checkpointing not supported'
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.last_linear
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -345,13 +350,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -411,7 +416,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.last_linear(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -424,7 +429,7 @@         pretrained,
         feature_cfg=dict(flatten_sequential=True),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 default_cfgs = generate_default_cfgs({
@@ -441,4 +446,4 @@ 
 @register_model
 def inception_v4(pretrained=False, **kwargs):
-    return _create_inception_v4('inception_v4', pretrained, **kwargs)
+    return _create_inception_v4('inception_v4', pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
