--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ LeViT
 
 Paper: `LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference`
@@ -27,8 +32,8 @@ from functools import partial
 from typing import Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN
 from timm.layers import to_ntuple, to_2tuple, get_act_layer, DropPath, trunc_normal_, ndgrid
@@ -40,7 +45,7 @@ __all__ = ['Levit']
 
 
-class ConvNorm(nn.Module):
+class ConvNorm(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -59,7 +64,7 @@         self.linear = nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, dilation, groups, bias=False, **dd)
         self.bn = nn.BatchNorm2d(out_chs, **dd)
 
-        nn.init.constant_(self.bn.weight, bn_weight_init)
+        nn.init.constant_(self.bn.weight, bn_weight_init)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.no_grad()
     def fuse(self):
@@ -68,17 +73,16 @@         w = c.weight * w[:, None, None, None]
         b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5
         m = nn.Conv2d(
-            w.size(1), w.size(0), w.shape[2:], stride=self.linear.stride,
-            padding=self.linear.padding, dilation=self.linear.dilation, groups=self.linear.groups)
+            w.size(1), w.size(0), w.shape[2:], stride = self.linear.stride, padding = self.linear.padding, dilation = self.linear.dilation, groups = self.linear.groups)
         m.weight.data.copy_(w)
         m.bias.data.copy_(b)
         return m
 
-    def forward(self, x):
+    def construct(self, x):
         return self.bn(self.linear(x))
 
 
-class LinearNorm(nn.Module):
+class LinearNorm(msnn.Cell):
     def __init__(
             self,
             in_features: int,
@@ -92,7 +96,7 @@         self.linear = nn.Linear(in_features, out_features, bias=False, **dd)
         self.bn = nn.BatchNorm1d(out_features, **dd)
 
-        nn.init.constant_(self.bn.weight, bn_weight_init)
+        nn.init.constant_(self.bn.weight, bn_weight_init)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.no_grad()
     def fuse(self):
@@ -105,12 +109,12 @@         m.bias.data.copy_(b)
         return m
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.linear(x)
         return self.bn(x.flatten(0, 1)).reshape_as(x)
 
 
-class NormLinear(nn.Module):
+class NormLinear(msnn.Cell):
     def __init__(
             self,
             in_features: int,
@@ -129,7 +133,7 @@ 
         trunc_normal_(self.linear.weight, std=std)
         if self.linear.bias is not None:
-            nn.init.constant_(self.linear.bias, 0)
+            nn.init.constant_(self.linear.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.no_grad()
     def fuse(self):
@@ -146,16 +150,16 @@         m.bias.data.copy_(b)
         return m
 
-    def forward(self, x):
+    def construct(self, x):
         return self.linear(self.drop(self.bn(x)))
 
 
-class Stem8(nn.Sequential):
+class Stem8(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             device=None,
             dtype=None,
     ):
@@ -170,12 +174,12 @@         self.add_module('conv3', ConvNorm(out_chs // 2, out_chs, 3, stride=2, padding=1, **dd))
 
 
-class Stem16(nn.Sequential):
+class Stem16(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             device=None,
             dtype=None,
     ):
@@ -192,7 +196,7 @@         self.add_module('conv4', ConvNorm(out_chs // 2, out_chs, 3, stride=2, padding=1, **dd))
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
     def __init__(
             self,
             stride: int,
@@ -204,9 +208,9 @@         super().__init__()
         self.stride = stride
         self.resolution = to_2tuple(resolution)
-        self.pool = nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False) if use_pool else None
-
-    def forward(self, x):
+        self.pool = nn.AvgPool2d(3, stride = stride, padding = 1, count_include_pad = False) if use_pool else None
+
+    def construct(self, x):
         B, N, C = x.shape
         x = x.view(B, self.resolution[0], self.resolution[1], C)
         if self.pool is not None:
@@ -216,8 +220,8 @@         return x.reshape(B, -1, C)
 
 
-class Attention(nn.Module):
-    attention_bias_cache: Dict[str, torch.Tensor]
+class Attention(msnn.Cell):
+    attention_bias_cache: Dict[str, ms.Tensor]
 
     def __init__(
             self,
@@ -227,7 +231,7 @@             attn_ratio: float = 4.,
             resolution: Union[int, Tuple[int, int]] = 14,
             use_conv: bool = False,
-            act_layer: Type[nn.Module] = nn.SiLU,
+            act_layer: Type[msnn.Cell] = nn.SiLU,
             device=None,
             dtype=None,
     ):
@@ -245,16 +249,18 @@         self.val_attn_dim = int(attn_ratio * key_dim) * num_heads
 
         self.qkv = ln_layer(dim, self.val_attn_dim + self.key_attn_dim * 2, **dd)
-        self.proj = nn.Sequential(OrderedDict([
+        self.proj = msnn.SequentialCell([
+            OrderedDict([
             ('act', act_layer()),
             ('ln', ln_layer(self.val_attn_dim, dim, bn_weight_init=0, **dd))
-        ]))
-
-        self.attention_biases = nn.Parameter(torch.zeros(num_heads, resolution[0] * resolution[1], **dd))
-        pos = torch.stack(ndgrid(
-            torch.arange(resolution[0], device=device, dtype=torch.long),
-            torch.arange(resolution[1], device=device, dtype=torch.long),
-        )).flatten(1)
+        ])
+        ])
+
+        self.attention_biases = ms.Parameter(mint.zeros(num_heads, resolution[0] * resolution[1], **dd))
+        pos = mint.stack(ndgrid(
+            mint.arange(resolution[0], dtype = torch.long),
+            mint.arange(resolution[1], dtype = torch.long),
+        )).flatten(1)  # 'torch.arange':没有对应的mindspore参数 'device' (position 6);
         rel_pos = (pos[..., :, None] - pos[..., None, :]).abs()
         rel_pos = (rel_pos[0] * resolution[1]) + rel_pos[1]
         self.register_buffer('attention_bias_idxs', rel_pos, persistent=False)
@@ -266,7 +272,9 @@         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def get_attention_biases(self, device: torch.device) -> ms.Tensor:
         if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
@@ -275,7 +283,7 @@                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
-    def forward(self, x):  # x (B,C,H,W)
+    def construct(self, x):  # x (B,C,H,W)
         if self.use_conv:
             B, C, H, W = x.shape
             q, k, v = self.qkv(x).view(
@@ -301,8 +309,8 @@         return x
 
 
-class AttentionDownsample(nn.Module):
-    attention_bias_cache: Dict[str, torch.Tensor]
+class AttentionDownsample(msnn.Cell):
+    attention_bias_cache: Dict[str, ms.Tensor]
 
     def __init__(
             self,
@@ -315,7 +323,7 @@             resolution: Union[int, Tuple[int, int]] = 14,
             use_conv: bool = False,
             use_pool: bool = False,
-            act_layer: Type[nn.Module] = nn.SiLU,
+            act_layer: Type[msnn.Cell] = nn.SiLU,
             device=None,
             dtype=None,
     ):
@@ -343,24 +351,28 @@             sub_layer = partial(Downsample, resolution=resolution, use_pool=use_pool, **dd)
 
         self.kv = ln_layer(in_dim, self.val_attn_dim + self.key_attn_dim, **dd)
-        self.q = nn.Sequential(OrderedDict([
+        self.q = msnn.SequentialCell([
+            OrderedDict([
             ('down', sub_layer(stride=stride)),
             ('ln', ln_layer(in_dim, self.key_attn_dim, **dd))
-        ]))
-        self.proj = nn.Sequential(OrderedDict([
+        ])
+        ])
+        self.proj = msnn.SequentialCell([
+            OrderedDict([
             ('act', act_layer()),
             ('ln', ln_layer(self.val_attn_dim, out_dim, **dd))
-        ]))
-
-        self.attention_biases = nn.Parameter(torch.zeros(num_heads, resolution[0] * resolution[1], **dd))
-        k_pos = torch.stack(ndgrid(
-            torch.arange(resolution[0], device=device, dtype=torch.long),
-            torch.arange(resolution[1], device=device, dtype=torch.long),
-        )).flatten(1)
-        q_pos = torch.stack(ndgrid(
-            torch.arange(0, resolution[0], step=stride, device=device, dtype=torch.long),
-            torch.arange(0, resolution[1], step=stride, device=device, dtype=torch.long),
-        )).flatten(1)
+        ])
+        ])
+
+        self.attention_biases = ms.Parameter(mint.zeros(num_heads, resolution[0] * resolution[1], **dd))
+        k_pos = mint.stack(ndgrid(
+            mint.arange(resolution[0], dtype = torch.long),
+            mint.arange(resolution[1], dtype = torch.long),
+        )).flatten(1)  # 'torch.arange':没有对应的mindspore参数 'device' (position 6);
+        q_pos = mint.stack(ndgrid(
+            mint.arange(0, resolution[0], step = stride, dtype = torch.long),
+            mint.arange(0, resolution[1], step = stride, dtype = torch.long),
+        )).flatten(1)  # 'torch.arange':没有对应的mindspore参数 'device' (position 6);
         rel_pos = (q_pos[..., :, None] - k_pos[..., None, :]).abs()
         rel_pos = (rel_pos[0] * resolution[1]) + rel_pos[1]
         self.register_buffer('attention_bias_idxs', rel_pos, persistent=False)
@@ -373,7 +385,9 @@         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def get_attention_biases(self, device: torch.device) -> ms.Tensor:
         if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
@@ -382,7 +396,7 @@                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
-    def forward(self, x):
+    def construct(self, x):
         if self.use_conv:
             B, C, H, W = x.shape
             HH, WW = (H - 1) // self.stride + 1, (W - 1) // self.stride + 1
@@ -408,7 +422,7 @@         return x
 
 
-class LevitMlp(nn.Module):
+class LevitMlp(msnn.Cell):
     """ MLP for Levit w/ normalization + ability to switch btw conv and linear
     """
     def __init__(
@@ -417,7 +431,7 @@             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
             use_conv: bool = False,
-            act_layer: Type[nn.Module] = nn.SiLU,
+            act_layer: Type[msnn.Cell] = nn.SiLU,
             drop: float = 0.,
             device=None,
             dtype=None,
@@ -433,7 +447,7 @@         self.drop = nn.Dropout(drop)
         self.ln2 = ln_layer(hidden_features, out_features, bn_weight_init=0, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.ln1(x)
         x = self.act(x)
         x = self.drop(x)
@@ -441,7 +455,7 @@         return x
 
 
-class LevitDownsample(nn.Module):
+class LevitDownsample(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
@@ -450,8 +464,8 @@             num_heads: int = 8,
             attn_ratio: float = 4.,
             mlp_ratio: float = 2.,
-            act_layer: Type[nn.Module] = nn.SiLU,
-            attn_act_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.SiLU,
+            attn_act_layer: Optional[Type[msnn.Cell]] = None,
             resolution: Union[int, Tuple[int, int]] = 14,
             use_conv: bool = False,
             use_pool: bool = False,
@@ -483,15 +497,15 @@             act_layer=act_layer,
             **dd,
         )
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         x = self.attn_downsample(x)
         x = x + self.drop_path(self.mlp(x))
         return x
 
 
-class LevitBlock(nn.Module):
+class LevitBlock(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -501,8 +515,8 @@             mlp_ratio: float = 2.,
             resolution: Union[int, Tuple[int, int]] = 14,
             use_conv: bool = False,
-            act_layer: Type[nn.Module] = nn.SiLU,
-            attn_act_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.SiLU,
+            attn_act_layer: Optional[Type[msnn.Cell]] = None,
             drop_path: float = 0.,
             device=None,
             dtype=None,
@@ -521,7 +535,7 @@             act_layer=attn_act_layer,
             **dd,
             )
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.mlp = LevitMlp(
             dim,
@@ -530,15 +544,15 @@             act_layer=act_layer,
             **dd,
         )
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         x = x + self.drop_path1(self.attn(x))
         x = x + self.drop_path2(self.mlp(x))
         return x
 
 
-class LevitStage(nn.Module):
+class LevitStage(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
@@ -548,8 +562,8 @@             num_heads: int = 8,
             attn_ratio: float = 4.0,
             mlp_ratio: float = 4.0,
-            act_layer: Type[nn.Module] = nn.SiLU,
-            attn_act_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.SiLU,
+            attn_act_layer: Optional[Type[msnn.Cell]] = None,
             resolution: Union[int, Tuple[int, int]] = 14,
             downsample: str = '',
             use_conv: bool = False,
@@ -579,7 +593,7 @@             resolution = [(r - 1) // 2 + 1 for r in resolution]
         else:
             assert in_dim == out_dim
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         blocks = []
         for _ in range(depth):
@@ -596,15 +610,17 @@                 drop_path=drop_path,
                 **dd,
             )]
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x):
         x = self.downsample(x)
         x = self.blocks(x)
         return x
 
 
-class Levit(nn.Module):
+class Levit(msnn.Cell):
     """ Vision Transformer with support for patch or hybrid CNN input stage
 
     NOTE: distillation is defaulted to True since pretrained weights use it, will cause problems
@@ -622,7 +638,7 @@             num_heads: Union[int, Tuple[int, ...]] = (3,),
             attn_ratio: Union[float, Tuple[float, ...]] = 2.,
             mlp_ratio: Union[float, Tuple[float, ...]] = 2.,
-            stem_backbone: Optional[nn.Module] = None,
+            stem_backbone: Optional[msnn.Cell] = None,
             stem_stride: Optional[int] = None,
             stem_type: str = 's16',
             down_op: str = 'subsample',
@@ -691,10 +707,12 @@             resolution = tuple([(r - 1) // stage_stride + 1 for r in resolution])
             self.feature_info += [dict(num_chs=embed_dim[i], reduction=stride, module=f'stages.{i}')]
             in_dim = embed_dim[i]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell([
+            stages
+        ])
 
         # Classifier head
-        self.head = NormLinear(embed_dim[-1], num_classes, drop=drop_rate, **dd) if num_classes > 0 else nn.Identity()
+        self.head = NormLinear(embed_dim[-1], num_classes, drop=drop_rate, **dd) if num_classes > 0 else msnn.Identity()
 
     @torch.jit.ignore
     def no_weight_decay(self):
@@ -713,7 +731,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head
 
     def reset_classifier(self, num_classes: int , global_pool: Optional[str] = None):
@@ -721,17 +739,17 @@         if global_pool is not None:
             self.global_pool = global_pool
         self.head = NormLinear(
-            self.num_features, num_classes, drop=self.drop_rate) if num_classes > 0 else nn.Identity()
+            self.num_features, num_classes, drop=self.drop_rate) if num_classes > 0 else msnn.Identity()
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -805,7 +823,7 @@             x = x.mean(dim=(-2, -1)) if self.use_conv else x.mean(dim=1)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -815,11 +833,11 @@     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         dd = {'device': kwargs.get('device', None), 'dtype': kwargs.get('dtype', None)}
-        self.head_dist = NormLinear(self.num_features, self.num_classes, **dd) if self.num_classes > 0 else nn.Identity()
+        self.head_dist = NormLinear(self.num_features, self.num_classes, **dd) if self.num_classes > 0 else msnn.Identity()
         self.distilled_training = False  # must set this True to train w/ distillation token
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head, self.head_dist
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -827,8 +845,8 @@         if global_pool is not None:
             self.global_pool = global_pool
         self.head = NormLinear(
-            self.num_features, num_classes, drop=self.drop_rate) if num_classes > 0 else nn.Identity()
-        self.head_dist = NormLinear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
+            self.num_features, num_classes, drop=self.drop_rate) if num_classes > 0 else msnn.Identity()
+        self.head_dist = NormLinear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
 
     @torch.jit.ignore
     def set_distilled_training(self, enable=True):
