--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 MambaOut models for image classification.
 Some implementations are modified from:
@@ -8,8 +13,8 @@ from collections import OrderedDict
 from typing import List, Optional, Tuple, Type, Union
 
-import torch
-from torch import nn
+# import torch
+# from torch import nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import trunc_normal_, DropPath, calculate_drop_path_rates, LayerNorm, LayerScale, ClNormMlpClassifierHead, get_act_layer
@@ -19,7 +24,7 @@ from ._registry import register_model, generate_default_cfgs
 
 
-class Stem(nn.Module):
+class Stem(msnn.Cell):
     r""" Code modified from InternImage:
         https://github.com/OpenGVLab/InternImage
     """
@@ -29,8 +34,8 @@             in_chs: int = 3,
             out_chs: int = 96,
             mid_norm: bool = True,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
             device=None,
             dtype=None,
     ):
@@ -43,8 +48,8 @@             stride=2,
             padding=1,
             **dd,
-        )
-        self.norm1 = norm_layer(out_chs // 2, **dd) if mid_norm else None
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm1 = norm_layer(out_chs // 2, **dd) if mid_norm else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act = act_layer()
         self.conv2 = nn.Conv2d(
             out_chs // 2,
@@ -53,10 +58,10 @@             stride=2,
             padding=1,
             **dd,
-        )
-        self.norm2 = norm_layer(out_chs, **dd)
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm2 = norm_layer(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.conv1(x)
         if self.norm1 is not None:
             x = x.permute(0, 2, 3, 1)
@@ -69,19 +74,19 @@         return x
 
 
-class DownsampleNormFirst(nn.Module):
+class DownsampleNormFirst(msnn.Cell):
 
     def __init__(
             self,
             in_chs: int = 96,
             out_chs: int = 198,
-            norm_layer: Type[nn.Module] = LayerNorm,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.norm = norm_layer(in_chs, **dd)
+        self.norm = norm_layer(in_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv = nn.Conv2d(
             in_chs,
             out_chs,
@@ -89,9 +94,9 @@             stride=2,
             padding=1,
             **dd,
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.norm(x)
         x = x.permute(0, 3, 1, 2)
         x = self.conv(x)
@@ -99,13 +104,13 @@         return x
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
 
     def __init__(
             self,
             in_chs: int = 96,
             out_chs: int = 198,
-            norm_layer: Type[nn.Module] = LayerNorm,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
             device=None,
             dtype=None,
     ):
@@ -118,10 +123,10 @@             stride=2,
             padding=1,
             **dd,
-        )
-        self.norm = norm_layer(out_chs, **dd)
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm = norm_layer(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = x.permute(0, 3, 1, 2)
         x = self.conv(x)
         x = x.permute(0, 2, 3, 1)
@@ -129,7 +134,7 @@         return x
 
 
-class MlpHead(nn.Module):
+class MlpHead(msnn.Cell):
     """ MLP classification head
     """
 
@@ -138,9 +143,9 @@             in_features: int,
             num_classes: int = 1000,
             pool_type: str = 'avg',
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             mlp_ratio: Optional[int] = 4,
-            norm_layer: Type[nn.Module] = LayerNorm,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
             drop_rate: float = 0.,
             bias: bool = True,
             device=None,
@@ -156,31 +161,31 @@         self.in_features = in_features
         self.hidden_size = hidden_size or in_features
 
-        self.norm = norm_layer(in_features, **dd)
+        self.norm = norm_layer(in_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if hidden_size:
-            self.pre_logits = nn.Sequential(OrderedDict([
+            self.pre_logits = msnn.SequentialCell(OrderedDict([
                 ('fc', nn.Linear(in_features, hidden_size, **dd)),
                 ('act', act_layer()),
                 ('norm', norm_layer(hidden_size, **dd))
-            ]))
+            ]))  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.num_features = hidden_size
         else:
             self.num_features = in_features
-            self.pre_logits = nn.Identity()
-
-        self.fc = nn.Linear(self.num_features, num_classes, bias=bias, **dd) if num_classes > 0 else nn.Identity()
+            self.pre_logits = msnn.Identity()
+
+        self.fc = nn.Linear(self.num_features, num_classes, bias=bias, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，需手动确认参数映射;
         self.head_dropout = nn.Dropout(drop_rate)
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None, reset_other: bool = False):
         if pool_type is not None:
             self.pool_type = pool_type
         if reset_other:
-            self.norm = nn.Identity()
-            self.pre_logits = nn.Identity()
+            self.norm = msnn.Identity()
+            self.pre_logits = msnn.Identity()
             self.num_features = self.in_features
-        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
+        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
+
+    def construct(self, x, pre_logits: bool = False):
         if self.pool_type == 'avg':
             x = x.mean((1, 2))
         x = self.norm(x)
@@ -192,7 +197,7 @@         return x
 
 
-class GatedConvBlock(nn.Module):
+class GatedConvBlock(msnn.Cell):
     r""" Our implementation of Gated CNN Block: https://arxiv.org/pdf/1612.08083
     Args:
         conv_ratio: control the number of channels to conduct depthwise convolution.
@@ -208,8 +213,8 @@             kernel_size: int = 7,
             conv_ratio: float = 1.0,
             ls_init_value: Optional[float] = None,
-            norm_layer: Type[nn.Module] = LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop_path: float = 0.,
             device=None,
             dtype=None,
@@ -217,9 +222,9 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.norm = norm_layer(dim, **dd)
+        self.norm = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         hidden = int(expansion_ratio * dim)
-        self.fc1 = nn.Linear(dim, hidden * 2, **dd)
+        self.fc1 = nn.Linear(dim, hidden * 2, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.act = act_layer()
         conv_channels = int(conv_ratio * dim)
         self.split_indices = (hidden, hidden - conv_channels, conv_channels)
@@ -230,26 +235,26 @@             padding=kernel_size // 2,
             groups=conv_channels,
             **dd,
-        )
-        self.fc2 = nn.Linear(hidden, dim, **dd)
-        self.ls = LayerScale(dim, **dd) if ls_init_value is not None else nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.fc2 = nn.Linear(hidden, dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.ls = LayerScale(dim, **dd) if ls_init_value is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x  # [B, H, W, C]
         x = self.norm(x)
         x = self.fc1(x)
-        g, i, c = torch.split(x, self.split_indices, dim=-1)
+        g, i, c = mint.split(x, self.split_indices, dim=-1)
         c = c.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]
         c = self.conv(c)
         c = c.permute(0, 2, 3, 1)  # [B, C, H, W] -> [B, H, W, C]
-        x = self.fc2(self.act(g) * torch.cat((i, c), dim=-1))
+        x = self.fc2(self.act(g) * mint.cat((i, c), dim=-1))
         x = self.ls(x)
         x = self.drop_path(x)
         return x + shortcut
 
 
-class MambaOutStage(nn.Module):
+class MambaOutStage(msnn.Cell):
 
     def __init__(
             self,
@@ -261,8 +266,8 @@             conv_ratio: float = 1.0,
             downsample: str = '',
             ls_init_value: Optional[float] = None,
-            norm_layer: Type[nn.Module] = LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop_path: float = 0.,
             device=None,
             dtype=None,
@@ -273,14 +278,14 @@         self.grad_checkpointing = False
 
         if downsample == 'conv':
-            self.downsample = Downsample(dim, dim_out, norm_layer=norm_layer, **dd)
+            self.downsample = Downsample(dim, dim_out, norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         elif downsample == 'conv_nf':
-            self.downsample = DownsampleNormFirst(dim, dim_out, norm_layer=norm_layer, **dd)
+            self.downsample = DownsampleNormFirst(dim, dim_out, norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             assert dim == dim_out
-            self.downsample = nn.Identity()
-
-        self.blocks = nn.Sequential(*[
+            self.downsample = msnn.Identity()
+
+        self.blocks = msnn.SequentialCell(*[
             GatedConvBlock(
                 dim=dim_out,
                 expansion_ratio=expansion_ratio,
@@ -293,9 +298,9 @@                 **dd,
             )
             for j in range(depth)
-        ])
-
-    def forward(self, x):
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
@@ -304,7 +309,7 @@         return x
 
 
-class MambaOut(nn.Module):
+class MambaOut(msnn.Cell):
     r""" MetaFormer
         A PyTorch impl of : `MetaFormer Baselines for Vision`  -
           https://arxiv.org/abs/2210.13452
@@ -328,8 +333,8 @@             global_pool: str = 'avg',
             depths: Tuple[int, ...] = (3, 3, 9, 3),
             dims: Tuple[int, ...] = (96, 192, 384, 576),
-            norm_layer: Type[nn.Module] = LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             conv_ratio: float = 1.0,
             expansion_ratio: float = 8/3,
             kernel_size: int = 7,
@@ -364,12 +369,12 @@             act_layer=act_layer,
             norm_layer=norm_layer,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         prev_dim = dims[0]
         dp_rates = calculate_drop_path_rates(drop_path_rate, depths, stagewise=True)
         cur = 0
         curr_stride = 4
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         for i in range(num_stage):
             dim = dims[i]
             stride = 2 if curr_stride == 2 or i > 0 else 1
@@ -387,7 +392,7 @@                 act_layer=act_layer,
                 drop_path=dp_rates[i],
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.stages.append(stage)
             prev_dim = dim
             # NOTE feature_info use currently assumes stage 0 == stride 1, rest are stride 2
@@ -403,7 +408,7 @@                 drop_rate=drop_rate,
                 norm_layer=norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             # more typical norm -> pool -> fc -> act -> fc
             self.head = ClNormMlpClassifierHead(
@@ -414,7 +419,7 @@                 norm_layer=norm_layer,
                 drop_rate=drop_rate,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.num_features = prev_dim
         self.head_hidden_size = self.head.num_features
 
@@ -424,9 +429,9 @@         if isinstance(m, (nn.Conv2d, nn.Linear)):
             trunc_normal_(m.weight, std=.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-    @torch.jit.ignore
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',
@@ -436,13 +441,13 @@             ]
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         for s in self.stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -451,13 +456,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -519,7 +524,7 @@         x = self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
         return x
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -612,7 +617,7 @@         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(out_indices=(0, 1, 2, 3), flatten_sequential=True),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -620,30 +625,30 @@ @register_model
 def mambaout_femto(pretrained=False, **kwargs):
     model_args = dict(depths=(3, 3, 9, 3), dims=(48, 96, 192, 288))
-    return _create_mambaout('mambaout_femto', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('mambaout_femto', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 # Kobe Memorial Version with 24 Gated CNN blocks
 @register_model
 def mambaout_kobe(pretrained=False, **kwargs):
     model_args = dict(depths=[3, 3, 15, 3], dims=[48, 96, 192, 288])
-    return _create_mambaout('mambaout_kobe', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('mambaout_kobe', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 @register_model
 def mambaout_tiny(pretrained=False, **kwargs):
     model_args = dict(depths=[3, 3, 9, 3], dims=[96, 192, 384, 576])
-    return _create_mambaout('mambaout_tiny', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('mambaout_tiny', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mambaout_small(pretrained=False, **kwargs):
     model_args = dict(depths=[3, 4, 27, 3], dims=[96, 192, 384, 576])
-    return _create_mambaout('mambaout_small', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('mambaout_small', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def mambaout_base(pretrained=False, **kwargs):
     model_args = dict(depths=[3, 4, 27, 3], dims=[128, 256, 512, 768])
-    return _create_mambaout('mambaout_base', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('mambaout_base', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -656,7 +661,7 @@         ls_init_value=1e-6,
         head_fn='norm_mlp',
     )
-    return _create_mambaout('mambaout_small_rw', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('mambaout_small_rw', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -671,7 +676,7 @@         ls_init_value=1e-6,
         head_fn='norm_mlp',
     )
-    return _create_mambaout('mambaout_base_short_rw', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('mambaout_base_short_rw', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -686,7 +691,7 @@         ls_init_value=1e-6,
         head_fn='norm_mlp',
     )
-    return _create_mambaout('mambaout_base_tall_rw', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('mambaout_base_tall_rw', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -702,7 +707,7 @@         act_layer='silu',
         head_fn='norm_mlp',
     )
-    return _create_mambaout('mambaout_base_wide_rw', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('mambaout_base_wide_rw', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -718,7 +723,7 @@         act_layer='silu',
         head_fn='norm_mlp',
     )
-    return _create_mambaout('mambaout_base_plus_rw', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('mambaout_base_plus_rw', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -733,4 +738,4 @@         act_layer='silu',
         head_fn='norm_mlp',
     )
-    return _create_mambaout('test_mambaout', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_mambaout('test_mambaout', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
