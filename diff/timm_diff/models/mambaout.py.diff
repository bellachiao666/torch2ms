--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 MambaOut models for image classification.
 Some implementations are modified from:
@@ -8,8 +13,8 @@ from collections import OrderedDict
 from typing import List, Optional, Tuple, Type, Union
 
-import torch
-from torch import nn
+# import torch
+# from torch import nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import trunc_normal_, DropPath, calculate_drop_path_rates, LayerNorm, LayerScale, ClNormMlpClassifierHead, get_act_layer
@@ -19,7 +24,7 @@ from ._registry import register_model, generate_default_cfgs
 
 
-class Stem(nn.Module):
+class Stem(msnn.Cell):
     r""" Code modified from InternImage:
         https://github.com/OpenGVLab/InternImage
     """
@@ -29,8 +34,8 @@             in_chs: int = 3,
             out_chs: int = 96,
             mid_norm: bool = True,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
             device=None,
             dtype=None,
     ):
@@ -56,7 +61,7 @@         )
         self.norm2 = norm_layer(out_chs, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         if self.norm1 is not None:
             x = x.permute(0, 2, 3, 1)
@@ -69,13 +74,13 @@         return x
 
 
-class DownsampleNormFirst(nn.Module):
+class DownsampleNormFirst(msnn.Cell):
 
     def __init__(
             self,
             in_chs: int = 96,
             out_chs: int = 198,
-            norm_layer: Type[nn.Module] = LayerNorm,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
             device=None,
             dtype=None,
     ):
@@ -91,7 +96,7 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.norm(x)
         x = x.permute(0, 3, 1, 2)
         x = self.conv(x)
@@ -99,13 +104,13 @@         return x
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
 
     def __init__(
             self,
             in_chs: int = 96,
             out_chs: int = 198,
-            norm_layer: Type[nn.Module] = LayerNorm,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
             device=None,
             dtype=None,
     ):
@@ -121,7 +126,7 @@         )
         self.norm = norm_layer(out_chs, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = x.permute(0, 3, 1, 2)
         x = self.conv(x)
         x = x.permute(0, 2, 3, 1)
@@ -129,7 +134,7 @@         return x
 
 
-class MlpHead(nn.Module):
+class MlpHead(msnn.Cell):
     """ MLP classification head
     """
 
@@ -138,9 +143,9 @@             in_features: int,
             num_classes: int = 1000,
             pool_type: str = 'avg',
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             mlp_ratio: Optional[int] = 4,
-            norm_layer: Type[nn.Module] = LayerNorm,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
             drop_rate: float = 0.,
             bias: bool = True,
             device=None,
@@ -158,29 +163,31 @@ 
         self.norm = norm_layer(in_features, **dd)
         if hidden_size:
-            self.pre_logits = nn.Sequential(OrderedDict([
+            self.pre_logits = msnn.SequentialCell([
+                OrderedDict([
                 ('fc', nn.Linear(in_features, hidden_size, **dd)),
                 ('act', act_layer()),
                 ('norm', norm_layer(hidden_size, **dd))
-            ]))
+            ])
+            ])
             self.num_features = hidden_size
         else:
             self.num_features = in_features
-            self.pre_logits = nn.Identity()
-
-        self.fc = nn.Linear(self.num_features, num_classes, bias=bias, **dd) if num_classes > 0 else nn.Identity()
+            self.pre_logits = msnn.Identity()
+
+        self.fc = nn.Linear(self.num_features, num_classes, bias=bias, **dd) if num_classes > 0 else msnn.Identity()
         self.head_dropout = nn.Dropout(drop_rate)
 
     def reset(self, num_classes: int, pool_type: Optional[str] = None, reset_other: bool = False):
         if pool_type is not None:
             self.pool_type = pool_type
         if reset_other:
-            self.norm = nn.Identity()
-            self.pre_logits = nn.Identity()
+            self.norm = msnn.Identity()
+            self.pre_logits = msnn.Identity()
             self.num_features = self.in_features
-        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()
-
-    def forward(self, x, pre_logits: bool = False):
+        self.fc = nn.Linear(self.num_features, num_classes) if num_classes > 0 else msnn.Identity()
+
+    def construct(self, x, pre_logits: bool = False):
         if self.pool_type == 'avg':
             x = x.mean((1, 2))
         x = self.norm(x)
@@ -192,7 +199,7 @@         return x
 
 
-class GatedConvBlock(nn.Module):
+class GatedConvBlock(msnn.Cell):
     r""" Our implementation of Gated CNN Block: https://arxiv.org/pdf/1612.08083
     Args:
         conv_ratio: control the number of channels to conduct depthwise convolution.
@@ -208,8 +215,8 @@             kernel_size: int = 7,
             conv_ratio: float = 1.0,
             ls_init_value: Optional[float] = None,
-            norm_layer: Type[nn.Module] = LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop_path: float = 0.,
             device=None,
             dtype=None,
@@ -232,24 +239,24 @@             **dd,
         )
         self.fc2 = nn.Linear(hidden, dim, **dd)
-        self.ls = LayerScale(dim, **dd) if ls_init_value is not None else nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.ls = LayerScale(dim, **dd) if ls_init_value is not None else msnn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x  # [B, H, W, C]
         x = self.norm(x)
         x = self.fc1(x)
-        g, i, c = torch.split(x, self.split_indices, dim=-1)
+        g, i, c = mint.split(x, self.split_indices, dim = -1)
         c = c.permute(0, 3, 1, 2)  # [B, H, W, C] -> [B, C, H, W]
         c = self.conv(c)
         c = c.permute(0, 2, 3, 1)  # [B, C, H, W] -> [B, H, W, C]
-        x = self.fc2(self.act(g) * torch.cat((i, c), dim=-1))
+        x = self.fc2(self.act(g) * mint.cat((i, c), dim = -1))
         x = self.ls(x)
         x = self.drop_path(x)
         return x + shortcut
 
 
-class MambaOutStage(nn.Module):
+class MambaOutStage(msnn.Cell):
 
     def __init__(
             self,
@@ -261,8 +268,8 @@             conv_ratio: float = 1.0,
             downsample: str = '',
             ls_init_value: Optional[float] = None,
-            norm_layer: Type[nn.Module] = LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop_path: float = 0.,
             device=None,
             dtype=None,
@@ -278,9 +285,10 @@             self.downsample = DownsampleNormFirst(dim, dim_out, norm_layer=norm_layer, **dd)
         else:
             assert dim == dim_out
-            self.downsample = nn.Identity()
-
-        self.blocks = nn.Sequential(*[
+            self.downsample = msnn.Identity()
+
+        self.blocks = msnn.SequentialCell([
+            [
             GatedConvBlock(
                 dim=dim_out,
                 expansion_ratio=expansion_ratio,
@@ -293,9 +301,10 @@                 **dd,
             )
             for j in range(depth)
+        ]
         ])
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
@@ -304,7 +313,7 @@         return x
 
 
-class MambaOut(nn.Module):
+class MambaOut(msnn.Cell):
     r""" MetaFormer
         A PyTorch impl of : `MetaFormer Baselines for Vision`  -
           https://arxiv.org/abs/2210.13452
@@ -328,8 +337,8 @@             global_pool: str = 'avg',
             depths: Tuple[int, ...] = (3, 3, 9, 3),
             dims: Tuple[int, ...] = (96, 192, 384, 576),
-            norm_layer: Type[nn.Module] = LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             conv_ratio: float = 1.0,
             expansion_ratio: float = 8/3,
             kernel_size: int = 7,
@@ -369,7 +378,7 @@         dp_rates = calculate_drop_path_rates(drop_path_rate, depths, stagewise=True)
         cur = 0
         curr_stride = 4
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         for i in range(num_stage):
             dim = dims[i]
             stride = 2 if curr_stride == 2 or i > 0 else 1
@@ -424,7 +433,7 @@         if isinstance(m, (nn.Conv2d, nn.Linear)):
             trunc_normal_(m.weight, std=.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -442,7 +451,7 @@             s.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -451,13 +460,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -519,7 +528,7 @@         x = self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
         return x
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
