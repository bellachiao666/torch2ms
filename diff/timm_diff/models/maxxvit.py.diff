--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ MaxVit and CoAtNet Vision Transformer - CNN Hybrids in PyTorch
 
 This is a from-scratch implementation of both CoAtNet and MaxVit in PyTorch.
@@ -40,9 +45,9 @@ from functools import partial
 from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
 
-import torch
-from torch import nn
-from torch.jit import Final
+# import torch
+# from torch import nn
+# from torch.jit import Final
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -166,9 +171,9 @@     weight_init: str = 'vit_eff'
 
 
-class Attention2d(nn.Module):
+class Attention2d(msnn.Cell):
     """Multi-head attention for 2D NCHW tensors."""
-    fused_attn: Final[bool]
+    fused_attn: Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -206,13 +211,13 @@         self.scale = dim_head ** -0.5
         self.fused_attn = use_fused_attn()
 
-        self.qkv = nn.Conv2d(dim, dim_attn * 3, 1, bias=bias, **dd)
-        self.rel_pos = rel_pos_cls(num_heads=self.num_heads, **dd) if rel_pos_cls else None
+        self.qkv = nn.Conv2d(dim, dim_attn * 3, 1, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.rel_pos = rel_pos_cls(num_heads=self.num_heads, **dd) if rel_pos_cls else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.attn_drop = nn.Dropout(attn_drop)
-        self.proj = nn.Conv2d(dim_attn, dim_out, 1, bias=bias, **dd)
+        self.proj = nn.Conv2d(dim_attn, dim_out, 1, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x: torch.Tensor, shared_rel_pos: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, shared_rel_pos: Optional[ms.Tensor] = None) -> ms.Tensor:
         B, C, H, W = x.shape
 
         if self.head_first:
@@ -227,13 +232,7 @@             elif shared_rel_pos is not None:
                 attn_bias = shared_rel_pos
 
-            x = torch.nn.functional.scaled_dot_product_attention(
-                q.transpose(-1, -2).contiguous(),
-                k.transpose(-1, -2).contiguous(),
-                v.transpose(-1, -2).contiguous(),
-                attn_mask=attn_bias,
-                dropout_p=self.attn_drop.p if self.training else 0.,
-            ).transpose(-1, -2).reshape(B, -1, H, W)
+            x = mint.transpose(-1, -2).reshape(B, -1, H, W)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q.transpose(-2, -1) @ k
@@ -250,9 +249,9 @@         return x
 
 
-class AttentionCl(nn.Module):
+class AttentionCl(msnn.Cell):
     """Channels-last multi-head attention (B, ..., C)."""
-    fused_attn: Final[bool]
+    fused_attn: Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -291,13 +290,13 @@         self.scale = dim_head ** -0.5
         self.fused_attn = use_fused_attn()
 
-        self.qkv = nn.Linear(dim, dim_attn * 3, bias=bias, **dd)
-        self.rel_pos = rel_pos_cls(num_heads=self.num_heads, **dd) if rel_pos_cls else None
+        self.qkv = nn.Linear(dim, dim_attn * 3, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.rel_pos = rel_pos_cls(num_heads=self.num_heads, **dd) if rel_pos_cls else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.attn_drop = nn.Dropout(attn_drop)
-        self.proj = nn.Linear(dim_attn, dim_out, bias=bias, **dd)
+        self.proj = nn.Linear(dim_attn, dim_out, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x: torch.Tensor, shared_rel_pos: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, shared_rel_pos: Optional[ms.Tensor] = None) -> ms.Tensor:
         B = x.shape[0]
         restore_shape = x.shape[:-1]
 
@@ -317,7 +316,7 @@                 q, k, v,
                 attn_mask=attn_bias,
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
@@ -335,7 +334,7 @@         return x
 
 
-class Downsample2d(nn.Module):
+class Downsample2d(msnn.Cell):
     """A downsample pooling module supporting several maxpool and avgpool modes.
 
     * 'max' - MaxPool2d w/ kernel_size 3, stride 2, padding 1
@@ -375,42 +374,42 @@             self.pool = create_pool2d('avg', 2, padding=padding or 0)
 
         if dim != dim_out:
-            self.expand = nn.Conv2d(dim, dim_out, 1, bias=bias, device=device, dtype=dtype)
-        else:
-            self.expand = nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.expand = nn.Conv2d(dim, dim_out, 1, bias = bias, dtype = dtype)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device' (position 9);
+        else:
+            self.expand = msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.pool(x)  # spatial downsample
         x = self.expand(x)  # expand chs
         return x
 
 
-def _init_transformer(module: nn.Module, name: str, scheme: str = '') -> None:
+def _init_transformer(module: msnn.Cell, name: str, scheme: str = '') -> None:
     """Initialize transformer module weights."""
     if isinstance(module, (nn.Conv2d, nn.Linear)):
         if scheme == 'normal':
-            nn.init.normal_(module.weight, std=.02)
+            nn.init.normal_(module.weight, std=.02)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif scheme == 'trunc_normal':
             trunc_normal_tf_(module.weight, std=.02)
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif scheme == 'xavier_normal':
-            nn.init.xavier_normal_(module.weight)
+            nn.init.xavier_normal_(module.weight)  # 'torch.nn.init.xavier_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             # vit like
-            nn.init.xavier_uniform_(module.weight)
+            nn.init.xavier_uniform_(module.weight)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
                 if 'mlp' in name:
-                    nn.init.normal_(module.bias, std=1e-6)
+                    nn.init.normal_(module.bias, std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 else:
-                    nn.init.zeros_(module.bias)
-
-
-class TransformerBlock2d(nn.Module):
+                    nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+
+class TransformerBlock2d(msnn.Cell):
     """Transformer block with 2D downsampling.
 
     '2D' NCHW tensor layout
@@ -447,15 +446,15 @@         act_layer = get_act_layer(cfg.act_layer)
 
         if stride == 2:
-            self.shortcut = Downsample2d(dim, dim_out, pool_type=cfg.pool_type, bias=cfg.shortcut_bias, **dd)
-            self.norm1 = nn.Sequential(OrderedDict([
+            self.shortcut = Downsample2d(dim, dim_out, pool_type=cfg.pool_type, bias=cfg.shortcut_bias, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.norm1 = msnn.SequentialCell(OrderedDict([
                 ('norm', norm_layer(dim, **dd)),
                 ('down', Downsample2d(dim, dim, pool_type=cfg.pool_type, **dd)),
-            ]))
+            ]))  # 存在 *args/**kwargs，需手动确认参数映射;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             assert dim == dim_out
-            self.shortcut = nn.Identity()
-            self.norm1 = norm_layer(dim, **dd)
+            self.shortcut = msnn.Identity()
+            self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
 
         self.attn = Attention2d(
             dim,
@@ -467,52 +466,52 @@             attn_drop=cfg.attn_drop,
             proj_drop=cfg.proj_drop,
             **dd,
-        )
-        self.ls1 = LayerScale2d(dim_out, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        self.norm2 = norm_layer(dim_out, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls1 = LayerScale2d(dim_out, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+        self.norm2 = norm_layer(dim_out, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.mlp = ConvMlp(
             in_features=dim_out,
             hidden_features=int(dim_out * cfg.expand_ratio),
             act_layer=act_layer,
             drop=cfg.proj_drop,
             **dd,
-        )
-        self.ls2 = LayerScale2d(dim_out, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls2 = LayerScale2d(dim_out, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
     def init_weights(self, scheme: str = '') -> None:
         named_apply(partial(_init_transformer, scheme=scheme), self)
 
-    def forward(self, x: torch.Tensor, shared_rel_pos: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, shared_rel_pos: Optional[ms.Tensor] = None) -> ms.Tensor:
         x = self.shortcut(x) + self.drop_path1(self.ls1(self.attn(self.norm1(x), shared_rel_pos=shared_rel_pos)))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-def _init_conv(module: nn.Module, name: str, scheme: str = '') -> None:
+def _init_conv(module: msnn.Cell, name: str, scheme: str = '') -> None:
     """Initialize convolution module weights."""
     if isinstance(module, nn.Conv2d):
         if scheme == 'normal':
-            nn.init.normal_(module.weight, std=.02)
+            nn.init.normal_(module.weight, std=.02)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif scheme == 'trunc_normal':
             trunc_normal_tf_(module.weight, std=.02)
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif scheme == 'xavier_normal':
-            nn.init.xavier_normal_(module.weight)
+            nn.init.xavier_normal_(module.weight)  # 'torch.nn.init.xavier_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             # efficientnet like
             fan_out = module.kernel_size[0] * module.kernel_size[1] * module.out_channels
             fan_out //= module.groups
-            nn.init.normal_(module.weight, 0, math.sqrt(2.0 / fan_out))
+            nn.init.normal_(module.weight, 0, math.sqrt(2.0 / fan_out))  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.zeros_(module.bias)
+                nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 
 def num_groups(group_size: Optional[int], channels: int) -> int:
@@ -525,7 +524,7 @@         return channels // group_size
 
 
-class MbConvBlock(nn.Module):
+class MbConvBlock(msnn.Cell):
     """Pre-Norm Conv Block - 1x1 - kxk - 1x1, w/ inverted bottleneck (expand)."""
 
     def __init__(
@@ -556,9 +555,9 @@ 
         if stride == 2:
             self.shortcut = Downsample2d(
-                in_chs, out_chs, pool_type=cfg.pool_type, bias=cfg.output_bias, padding=cfg.padding, **dd)
-        else:
-            self.shortcut = nn.Identity()
+                in_chs, out_chs, pool_type=cfg.pool_type, bias=cfg.output_bias, padding=cfg.padding, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.shortcut = msnn.Identity()
 
         assert cfg.stride_mode in ('pool', '1x1', 'dw')
         stride_pool, stride_1, stride_2 = 1, 1, 1
@@ -572,13 +571,13 @@         else:
             stride_2, dilation_2 = stride, dilation[0]
 
-        self.pre_norm = norm_act_layer(in_chs, apply_act=cfg.pre_norm_act, **dd)
+        self.pre_norm = norm_act_layer(in_chs, apply_act=cfg.pre_norm_act, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if stride_pool > 1:
-            self.down = Downsample2d(in_chs, in_chs, pool_type=cfg.downsample_pool_type, padding=cfg.padding, **dd)
-        else:
-            self.down = nn.Identity()
-        self.conv1_1x1 = create_conv2d(in_chs, mid_chs, 1, stride=stride_1, **dd)
-        self.norm1 = norm_act_layer(mid_chs, **dd)
+            self.down = Downsample2d(in_chs, in_chs, pool_type=cfg.downsample_pool_type, padding=cfg.padding, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.down = msnn.Identity()
+        self.conv1_1x1 = create_conv2d(in_chs, mid_chs, 1, stride=stride_1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm1 = norm_act_layer(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.conv2_kxk = create_conv2d(
             mid_chs,
@@ -589,7 +588,7 @@             groups=groups,
             padding=cfg.padding,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         attn_kwargs = {}
         if isinstance(cfg.attn_layer, str):
@@ -599,21 +598,21 @@ 
         # two different orderings for SE and norm2 (due to some weights and trials using SE before norm2)
         if cfg.attn_early:
-            self.se_early = create_attn(cfg.attn_layer, mid_chs, **attn_kwargs, **dd)
-            self.norm2 = norm_act_layer(mid_chs, **dd)
+            self.se_early = create_attn(cfg.attn_layer, mid_chs, **attn_kwargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.norm2 = norm_act_layer(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.se = None
         else:
             self.se_early = None
-            self.norm2 = norm_act_layer(mid_chs, **dd)
-            self.se = create_attn(cfg.attn_layer, mid_chs, **attn_kwargs, **dd)
-
-        self.conv3_1x1 = create_conv2d(mid_chs, out_chs, 1, bias=cfg.output_bias, **dd)
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+            self.norm2 = norm_act_layer(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.se = create_attn(cfg.attn_layer, mid_chs, **attn_kwargs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.conv3_1x1 = create_conv2d(mid_chs, out_chs, 1, bias=cfg.output_bias, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
     def init_weights(self, scheme: str = '') -> None:
         named_apply(partial(_init_conv, scheme=scheme), self)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = self.shortcut(x)
         x = self.pre_norm(x)
         x = self.down(x)
@@ -636,7 +635,7 @@         return x
 
 
-class ConvNeXtBlock(nn.Module):
+class ConvNeXtBlock(msnn.Cell):
     """ConvNeXt Block."""
 
     def __init__(
@@ -677,11 +676,11 @@         self.use_conv_mlp = conv_mlp
 
         if stride == 2:
-            self.shortcut = Downsample2d(in_chs, out_chs, **dd)
+            self.shortcut = Downsample2d(in_chs, out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         elif in_chs != out_chs:
-            self.shortcut = nn.Conv2d(in_chs, out_chs, kernel_size=1, bias=cfg.output_bias, **dd)
-        else:
-            self.shortcut = nn.Identity()
+            self.shortcut = nn.Conv2d(in_chs, out_chs, kernel_size=1, bias=cfg.output_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        else:
+            self.shortcut = msnn.Identity()
 
         assert cfg.stride_mode in ('pool', 'dw')
         stride_pool, stride_dw = 1, 1
@@ -692,9 +691,9 @@             stride_dw = stride
 
         if stride_pool == 2:
-            self.down = Downsample2d(in_chs, in_chs, pool_type=cfg.downsample_pool_type, **dd)
-        else:
-            self.down = nn.Identity()
+            self.down = Downsample2d(in_chs, in_chs, pool_type=cfg.downsample_pool_type, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.down = msnn.Identity()
 
         self.conv_dw = create_conv2d(
             in_chs,
@@ -705,22 +704,22 @@             depthwise=True,
             bias=cfg.output_bias,
             **dd,
-        )
-        self.norm = norm_layer(out_chs, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm = norm_layer(out_chs, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.mlp = mlp_layer(
             out_chs,
             int(cfg.expand_ratio * out_chs),
             bias=cfg.output_bias,
             act_layer=act_layer,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if conv_mlp:
-            self.ls = LayerScale2d(out_chs, cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        else:
-            self.ls = LayerScale(out_chs, cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.ls = LayerScale2d(out_chs, cfg.init_values, **dd) if cfg.init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.ls = LayerScale(out_chs, cfg.init_values, **dd) if cfg.init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = self.shortcut(x)
         x = self.down(x)
         x = self.conv_dw(x)
@@ -739,7 +738,7 @@         return x
 
 
-def window_partition(x: torch.Tensor, window_size: List[int]) -> torch.Tensor:
+def window_partition(x: ms.Tensor, window_size: List[int]) -> ms.Tensor:
     """Partition into non-overlapping windows."""
     B, H, W, C = x.shape
     _assert(H % window_size[0] == 0, f'height ({H}) must be divisible by window ({window_size[0]})')
@@ -750,7 +749,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def window_reverse(windows: torch.Tensor, window_size: List[int], img_size: List[int]) -> torch.Tensor:
+def window_reverse(windows: ms.Tensor, window_size: List[int], img_size: List[int]) -> ms.Tensor:
     """Reverse window partition."""
     H, W = img_size
     C = windows.shape[-1]
@@ -759,7 +758,7 @@     return x
 
 
-def grid_partition(x: torch.Tensor, grid_size: List[int]) -> torch.Tensor:
+def grid_partition(x: ms.Tensor, grid_size: List[int]) -> ms.Tensor:
     """Partition into overlapping windows with grid striding."""
     B, H, W, C = x.shape
     _assert(H % grid_size[0] == 0, f'height {H} must be divisible by grid {grid_size[0]}')
@@ -770,7 +769,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def grid_reverse(windows: torch.Tensor, grid_size: List[int], img_size: List[int]) -> torch.Tensor:
+def grid_reverse(windows: ms.Tensor, grid_size: List[int], img_size: List[int]) -> ms.Tensor:
     """Reverse grid partition."""
     H, W = img_size
     C = windows.shape[-1]
@@ -791,7 +790,7 @@     return rel_pos_cls
 
 
-class PartitionAttentionCl(nn.Module):
+class PartitionAttentionCl(msnn.Cell):
     """Grid or Block partition + Attn + FFN.
 
     NxC 'channels last' tensor layout.
@@ -815,7 +814,7 @@         self.partition_size = to_2tuple(cfg.window_size if self.partition_block else cfg.grid_size)
         rel_pos_cls = get_rel_pos_cls(cfg, self.partition_size)
 
-        self.norm1 = norm_layer(dim, **dd)
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn = AttentionCl(
             dim,
             dim,
@@ -826,20 +825,20 @@             attn_drop=cfg.attn_drop,
             proj_drop=cfg.proj_drop,
             **dd,
-        )
-        self.ls1 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        self.norm2 = norm_layer(dim, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls1 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.mlp = Mlp(
             in_features=dim,
             hidden_features=int(dim * cfg.expand_ratio),
             act_layer=act_layer,
             drop=cfg.proj_drop,
             **dd,
-        )
-        self.ls2 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls2 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
     def _partition_attn(self, x):
         img_size = x.shape[1:3]
@@ -856,13 +855,13 @@             x = grid_reverse(partitioned, self.partition_size, img_size)
         return x
 
-    def forward(self, x):
+    def construct(self, x):
         x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-class ParallelPartitionAttention(nn.Module):
+class ParallelPartitionAttention(msnn.Cell):
     """Experimental. Grid and Block partition + single FFN.
 
     NxC tensor layout.
@@ -892,7 +891,7 @@         self.partition_size = to_2tuple(cfg.window_size)
         rel_pos_cls = get_rel_pos_cls(cfg, self.partition_size)
 
-        self.norm1 = norm_layer(dim, **dd)
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn_block = AttentionCl(
             dim,
             dim // 2,
@@ -903,7 +902,7 @@             attn_drop=cfg.attn_drop,
             proj_drop=cfg.proj_drop,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.attn_grid = AttentionCl(
             dim,
             dim // 2,
@@ -914,11 +913,11 @@             attn_drop=cfg.attn_drop,
             proj_drop=cfg.proj_drop,
             **dd,
-        )
-        self.ls1 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        self.norm2 = norm_layer(dim, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls1 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.mlp = Mlp(
             in_features=dim,
             hidden_features=int(dim * cfg.expand_ratio),
@@ -926,11 +925,11 @@             act_layer=act_layer,
             drop=cfg.proj_drop,
             **dd,
-        )
-        self.ls2 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def _partition_attn(self, x: torch.Tensor) -> torch.Tensor:
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls2 = LayerScale(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def _partition_attn(self, x: ms.Tensor) -> ms.Tensor:
         img_size = x.shape[1:3]
 
         partitioned_block = window_partition(x, self.partition_size)
@@ -941,15 +940,15 @@         partitioned_grid = self.attn_grid(partitioned_grid)
         x_grid = grid_reverse(partitioned_grid, self.partition_size, img_size)
 
-        return torch.cat([x_window, x_grid], dim=-1)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        return mint.cat([x_window, x_grid], dim=-1)
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-def window_partition_nchw(x: torch.Tensor, window_size: List[int]) -> torch.Tensor:
+def window_partition_nchw(x: ms.Tensor, window_size: List[int]) -> ms.Tensor:
     """Partition windows for NCHW tensors."""
     B, C, H, W = x.shape
     _assert(H % window_size[0] == 0, f'height ({H}) must be divisible by window ({window_size[0]})')
@@ -960,7 +959,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def window_reverse_nchw(windows: torch.Tensor, window_size: List[int], img_size: List[int]) -> torch.Tensor:
+def window_reverse_nchw(windows: ms.Tensor, window_size: List[int], img_size: List[int]) -> ms.Tensor:
     """Reverse window partition for NCHW tensors."""
     H, W = img_size
     C = windows.shape[1]
@@ -969,7 +968,7 @@     return x
 
 
-def grid_partition_nchw(x: torch.Tensor, grid_size: List[int]) -> torch.Tensor:
+def grid_partition_nchw(x: ms.Tensor, grid_size: List[int]) -> ms.Tensor:
     """Grid partition for NCHW tensors."""
     B, C, H, W = x.shape
     _assert(H % grid_size[0] == 0, f'height {H} must be divisible by grid {grid_size[0]}')
@@ -980,7 +979,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def grid_reverse_nchw(windows: torch.Tensor, grid_size: List[int], img_size: List[int]) -> torch.Tensor:
+def grid_reverse_nchw(windows: ms.Tensor, grid_size: List[int], img_size: List[int]) -> ms.Tensor:
     """Reverse grid partition for NCHW tensors."""
     H, W = img_size
     C = windows.shape[1]
@@ -989,7 +988,7 @@     return x
 
 
-class PartitionAttention2d(nn.Module):
+class PartitionAttention2d(msnn.Cell):
     """Grid or Block partition + Attn + FFN.
 
     '2D' NCHW tensor layout.
@@ -1020,7 +1019,7 @@         self.partition_size = to_2tuple(cfg.window_size if self.partition_block else cfg.grid_size)
         rel_pos_cls = get_rel_pos_cls(cfg, self.partition_size)
 
-        self.norm1 = norm_layer(dim, **dd)
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn = Attention2d(
             dim,
             dim,
@@ -1031,22 +1030,22 @@             attn_drop=cfg.attn_drop,
             proj_drop=cfg.proj_drop,
             **dd,
-        )
-        self.ls1 = LayerScale2d(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        self.norm2 = norm_layer(dim, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls1 = LayerScale2d(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.mlp = ConvMlp(
             in_features=dim,
             hidden_features=int(dim * cfg.expand_ratio),
             act_layer=act_layer,
             drop=cfg.proj_drop,
             **dd,
-        )
-        self.ls2 = LayerScale2d(dim, init_values=cfg.init_values, **dd) if cfg.init_values else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def _partition_attn(self, x: torch.Tensor) -> torch.Tensor:
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls2 = LayerScale2d(dim, init_values=cfg.init_values, **dd) if cfg.init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def _partition_attn(self, x: ms.Tensor) -> ms.Tensor:
         img_size = x.shape[-2:]
         if self.partition_block:
             partitioned = window_partition_nchw(x, self.partition_size)
@@ -1061,13 +1060,13 @@             x = grid_reverse_nchw(partitioned, self.partition_size, img_size)
         return x
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = x + self.drop_path1(self.ls1(self._partition_attn(self.norm1(x))))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-class MaxxVitBlock(nn.Module):
+class MaxxVitBlock(msnn.Cell):
     """MaxVit conv, window partition + FFN , grid partition + FFN."""
 
     def __init__(
@@ -1096,12 +1095,12 @@         self.nchw_attn = transformer_cfg.use_nchw_attn
 
         conv_cls = ConvNeXtBlock if conv_cfg.block_type == 'convnext' else MbConvBlock
-        self.conv = conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path, **dd)
-
-        attn_kwargs = dict(dim=dim_out, cfg=transformer_cfg, drop_path=drop_path, **dd)
+        self.conv = conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        attn_kwargs = dict(dim=dim_out, cfg=transformer_cfg, drop_path=drop_path, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         partition_layer = PartitionAttention2d if self.nchw_attn else PartitionAttentionCl
-        self.attn_block = None if transformer_cfg.no_block_attn else partition_layer(**attn_kwargs)
-        self.attn_grid = partition_layer(partition_type='grid', **attn_kwargs)
+        self.attn_block = None if transformer_cfg.no_block_attn else partition_layer(**attn_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn_grid = partition_layer(partition_type='grid', **attn_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def init_weights(self, scheme=''):
         if self.attn_block is not None:
@@ -1109,7 +1108,7 @@         named_apply(partial(_init_transformer, scheme=scheme), self.attn_grid)
         named_apply(partial(_init_conv, scheme=scheme), self.conv)
 
-    def forward(self, x):
+    def construct(self, x):
         # NCHW format
         x = self.conv(x)
 
@@ -1123,7 +1122,7 @@         return x
 
 
-class ParallelMaxxVitBlock(nn.Module):
+class ParallelMaxxVitBlock(msnn.Cell):
     """MaxVit block with parallel cat(window + grid), one FF.
 
     Experimental timm block.
@@ -1156,18 +1155,18 @@ 
         conv_cls = ConvNeXtBlock if conv_cfg.block_type == 'convnext' else MbConvBlock
         if num_conv > 1:
-            convs = [conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path, **dd)]
-            convs += [conv_cls(dim_out, dim_out, cfg=conv_cfg, drop_path=drop_path, **dd)] * (num_conv - 1)
-            self.conv = nn.Sequential(*convs)
-        else:
-            self.conv = conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path, **dd)
-        self.attn = ParallelPartitionAttention(dim=dim_out, cfg=transformer_cfg, drop_path=drop_path, **dd)
+            convs = [conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path, **dd)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            convs += [conv_cls(dim_out, dim_out, cfg=conv_cfg, drop_path=drop_path, **dd)] * (num_conv - 1)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.conv = msnn.SequentialCell(*convs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        else:
+            self.conv = conv_cls(dim, dim_out, stride=stride, cfg=conv_cfg, drop_path=drop_path, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.attn = ParallelPartitionAttention(dim=dim_out, cfg=transformer_cfg, drop_path=drop_path, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def init_weights(self, scheme: str = '') -> None:
         named_apply(partial(_init_transformer, scheme=scheme), self.attn)
         named_apply(partial(_init_conv, scheme=scheme), self.conv)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.conv(x)
         x = x.permute(0, 2, 3, 1)
         x = self.attn(x)
@@ -1175,7 +1174,7 @@         return x
 
 
-class MaxxVitStage(nn.Module):
+class MaxxVitStage(msnn.Cell):
     """MaxxVit stage consisting of mixed convolution and transformer blocks."""
 
     def __init__(
@@ -1222,7 +1221,7 @@                     cfg=conv_cfg,
                     drop_path=drop_path[i],
                     **dd,
-                )]
+                )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             elif t == 'T':
                 rel_pos_cls = get_rel_pos_cls(transformer_cfg, feat_size)
                 blocks += [TransformerBlock2d(
@@ -1233,7 +1232,7 @@                     cfg=transformer_cfg,
                     drop_path=drop_path[i],
                     **dd,
-                )]
+                )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             elif t == 'M':
                 blocks += [MaxxVitBlock(
                     in_chs,
@@ -1243,7 +1242,7 @@                     transformer_cfg=transformer_cfg,
                     drop_path=drop_path[i],
                     **dd,
-                )]
+                )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             elif t == 'PM':
                 blocks += [ParallelMaxxVitBlock(
                     in_chs,
@@ -1253,11 +1252,11 @@                     transformer_cfg=transformer_cfg,
                     drop_path=drop_path[i],
                     **dd,
-                )]
+                )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             in_chs = out_chs
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.blocks = msnn.SequentialCell(*blocks)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
@@ -1265,7 +1264,7 @@         return x
 
 
-class Stem(nn.Module):
+class Stem(msnn.Cell):
     """Stem layer for feature extraction."""
 
     def __init__(
@@ -1301,14 +1300,14 @@         self.out_chs = out_chs[-1]
         self.stride = 2
 
-        self.conv1 = create_conv2d(in_chs, out_chs[0], kernel_size, stride=2, padding=padding, bias=bias, **dd)
-        self.norm1 = norm_act_layer(out_chs[0], **dd)
-        self.conv2 = create_conv2d(out_chs[0], out_chs[1], kernel_size, stride=1, padding=padding, bias=bias, **dd)
+        self.conv1 = create_conv2d(in_chs, out_chs[0], kernel_size, stride=2, padding=padding, bias=bias, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm1 = norm_act_layer(out_chs[0], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv2 = create_conv2d(out_chs[0], out_chs[1], kernel_size, stride=1, padding=padding, bias=bias, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     def init_weights(self, scheme: str = '') -> None:
         named_apply(partial(_init_conv, scheme=scheme), self)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.conv1(x)
         x = self.norm1(x)
         x = self.conv2(x)
@@ -1342,11 +1341,11 @@         transformer_cfg=replace(cfg.transformer_cfg, **transformer_kwargs),
         conv_cfg=replace(cfg.conv_cfg, **conv_kwargs),
         **base_kwargs
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return cfg
 
 
-class MaxxVit(nn.Module):
+class MaxxVit(msnn.Cell):
     """CoaTNet + MaxVit base model.
 
     Highly configurable for different block compositions, tensor layouts, pooling types.
@@ -1380,7 +1379,7 @@         dd = {'device': device, 'dtype': dtype}
         img_size = to_2tuple(img_size)
         if kwargs:
-            cfg = _overlay_kwargs(cfg, **kwargs)
+            cfg = _overlay_kwargs(cfg, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         transformer_cfg = cfg_window_size(cfg.transformer_cfg, img_size)
         self.num_classes = num_classes
         self.global_pool = global_pool
@@ -1398,7 +1397,7 @@             norm_layer=cfg.conv_cfg.norm_layer,
             norm_eps=cfg.conv_cfg.norm_eps,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stride = self.stem.stride
         self.feature_info += [dict(num_chs=self.stem.out_chs, reduction=2, module='stem')]
         feat_size = tuple([i // s for i, s in zip(img_size, to_2tuple(stride))])
@@ -1422,15 +1421,15 @@                 feat_size=feat_size,
                 drop_path=dpr[i],
                 **dd,
-            )]
+            )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             stride *= stage_stride
             in_chs = out_chs
             self.feature_info += [dict(num_chs=out_chs, reduction=stride, module=f'stages.{i}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         final_norm_layer = partial(get_norm_layer(cfg.transformer_cfg.norm_layer), eps=cfg.transformer_cfg.norm_eps)
         if cfg.head_hidden_size:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
             self.head_hidden_size = cfg.head_hidden_size
             self.head = NormMlpClassifierHead(
                 self.num_features,
@@ -1440,38 +1439,38 @@                 drop_rate=drop_rate,
                 norm_layer=final_norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             # standard classifier head w/ norm, pooling, fc classifier
             self.head_hidden_size = self.num_features
-            self.norm = final_norm_layer(self.num_features, **dd)
+            self.norm = final_norm_layer(self.num_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.head = ClassifierHead(
                 self.num_features,
                 num_classes,
                 pool_type=global_pool,
                 drop_rate=drop_rate,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # Weight init (default PyTorch init works well for AdamW if scheme not set)
         assert cfg.weight_init in ('', 'normal', 'trunc_normal', 'xavier_normal', 'vit_eff')
         if cfg.weight_init:
             named_apply(partial(self._init_weights, scheme=cfg.weight_init), self)
 
-    def _init_weights(self, module: nn.Module, name: str, scheme: str = '') -> None:
+    def _init_weights(self, module: msnn.Cell, name: str, scheme: str = '') -> None:
         if hasattr(module, 'init_weights'):
             try:
                 module.init_weights(scheme=scheme)
             except TypeError:
                 module.init_weights()
 
-    @torch.jit.ignore
+    @ms.jit
     def no_weight_decay(self) -> Set[str]:
         return {
             k for k, _ in self.named_parameters()
             if any(n in k for n in ["relative_position_bias_table", "rel_pos.mlp"])}
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse: bool = False) -> Dict[str, Any]:
         matcher = dict(
             stem=r'^stem',  # stem and embed
@@ -1479,13 +1478,13 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True) -> None:
         for s in self.stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None) -> None:
@@ -1494,13 +1493,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -1556,21 +1555,21 @@         take_indices, max_index = feature_take_indices(len(self.stages) + 1, indices)
         self.stages = self.stages[:max_index]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.head = self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         x = self.stem(x)
         x = self.stages(x)
         x = self.norm(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -2103,10 +2102,10 @@         head_hidden_size=1536,
         **_tf_cfg(),
     ),
-)
-
-
-def checkpoint_filter_fn(state_dict: Dict[str, torch.Tensor], model: nn.Module) -> Dict[str, torch.Tensor]:
+)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+
+def checkpoint_filter_fn(state_dict: Dict[str, ms.Tensor], model: msnn.Cell) -> Dict[str, ms.Tensor]:
     """Filter checkpoint state dict for compatibility."""
     model_state_dict = model.state_dict()
     out_dict = {}
@@ -2140,7 +2139,7 @@         model_cfg=model_cfgs[cfg_variant],
         feature_cfg=dict(flatten_sequential=True),
         pretrained_filter_fn=checkpoint_filter_fn,
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url: str = '', **kwargs: Any) -> Dict[str, Any]:
@@ -2377,334 +2376,334 @@ @register_model
 def coatnet_pico_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet Pico model with RW configuration."""
-    return _create_maxxvit('coatnet_pico_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_pico_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_nano_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet Nano model with RW configuration."""
-    return _create_maxxvit('coatnet_nano_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_nano_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_0_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-0 model with RW configuration."""
-    return _create_maxxvit('coatnet_0_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_0_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_1_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-1 model with RW configuration."""
-    return _create_maxxvit('coatnet_1_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_1_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_2_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-2 model with RW configuration."""
-    return _create_maxxvit('coatnet_2_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_2_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_3_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-3 model with RW configuration."""
-    return _create_maxxvit('coatnet_3_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_3_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_bn_0_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-0 model with BatchNorm and RW configuration."""
-    return _create_maxxvit('coatnet_bn_0_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_bn_0_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_rmlp_nano_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet Nano model with Relative Position MLP."""
-    return _create_maxxvit('coatnet_rmlp_nano_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_rmlp_nano_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_rmlp_0_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-0 model with Relative Position MLP."""
-    return _create_maxxvit('coatnet_rmlp_0_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_rmlp_0_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_rmlp_1_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-1 model with Relative Position MLP."""
-    return _create_maxxvit('coatnet_rmlp_1_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_rmlp_1_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_rmlp_1_rw2_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-1 model with Relative Position MLP v2."""
-    return _create_maxxvit('coatnet_rmlp_1_rw2_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_rmlp_1_rw2_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_rmlp_2_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-2 model with Relative Position MLP."""
-    return _create_maxxvit('coatnet_rmlp_2_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_rmlp_2_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_rmlp_2_rw_384(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-2 model with Relative Position MLP at 384x384."""
-    return _create_maxxvit('coatnet_rmlp_2_rw_384', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_rmlp_2_rw_384', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_rmlp_3_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-3 model with Relative Position MLP."""
-    return _create_maxxvit('coatnet_rmlp_3_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_rmlp_3_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_nano_cc_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet Nano model with ConvNeXt blocks."""
-    return _create_maxxvit('coatnet_nano_cc_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_nano_cc_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnext_nano_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoAtNeXt Nano model with RW configuration."""
-    return _create_maxxvit('coatnext_nano_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnext_nano_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_0_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-0 model."""
-    return _create_maxxvit('coatnet_0_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_0_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_1_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-1 model."""
-    return _create_maxxvit('coatnet_1_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_1_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_2_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-2 model."""
-    return _create_maxxvit('coatnet_2_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_2_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_3_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-3 model."""
-    return _create_maxxvit('coatnet_3_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_3_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_4_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-4 model."""
-    return _create_maxxvit('coatnet_4_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_4_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def coatnet_5_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """CoatNet-5 model."""
-    return _create_maxxvit('coatnet_5_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('coatnet_5_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_pico_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Pico model with RW configuration."""
-    return _create_maxxvit('maxvit_pico_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_pico_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_nano_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Nano model with RW configuration."""
-    return _create_maxxvit('maxvit_nano_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_nano_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_tiny_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Tiny model with RW configuration."""
-    return _create_maxxvit('maxvit_tiny_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_tiny_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_tiny_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Tiny model with RW configuration at 256x256."""
-    return _create_maxxvit('maxvit_tiny_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_tiny_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_rmlp_pico_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Relative Position MLP Pico RW 256x256 model."""
-    return _create_maxxvit('maxvit_rmlp_pico_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_rmlp_pico_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_rmlp_nano_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Relative Position MLP Nano RW 256x256 model."""
-    return _create_maxxvit('maxvit_rmlp_nano_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_rmlp_nano_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_rmlp_tiny_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Relative Position MLP Tiny RW 256x256 model."""
-    return _create_maxxvit('maxvit_rmlp_tiny_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_rmlp_tiny_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_rmlp_small_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Relative Position MLP Small RW 224x224 model."""
-    return _create_maxxvit('maxvit_rmlp_small_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_rmlp_small_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_rmlp_small_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Small model with Relative Position MLP at 256x256."""
-    return _create_maxxvit('maxvit_rmlp_small_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_rmlp_small_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_rmlp_base_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Base model with Relative Position MLP."""
-    return _create_maxxvit('maxvit_rmlp_base_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_rmlp_base_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_rmlp_base_rw_384(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Base model with Relative Position MLP at 384x384."""
-    return _create_maxxvit('maxvit_rmlp_base_rw_384', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_rmlp_base_rw_384', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_tiny_pm_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Tiny model with parallel blocks."""
-    return _create_maxxvit('maxvit_tiny_pm_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_tiny_pm_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxxvit_rmlp_nano_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxxViT Relative Position MLP Nano RW 256x256 model."""
-    return _create_maxxvit('maxxvit_rmlp_nano_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxxvit_rmlp_nano_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxxvit_rmlp_tiny_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxxViT Tiny model with Relative Position MLP."""
-    return _create_maxxvit('maxxvit_rmlp_tiny_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxxvit_rmlp_tiny_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxxvit_rmlp_small_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxxViT Small model with Relative Position MLP."""
-    return _create_maxxvit('maxxvit_rmlp_small_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxxvit_rmlp_small_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxxvitv2_nano_rw_256(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxxViT-V2 Nano model."""
-    return _create_maxxvit('maxxvitv2_nano_rw_256', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxxvitv2_nano_rw_256', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxxvitv2_rmlp_base_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxxViT-V2 Base model with Relative Position MLP."""
-    return _create_maxxvit('maxxvitv2_rmlp_base_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxxvitv2_rmlp_base_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxxvitv2_rmlp_base_rw_384(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxxViT-V2 Base model with Relative Position MLP at 384x384."""
-    return _create_maxxvit('maxxvitv2_rmlp_base_rw_384', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxxvitv2_rmlp_base_rw_384', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxxvitv2_rmlp_large_rw_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxxViT-V2 Large model with Relative Position MLP."""
-    return _create_maxxvit('maxxvitv2_rmlp_large_rw_224', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxxvitv2_rmlp_large_rw_224', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_tiny_tf_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Tiny model from TensorFlow."""
-    return _create_maxxvit('maxvit_tiny_tf_224', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_tiny_tf_224', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_tiny_tf_384(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Tiny model from TensorFlow at 384x384."""
-    return _create_maxxvit('maxvit_tiny_tf_384', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_tiny_tf_384', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_tiny_tf_512(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Tiny model from TensorFlow at 512x512."""
-    return _create_maxxvit('maxvit_tiny_tf_512', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_tiny_tf_512', 'maxvit_tiny_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_small_tf_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Small model from TensorFlow."""
-    return _create_maxxvit('maxvit_small_tf_224', 'maxvit_small_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_small_tf_224', 'maxvit_small_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_small_tf_384(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Small model from TensorFlow at 384x384."""
-    return _create_maxxvit('maxvit_small_tf_384', 'maxvit_small_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_small_tf_384', 'maxvit_small_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_small_tf_512(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Small model from TensorFlow at 512x512."""
-    return _create_maxxvit('maxvit_small_tf_512', 'maxvit_small_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_small_tf_512', 'maxvit_small_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_base_tf_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Base model from TensorFlow."""
-    return _create_maxxvit('maxvit_base_tf_224', 'maxvit_base_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_base_tf_224', 'maxvit_base_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_base_tf_384(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Base model from TensorFlow at 384x384."""
-    return _create_maxxvit('maxvit_base_tf_384', 'maxvit_base_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_base_tf_384', 'maxvit_base_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_base_tf_512(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Base model from TensorFlow at 512x512."""
-    return _create_maxxvit('maxvit_base_tf_512', 'maxvit_base_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_base_tf_512', 'maxvit_base_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_large_tf_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Large model from TensorFlow."""
-    return _create_maxxvit('maxvit_large_tf_224', 'maxvit_large_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_large_tf_224', 'maxvit_large_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_large_tf_384(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Large model from TensorFlow at 384x384."""
-    return _create_maxxvit('maxvit_large_tf_384', 'maxvit_large_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_large_tf_384', 'maxvit_large_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_large_tf_512(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT Large model from TensorFlow at 512x512."""
-    return _create_maxxvit('maxvit_large_tf_512', 'maxvit_large_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_large_tf_512', 'maxvit_large_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_xlarge_tf_224(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT XLarge model from TensorFlow."""
-    return _create_maxxvit('maxvit_xlarge_tf_224', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_xlarge_tf_224', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_xlarge_tf_384(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT XLarge model from TensorFlow at 384x384."""
-    return _create_maxxvit('maxvit_xlarge_tf_384', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_xlarge_tf_384', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def maxvit_xlarge_tf_512(pretrained: bool = False, **kwargs: Any) -> MaxxVit:
     """MaxViT XLarge model from TensorFlow at 512x512."""
-    return _create_maxxvit('maxvit_xlarge_tf_512', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)
+    return _create_maxxvit('maxvit_xlarge_tf_512', 'maxvit_xlarge_tf', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
