--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 Poolformer from MetaFormer is Actually What You Need for Vision https://arxiv.org/abs/2111.11418
 
@@ -30,11 +35,11 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch import Tensor
-from torch.jit import Final
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
+# from torch import Tensor
+# from torch.jit import Final
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -56,7 +61,7 @@ __all__ = ['MetaFormer']
 
 
-class Stem(nn.Module):
+class Stem(msnn.Cell):
     """
     Stem implemented by a layer of convolution.
     Conv2d params constant across all models.
@@ -66,7 +71,7 @@             self,
             in_channels: int,
             out_channels: int,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -79,16 +84,16 @@             stride=4,
             padding=2,
             **dd,
-        )
-        self.norm = norm_layer(out_channels, **dd) if norm_layer else nn.Identity()
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.norm = norm_layer(out_channels, **dd) if norm_layer else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.conv(x)
         x = self.norm(x)
         return x
 
 
-class Downsampling(nn.Module):
+class Downsampling(msnn.Cell):
     """
     Downsampling implemented by a layer of convolution.
     """
@@ -100,13 +105,13 @@             kernel_size: int,
             stride: int = 1,
             padding: int = 0,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.norm = norm_layer(in_channels, **dd) if norm_layer else nn.Identity()
+        self.norm = norm_layer(in_channels, **dd) if norm_layer else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv = nn.Conv2d(
             in_channels,
             out_channels,
@@ -114,15 +119,15 @@             stride=stride,
             padding=padding,
             **dd
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.norm(x)
         x = self.conv(x)
         return x
 
 
-class Scale(nn.Module):
+class Scale(msnn.Cell):
     """
     Scale vector by element multiplications.
     """
@@ -139,26 +144,26 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.shape = (dim, 1, 1) if use_nchw else (dim,)
-        self.scale = nn.Parameter(init_value * torch.ones(dim, **dd), requires_grad=trainable)
-
-    def forward(self, x):
+        self.scale = ms.Parameter(init_value * mint.ones(dim, **dd), requires_grad=trainable)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         return x * self.scale.view(self.shape)
 
 
-class SquaredReLU(nn.Module):
+class SquaredReLU(msnn.Cell):
     """
         Squared ReLU: https://arxiv.org/abs/2109.08668
     """
 
     def __init__(self, inplace: bool = False):
         super().__init__()
-        self.relu = nn.ReLU(inplace=inplace)
-
-    def forward(self, x):
-        return torch.square(self.relu(x))
-
-
-class StarReLU(nn.Module):
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+    def construct(self, x):
+        return mint.square(self.relu(x))
+
+
+class StarReLU(msnn.Cell):
     """
     StarReLU: s * relu(x) ** 2 + b
     """
@@ -177,20 +182,20 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.inplace = inplace
-        self.relu = nn.ReLU(inplace=inplace)
-        self.scale = nn.Parameter(scale_value * torch.ones(1, **dd), requires_grad=scale_learnable)
-        self.bias = nn.Parameter(bias_value * torch.ones(1, **dd), requires_grad=bias_learnable)
-
-    def forward(self, x):
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.scale = ms.Parameter(scale_value * mint.ones(1, **dd), requires_grad=scale_learnable)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.bias = ms.Parameter(bias_value * mint.ones(1, **dd), requires_grad=bias_learnable)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         return self.scale * self.relu(x) ** 2 + self.bias
 
 
-class Attention(nn.Module):
+class Attention(msnn.Cell):
     """
     Vanilla self-attention from Transformer: https://arxiv.org/abs/1706.03762.
     Modified from timm.
     """
-    fused_attn: Final[bool]
+    fused_attn: Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -218,12 +223,12 @@ 
         self.attention_dim = self.num_heads * self.head_dim
 
-        self.qkv = nn.Linear(dim, self.attention_dim * 3, bias=qkv_bias, **dd)
+        self.qkv = nn.Linear(dim, self.attention_dim * 3, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn_drop = nn.Dropout(attn_drop)
-        self.proj = nn.Linear(self.attention_dim, dim, bias=proj_bias, **dd)
+        self.proj = nn.Linear(self.attention_dim, dim, bias=proj_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x):
+    def construct(self, x):
         B, N, C = x.shape
         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
         q, k, v = qkv.unbind(0)
@@ -232,7 +237,7 @@             x = F.scaled_dot_product_attention(
                 q, k, v,
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             attn = (q @ k.transpose(-2, -1)) * self.scale
             attn = attn.softmax(dim=-1)
@@ -250,26 +255,26 @@ 
 class GroupNorm1NoBias(GroupNorm1):
     def __init__(self, num_channels: int, **kwargs):
-        super().__init__(num_channels, **kwargs)
+        super().__init__(num_channels, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.eps = kwargs.get('eps', 1e-6)
         self.bias = None
 
 
 class LayerNorm2dNoBias(LayerNorm2d):
     def __init__(self, num_channels: int, **kwargs):
-        super().__init__(num_channels, **kwargs)
+        super().__init__(num_channels, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.eps = kwargs.get('eps', 1e-6)
         self.bias = None
 
 
 class LayerNormNoBias(nn.LayerNorm):
     def __init__(self, num_channels: int, **kwargs):
-        super().__init__(num_channels, **kwargs)
+        super().__init__(num_channels, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.eps = kwargs.get('eps', 1e-6)
         self.bias = None
 
 
-class SepConv(nn.Module):
+class SepConv(msnn.Cell):
     r"""
     Inverted separable convolution from MobileNetV2: https://arxiv.org/abs/1801.04381.
     """
@@ -278,8 +283,8 @@             self,
             dim: int,
             expansion_ratio: float = 2,
-            act1_layer: Type[nn.Module] = StarReLU,
-            act2_layer: Type[nn.Module] = nn.Identity,
+            act1_layer: Type[msnn.Cell] = StarReLU,
+            act2_layer: Type[msnn.Cell] = msnn.Identity,
             bias: bool = False,
             kernel_size: int = 7,
             padding: int = 3,
@@ -290,8 +295,8 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         mid_channels = int(expansion_ratio * dim)
-        self.pwconv1 = nn.Conv2d(dim, mid_channels, kernel_size=1, bias=bias, **dd)
-        self.act1 = act1_layer(**dd) if issubclass(act1_layer, StarReLU) else act1_layer()
+        self.pwconv1 = nn.Conv2d(dim, mid_channels, kernel_size=1, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.act1 = act1_layer(**dd) if issubclass(act1_layer, StarReLU) else act1_layer()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.dwconv = nn.Conv2d(
             mid_channels,
             mid_channels,
@@ -300,11 +305,11 @@             groups=mid_channels,
             bias=bias,
             **dd,
-        )  # depthwise conv
-        self.act2 = act2_layer(**dd) if issubclass(act2_layer, StarReLU) else act2_layer()
-        self.pwconv2 = nn.Conv2d(mid_channels, dim, kernel_size=1, bias=bias, **dd)
-
-    def forward(self, x):
+        )  # depthwise conv; 存在 *args/**kwargs，需手动确认参数映射;
+        self.act2 = act2_layer(**dd) if issubclass(act2_layer, StarReLU) else act2_layer()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.pwconv2 = nn.Conv2d(mid_channels, dim, kernel_size=1, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.pwconv1(x)
         x = self.act1(x)
         x = self.dwconv(x)
@@ -313,21 +318,21 @@         return x
 
 
-class Pooling(nn.Module):
+class Pooling(msnn.Cell):
     """
     Implementation of pooling for PoolFormer: https://arxiv.org/abs/2111.11418
     """
 
     def __init__(self, pool_size: int = 3, **kwargs):
         super().__init__()
-        self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size // 2, count_include_pad=False)
-
-    def forward(self, x):
+        self.pool = nn.AvgPool2d(pool_size, stride = 1, padding = pool_size // 2, count_include_pad = False)
+
+    def construct(self, x):
         y = self.pool(x)
         return y - x
 
 
-class MlpHead(nn.Module):
+class MlpHead(msnn.Cell):
     """ MLP classification head
     """
 
@@ -336,8 +341,8 @@             dim: int,
             num_classes: int = 1000,
             mlp_ratio: float = 4,
-            act_layer: Type[nn.Module] = SquaredReLU,
-            norm_layer: Type[nn.Module] = LayerNorm,
+            act_layer: Type[msnn.Cell] = SquaredReLU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
             drop_rate: float = 0.,
             bias: bool = True,
             device=None,
@@ -346,13 +351,13 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         hidden_features = int(mlp_ratio * dim)
-        self.fc1 = nn.Linear(dim, hidden_features, bias=bias, **dd)
+        self.fc1 = nn.Linear(dim, hidden_features, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.act = act_layer()
-        self.norm = norm_layer(hidden_features, **dd)
-        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias, **dd)
+        self.norm = norm_layer(hidden_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.head_drop = nn.Dropout(drop_rate)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.fc1(x)
         x = self.act(x)
         x = self.norm(x)
@@ -361,7 +366,7 @@         return x
 
 
-class MetaFormerBlock(nn.Module):
+class MetaFormerBlock(msnn.Cell):
     """
     Implementation of one MetaFormer block.
     """
@@ -369,10 +374,10 @@     def __init__(
             self,
             dim: int,
-            token_mixer: Type[nn.Module] = Pooling,
-            mlp_act: Type[nn.Module] = StarReLU,
+            token_mixer: Type[msnn.Cell] = Pooling,
+            mlp_act: Type[msnn.Cell] = StarReLU,
             mlp_bias: bool = False,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             proj_drop: float = 0.,
             drop_path: float = 0.,
             use_nchw: bool = True,
@@ -384,16 +389,16 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        ls_layer = partial(Scale, dim=dim, init_value=layer_scale_init_value, use_nchw=use_nchw, **dd)
-        rs_layer = partial(Scale, dim=dim, init_value=res_scale_init_value, use_nchw=use_nchw, **dd)
-
-        self.norm1 = norm_layer(dim, **dd)
-        self.token_mixer = token_mixer(dim=dim, proj_drop=proj_drop, **dd, **kwargs)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.layer_scale1 = ls_layer() if layer_scale_init_value is not None else nn.Identity()
-        self.res_scale1 = rs_layer() if res_scale_init_value is not None else nn.Identity()
-
-        self.norm2 = norm_layer(dim, **dd)
+        ls_layer = partial(Scale, dim=dim, init_value=layer_scale_init_value, use_nchw=use_nchw, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        rs_layer = partial(Scale, dim=dim, init_value=res_scale_init_value, use_nchw=use_nchw, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.token_mixer = token_mixer(dim=dim, proj_drop=proj_drop, **dd, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+        self.layer_scale1 = ls_layer() if layer_scale_init_value is not None else msnn.Identity()
+        self.res_scale1 = rs_layer() if res_scale_init_value is not None else msnn.Identity()
+
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.mlp = Mlp(
             dim,
             int(4 * dim),
@@ -402,12 +407,12 @@             drop=proj_drop,
             use_conv=use_nchw,
             **dd
-        )
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.layer_scale2 = ls_layer() if layer_scale_init_value is not None else nn.Identity()
-        self.res_scale2 = rs_layer() if res_scale_init_value is not None else nn.Identity()
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+        self.layer_scale2 = ls_layer() if layer_scale_init_value is not None else msnn.Identity()
+        self.res_scale2 = rs_layer() if res_scale_init_value is not None else msnn.Identity()
+
+    def construct(self, x):
         x = self.res_scale1(x) + \
             self.layer_scale1(
                 self.drop_path1(
@@ -423,18 +428,18 @@         return x
 
 
-class MetaFormerStage(nn.Module):
+class MetaFormerStage(msnn.Cell):
 
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             depth: int = 2,
-            token_mixer: Type[nn.Module] = nn.Identity,
-            mlp_act: Type[nn.Module] = StarReLU,
+            token_mixer: Type[msnn.Cell] = msnn.Identity,
+            mlp_act: Type[msnn.Cell] = StarReLU,
             mlp_bias: bool = False,
-            downsample_norm: Optional[Type[nn.Module]] = LayerNorm2d,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            downsample_norm: Optional[Type[msnn.Cell]] = LayerNorm2d,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             proj_drop: float = 0.,
             dp_rates: List[float] = [0.] * 2,
             layer_scale_init_value: Optional[float] = None,
@@ -449,7 +454,7 @@         self.use_nchw = not issubclass(token_mixer, Attention)
 
         # don't downsample if in_chs and out_chs are the same
-        self.downsample = nn.Identity() if in_chs == out_chs else Downsampling(
+        self.downsample = msnn.Identity() if in_chs == out_chs else Downsampling(
             in_chs,
             out_chs,
             kernel_size=3,
@@ -457,9 +462,9 @@             padding=1,
             norm_layer=downsample_norm,
             **dd,
-        )
-
-        self.blocks = nn.Sequential(*[MetaFormerBlock(
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.blocks = msnn.SequentialCell(*[MetaFormerBlock(
             dim=out_chs,
             token_mixer=token_mixer,
             mlp_act=mlp_act,
@@ -472,13 +477,13 @@             use_nchw=self.use_nchw,
             **dd,
             **kwargs,
-        ) for i in range(depth)])
-
-    @torch.jit.ignore
+        ) for i in range(depth)])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    def forward(self, x: Tensor):
+    def construct(self, x: ms.Tensor):
         x = self.downsample(x)
         B, C, H, W = x.shape
 
@@ -496,7 +501,7 @@         return x
 
 
-class MetaFormer(nn.Module):
+class MetaFormer(msnn.Cell):
     r""" MetaFormer
         A PyTorch impl of : `MetaFormer Baselines for Vision`  -
           https://arxiv.org/abs/2210.13452
@@ -529,17 +534,17 @@             global_pool: str = 'avg',
             depths: Tuple[int, ...] = (2, 2, 6, 2),
             dims: Tuple[int, ...] = (64, 128, 320, 512),
-            token_mixers: Union[Type[nn.Module], List[Type[nn.Module]]] = Pooling,
-            mlp_act: Type[nn.Module] = StarReLU,
+            token_mixers: Union[Type[msnn.Cell], List[Type[msnn.Cell]]] = Pooling,
+            mlp_act: Type[msnn.Cell] = StarReLU,
             mlp_bias: bool = False,
             drop_path_rate: float = 0.,
             proj_drop_rate: float = 0.,
             drop_rate: float = 0.0,
             layer_scale_init_values: Optional[Union[float, List[float]]] = None,
             res_scale_init_values: Union[Tuple[Optional[float], ...], List[Optional[float]]] = (None, None, 1.0, 1.0),
-            downsample_norm: Optional[Type[nn.Module]] = LayerNorm2dNoBias,
-            norm_layers: Union[Type[nn.Module], List[Type[nn.Module]]] = LayerNorm2dNoBias,
-            output_norm: Type[nn.Module] = LayerNorm2d,
+            downsample_norm: Optional[Type[msnn.Cell]] = LayerNorm2dNoBias,
+            norm_layers: Union[Type[msnn.Cell], List[Type[msnn.Cell]]] = LayerNorm2dNoBias,
+            output_norm: Type[msnn.Cell] = LayerNorm2d,
             use_mlp_head: bool = True,
             device=None,
             dtype=None,
@@ -549,7 +554,7 @@         dd = {'device': device, 'dtype': dtype}
         # Bind dd kwargs to activation layers that need them
         if mlp_act in (StarReLU,):
-            mlp_act = partial(mlp_act, **dd)
+            mlp_act = partial(mlp_act, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.num_classes = num_classes
         self.num_features = dims[-1]
@@ -579,7 +584,7 @@             dims[0],
             norm_layer=downsample_norm,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         stages = []
         prev_dim = dims[0]
@@ -600,31 +605,31 @@                 norm_layer=norm_layers[i],
                 **dd,
                 **kwargs,
-            )]
+            )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_dim = dims[i]
             self.feature_info += [dict(num_chs=dims[i], reduction=2**(i+2), module=f'stages.{i}')]
 
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # if using MlpHead, dropout is handled by MlpHead
         if num_classes > 0:
             if self.use_mlp_head:
                 # FIXME not actually returning mlp hidden state right now as pre-logits.
-                final = MlpHead(self.num_features, num_classes, drop_rate=self.drop_rate, **dd)
+                final = MlpHead(self.num_features, num_classes, drop_rate=self.drop_rate, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
                 self.head_hidden_size = self.num_features
             else:
-                final = nn.Linear(self.num_features, num_classes, **dd)
+                final = nn.Linear(self.num_features, num_classes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
                 self.head_hidden_size = self.num_features
         else:
-            final = nn.Identity()
-
-        self.head = nn.Sequential(OrderedDict([
+            final = msnn.Identity()
+
+        self.head = msnn.SequentialCell(OrderedDict([
             ('global_pool', SelectAdaptivePool2d(pool_type=global_pool)),
             ('norm', output_norm(self.num_features, **dd)),
-            ('flatten', nn.Flatten(1) if global_pool else nn.Identity()),
-            ('drop', nn.Dropout(drop_rate) if self.use_mlp_head else nn.Identity()),
+            ('flatten', mint.flatten(1) if global_pool else msnn.Identity()),
+            ('drop', nn.Dropout(drop_rate) if self.use_mlp_head else msnn.Identity()),
             ('fc', final)
-        ]))
+        ]))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.apply(self._init_weights)
 
@@ -632,16 +637,16 @@         if isinstance(m, (nn.Conv2d, nn.Linear)):
             trunc_normal_(m.weight, std=.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-    @torch.jit.ignore
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
         for stage in self.stages:
             stage.set_grad_checkpointing(enable=enable)
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None, device=None, dtype=None):
@@ -649,25 +654,25 @@         self.num_classes = num_classes
         if global_pool is not None:
             self.head.global_pool = SelectAdaptivePool2d(pool_type=global_pool)
-            self.head.flatten = nn.Flatten(1) if global_pool else nn.Identity()
+            self.head.flatten = mint.flatten(1) if global_pool else msnn.Identity()
         if num_classes > 0:
             if self.use_mlp_head:
-                final = MlpHead(self.num_features, num_classes, drop_rate=self.drop_rate, **dd)
+                final = MlpHead(self.num_features, num_classes, drop_rate=self.drop_rate, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
-                final = nn.Linear(self.num_features, num_classes, **dd)
+                final = nn.Linear(self.num_features, num_classes, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         else:
-            final = nn.Identity()
+            final = msnn.Identity()
         self.head.fc = final
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -718,7 +723,7 @@             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_head(self, x: Tensor, pre_logits: bool = False):
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False):
         # NOTE nn.Sequential in head broken down since can't call head[:-1](x) in torchscript :(
         x = self.head.global_pool(x)
         x = self.head.norm(x)
@@ -726,7 +731,7 @@         x = self.head.drop(x)
         return x if pre_logits else self.head.fc(x)
 
-    def forward_features(self, x: Tensor):
+    def forward_features(self, x: ms.Tensor):
         x = self.stem(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stages, x)
@@ -734,7 +739,7 @@             x = self.stages(x)
         return x
 
-    def forward(self, x: Tensor):
+    def construct(self, x: ms.Tensor):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -789,7 +794,7 @@         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     return model
 
@@ -971,8 +976,8 @@         layer_scale_init_values=1e-5,
         res_scale_init_values=None,
         use_mlp_head=False,
-        **kwargs)
-    return _create_metaformer('poolformer_s12', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('poolformer_s12', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -987,8 +992,8 @@         layer_scale_init_values=1e-5,
         res_scale_init_values=None,
         use_mlp_head=False,
-        **kwargs)
-    return _create_metaformer('poolformer_s24', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('poolformer_s24', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1003,8 +1008,8 @@         layer_scale_init_values=1e-6,
         res_scale_init_values=None,
         use_mlp_head=False,
-        **kwargs)
-    return _create_metaformer('poolformer_s36', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('poolformer_s36', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1019,8 +1024,8 @@         layer_scale_init_values=1e-6,
         res_scale_init_values=None,
         use_mlp_head=False,
-        **kwargs)
-    return _create_metaformer('poolformer_m36', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('poolformer_m36', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1035,8 +1040,8 @@         layer_scale_init_values=1e-6,
         res_scale_init_values=None,
         use_mlp_head=False,
-        **kwargs)
-    return _create_metaformer('poolformer_m48', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('poolformer_m48', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1046,8 +1051,8 @@         dims=[64, 128, 320, 512],
         norm_layers=GroupNorm1NoBias,
         use_mlp_head=False,
-        **kwargs)
-    return _create_metaformer('poolformerv2_s12', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('poolformerv2_s12', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1057,8 +1062,8 @@         dims=[64, 128, 320, 512],
         norm_layers=GroupNorm1NoBias,
         use_mlp_head=False,
-        **kwargs)
-    return _create_metaformer('poolformerv2_s24', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('poolformerv2_s24', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1068,8 +1073,8 @@         dims=[64, 128, 320, 512],
         norm_layers=GroupNorm1NoBias,
         use_mlp_head=False,
-        **kwargs)
-    return _create_metaformer('poolformerv2_s36', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('poolformerv2_s36', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1079,8 +1084,8 @@         dims=[96, 192, 384, 768],
         norm_layers=GroupNorm1NoBias,
         use_mlp_head=False,
-        **kwargs)
-    return _create_metaformer('poolformerv2_m36', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('poolformerv2_m36', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1090,8 +1095,8 @@         dims=[96, 192, 384, 768],
         norm_layers=GroupNorm1NoBias,
         use_mlp_head=False,
-        **kwargs)
-    return _create_metaformer('poolformerv2_m48', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('poolformerv2_m48', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1101,8 +1106,8 @@         dims=[64, 128, 320, 512],
         token_mixers=SepConv,
         norm_layers=LayerNorm2dNoBias,
-        **kwargs)
-    return _create_metaformer('convformer_s18', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('convformer_s18', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1112,8 +1117,8 @@         dims=[64, 128, 320, 512],
         token_mixers=SepConv,
         norm_layers=LayerNorm2dNoBias,
-        **kwargs)
-    return _create_metaformer('convformer_s36', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('convformer_s36', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1123,8 +1128,8 @@         dims=[96, 192, 384, 576],
         token_mixers=SepConv,
         norm_layers=LayerNorm2dNoBias,
-        **kwargs)
-    return _create_metaformer('convformer_m36', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('convformer_m36', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1134,8 +1139,8 @@         dims=[128, 256, 512, 768],
         token_mixers=SepConv,
         norm_layers=LayerNorm2dNoBias,
-        **kwargs)
-    return _create_metaformer('convformer_b36', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('convformer_b36', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1145,8 +1150,8 @@         dims=[64, 128, 320, 512],
         token_mixers=[SepConv, SepConv, Attention, Attention],
         norm_layers=[LayerNorm2dNoBias] * 2 + [LayerNormNoBias] * 2,
-        **kwargs)
-    return _create_metaformer('caformer_s18', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('caformer_s18', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1156,8 +1161,8 @@         dims=[64, 128, 320, 512],
         token_mixers=[SepConv, SepConv, Attention, Attention],
         norm_layers=[LayerNorm2dNoBias] * 2 + [LayerNormNoBias] * 2,
-        **kwargs)
-    return _create_metaformer('caformer_s36', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('caformer_s36', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1167,8 +1172,8 @@         dims=[96, 192, 384, 576],
         token_mixers=[SepConv, SepConv, Attention, Attention],
         norm_layers=[LayerNorm2dNoBias] * 2 + [LayerNormNoBias] * 2,
-        **kwargs)
-    return _create_metaformer('caformer_m36', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('caformer_m36', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1178,5 +1183,5 @@         dims=[128, 256, 512, 768],
         token_mixers=[SepConv, SepConv, Attention, Attention],
         norm_layers=[LayerNorm2dNoBias] * 2 + [LayerNormNoBias] * 2,
-        **kwargs)
-    return _create_metaformer('caformer_b36', pretrained=pretrained, **model_kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_metaformer('caformer_b36', pretrained=pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
