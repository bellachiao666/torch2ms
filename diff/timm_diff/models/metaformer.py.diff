--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 Poolformer from MetaFormer is Actually What You Need for Vision https://arxiv.org/abs/2111.11418
 
@@ -30,11 +35,11 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch import Tensor
-from torch.jit import Final
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
+# from torch import Tensor
+# from torch.jit import Final
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import (
@@ -56,7 +61,7 @@ __all__ = ['MetaFormer']
 
 
-class Stem(nn.Module):
+class Stem(msnn.Cell):
     """
     Stem implemented by a layer of convolution.
     Conv2d params constant across all models.
@@ -66,7 +71,7 @@             self,
             in_channels: int,
             out_channels: int,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -80,15 +85,15 @@             padding=2,
             **dd,
         )
-        self.norm = norm_layer(out_channels, **dd) if norm_layer else nn.Identity()
-
-    def forward(self, x):
+        self.norm = norm_layer(out_channels, **dd) if norm_layer else msnn.Identity()
+
+    def construct(self, x):
         x = self.conv(x)
         x = self.norm(x)
         return x
 
 
-class Downsampling(nn.Module):
+class Downsampling(msnn.Cell):
     """
     Downsampling implemented by a layer of convolution.
     """
@@ -100,13 +105,13 @@             kernel_size: int,
             stride: int = 1,
             padding: int = 0,
-            norm_layer: Optional[Type[nn.Module]] = None,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.norm = norm_layer(in_channels, **dd) if norm_layer else nn.Identity()
+        self.norm = norm_layer(in_channels, **dd) if norm_layer else msnn.Identity()
         self.conv = nn.Conv2d(
             in_channels,
             out_channels,
@@ -116,13 +121,13 @@             **dd
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.norm(x)
         x = self.conv(x)
         return x
 
 
-class Scale(nn.Module):
+class Scale(msnn.Cell):
     """
     Scale vector by element multiplications.
     """
@@ -139,26 +144,26 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.shape = (dim, 1, 1) if use_nchw else (dim,)
-        self.scale = nn.Parameter(init_value * torch.ones(dim, **dd), requires_grad=trainable)
-
-    def forward(self, x):
+        self.scale = ms.Parameter(init_value * mint.ones(dim, **dd), requires_grad=trainable)
+
+    def construct(self, x):
         return x * self.scale.view(self.shape)
 
 
-class SquaredReLU(nn.Module):
+class SquaredReLU(msnn.Cell):
     """
         Squared ReLU: https://arxiv.org/abs/2109.08668
     """
 
     def __init__(self, inplace: bool = False):
         super().__init__()
-        self.relu = nn.ReLU(inplace=inplace)
-
-    def forward(self, x):
-        return torch.square(self.relu(x))
-
-
-class StarReLU(nn.Module):
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+    def construct(self, x):
+        return mint.square(self.relu(x))
+
+
+class StarReLU(msnn.Cell):
     """
     StarReLU: s * relu(x) ** 2 + b
     """
@@ -177,15 +182,15 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.inplace = inplace
-        self.relu = nn.ReLU(inplace=inplace)
-        self.scale = nn.Parameter(scale_value * torch.ones(1, **dd), requires_grad=scale_learnable)
-        self.bias = nn.Parameter(bias_value * torch.ones(1, **dd), requires_grad=bias_learnable)
-
-    def forward(self, x):
+        self.relu = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+        self.scale = ms.Parameter(scale_value * mint.ones(1, **dd), requires_grad=scale_learnable)
+        self.bias = ms.Parameter(bias_value * mint.ones(1, **dd), requires_grad=bias_learnable)
+
+    def construct(self, x):
         return self.scale * self.relu(x) ** 2 + self.bias
 
 
-class Attention(nn.Module):
+class Attention(msnn.Cell):
     """
     Vanilla self-attention from Transformer: https://arxiv.org/abs/1706.03762.
     Modified from timm.
@@ -223,7 +228,7 @@         self.proj = nn.Linear(self.attention_dim, dim, bias=proj_bias, **dd)
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x):
+    def construct(self, x):
         B, N, C = x.shape
         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
         q, k, v = qkv.unbind(0)
@@ -232,7 +237,7 @@             x = F.scaled_dot_product_attention(
                 q, k, v,
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             attn = (q @ k.transpose(-2, -1)) * self.scale
             attn = attn.softmax(dim=-1)
@@ -269,7 +274,7 @@         self.bias = None
 
 
-class SepConv(nn.Module):
+class SepConv(msnn.Cell):
     r"""
     Inverted separable convolution from MobileNetV2: https://arxiv.org/abs/1801.04381.
     """
@@ -278,8 +283,8 @@             self,
             dim: int,
             expansion_ratio: float = 2,
-            act1_layer: Type[nn.Module] = StarReLU,
-            act2_layer: Type[nn.Module] = nn.Identity,
+            act1_layer: Type[msnn.Cell] = StarReLU,
+            act2_layer: Type[msnn.Cell] = nn.Identity,
             bias: bool = False,
             kernel_size: int = 7,
             padding: int = 3,
@@ -304,7 +309,7 @@         self.act2 = act2_layer(**dd) if issubclass(act2_layer, StarReLU) else act2_layer()
         self.pwconv2 = nn.Conv2d(mid_channels, dim, kernel_size=1, bias=bias, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.pwconv1(x)
         x = self.act1(x)
         x = self.dwconv(x)
@@ -313,21 +318,21 @@         return x
 
 
-class Pooling(nn.Module):
+class Pooling(msnn.Cell):
     """
     Implementation of pooling for PoolFormer: https://arxiv.org/abs/2111.11418
     """
 
     def __init__(self, pool_size: int = 3, **kwargs):
         super().__init__()
-        self.pool = nn.AvgPool2d(pool_size, stride=1, padding=pool_size // 2, count_include_pad=False)
-
-    def forward(self, x):
+        self.pool = nn.AvgPool2d(pool_size, stride = 1, padding = pool_size // 2, count_include_pad = False)
+
+    def construct(self, x):
         y = self.pool(x)
         return y - x
 
 
-class MlpHead(nn.Module):
+class MlpHead(msnn.Cell):
     """ MLP classification head
     """
 
@@ -336,8 +341,8 @@             dim: int,
             num_classes: int = 1000,
             mlp_ratio: float = 4,
-            act_layer: Type[nn.Module] = SquaredReLU,
-            norm_layer: Type[nn.Module] = LayerNorm,
+            act_layer: Type[msnn.Cell] = SquaredReLU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
             drop_rate: float = 0.,
             bias: bool = True,
             device=None,
@@ -352,7 +357,7 @@         self.fc2 = nn.Linear(hidden_features, num_classes, bias=bias, **dd)
         self.head_drop = nn.Dropout(drop_rate)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.fc1(x)
         x = self.act(x)
         x = self.norm(x)
@@ -361,7 +366,7 @@         return x
 
 
-class MetaFormerBlock(nn.Module):
+class MetaFormerBlock(msnn.Cell):
     """
     Implementation of one MetaFormer block.
     """
@@ -369,10 +374,10 @@     def __init__(
             self,
             dim: int,
-            token_mixer: Type[nn.Module] = Pooling,
-            mlp_act: Type[nn.Module] = StarReLU,
+            token_mixer: Type[msnn.Cell] = Pooling,
+            mlp_act: Type[msnn.Cell] = StarReLU,
             mlp_bias: bool = False,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             proj_drop: float = 0.,
             drop_path: float = 0.,
             use_nchw: bool = True,
@@ -389,9 +394,9 @@ 
         self.norm1 = norm_layer(dim, **dd)
         self.token_mixer = token_mixer(dim=dim, proj_drop=proj_drop, **dd, **kwargs)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.layer_scale1 = ls_layer() if layer_scale_init_value is not None else nn.Identity()
-        self.res_scale1 = rs_layer() if res_scale_init_value is not None else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+        self.layer_scale1 = ls_layer() if layer_scale_init_value is not None else msnn.Identity()
+        self.res_scale1 = rs_layer() if res_scale_init_value is not None else msnn.Identity()
 
         self.norm2 = norm_layer(dim, **dd)
         self.mlp = Mlp(
@@ -403,11 +408,11 @@             use_conv=use_nchw,
             **dd
         )
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.layer_scale2 = ls_layer() if layer_scale_init_value is not None else nn.Identity()
-        self.res_scale2 = rs_layer() if res_scale_init_value is not None else nn.Identity()
-
-    def forward(self, x):
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+        self.layer_scale2 = ls_layer() if layer_scale_init_value is not None else msnn.Identity()
+        self.res_scale2 = rs_layer() if res_scale_init_value is not None else msnn.Identity()
+
+    def construct(self, x):
         x = self.res_scale1(x) + \
             self.layer_scale1(
                 self.drop_path1(
@@ -423,18 +428,18 @@         return x
 
 
-class MetaFormerStage(nn.Module):
+class MetaFormerStage(msnn.Cell):
 
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             depth: int = 2,
-            token_mixer: Type[nn.Module] = nn.Identity,
-            mlp_act: Type[nn.Module] = StarReLU,
+            token_mixer: Type[msnn.Cell] = nn.Identity,
+            mlp_act: Type[msnn.Cell] = StarReLU,
             mlp_bias: bool = False,
-            downsample_norm: Optional[Type[nn.Module]] = LayerNorm2d,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            downsample_norm: Optional[Type[msnn.Cell]] = LayerNorm2d,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             proj_drop: float = 0.,
             dp_rates: List[float] = [0.] * 2,
             layer_scale_init_value: Optional[float] = None,
@@ -449,7 +454,7 @@         self.use_nchw = not issubclass(token_mixer, Attention)
 
         # don't downsample if in_chs and out_chs are the same
-        self.downsample = nn.Identity() if in_chs == out_chs else Downsampling(
+        self.downsample = msnn.Identity() if in_chs == out_chs else Downsampling(
             in_chs,
             out_chs,
             kernel_size=3,
@@ -459,7 +464,8 @@             **dd,
         )
 
-        self.blocks = nn.Sequential(*[MetaFormerBlock(
+        self.blocks = msnn.SequentialCell([
+            [MetaFormerBlock(
             dim=out_chs,
             token_mixer=token_mixer,
             mlp_act=mlp_act,
@@ -472,13 +478,14 @@             use_nchw=self.use_nchw,
             **dd,
             **kwargs,
-        ) for i in range(depth)])
+        ) for i in range(depth)]
+        ])
 
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    def forward(self, x: Tensor):
+    def construct(self, x: ms.Tensor):
         x = self.downsample(x)
         B, C, H, W = x.shape
 
@@ -496,7 +503,7 @@         return x
 
 
-class MetaFormer(nn.Module):
+class MetaFormer(msnn.Cell):
     r""" MetaFormer
         A PyTorch impl of : `MetaFormer Baselines for Vision`  -
           https://arxiv.org/abs/2210.13452
@@ -529,17 +536,17 @@             global_pool: str = 'avg',
             depths: Tuple[int, ...] = (2, 2, 6, 2),
             dims: Tuple[int, ...] = (64, 128, 320, 512),
-            token_mixers: Union[Type[nn.Module], List[Type[nn.Module]]] = Pooling,
-            mlp_act: Type[nn.Module] = StarReLU,
+            token_mixers: Union[Type[msnn.Cell], List[Type[msnn.Cell]]] = Pooling,
+            mlp_act: Type[msnn.Cell] = StarReLU,
             mlp_bias: bool = False,
             drop_path_rate: float = 0.,
             proj_drop_rate: float = 0.,
             drop_rate: float = 0.0,
             layer_scale_init_values: Optional[Union[float, List[float]]] = None,
             res_scale_init_values: Union[Tuple[Optional[float], ...], List[Optional[float]]] = (None, None, 1.0, 1.0),
-            downsample_norm: Optional[Type[nn.Module]] = LayerNorm2dNoBias,
-            norm_layers: Union[Type[nn.Module], List[Type[nn.Module]]] = LayerNorm2dNoBias,
-            output_norm: Type[nn.Module] = LayerNorm2d,
+            downsample_norm: Optional[Type[msnn.Cell]] = LayerNorm2dNoBias,
+            norm_layers: Union[Type[msnn.Cell], List[Type[msnn.Cell]]] = LayerNorm2dNoBias,
+            output_norm: Type[msnn.Cell] = LayerNorm2d,
             use_mlp_head: bool = True,
             device=None,
             dtype=None,
@@ -604,7 +611,9 @@             prev_dim = dims[i]
             self.feature_info += [dict(num_chs=dims[i], reduction=2**(i+2), module=f'stages.{i}')]
 
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell([
+            stages
+        ])
 
         # if using MlpHead, dropout is handled by MlpHead
         if num_classes > 0:
@@ -616,15 +625,17 @@                 final = nn.Linear(self.num_features, num_classes, **dd)
                 self.head_hidden_size = self.num_features
         else:
-            final = nn.Identity()
-
-        self.head = nn.Sequential(OrderedDict([
+            final = msnn.Identity()
+
+        self.head = msnn.SequentialCell([
+            OrderedDict([
             ('global_pool', SelectAdaptivePool2d(pool_type=global_pool)),
             ('norm', output_norm(self.num_features, **dd)),
-            ('flatten', nn.Flatten(1) if global_pool else nn.Identity()),
-            ('drop', nn.Dropout(drop_rate) if self.use_mlp_head else nn.Identity()),
+            ('flatten', mint.flatten(1) if global_pool else msnn.Identity()),
+            ('drop', nn.Dropout(drop_rate) if self.use_mlp_head else msnn.Identity()),
             ('fc', final)
-        ]))
+        ])
+        ])
 
         self.apply(self._init_weights)
 
@@ -632,7 +643,7 @@         if isinstance(m, (nn.Conv2d, nn.Linear)):
             trunc_normal_(m.weight, std=.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def set_grad_checkpointing(self, enable=True):
@@ -641,7 +652,7 @@             stage.set_grad_checkpointing(enable=enable)
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None, device=None, dtype=None):
@@ -649,25 +660,25 @@         self.num_classes = num_classes
         if global_pool is not None:
             self.head.global_pool = SelectAdaptivePool2d(pool_type=global_pool)
-            self.head.flatten = nn.Flatten(1) if global_pool else nn.Identity()
+            self.head.flatten = mint.flatten(1) if global_pool else msnn.Identity()
         if num_classes > 0:
             if self.use_mlp_head:
                 final = MlpHead(self.num_features, num_classes, drop_rate=self.drop_rate, **dd)
             else:
                 final = nn.Linear(self.num_features, num_classes, **dd)
         else:
-            final = nn.Identity()
+            final = msnn.Identity()
         self.head.fc = final
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -718,7 +729,7 @@             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_head(self, x: Tensor, pre_logits: bool = False):
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False):
         # NOTE nn.Sequential in head broken down since can't call head[:-1](x) in torchscript :(
         x = self.head.global_pool(x)
         x = self.head.norm(x)
@@ -726,7 +737,7 @@         x = self.head.drop(x)
         return x if pre_logits else self.head.fc(x)
 
-    def forward_features(self, x: Tensor):
+    def forward_features(self, x: ms.Tensor):
         x = self.stem(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stages, x)
@@ -734,7 +745,7 @@             x = self.stages(x)
         return x
 
-    def forward(self, x: Tensor):
+    def construct(self, x: ms.Tensor):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
