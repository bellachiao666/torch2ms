--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ MLP-Mixer, ResMLP, and gMLP in PyTorch
 
 This impl originally based on MLP-Mixer paper.
@@ -42,8 +47,8 @@ from functools import partial
 from typing import Any, Dict, List, Optional, Type, Union, Tuple
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import PatchEmbed, Mlp, GluMlp, GatedMlp, DropPath, lecun_normal_, to_2tuple
@@ -56,7 +61,7 @@ __all__ = ['MixerBlock', 'MlpMixer']  # model_registry will add each entrypoint fn to this
 
 
-class MixerBlock(nn.Module):
+class MixerBlock(msnn.Cell):
     """Residual Block w/ token mixing and channel MLPs.
 
     Based on: 'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
@@ -66,9 +71,9 @@             dim: int,
             seq_len: int,
             mlp_ratio: Union[float, Tuple[float, float]] = (0.5, 4.0),
-            mlp_layer: Type[nn.Module] = Mlp,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
+            mlp_layer: Type[msnn.Cell] = Mlp,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop: float = 0.,
             drop_path: float = 0.,
             device=None,
@@ -91,18 +96,18 @@         tokens_dim, channels_dim = [int(x * dim) for x in to_2tuple(mlp_ratio)]
         self.norm1 = norm_layer(dim, **dd)
         self.mlp_tokens = mlp_layer(seq_len, tokens_dim, act_layer=act_layer, drop=drop, **dd)
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         self.norm2 = norm_layer(dim, **dd)
         self.mlp_channels = mlp_layer(dim, channels_dim, act_layer=act_layer, drop=drop, **dd)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = x + self.drop_path(self.mlp_tokens(self.norm1(x).transpose(1, 2)).transpose(1, 2))
         x = x + self.drop_path(self.mlp_channels(self.norm2(x)))
         return x
 
 
-class Affine(nn.Module):
+class Affine(msnn.Cell):
     """Affine transformation layer."""
 
     def __init__(self, dim: int, device=None, dtype=None) -> None:
@@ -113,15 +118,15 @@         """
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.alpha = nn.Parameter(torch.ones((1, 1, dim), **dd))
-        self.beta = nn.Parameter(torch.zeros((1, 1, dim), **dd))
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.alpha = ms.Parameter(mint.ones((1, 1, dim), **dd))
+        self.beta = ms.Parameter(mint.zeros((1, 1, dim), **dd))
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Apply affine transformation."""
-        return torch.addcmul(self.beta, self.alpha, x)
-
-
-class ResBlock(nn.Module):
+        return torch.addcmul(self.beta, self.alpha, x)  # 'torch.addcmul' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+
+class ResBlock(msnn.Cell):
     """Residual MLP block w/ LayerScale and Affine 'norm'.
 
     Based on: `ResMLP: Feedforward networks for image classification...` - https://arxiv.org/abs/2105.03404
@@ -131,9 +136,9 @@             dim: int,
             seq_len: int,
             mlp_ratio: float = 4,
-            mlp_layer: Type[nn.Module] = Mlp,
-            norm_layer: Type[nn.Module] = Affine,
-            act_layer: Type[nn.Module] = nn.GELU,
+            mlp_layer: Type[msnn.Cell] = Mlp,
+            norm_layer: Type[msnn.Cell] = Affine,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             init_values: float = 1e-4,
             drop: float = 0.,
             drop_path: float = 0.,
@@ -158,20 +163,20 @@         channel_dim = int(dim * mlp_ratio)
         self.norm1 = norm_layer(dim, **dd)
         self.linear_tokens = nn.Linear(seq_len, seq_len, **dd)
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         self.norm2 = norm_layer(dim, **dd)
         self.mlp_channels = mlp_layer(dim, channel_dim, act_layer=act_layer, drop=drop, **dd)
-        self.ls1 = nn.Parameter(init_values * torch.ones(dim, **dd))
-        self.ls2 = nn.Parameter(init_values * torch.ones(dim, **dd))
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.ls1 = ms.Parameter(init_values * mint.ones(dim, **dd))
+        self.ls2 = ms.Parameter(init_values * mint.ones(dim, **dd))
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = x + self.drop_path(self.ls1 * self.linear_tokens(self.norm1(x).transpose(1, 2)).transpose(1, 2))
         x = x + self.drop_path(self.ls2 * self.mlp_channels(self.norm2(x)))
         return x
 
 
-class SpatialGatingUnit(nn.Module):
+class SpatialGatingUnit(msnn.Cell):
     """Spatial Gating Unit.
 
     Based on: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050
@@ -180,7 +185,7 @@             self,
             dim: int,
             seq_len: int,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
             device=None,
             dtype=None,
     ) -> None:
@@ -200,10 +205,10 @@     def init_weights(self) -> None:
         """Initialize weights for projection gate."""
         # special init for the projection gate, called as override by base model init
-        nn.init.normal_(self.proj.weight, std=1e-6)
-        nn.init.ones_(self.proj.bias)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        nn.init.normal_(self.proj.weight, std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.ones_(self.proj.bias)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Apply spatial gating."""
         u, v = x.chunk(2, dim=-1)
         v = self.norm(v)
@@ -211,7 +216,7 @@         return u * v.transpose(-1, -2)
 
 
-class SpatialGatingBlock(nn.Module):
+class SpatialGatingBlock(msnn.Cell):
     """Residual Block w/ Spatial Gating.
 
     Based on: `Pay Attention to MLPs` - https://arxiv.org/abs/2105.08050
@@ -221,9 +226,9 @@             dim: int,
             seq_len: int,
             mlp_ratio: float = 4,
-            mlp_layer: Type[nn.Module] = GatedMlp,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
+            mlp_layer: Type[msnn.Cell] = GatedMlp,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop: float = 0.,
             drop_path: float = 0.,
             device=None,
@@ -254,15 +259,15 @@             drop=drop,
             **dd,
         )
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = x + self.drop_path(self.mlp_channels(self.norm(x)))
         return x
 
 
-class MlpMixer(nn.Module):
+class MlpMixer(msnn.Cell):
     """MLP-Mixer model architecture.
 
     Based on: 'MLP-Mixer: An all-MLP Architecture for Vision' - https://arxiv.org/abs/2105.01601
@@ -277,10 +282,10 @@             num_blocks: int = 8,
             embed_dim: int = 512,
             mlp_ratio: Union[float, Tuple[float, float]] = (0.5, 4.0),
-            block_layer: Type[nn.Module] = MixerBlock,
-            mlp_layer: Type[nn.Module] = Mlp,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
+            block_layer: Type[msnn.Cell] = MixerBlock,
+            mlp_layer: Type[msnn.Cell] = Mlp,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop_rate: float = 0.,
             proj_drop_rate: float = 0.,
             drop_path_rate: float = 0.,
@@ -328,7 +333,8 @@         )
         reduction = self.stem.feat_ratio() if hasattr(self.stem, 'feat_ratio') else patch_size
         # FIXME drop_path (stochastic depth scaling rule or all the same?)
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell([
+            [
             block_layer(
                 embed_dim,
                 self.stem.num_patches,
@@ -340,12 +346,13 @@                 drop_path=drop_path_rate,
                 **dd,
             )
-            for _ in range(num_blocks)])
+            for _ in range(num_blocks)]
+        ])
         self.feature_info = [
             dict(module=f'blocks.{i}', num_chs=embed_dim, reduction=reduction) for i in range(num_blocks)]
         self.norm = norm_layer(embed_dim, **dd)
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(embed_dim, self.num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(embed_dim, self.num_classes, **dd) if num_classes > 0 else msnn.Identity()
 
         self.init_weights(nlhb=nlhb)
 
@@ -384,7 +391,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         """Get the classifier module."""
         return self.head
 
@@ -400,17 +407,17 @@             assert global_pool in ('', 'avg')
             self.global_pool = global_pool
         device, dtype = self.head.weight.device, self.head.weight.dtype if hasattr(self.head, 'weight') else (None, None)
-        self.head = nn.Linear(self.embed_dim, num_classes, device=device, dtype=dtype) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.embed_dim, num_classes, dtype = dtype) if num_classes > 0 else msnn.Identity()  # 'torch.nn.Linear':没有对应的mindspore参数 'device' (position 3);
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """Forward features that returns intermediates.
 
         Args:
@@ -478,12 +485,12 @@         take_indices, max_index = feature_take_indices(len(self.blocks), indices)
         self.blocks = self.blocks[:max_index + 1]  # truncate blocks
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers."""
         x = self.stem(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
@@ -493,7 +500,7 @@         x = self.norm(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -508,14 +515,14 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-def _init_weights(module: nn.Module, name: str, head_bias: float = 0., flax: bool = False) -> None:
+def _init_weights(module: msnn.Cell, name: str, head_bias: float = 0., flax: bool = False) -> None:
     """Mixer weight initialization (trying to match Flax defaults).
 
     Args:
@@ -526,29 +533,29 @@     """
     if isinstance(module, nn.Linear):
         if name.startswith('head'):
-            nn.init.zeros_(module.weight)
-            nn.init.constant_(module.bias, head_bias)
+            nn.init.zeros_(module.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(module.bias, head_bias)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             if flax:
                 # Flax defaults
                 lecun_normal_(module.weight)
                 if module.bias is not None:
-                    nn.init.zeros_(module.bias)
+                    nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
                 # like MLP init in vit (my original init)
-                nn.init.xavier_uniform_(module.weight)
+                nn.init.xavier_uniform_(module.weight)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 if module.bias is not None:
                     if 'mlp' in name:
-                        nn.init.normal_(module.bias, std=1e-6)
+                        nn.init.normal_(module.bias, std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                     else:
-                        nn.init.zeros_(module.bias)
+                        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Conv2d):
         lecun_normal_(module.weight)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):
-        nn.init.ones_(module.weight)
-        nn.init.zeros_(module.bias)
+        nn.init.ones_(module.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         # NOTE if a parent module contains init_weights method, it can override the init of the
         # child modules as this will be called in depth-first order.
