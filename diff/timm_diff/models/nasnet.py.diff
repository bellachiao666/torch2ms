--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ NasNet-A (Large)
  nasnetalarge implementation grabbed from Cadene's pretrained models
  https://github.com/Cadene/pretrained-models.pytorch
@@ -5,8 +10,8 @@ from functools import partial
 from typing import Optional, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier
 from ._builder import build_model_with_cfg
@@ -16,7 +21,7 @@ 
 
 
-class ActConvBn(nn.Module):
+class ActConvBn(msnn.Cell):
 
     def __init__(
             self,
@@ -35,14 +40,14 @@             in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, **dd)
         self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.act(x)
         x = self.conv(x)
         x = self.bn(x)
         return x
 
 
-class SeparableConv2d(nn.Module):
+class SeparableConv2d(msnn.Cell):
 
     def __init__(
             self,
@@ -73,13 +78,13 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.depthwise_conv2d(x)
         x = self.pointwise_conv2d(x)
         return x
 
 
-class BranchSeparables(nn.Module):
+class BranchSeparables(msnn.Cell):
 
     def __init__(
             self,
@@ -99,12 +104,12 @@         self.separable_1 = SeparableConv2d(
             in_channels, middle_channels, kernel_size, stride=stride, padding=pad_type, **dd)
         self.bn_sep_1 = nn.BatchNorm2d(middle_channels, eps=0.001, momentum=0.1, **dd)
-        self.act_2 = nn.ReLU(inplace=True)
+        self.act_2 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.separable_2 = SeparableConv2d(
             middle_channels, out_channels, kernel_size, stride=1, padding=pad_type, **dd)
         self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.act_1(x)
         x = self.separable_1(x)
         x = self.bn_sep_1(x)
@@ -114,7 +119,7 @@         return x
 
 
-class CellStem0(nn.Module):
+class CellStem0(msnn.Cell):
     def __init__(
             self,
             stem_size: int,
@@ -148,7 +153,7 @@             self.num_channels, self.num_channels, 3, 1, pad_type, **dd)
         self.comb_iter_4_right = create_pool2d('max', 3, 2, padding=pad_type)
 
-    def forward(self, x):
+    def construct(self, x):
         x1 = self.conv_1x1(x)
 
         x_comb_iter_0_left = self.comb_iter_0_left(x1)
@@ -170,11 +175,11 @@         x_comb_iter_4_right = self.comb_iter_4_right(x1)
         x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right
 
-        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
+        x_out = mint.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
         return x_out
 
 
-class CellStem1(nn.Module):
+class CellStem1(msnn.Cell):
 
     def __init__(
             self,
@@ -191,13 +196,13 @@         self.conv_1x1 = ActConvBn(2 * self.num_channels, self.num_channels, 1, stride=1, **dd)
 
         self.act = nn.ReLU()
-        self.path_1 = nn.Sequential()
-        self.path_1.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))
+        self.path_1 = msnn.SequentialCell()
+        self.path_1.add_module('avgpool', nn.AvgPool2d(1, stride = 2, count_include_pad = False))
         self.path_1.add_module('conv', nn.Conv2d(self.stem_size, self.num_channels // 2, 1, stride=1, bias=False, **dd))
 
-        self.path_2 = nn.Sequential()
+        self.path_2 = msnn.SequentialCell()
         self.path_2.add_module('pad', nn.ZeroPad2d((-1, 1, -1, 1)))
-        self.path_2.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))
+        self.path_2.add_module('avgpool', nn.AvgPool2d(1, stride = 2, count_include_pad = False))
         self.path_2.add_module('conv', nn.Conv2d(self.stem_size, self.num_channels // 2, 1, stride=1, bias=False, **dd))
 
         self.final_path_bn = nn.BatchNorm2d(self.num_channels, eps=0.001, momentum=0.1, **dd)
@@ -216,7 +221,7 @@         self.comb_iter_4_left = BranchSeparables(self.num_channels, self.num_channels, 3, 1, pad_type, **dd)
         self.comb_iter_4_right = create_pool2d('max', 3, 2, padding=pad_type)
 
-    def forward(self, x_conv0, x_stem_0):
+    def construct(self, x_conv0, x_stem_0):
         x_left = self.conv_1x1(x_stem_0)
 
         x_relu = self.act(x_conv0)
@@ -225,7 +230,7 @@         # path 2
         x_path2 = self.path_2(x_relu)
         # final path
-        x_right = self.final_path_bn(torch.cat([x_path1, x_path2], 1))
+        x_right = self.final_path_bn(mint.cat([x_path1, x_path2], 1))
 
         x_comb_iter_0_left = self.comb_iter_0_left(x_left)
         x_comb_iter_0_right = self.comb_iter_0_right(x_right)
@@ -246,11 +251,11 @@         x_comb_iter_4_right = self.comb_iter_4_right(x_left)
         x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right
 
-        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
+        x_out = mint.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
         return x_out
 
 
-class FirstCell(nn.Module):
+class FirstCell(msnn.Cell):
 
     def __init__(
             self,
@@ -267,13 +272,13 @@         self.conv_1x1 = ActConvBn(in_chs_right, out_chs_right, 1, stride=1, **dd)
 
         self.act = nn.ReLU()
-        self.path_1 = nn.Sequential()
-        self.path_1.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))
+        self.path_1 = msnn.SequentialCell()
+        self.path_1.add_module('avgpool', nn.AvgPool2d(1, stride = 2, count_include_pad = False))
         self.path_1.add_module('conv', nn.Conv2d(in_chs_left, out_chs_left, 1, stride=1, bias=False, **dd))
 
-        self.path_2 = nn.Sequential()
+        self.path_2 = msnn.SequentialCell()
         self.path_2.add_module('pad', nn.ZeroPad2d((-1, 1, -1, 1)))
-        self.path_2.add_module('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False))
+        self.path_2.add_module('avgpool', nn.AvgPool2d(1, stride = 2, count_include_pad = False))
         self.path_2.add_module('conv', nn.Conv2d(in_chs_left, out_chs_left, 1, stride=1, bias=False, **dd))
 
         self.final_path_bn = nn.BatchNorm2d(out_chs_left * 2, eps=0.001, momentum=0.1, **dd)
@@ -291,11 +296,11 @@ 
         self.comb_iter_4_left = BranchSeparables(out_chs_right, out_chs_right, 3, 1, pad_type, **dd)
 
-    def forward(self, x, x_prev):
+    def construct(self, x, x_prev):
         x_relu = self.act(x_prev)
         x_path1 = self.path_1(x_relu)
         x_path2 = self.path_2(x_relu)
-        x_left = self.final_path_bn(torch.cat([x_path1, x_path2], 1))
+        x_left = self.final_path_bn(mint.cat([x_path1, x_path2], 1))
         x_right = self.conv_1x1(x)
 
         x_comb_iter_0_left = self.comb_iter_0_left(x_right)
@@ -316,11 +321,11 @@         x_comb_iter_4_left = self.comb_iter_4_left(x_right)
         x_comb_iter_4 = x_comb_iter_4_left + x_right
 
-        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
+        x_out = mint.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
         return x_out
 
 
-class NormalCell(nn.Module):
+class NormalCell(msnn.Cell):
 
     def __init__(
             self,
@@ -350,7 +355,7 @@ 
         self.comb_iter_4_left = BranchSeparables(out_chs_right, out_chs_right, 3, 1, pad_type, **dd)
 
-    def forward(self, x, x_prev):
+    def construct(self, x, x_prev):
         x_left = self.conv_prev_1x1(x_prev)
         x_right = self.conv_1x1(x)
 
@@ -372,11 +377,11 @@         x_comb_iter_4_left = self.comb_iter_4_left(x_right)
         x_comb_iter_4 = x_comb_iter_4_left + x_right
 
-        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
+        x_out = mint.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
         return x_out
 
 
-class ReductionCell0(nn.Module):
+class ReductionCell0(msnn.Cell):
 
     def __init__(
             self,
@@ -407,7 +412,7 @@         self.comb_iter_4_left = BranchSeparables(out_chs_right, out_chs_right, 3, 1, pad_type, **dd)
         self.comb_iter_4_right = create_pool2d('max', 3, 2, padding=pad_type)
 
-    def forward(self, x, x_prev):
+    def construct(self, x, x_prev):
         x_left = self.conv_prev_1x1(x_prev)
         x_right = self.conv_1x1(x)
 
@@ -430,11 +435,11 @@         x_comb_iter_4_right = self.comb_iter_4_right(x_right)
         x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right
 
-        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
+        x_out = mint.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
         return x_out
 
 
-class ReductionCell1(nn.Module):
+class ReductionCell1(msnn.Cell):
 
     def __init__(
             self,
@@ -465,7 +470,7 @@         self.comb_iter_4_left = BranchSeparables(out_chs_right, out_chs_right, 3, 1, pad_type, **dd)
         self.comb_iter_4_right = create_pool2d('max', 3, 2, padding=pad_type)
 
-    def forward(self, x, x_prev):
+    def construct(self, x, x_prev):
         x_left = self.conv_prev_1x1(x_prev)
         x_right = self.conv_1x1(x)
 
@@ -488,11 +493,11 @@         x_comb_iter_4_right = self.comb_iter_4_right(x_right)
         x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right
 
-        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
+        x_out = mint.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
         return x_out
 
 
-class NASNetALarge(nn.Module):
+class NASNetALarge(msnn.Cell):
     """NASNetALarge (6 @ 4032) """
 
     def __init__(
@@ -598,7 +603,7 @@         self.cell_17 = NormalCell(
             in_chs_left=24 * channels, out_chs_left=4 * channels,
             in_chs_right=24 * channels, out_chs_right=4 * channels, pad_type=pad_type, **dd)
-        self.act = nn.ReLU(inplace=True)
+        self.act = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.feature_info = [
             dict(num_chs=96, reduction=2, module='conv0'),
             dict(num_chs=168, reduction=4, module='cell_stem_1.conv_1x1.act'),
@@ -632,7 +637,7 @@         assert not enable, 'gradient checkpointing not supported'
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.last_linear
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -676,7 +681,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.last_linear(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
