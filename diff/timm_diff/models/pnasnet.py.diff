--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
  pnasnet5large implementation grabbed from Cadene's pretrained models
  Additional credit to https://github.com/creafz
@@ -9,8 +14,8 @@ from functools import partial
 from typing import Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.layers import ConvNormAct, create_conv2d, create_pool2d, create_classifier
 from ._builder import build_model_with_cfg
@@ -19,7 +24,7 @@ __all__ = ['PNASNet5Large']
 
 
-class SeparableConv2d(nn.Module):
+class SeparableConv2d(msnn.Cell):
 
     def __init__(
             self,
@@ -50,13 +55,13 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.depthwise_conv2d(x)
         x = self.pointwise_conv2d(x)
         return x
 
 
-class BranchSeparables(nn.Module):
+class BranchSeparables(msnn.Cell):
 
     def __init__(
             self,
@@ -93,7 +98,7 @@         )
         self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.act_1(x)
         x = self.separable_1(x)
         x = self.bn_sep_1(x)
@@ -103,7 +108,7 @@         return x
 
 
-class ActConvBn(nn.Module):
+class ActConvBn(msnn.Cell):
 
     def __init__(
             self,
@@ -128,14 +133,14 @@         )
         self.bn = nn.BatchNorm2d(out_channels, eps=0.001, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.act(x)
         x = self.conv(x)
         x = self.bn(x)
         return x
 
 
-class FactorizedReduction(nn.Module):
+class FactorizedReduction(msnn.Cell):
 
     def __init__(
             self,
@@ -148,26 +153,30 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.act = nn.ReLU()
-        self.path_1 = nn.Sequential(OrderedDict([
-            ('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False)),
+        self.path_1 = msnn.SequentialCell([
+            OrderedDict([
+            ('avgpool', nn.AvgPool2d(1, stride = 2, count_include_pad = False)),
             ('conv', create_conv2d(in_channels, out_channels // 2, kernel_size=1, padding=padding, **dd)),
-        ]))
-        self.path_2 = nn.Sequential(OrderedDict([
+        ])
+        ])
+        self.path_2 = msnn.SequentialCell([
+            OrderedDict([
             ('pad', nn.ZeroPad2d((-1, 1, -1, 1))),  # shift
-            ('avgpool', nn.AvgPool2d(1, stride=2, count_include_pad=False)),
+            ('avgpool', nn.AvgPool2d(1, stride = 2, count_include_pad = False)),
             ('conv', create_conv2d(in_channels, out_channels // 2, kernel_size=1, padding=padding, **dd)),
-        ]))
+        ])
+        ])
         self.final_path_bn = nn.BatchNorm2d(out_channels, eps=0.001, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.act(x)
         x_path1 = self.path_1(x)
         x_path2 = self.path_2(x)
-        out = self.final_path_bn(torch.cat([x_path1, x_path2], 1))
+        out = self.final_path_bn(mint.cat([x_path1, x_path2], 1))
         return out
 
 
-class CellBase(nn.Module):
+class CellBase(msnn.Cell):
 
     def cell_forward(self, x_left, x_right):
         x_comb_iter_0_left = self.comb_iter_0_left(x_left)
@@ -193,7 +202,7 @@             x_comb_iter_4_right = x_right
         x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right
 
-        x_out = torch.cat([x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
+        x_out = mint.cat([x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)
         return x_out
 
 
@@ -215,11 +224,13 @@ 
         self.comb_iter_0_left = BranchSeparables(
             in_chs_left, out_chs_left, kernel_size=5, stride=2, stem_cell=True, padding=pad_type, **dd)
-        self.comb_iter_0_right = nn.Sequential(OrderedDict([
+        self.comb_iter_0_right = msnn.SequentialCell([
+            OrderedDict([
             ('max_pool', create_pool2d('max', 3, stride=2, padding=pad_type)),
             ('conv', create_conv2d(in_chs_left, out_chs_left, kernel_size=1, padding=pad_type, **dd)),
             ('bn', nn.BatchNorm2d(out_chs_left, eps=0.001, **dd)),
-        ]))
+        ])
+        ])
 
         self.comb_iter_1_left = BranchSeparables(
             out_chs_right, out_chs_right, kernel_size=7, stride=2, padding=pad_type, **dd)
@@ -307,7 +318,7 @@         return x_out
 
 
-class PNASNet5Large(nn.Module):
+class PNASNet5Large(msnn.Cell):
     def __init__(
             self,
             num_classes: int = 1000,
@@ -387,7 +398,7 @@         assert not enable, 'gradient checkpointing not supported'
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.last_linear
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg', device=None, dtype=None):
@@ -420,7 +431,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.last_linear(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
