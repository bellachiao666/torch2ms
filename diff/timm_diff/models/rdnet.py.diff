--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 RDNet
 Copyright (c) 2024-present NAVER Cloud Corp.
@@ -7,8 +12,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Callable, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, calculate_drop_path_rates, NormMlpClassifierHead, ClassifierHead, EffectiveSEModule, \
@@ -21,7 +26,7 @@ __all__ = ["RDNet"]
 
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -34,19 +39,20 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.layers = nn.Sequential(
+        self.layers = msnn.SequentialCell(
+            [
             nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3, **dd),
             norm_layer(in_chs, **dd),
             nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0, **dd),
             act_layer(),
-            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0, **dd),
-        )
-
-    def forward(self, x):
+            nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0, **dd)
+        ])
+
+    def construct(self, x):
         return self.layers(x)
 
 
-class BlockESE(nn.Module):
+class BlockESE(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -59,16 +65,17 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.layers = nn.Sequential(
+        self.layers = msnn.SequentialCell(
+            [
             nn.Conv2d(in_chs, in_chs, groups=in_chs, kernel_size=7, stride=1, padding=3, **dd),
             norm_layer(in_chs, **dd),
             nn.Conv2d(in_chs, inter_chs, kernel_size=1, stride=1, padding=0, **dd),
             act_layer(),
             nn.Conv2d(inter_chs, out_chs, kernel_size=1, stride=1, padding=0, **dd),
-            EffectiveSEModule(out_chs, **dd),
-        )
-
-    def forward(self, x):
+            EffectiveSEModule(out_chs, **dd)
+        ])
+
+    def construct(self, x):
         return self.layers(x)
 
 
@@ -82,7 +89,7 @@         assert False, f"Unknown block type ({block})."
 
 
-class DenseBlock(nn.Module):
+class DenseBlock(msnn.Cell):
     def __init__(
             self,
             num_input_features: int = 64,
@@ -107,7 +114,7 @@         self.block_idx = block_idx
         self.growth_rate = growth_rate
 
-        self.gamma = nn.Parameter(ls_init_value * torch.ones(growth_rate, **dd)) if ls_init_value > 0 else None
+        self.gamma = ms.Parameter(ls_init_value * mint.ones(growth_rate, **dd)) if ls_init_value > 0 else None
         growth_rate = int(growth_rate)
         inter_chs = int(num_input_features * bottleneck_width_ratio / 8) * 8
 
@@ -122,8 +129,8 @@             **dd,
         )
 
-    def forward(self, x: List[torch.Tensor]) -> torch.Tensor:
-        x = torch.cat(x, 1)
+    def construct(self, x: List[torch.Tensor]) -> ms.Tensor:
+        x = mint.cat(x, 1)
         x = self.layers(x)
 
         if self.gamma is not None:
@@ -133,7 +140,7 @@         return x
 
 
-class DenseStage(nn.Sequential):
+class DenseStage(msnn.SequentialCell):
     def __init__(
             self,
             num_block: int,
@@ -159,15 +166,15 @@             self.add_module(f"dense_block{i}", layer)
         self.num_out_features = num_input_features
 
-    def forward(self, init_feature: torch.Tensor) -> torch.Tensor:
+    def forward(self, init_feature: ms.Tensor) -> ms.Tensor:
         features = [init_feature]
         for module in self:
             new_feature = module(features)
             features.append(new_feature)
-        return torch.cat(features, 1)
-
-
-class RDNet(nn.Module):
+        return mint.cat(features, 1)
+
+
+class RDNet(msnn.Cell):
     def __init__(
             self,
             in_chans: int = 3,  # timm option [--in-chans]
@@ -232,18 +239,20 @@         assert stem_type in ('patch', 'overlap', 'overlap_tiered')
         if stem_type == 'patch':
             # NOTE: this stem is a minimal form of ViT PatchEmbed, as used in SwinTransformer w/ patch_size = 4
-            self.stem = nn.Sequential(
+            self.stem = msnn.SequentialCell(
+                [
                 nn.Conv2d(in_chans, num_init_features, kernel_size=patch_size, stride=patch_size, bias=conv_bias, **dd),
-                norm_layer(num_init_features, **dd),
-            )
+                norm_layer(num_init_features, **dd)
+            ])
             stem_stride = patch_size
         else:
             mid_chs = make_divisible(num_init_features // 2) if 'tiered' in stem_type else num_init_features
-            self.stem = nn.Sequential(
+            self.stem = msnn.SequentialCell(
+                [
                 nn.Conv2d(in_chans, mid_chs, kernel_size=3, stride=2, padding=1, bias=conv_bias, **dd),
                 nn.Conv2d(mid_chs, num_init_features, kernel_size=3, stride=2, padding=1, bias=conv_bias, **dd),
-                norm_layer(num_init_features, **dd),
-            )
+                norm_layer(num_init_features, **dd)
+            ])
             stem_stride = 4
 
         # features
@@ -299,8 +308,12 @@                         growth_rate=growth_rates[i],
                     )
                 ]
-            dense_stages.append(nn.Sequential(*dense_stage_layers))
-        self.dense_stages = nn.Sequential(*dense_stages)
+            dense_stages.append(msnn.SequentialCell([
+                dense_stage_layers
+            ]))
+        self.dense_stages = msnn.SequentialCell([
+            dense_stages
+        ])
         self.num_features = self.head_hidden_size = num_features
 
         # if head_norm_first == true, norm -> global pool -> fc ordering, like most other nets
@@ -315,7 +328,7 @@                 **dd,
             )
         else:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
             self.head = NormMlpClassifierHead(
                 self.num_features,
                 num_classes,
@@ -340,6 +353,7 @@         for s in self.dense_stages:
             s.grad_checkpointing = enable
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         return self.head.fc
@@ -350,7 +364,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
@@ -412,7 +426,7 @@         max_index = stage_ends[max_index]
         self.dense_stages = self.dense_stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm_pre = nn.Identity()
+            self.norm_pre = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
@@ -426,7 +440,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -434,12 +448,12 @@ 
 def _init_weights(module, name=None, head_init_scale=1.0):
     if isinstance(module, nn.Conv2d):
-        nn.init.kaiming_normal_(module.weight)
+        nn.init.kaiming_normal_(module.weight)  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.BatchNorm2d):
-        nn.init.constant_(module.weight, 1)
-        nn.init.constant_(module.bias, 0)
+        nn.init.constant_(module.weight, 1)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.constant_(module.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Linear):
-        nn.init.constant_(module.bias, 0)
+        nn.init.constant_(module.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if name and 'head.' in name:
             module.weight.data.mul_(head_init_scale)
             module.bias.data.mul_(head_init_scale)
