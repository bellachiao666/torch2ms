--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """RegNet X, Y, Z, and more
 
 Paper: `Designing Network Design Spaces` - https://arxiv.org/abs/2003.13678
@@ -28,8 +33,8 @@ from functools import partial
 from typing import Any, Callable, Dict, List, Optional, Union, Tuple, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ClassifierHead, AvgPool2dSame, ConvNormAct, SEModule, DropPath, GroupNormAct, calculate_drop_path_rates
@@ -128,11 +133,11 @@     # TODO dWr scaling?
     # depth = int(depth * (scale ** 0.1))
     # width_scale = scale ** 0.4  # dWr scale, exp 0.8 / 2, applied to both group and layer widths
-    widths_cont = torch.arange(depth, dtype=torch.float32) * width_slope + width_initial
-    width_exps = torch.round(torch.log(widths_cont / width_initial) / math.log(width_mult))
-    widths = torch.round((width_initial * torch.pow(width_mult, width_exps)) / quant) * quant
-    num_stages, max_stage = len(torch.unique(widths)), int(width_exps.max().item()) + 1
-    groups = torch.tensor([group_size for _ in range(num_stages)], dtype=torch.int32)
+    widths_cont = mint.arange(depth, dtype = ms.float32) * width_slope + width_initial
+    width_exps = ms.Tensor.round(mint.log(widths_cont / width_initial) / math.log(width_mult))
+    widths = ms.Tensor.round((width_initial * mint.pow(width_mult, width_exps)) / quant) * quant
+    num_stages, max_stage = len(mint.unique(widths)), int(width_exps.max().item()) + 1
+    groups = ms.Tensor([group_size for _ in range(num_stages)], dtype = ms.int32)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;
     return widths.int().tolist(), num_stages, groups.tolist()
 
 
@@ -142,11 +147,11 @@         kernel_size: int = 1,
         stride: int = 1,
         dilation: int = 1,
-        norm_layer: Optional[Type[nn.Module]] = None,
+        norm_layer: Optional[Type[msnn.Cell]] = None,
         preact: bool = False,
         device=None,
         dtype=None,
-) -> nn.Module:
+) -> msnn.Cell:
     """Create convolutional downsampling module.
 
     Args:
@@ -193,11 +198,11 @@         kernel_size: int = 1,
         stride: int = 1,
         dilation: int = 1,
-        norm_layer: Optional[Type[nn.Module]] = None,
+        norm_layer: Optional[Type[msnn.Cell]] = None,
         preact: bool = False,
         device=None,
         dtype=None,
-) -> nn.Sequential:
+) -> msnn.SequentialCell:
     """Create average pool downsampling module.
 
     AvgPool Downsampling as in 'D' ResNet variants. This is not in RegNet space but I might experiment.
@@ -217,7 +222,7 @@     dd = {'device': device, 'dtype': dtype}
     norm_layer = norm_layer or nn.BatchNorm2d
     avg_stride = stride if dilation == 1 else 1
-    pool = nn.Identity()
+    pool = msnn.Identity()
     if stride > 1 or dilation > 1:
         avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
         pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
@@ -225,7 +230,7 @@         conv = create_conv2d(in_chs, out_chs, 1, stride=1, **dd)
     else:
         conv = ConvNormAct(in_chs, out_chs, 1, stride=1, norm_layer=norm_layer, apply_act=False, **dd)
-    return nn.Sequential(*[pool, conv])
+    return msnn.SequentialCell(*[pool, conv])
 
 
 def create_shortcut(
@@ -235,11 +240,11 @@         kernel_size: int,
         stride: int,
         dilation: Tuple[int, int] = (1, 1),
-        norm_layer: Optional[Type[nn.Module]] = None,
+        norm_layer: Optional[Type[msnn.Cell]] = None,
         preact: bool = False,
         device=None,
         dtype=None,
-) -> Optional[nn.Module]:
+) -> Optional[msnn.Cell]:
     """Create shortcut connection for residual blocks.
 
     Args:
@@ -266,10 +271,10 @@         else:
             return downsample_conv(in_chs, out_chs, kernel_size=kernel_size, **dargs)
     else:
-        return nn.Identity()  # identity shortcut (no downsample)
-
-
-class Bottleneck(nn.Module):
+        return msnn.Identity()  # identity shortcut (no downsample)
+
+
+class Bottleneck(msnn.Cell):
     """RegNet Bottleneck block.
 
     This is almost exactly the same as a ResNet Bottleneck. The main difference is the SE block is moved from
@@ -287,9 +292,9 @@             se_ratio: float = 0.25,
             downsample: str = 'conv1x1',
             linear_out: bool = False,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            drop_block: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            drop_block: Optional[Type[msnn.Cell]] = None,
             drop_path_rate: float = 0.,
             device=None,
             dtype=None,
@@ -334,9 +339,9 @@             se_channels = int(round(in_chs * se_ratio))
             self.se = SEModule(bottleneck_chs, rd_channels=se_channels, act_layer=act_layer, **dd)
         else:
-            self.se = nn.Identity()
+            self.se = msnn.Identity()
         self.conv3 = ConvNormAct(bottleneck_chs, out_chs, kernel_size=1, apply_act=False, **cargs, **dd)
-        self.act3 = nn.Identity() if linear_out else act_layer()
+        self.act3 = msnn.Identity() if linear_out else act_layer()
         self.downsample = create_shortcut(
             downsample,
             in_chs,
@@ -347,13 +352,13 @@             norm_layer=norm_layer,
             **dd,
         )
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
 
     def zero_init_last(self) -> None:
         """Zero-initialize the last batch norm in the block."""
-        nn.init.zeros_(self.conv3.bn.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        nn.init.zeros_(self.conv3.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -375,7 +380,7 @@         return x
 
 
-class PreBottleneck(nn.Module):
+class PreBottleneck(msnn.Cell):
     """Pre-activation RegNet Bottleneck block.
 
     Similar to Bottleneck but with pre-activation normalization.
@@ -392,9 +397,9 @@             se_ratio: float = 0.25,
             downsample: str = 'conv1x1',
             linear_out: bool = False,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            drop_block: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            drop_block: Optional[Type[msnn.Cell]] = None,
             drop_path_rate: float = 0.,
             device=None,
             dtype=None,
@@ -438,7 +443,7 @@             se_channels = int(round(in_chs * se_ratio))
             self.se = SEModule(bottleneck_chs, rd_channels=se_channels, act_layer=act_layer, **dd)
         else:
-            self.se = nn.Identity()
+            self.se = msnn.Identity()
         self.norm3 = norm_act_layer(bottleneck_chs, **dd)
         self.conv3 = create_conv2d(bottleneck_chs, out_chs, kernel_size=1, **dd)
         self.downsample = create_shortcut(
@@ -451,13 +456,13 @@             preact=True,
             **dd,
         )
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
 
     def zero_init_last(self) -> None:
         """Zero-initialize the last batch norm (no-op for pre-activation)."""
         pass
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -481,7 +486,7 @@         return x
 
 
-class RegStage(nn.Module):
+class RegStage(msnn.Cell):
     """RegNet stage (sequence of blocks with the same output shape).
 
     A stage consists of multiple bottleneck blocks with the same output dimensions.
@@ -495,7 +500,7 @@             stride: int,
             dilation: int,
             drop_path_rates: Optional[List[float]] = None,
-            block_fn: Type[nn.Module] = Bottleneck,
+            block_fn: Type[msnn.Cell] = Bottleneck,
             **block_kwargs,
     ):
         """Initialize RegNet stage.
@@ -533,7 +538,7 @@             )
             first_dilation = dilation
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through all blocks in the stage.
 
         Args:
@@ -550,7 +555,7 @@         return x
 
 
-class RegNet(nn.Module):
+class RegNet(msnn.Cell):
     """RegNet-X, Y, and Z Models.
 
     Paper: https://arxiv.org/abs/2003.13678
@@ -632,7 +637,7 @@             self.num_features = cfg.num_features
         else:
             final_act = cfg.linear_out or cfg.preact
-            self.final_conv = get_act_layer(cfg.act_layer)() if final_act else nn.Identity()
+            self.final_conv = get_act_layer(cfg.act_layer)() if final_act else msnn.Identity()
             self.num_features = prev_width
         self.head_hidden_size = self.num_features
         self.head = ClassifierHead(
@@ -667,7 +672,7 @@         widths, num_stages, stage_gs = generate_regnet(cfg.wa, cfg.w0, cfg.wm, cfg.depth, cfg.group_size)
 
         # Convert to per stage format
-        stage_widths, stage_depths = torch.unique(torch.tensor(widths), return_counts=True)
+        stage_widths, stage_depths = mint.unique(ms.Tensor(widths), return_counts = True)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;
         stage_widths, stage_depths = stage_widths.tolist(), stage_depths.tolist()
         stage_br = [cfg.bottle_ratio for _ in range(num_stages)]
         stage_strides = []
@@ -716,7 +721,7 @@             s.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         """Get the classifier head."""
         return self.head.fc
 
@@ -732,13 +737,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -798,14 +803,14 @@         layer_names = ('s1', 's2', 's3', 's4')
         layer_names = layer_names[max_index:]
         for n in layer_names:
-            setattr(self, n, nn.Identity())
+            setattr(self, n, msnn.Identity())
         if max_index < 4:
-            self.final_conv = nn.Identity()
+            self.final_conv = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers.
 
         Args:
@@ -822,7 +827,7 @@         x = self.final_conv(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -834,7 +839,7 @@         """
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -848,7 +853,7 @@         return x
 
 
-def _init_weights(module: nn.Module, name: str = '', zero_init_last: bool = False) -> None:
+def _init_weights(module: msnn.Cell, name: str = '', zero_init_last: bool = False) -> None:
     """Initialize module weights.
 
     Args:
@@ -863,9 +868,9 @@         if module.bias is not None:
             module.bias.data.zero_()
     elif isinstance(module, nn.Linear):
-        nn.init.normal_(module.weight, mean=0.0, std=0.01)
+        nn.init.normal_(module.weight, mean=0.0, std=0.01)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif zero_init_last and hasattr(module, 'zero_init_last'):
         module.zero_init_last()
 
