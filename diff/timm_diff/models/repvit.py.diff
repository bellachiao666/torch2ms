--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ RepViT
 
 Paper: `RepViT: Revisiting Mobile CNN From ViT Perspective`
@@ -16,8 +21,8 @@ """
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SqueezeExcite, trunc_normal_, to_ntuple, to_2tuple
@@ -29,7 +34,7 @@ __all__ = ['RepVit']
 
 
-class ConvNorm(nn.Sequential):
+class ConvNorm(msnn.SequentialCell):
     def __init__(
             self,
             in_dim: int,
@@ -45,10 +50,10 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.add_module('c', nn.Conv2d(in_dim, out_dim, ks, stride, pad, dilation, groups, bias=False, **dd))
-        self.add_module('bn', nn.BatchNorm2d(out_dim, **dd))
-        nn.init.constant_(self.bn.weight, bn_weight_init)
-        nn.init.constant_(self.bn.bias, 0)
+        self.add_module('c', nn.Conv2d(in_dim, out_dim, ks, stride, pad, dilation, groups, bias=False, **dd))  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.add_module('bn', nn.BatchNorm2d(out_dim, **dd))  # 存在 *args/**kwargs，需手动确认参数映射;
+        nn.init.constant_(self.bn.weight, bn_weight_init)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.constant_(self.bn.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.no_grad()
     def fuse(self):
@@ -57,21 +62,13 @@         w = c.weight * w[:, None, None, None]
         b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5
         m = nn.Conv2d(
-            w.size(1) * self.c.groups,
-            w.size(0),
-            w.shape[2:],
-            stride=self.c.stride,
-            padding=self.c.padding,
-            dilation=self.c.dilation,
-            groups=self.c.groups,
-            device=c.weight.device,
-        )
+            w.size(1) * self.c.groups, w.size(0), w.shape[2:], stride = self.c.stride, padding = self.c.padding, dilation = self.c.dilation, groups = self.c.groups)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device' (position 9);
         m.weight.data.copy_(w)
         m.bias.data.copy_(b)
         return m
 
 
-class NormLinear(nn.Sequential):
+class NormLinear(msnn.SequentialCell):
     def __init__(
             self,
             in_dim: int,
@@ -83,11 +80,11 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.add_module('bn', nn.BatchNorm1d(in_dim, **dd))
-        self.add_module('l', nn.Linear(in_dim, out_dim, bias=bias, **dd))
+        self.add_module('bn', nn.BatchNorm1d(in_dim, **dd))  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.add_module('l', nn.Linear(in_dim, out_dim, bias=bias, **dd))  # 存在 *args/**kwargs，需手动确认参数映射;
         trunc_normal_(self.l.weight, std=std)
         if bias:
-            nn.init.constant_(self.l.bias, 0)
+            nn.init.constant_(self.l.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.no_grad()
     def fuse(self):
@@ -99,13 +96,13 @@             b = b @ self.l.weight.T
         else:
             b = (l.weight @ b[:, None]).view(-1) + self.l.bias
-        m = nn.Linear(w.size(1), w.size(0), device=l.weight.device)
+        m = nn.Linear(w.size(1), w.size(0))  # 'torch.nn.Linear':没有对应的mindspore参数 'device' (position 3);
         m.weight.data.copy_(w)
         m.bias.data.copy_(b)
         return m
 
 
-class RepVggDw(nn.Module):
+class RepVggDw(msnn.Cell):
     def __init__(
             self,
             ed: int,
@@ -116,18 +113,18 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.conv = ConvNorm(ed, ed, kernel_size, 1, (kernel_size - 1) // 2, groups=ed, **dd)
+        self.conv = ConvNorm(ed, ed, kernel_size, 1, (kernel_size - 1) // 2, groups=ed, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if legacy:
-            self.conv1 = ConvNorm(ed, ed, 1, 1, 0, groups=ed, **dd)
+            self.conv1 = ConvNorm(ed, ed, 1, 1, 0, groups=ed, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             # Make torchscript happy.
-            self.bn = nn.Identity()
-        else:
-            self.conv1 = nn.Conv2d(ed, ed, 1, 1, 0, groups=ed, **dd)
-            self.bn = nn.BatchNorm2d(ed, **dd)
+            self.bn = msnn.Identity()
+        else:
+            self.conv1 = nn.Conv2d(ed, ed, 1, 1, 0, groups=ed, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+            self.bn = nn.BatchNorm2d(ed, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.dim = ed
         self.legacy = legacy
 
-    def forward(self, x):
+    def construct(self, x):
         return self.bn(self.conv(x) + self.conv1(x) + x)
 
     @torch.no_grad()
@@ -147,8 +144,7 @@         conv1_w = nn.functional.pad(conv1_w, [1, 1, 1, 1])
 
         identity = nn.functional.pad(
-            torch.ones(conv1_w.shape[0], conv1_w.shape[1], 1, 1, device=conv1_w.device), [1, 1, 1, 1]
-        )
+            mint.ones(conv1_w.shape[0], conv1_w.shape[1], 1, 1, device=conv1_w.device), [1, 1, 1, 1])
 
         final_conv_w = conv_w + conv1_w + identity
         final_conv_b = conv_b + conv1_b
@@ -166,44 +162,44 @@         return conv
 
 
-class RepVitMlp(nn.Module):
+class RepVitMlp(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
             hidden_dim: int,
-            act_layer: Type[nn.Module],
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-        self.conv1 = ConvNorm(in_dim, hidden_dim, 1, 1, 0, **dd)
+            act_layer: Type[msnn.Cell],
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+        self.conv1 = ConvNorm(in_dim, hidden_dim, 1, 1, 0, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act = act_layer()
-        self.conv2 = ConvNorm(hidden_dim, in_dim, 1, 1, 0, bn_weight_init=0, **dd)
-
-    def forward(self, x):
+        self.conv2 = ConvNorm(hidden_dim, in_dim, 1, 1, 0, bn_weight_init=0, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         return self.conv2(self.act(self.conv1(x)))
 
 
-class RepViTBlock(nn.Module):
+class RepViTBlock(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
             mlp_ratio: float,
             kernel_size: int,
             use_se: bool,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             legacy: bool = False,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.token_mixer = RepVggDw(in_dim, kernel_size, legacy, **dd)
-        self.se = SqueezeExcite(in_dim, 0.25, **dd) if use_se else nn.Identity()
-        self.channel_mixer = RepVitMlp(in_dim, in_dim * mlp_ratio, act_layer, **dd)
-
-    def forward(self, x):
+        self.token_mixer = RepVggDw(in_dim, kernel_size, legacy, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.se = SqueezeExcite(in_dim, 0.25, **dd) if use_se else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.channel_mixer = RepVitMlp(in_dim, in_dim * mlp_ratio, act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.token_mixer(x)
         x = self.se(x)
         identity = x
@@ -211,34 +207,34 @@         return identity + x
 
 
-class RepVitStem(nn.Module):
+class RepVitStem(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
-            act_layer: Type[nn.Module],
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-        self.conv1 = ConvNorm(in_chs, out_chs // 2, 3, 2, 1, **dd)
+            act_layer: Type[msnn.Cell],
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+        self.conv1 = ConvNorm(in_chs, out_chs // 2, 3, 2, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act1 = act_layer()
-        self.conv2 = ConvNorm(out_chs // 2, out_chs, 3, 2, 1, **dd)
+        self.conv2 = ConvNorm(out_chs // 2, out_chs, 3, 2, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.stride = 4
 
-    def forward(self, x):
+    def construct(self, x):
         return self.conv2(self.act1(self.conv1(x)))
 
 
-class RepVitDownsample(nn.Module):
+class RepVitDownsample(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
             mlp_ratio: float,
             out_dim: int,
             kernel_size: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             legacy: bool = False,
             device=None,
             dtype=None,
@@ -253,7 +249,7 @@             act_layer=act_layer,
             legacy=legacy,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.spatial_downsample = ConvNorm(
             in_dim,
             in_dim,
@@ -262,11 +258,11 @@             pad=(kernel_size - 1) // 2,
             groups=in_dim,
             **dd,
-        )
-        self.channel_downsample = ConvNorm(in_dim, out_dim, 1, 1, **dd)
-        self.ffn = RepVitMlp(out_dim, out_dim * mlp_ratio, act_layer, **dd)
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.channel_downsample = ConvNorm(in_dim, out_dim, 1, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ffn = RepVitMlp(out_dim, out_dim * mlp_ratio, act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.pre_block(x)
         x = self.spatial_downsample(x)
         x = self.channel_downsample(x)
@@ -275,7 +271,7 @@         return x + identity
 
 
-class RepVitClassifier(nn.Module):
+class RepVitClassifier(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -288,14 +284,14 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.head_drop = nn.Dropout(drop)
-        self.head = NormLinear(dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = NormLinear(dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.distillation = distillation
         self.distilled_training = False
         self.num_classes = num_classes
         if distillation:
-            self.head_dist = NormLinear(dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
-
-    def forward(self, x):
+            self.head_dist = NormLinear(dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.head_drop(x)
         if self.distillation:
             x1, x2 = self.head(x), self.head_dist(x)
@@ -310,7 +306,7 @@     @torch.no_grad()
     def fuse(self):
         if not self.num_classes > 0:
-            return nn.Identity()
+            return msnn.Identity()
         head = self.head.fuse()
         if self.distillation:
             head_dist = self.head_dist.fuse()
@@ -323,14 +319,14 @@             return head
 
 
-class RepVitStage(nn.Module):
+class RepVitStage(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
             out_dim: int,
             depth: int,
             mlp_ratio: float,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             kernel_size: int = 3,
             downsample: bool = True,
             legacy: bool = False,
@@ -348,26 +344,26 @@                 act_layer=act_layer,
                 legacy=legacy,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             assert in_dim == out_dim
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         blocks = []
         use_se = True
         for _ in range(depth):
-            blocks.append(RepViTBlock(out_dim, mlp_ratio, kernel_size, use_se, act_layer, legacy, **dd))
+            blocks.append(RepViTBlock(out_dim, mlp_ratio, kernel_size, use_se, act_layer, legacy, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             use_se = not use_se
 
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell(*blocks)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.downsample(x)
         x = self.blocks(x)
         return x
 
 
-class RepVit(nn.Module):
+class RepVit(msnn.Cell):
     def __init__(
         self,
         in_chans: int = 3,
@@ -378,7 +374,7 @@         global_pool: str = 'avg',
         kernel_size: int = 3,
         num_classes: int = 1000,
-        act_layer: Type[nn.Module] = nn.GELU,
+        act_layer: Type[msnn.Cell] = nn.GELU,
         distillation: bool = True,
         drop_rate: float = 0.0,
         legacy: bool = False,
@@ -393,7 +389,7 @@         self.num_classes = num_classes
 
         in_dim = embed_dim[0]
-        self.stem = RepVitStem(in_chans, in_dim, act_layer, **dd)
+        self.stem = RepVitStem(in_chans, in_dim, act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stride = self.stem.stride
         resolution = tuple([i // p for i, p in zip(to_2tuple(img_size), to_2tuple(stride))])
 
@@ -416,29 +412,29 @@                     legacy=legacy,
                     **dd,
                 )
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             stage_stride = 2 if downsample else 1
             stride *= stage_stride
             resolution = tuple([(r - 1) // stage_stride + 1 for r in resolution])
             self.feature_info += [dict(num_chs=embed_dim[i], reduction=stride, module=f'stages.{i}')]
             in_dim = embed_dim[i]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.num_features = self.head_hidden_size = embed_dim[-1]
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = RepVitClassifier(embed_dim[-1], num_classes, distillation, **dd)
-
-    @torch.jit.ignore
+        self.head = RepVitClassifier(embed_dim[-1], num_classes, distillation, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(stem=r'^stem', blocks=[(r'^blocks\.(\d+)', None), (r'^norm', (99999,))])  # stem and embed
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None, distillation: bool = False, device=None, dtype=None):
@@ -446,21 +442,21 @@         if global_pool is not None:
             self.global_pool = global_pool
         dd = {'device': device, 'dtype': dtype}
-        self.head = RepVitClassifier(self.embed_dim[-1], num_classes, distillation, **dd)
-
-    @torch.jit.ignore
+        self.head = RepVitClassifier(self.embed_dim[-1], num_classes, distillation, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def set_distilled_training(self, enable=True):
         self.head.distilled_training = enable
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -527,7 +523,7 @@             return x
         return self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -616,7 +612,7 @@         pretrained,
         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -626,7 +622,7 @@     Constructs a RepViT-M1 model
     """
     model_args = dict(embed_dim=(48, 96, 192, 384), depth=(2, 2, 14, 2), legacy=True)
-    return _create_repvit('repvit_m1', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_repvit('repvit_m1', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -635,7 +631,7 @@     Constructs a RepViT-M2 model
     """
     model_args = dict(embed_dim=(64, 128, 256, 512), depth=(2, 2, 12, 2), legacy=True)
-    return _create_repvit('repvit_m2', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_repvit('repvit_m2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -644,7 +640,7 @@     Constructs a RepViT-M3 model
     """
     model_args = dict(embed_dim=(64, 128, 256, 512), depth=(4, 4, 18, 2), legacy=True)
-    return _create_repvit('repvit_m3', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_repvit('repvit_m3', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -653,7 +649,7 @@     Constructs a RepViT-M0.9 model
     """
     model_args = dict(embed_dim=(48, 96, 192, 384), depth=(2, 2, 14, 2))
-    return _create_repvit('repvit_m0_9', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_repvit('repvit_m0_9', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -662,7 +658,7 @@     Constructs a RepViT-M1.0 model
     """
     model_args = dict(embed_dim=(56, 112, 224, 448), depth=(2, 2, 14, 2))
-    return _create_repvit('repvit_m1_0', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_repvit('repvit_m1_0', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -671,7 +667,7 @@     Constructs a RepViT-M1.1 model
     """
     model_args = dict(embed_dim=(64, 128, 256, 512), depth=(2, 2, 12, 2))
-    return _create_repvit('repvit_m1_1', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_repvit('repvit_m1_1', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -680,7 +676,7 @@     Constructs a RepViT-M1.5 model
     """
     model_args = dict(embed_dim=(64, 128, 256, 512), depth=(4, 4, 24, 4))
-    return _create_repvit('repvit_m1_5', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_repvit('repvit_m1_5', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -689,4 +685,4 @@     Constructs a RepViT-M2.3 model
     """
     model_args = dict(embed_dim=(80, 160, 320, 640), depth=(6, 6, 34, 2))
-    return _create_repvit('repvit_m2_3', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_repvit('repvit_m2_3', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
