--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ RepViT
 
 Paper: `RepViT: Revisiting Mobile CNN From ViT Perspective`
@@ -16,8 +21,8 @@ """
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SqueezeExcite, trunc_normal_, to_ntuple, to_2tuple
@@ -29,7 +34,7 @@ __all__ = ['RepVit']
 
 
-class ConvNorm(nn.Sequential):
+class ConvNorm(msnn.SequentialCell):
     def __init__(
             self,
             in_dim: int,
@@ -47,8 +52,8 @@         super().__init__()
         self.add_module('c', nn.Conv2d(in_dim, out_dim, ks, stride, pad, dilation, groups, bias=False, **dd))
         self.add_module('bn', nn.BatchNorm2d(out_dim, **dd))
-        nn.init.constant_(self.bn.weight, bn_weight_init)
-        nn.init.constant_(self.bn.bias, 0)
+        nn.init.constant_(self.bn.weight, bn_weight_init)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.constant_(self.bn.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.no_grad()
     def fuse(self):
@@ -57,21 +62,13 @@         w = c.weight * w[:, None, None, None]
         b = bn.bias - bn.running_mean * bn.weight / (bn.running_var + bn.eps) ** 0.5
         m = nn.Conv2d(
-            w.size(1) * self.c.groups,
-            w.size(0),
-            w.shape[2:],
-            stride=self.c.stride,
-            padding=self.c.padding,
-            dilation=self.c.dilation,
-            groups=self.c.groups,
-            device=c.weight.device,
-        )
+            w.size(1) * self.c.groups, w.size(0), w.shape[2:], stride = self.c.stride, padding = self.c.padding, dilation = self.c.dilation, groups = self.c.groups)  # 'torch.nn.Conv2d':没有对应的mindspore参数 'device' (position 9);
         m.weight.data.copy_(w)
         m.bias.data.copy_(b)
         return m
 
 
-class NormLinear(nn.Sequential):
+class NormLinear(msnn.SequentialCell):
     def __init__(
             self,
             in_dim: int,
@@ -87,7 +84,7 @@         self.add_module('l', nn.Linear(in_dim, out_dim, bias=bias, **dd))
         trunc_normal_(self.l.weight, std=std)
         if bias:
-            nn.init.constant_(self.l.bias, 0)
+            nn.init.constant_(self.l.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.no_grad()
     def fuse(self):
@@ -99,13 +96,13 @@             b = b @ self.l.weight.T
         else:
             b = (l.weight @ b[:, None]).view(-1) + self.l.bias
-        m = nn.Linear(w.size(1), w.size(0), device=l.weight.device)
+        m = nn.Linear(w.size(1), w.size(0))  # 'torch.nn.Linear':没有对应的mindspore参数 'device' (position 3);
         m.weight.data.copy_(w)
         m.bias.data.copy_(b)
         return m
 
 
-class RepVggDw(nn.Module):
+class RepVggDw(msnn.Cell):
     def __init__(
             self,
             ed: int,
@@ -120,14 +117,14 @@         if legacy:
             self.conv1 = ConvNorm(ed, ed, 1, 1, 0, groups=ed, **dd)
             # Make torchscript happy.
-            self.bn = nn.Identity()
+            self.bn = msnn.Identity()
         else:
             self.conv1 = nn.Conv2d(ed, ed, 1, 1, 0, groups=ed, **dd)
             self.bn = nn.BatchNorm2d(ed, **dd)
         self.dim = ed
         self.legacy = legacy
 
-    def forward(self, x):
+    def construct(self, x):
         return self.bn(self.conv(x) + self.conv1(x) + x)
 
     @torch.no_grad()
@@ -147,8 +144,7 @@         conv1_w = nn.functional.pad(conv1_w, [1, 1, 1, 1])
 
         identity = nn.functional.pad(
-            torch.ones(conv1_w.shape[0], conv1_w.shape[1], 1, 1, device=conv1_w.device), [1, 1, 1, 1]
-        )
+            mint.ones(size = (conv1_w.shape[0], conv1_w.shape[1], 1, 1)), [1, 1, 1, 1])  # 'torch.ones':没有对应的mindspore参数 'device' (position 4);
 
         final_conv_w = conv_w + conv1_w + identity
         final_conv_b = conv_b + conv1_b
@@ -166,12 +162,12 @@         return conv
 
 
-class RepVitMlp(nn.Module):
+class RepVitMlp(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
             hidden_dim: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             device=None,
             dtype=None,
     ):
@@ -181,18 +177,18 @@         self.act = act_layer()
         self.conv2 = ConvNorm(hidden_dim, in_dim, 1, 1, 0, bn_weight_init=0, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         return self.conv2(self.act(self.conv1(x)))
 
 
-class RepViTBlock(nn.Module):
+class RepViTBlock(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
             mlp_ratio: float,
             kernel_size: int,
             use_se: bool,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             legacy: bool = False,
             device=None,
             dtype=None,
@@ -200,10 +196,10 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.token_mixer = RepVggDw(in_dim, kernel_size, legacy, **dd)
-        self.se = SqueezeExcite(in_dim, 0.25, **dd) if use_se else nn.Identity()
+        self.se = SqueezeExcite(in_dim, 0.25, **dd) if use_se else msnn.Identity()
         self.channel_mixer = RepVitMlp(in_dim, in_dim * mlp_ratio, act_layer, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.token_mixer(x)
         x = self.se(x)
         identity = x
@@ -211,12 +207,12 @@         return identity + x
 
 
-class RepVitStem(nn.Module):
+class RepVitStem(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             device=None,
             dtype=None,
     ):
@@ -227,18 +223,18 @@         self.conv2 = ConvNorm(out_chs // 2, out_chs, 3, 2, 1, **dd)
         self.stride = 4
 
-    def forward(self, x):
+    def construct(self, x):
         return self.conv2(self.act1(self.conv1(x)))
 
 
-class RepVitDownsample(nn.Module):
+class RepVitDownsample(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
             mlp_ratio: float,
             out_dim: int,
             kernel_size: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             legacy: bool = False,
             device=None,
             dtype=None,
@@ -266,7 +262,7 @@         self.channel_downsample = ConvNorm(in_dim, out_dim, 1, 1, **dd)
         self.ffn = RepVitMlp(out_dim, out_dim * mlp_ratio, act_layer, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.pre_block(x)
         x = self.spatial_downsample(x)
         x = self.channel_downsample(x)
@@ -275,7 +271,7 @@         return x + identity
 
 
-class RepVitClassifier(nn.Module):
+class RepVitClassifier(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -288,14 +284,14 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.head_drop = nn.Dropout(drop)
-        self.head = NormLinear(dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = NormLinear(dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()
         self.distillation = distillation
         self.distilled_training = False
         self.num_classes = num_classes
         if distillation:
-            self.head_dist = NormLinear(dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
-
-    def forward(self, x):
+            self.head_dist = NormLinear(dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()
+
+    def construct(self, x):
         x = self.head_drop(x)
         if self.distillation:
             x1, x2 = self.head(x), self.head_dist(x)
@@ -310,7 +306,7 @@     @torch.no_grad()
     def fuse(self):
         if not self.num_classes > 0:
-            return nn.Identity()
+            return msnn.Identity()
         head = self.head.fuse()
         if self.distillation:
             head_dist = self.head_dist.fuse()
@@ -323,14 +319,14 @@             return head
 
 
-class RepVitStage(nn.Module):
+class RepVitStage(msnn.Cell):
     def __init__(
             self,
             in_dim: int,
             out_dim: int,
             depth: int,
             mlp_ratio: float,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             kernel_size: int = 3,
             downsample: bool = True,
             legacy: bool = False,
@@ -351,7 +347,7 @@             )
         else:
             assert in_dim == out_dim
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         blocks = []
         use_se = True
@@ -359,15 +355,17 @@             blocks.append(RepViTBlock(out_dim, mlp_ratio, kernel_size, use_se, act_layer, legacy, **dd))
             use_se = not use_se
 
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x):
         x = self.downsample(x)
         x = self.blocks(x)
         return x
 
 
-class RepVit(nn.Module):
+class RepVit(msnn.Cell):
     def __init__(
         self,
         in_chans: int = 3,
@@ -378,7 +376,7 @@         global_pool: str = 'avg',
         kernel_size: int = 3,
         num_classes: int = 1000,
-        act_layer: Type[nn.Module] = nn.GELU,
+        act_layer: Type[msnn.Cell] = nn.GELU,
         distillation: bool = True,
         drop_rate: float = 0.0,
         legacy: bool = False,
@@ -422,7 +420,9 @@             resolution = tuple([(r - 1) // stage_stride + 1 for r in resolution])
             self.feature_info += [dict(num_chs=embed_dim[i], reduction=stride, module=f'stages.{i}')]
             in_dim = embed_dim[i]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell([
+            stages
+        ])
 
         self.num_features = self.head_hidden_size = embed_dim[-1]
         self.head_drop = nn.Dropout(drop_rate)
@@ -438,7 +438,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None, distillation: bool = False, device=None, dtype=None):
@@ -454,13 +454,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -527,7 +527,7 @@             return x
         return self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
