--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """PyTorch ResNet
 
 This started as a copy of https://github.com/pytorch/vision 'resnet.py' (BSD-3-Clause) with
@@ -11,9 +16,8 @@ from functools import partial
 from typing import Any, Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropBlock2d, DropPath, AvgPool2dSame, BlurPool2d, LayerType, create_attn, \
@@ -31,7 +35,7 @@     return padding
 
 
-class BasicBlock(nn.Module):
+class BasicBlock(msnn.Cell):
     """Basic residual block for ResNet.
 
     This is the standard residual block used in ResNet-18 and ResNet-34.
@@ -97,7 +101,7 @@             **dd,
         )
         self.bn1 = norm_layer(first_planes, **dd)
-        self.drop_block = drop_block() if drop_block is not None else nn.Identity()
+        self.drop_block = drop_block() if drop_block is not None else msnn.Identity()
         self.act1 = act_layer(inplace=True)
         self.aa = create_aa(aa_layer, channels=first_planes, stride=stride, enable=use_aa, **dd)
 
@@ -123,9 +127,9 @@     def zero_init_last(self) -> None:
         """Initialize the last batch norm layer weights to zero for better convergence."""
         if getattr(self.bn2, 'weight', None) is not None:
-            nn.init.zeros_(self.bn2.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            nn.init.zeros_(self.bn2.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = x
 
         x = self.conv1(x)
@@ -151,7 +155,7 @@         return x
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(msnn.Cell):
     """Bottleneck residual block for ResNet.
 
     This is the bottleneck block used in ResNet-50, ResNet-101, and ResNet-152.
@@ -221,7 +225,7 @@             **dd,
         )
         self.bn2 = norm_layer(width, **dd)
-        self.drop_block = drop_block() if drop_block is not None else nn.Identity()
+        self.drop_block = drop_block() if drop_block is not None else msnn.Identity()
         self.act2 = act_layer(inplace=True)
         self.aa = create_aa(aa_layer, channels=width, stride=stride, enable=use_aa, **dd)
 
@@ -239,9 +243,9 @@     def zero_init_last(self) -> None:
         """Initialize the last batch norm layer weights to zero for better convergence."""
         if getattr(self.bn3, 'weight', None) is not None:
-            nn.init.zeros_(self.bn3.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            nn.init.zeros_(self.bn3.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         shortcut = x
 
         x = self.conv1(x)
@@ -271,6 +275,7 @@         return x
 
 
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def downsample_conv(
         in_channels: int,
         out_channels: int,
@@ -288,7 +293,7 @@     first_dilation = (first_dilation or dilation) if kernel_size > 1 else 1
     p = get_padding(kernel_size, stride, first_dilation)
 
-    return nn.Sequential(*[
+    return msnn.SequentialCell(*[
         nn.Conv2d(
             in_channels,
             out_channels,
@@ -303,6 +308,7 @@     ])
 
 
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def downsample_avg(
         in_channels: int,
         out_channels: int,
@@ -318,12 +324,12 @@     norm_layer = norm_layer or nn.BatchNorm2d
     avg_stride = stride if dilation == 1 else 1
     if stride == 1 and dilation == 1:
-        pool = nn.Identity()
+        pool = msnn.Identity()
     else:
         avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
         pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
 
-    return nn.Sequential(*[
+    return msnn.SequentialCell(*[
         pool,
         nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False, **dd),
         norm_layer(out_channels, **dd)
@@ -428,13 +434,15 @@             inplanes = planes * block_fn.expansion
             net_block_idx += 1
 
-        stages.append((stage_name, nn.Sequential(*blocks)))
+        stages.append((stage_name, msnn.SequentialCell([
+            blocks
+        ])))
         feature_info.append(dict(num_chs=inplanes, reduction=net_stride, module=stage_name))
 
     return stages, feature_info
 
 
-class ResNet(nn.Module):
+class ResNet(msnn.Cell):
     """ResNet / ResNeXt / SE-ResNeXt / SE-Net
 
     This class implements all variants of ResNet, ResNeXt, SE-ResNeXt, and SENet that
@@ -542,7 +550,7 @@             stem_chs = (stem_width, stem_width)
             if 'tiered' in stem_type:
                 stem_chs = (3 * (stem_width // 4), stem_width)
-            self.conv1 = nn.Sequential(*[
+            self.conv1 = msnn.SequentialCell(*[
                 nn.Conv2d(in_chans, stem_chs[0], 3, stride=2, padding=1, bias=False, **dd),
                 norm_layer(stem_chs[0], **dd),
                 act_layer(inplace=True),
@@ -558,22 +566,24 @@ 
         # Stem pooling. The name 'maxpool' remains for weight compatibility.
         if replace_stem_pool:
-            self.maxpool = nn.Sequential(*filter(None, [
+            self.maxpool = msnn.SequentialCell([
+                filter(None, [
                 nn.Conv2d(inplanes, inplanes, 3, stride=1 if aa_layer else 2, padding=1, bias=False, **dd),
                 create_aa(aa_layer, channels=inplanes, stride=2, **dd) if aa_layer is not None else None,
                 norm_layer(inplanes, **dd),
                 act_layer(inplace=True),
-            ]))
+            ])
+            ])
         else:
             if aa_layer is not None:
                 if issubclass(aa_layer, nn.AvgPool2d):
                     self.maxpool = aa_layer(2)
                 else:
-                    self.maxpool = nn.Sequential(*[
-                        nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
+                    self.maxpool = msnn.SequentialCell(*[
+                        nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1),
                         aa_layer(channels=inplanes, stride=2, **dd)])
             else:
-                self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
+                self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
 
         # Feature Blocks
         block_fns = to_ntuple(len(channels))(block)
@@ -615,7 +625,7 @@         """
         for n, m in self.named_modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if zero_init_last:
             for m in self.modules():
                 if hasattr(m, 'zero_init_last'):
@@ -667,7 +677,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
@@ -734,12 +744,12 @@         layer_names = ('layer1', 'layer2', 'layer3', 'layer4')
         layer_names = layer_names[max_index:]
         for n in layer_names:
-            setattr(self, n, nn.Identity())
+            setattr(self, n, msnn.Identity())
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers."""
         x = self.conv1(x)
         x = self.bn1(x)
@@ -755,7 +765,7 @@             x = self.layer4(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -767,10 +777,10 @@         """
         x = self.global_pool(x)
         if self.drop_rate:
-            x = F.dropout(x, p=float(self.drop_rate), training=self.training)
+            x = nn.functional.dropout(x, p = float(self.drop_rate), training = self.training)
         return x if pre_logits else self.fc(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass."""
         x = self.forward_features(x)
         x = self.forward_head(x)
