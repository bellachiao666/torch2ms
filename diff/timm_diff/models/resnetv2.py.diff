--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """Pre-Activation ResNet v2 with GroupNorm and Weight Standardization.
 
 A PyTorch implementation of ResNetV2 adapted from the Google Big-Transfer (BiT) source code
@@ -33,8 +38,8 @@ from functools import partial
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
 from timm.layers import GroupNormAct, BatchNormAct2d, EvoNorm2dS0, FilterResponseNormTlu2d, ClassifierHead, \
@@ -47,7 +52,7 @@ __all__ = ['ResNetV2']  # model_registry will add each entrypoint fn to this
 
 
-class PreActBasic(nn.Module):
+class PreActBasic(msnn.Cell):
     """Pre-activation basic block (not in typical 'v2' implementations)."""
 
     def __init__(
@@ -110,13 +115,13 @@         self.conv1 = conv_layer(in_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups, **dd)
         self.norm2 = norm_layer(mid_chs, **dd)
         self.conv2 = conv_layer(mid_chs, out_chs, 3, dilation=dilation, groups=groups, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
 
     def zero_init_last(self) -> None:
         """Zero-initialize the last convolution weight (not applicable to basic block)."""
-        nn.init.zeros_(self.conv2.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        nn.init.zeros_(self.conv2.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -139,7 +144,7 @@         return x + shortcut
 
 
-class PreActBottleneck(nn.Module):
+class PreActBottleneck(msnn.Cell):
     """Pre-activation (v2) bottleneck block.
 
     Follows the implementation of "Identity Mappings in Deep Residual Networks":
@@ -210,13 +215,13 @@         self.conv2 = conv_layer(mid_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups, **dd)
         self.norm3 = norm_layer(mid_chs, **dd)
         self.conv3 = conv_layer(mid_chs, out_chs, 1, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
 
     def zero_init_last(self) -> None:
         """Zero-initialize the last convolution weight."""
-        nn.init.zeros_(self.conv3.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        nn.init.zeros_(self.conv3.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -240,7 +245,7 @@         return x + shortcut
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(msnn.Cell):
     """Non Pre-activation bottleneck block, equiv to V1.5/V1b Bottleneck. Used for ViT.
     """
     def __init__(
@@ -289,15 +294,15 @@         self.norm2 = norm_layer(mid_chs, **dd)
         self.conv3 = conv_layer(mid_chs, out_chs, 1, **dd)
         self.norm3 = norm_layer(out_chs, apply_act=False, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
         self.act3 = act_layer(inplace=True)
 
     def zero_init_last(self) -> None:
         """Zero-initialize the last batch norm weight."""
         if getattr(self.norm3, 'weight', None) is not None:
-            nn.init.zeros_(self.norm3.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            nn.init.zeros_(self.norm3.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -323,7 +328,7 @@         return x
 
 
-class DownsampleConv(nn.Module):
+class DownsampleConv(msnn.Cell):
     """1x1 convolution downsampling module."""
 
     def __init__(
@@ -342,9 +347,9 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.conv = conv_layer(in_chs, out_chs, 1, stride=stride, **dd)
-        self.norm = nn.Identity() if preact else norm_layer(out_chs, apply_act=False, **dd)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.norm = msnn.Identity() if preact else norm_layer(out_chs, apply_act=False, **dd)
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -356,7 +361,7 @@         return self.norm(self.conv(x))
 
 
-class DownsampleAvg(nn.Module):
+class DownsampleAvg(msnn.Cell):
     """AvgPool downsampling as in 'D' ResNet variants."""
 
     def __init__(
@@ -379,11 +384,11 @@             avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
             self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
         else:
-            self.pool = nn.Identity()
+            self.pool = msnn.Identity()
         self.conv = conv_layer(in_chs, out_chs, 1, stride=1, **dd)
-        self.norm = nn.Identity() if preact else norm_layer(out_chs, apply_act=False, **dd)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.norm = msnn.Identity() if preact else norm_layer(out_chs, apply_act=False, **dd)
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -395,7 +400,7 @@         return self.norm(self.conv(self.pool(x)))
 
 
-class ResNetStage(nn.Module):
+class ResNetStage(msnn.Cell):
     """ResNet Stage."""
     def __init__(
             self,
@@ -421,7 +426,7 @@         layer_kwargs = dict(act_layer=act_layer, conv_layer=conv_layer, norm_layer=norm_layer)
         proj_layer = DownsampleAvg if avg_down else DownsampleConv
         prev_chs = in_chs
-        self.blocks = nn.Sequential()
+        self.blocks = msnn.SequentialCell()
         for block_idx in range(depth):
             drop_path_rate = block_dpr[block_idx] if block_dpr else 0.
             stride = stride if block_idx == 0 else 1
@@ -442,7 +447,7 @@             first_dilation = dilation
             proj_layer = None
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through all blocks in the stage.
 
         Args:
@@ -479,7 +484,7 @@         norm_layer: Callable = partial(GroupNormAct, num_groups=32),
         device=None,
         dtype=None,
-) -> nn.Sequential:
+) -> msnn.SequentialCell:
     dd = {'device': device, 'dtype': dtype}
     stem = OrderedDict()
     assert stem_type in ('', 'fixed', 'same', 'deep', 'deep_fixed', 'deep_same', 'tiered')
@@ -507,18 +512,20 @@     if 'fixed' in stem_type:
         # 'fixed' SAME padding approximation that is used in BiT models
         stem['pad'] = nn.ConstantPad2d(1, 0.)
-        stem['pool'] = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)
+        stem['pool'] = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 0)
     elif 'same' in stem_type:
         # full, input size based 'SAME' padding, used in ViT Hybrid model
         stem['pool'] = create_pool2d('max', kernel_size=3, stride=2, padding='same')
     else:
         # the usual PyTorch symmetric padding
-        stem['pool'] = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
-
-    return nn.Sequential(stem)
-
-
-class ResNetV2(nn.Module):
+        stem['pool'] = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
+
+    return msnn.SequentialCell([
+        stem
+    ])
+
+
+class ResNetV2(msnn.Cell):
     """Implementation of Pre-activation (v2) ResNet mode.
     """
 
@@ -597,7 +604,7 @@         else:
             assert not basic
             block_fn = Bottleneck
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         for stage_idx, (d, c, bdpr) in enumerate(zip(layers, channels, block_dprs)):
             out_chs = make_divisible(c * wf)
             stride = 1 if stage_idx == 0 else 2
@@ -625,7 +632,7 @@             self.stages.add_module(str(stage_idx), stage)
 
         self.num_features = self.head_hidden_size = prev_chs
-        self.norm = norm_layer(self.num_features, **dd) if preact else nn.Identity()
+        self.norm = norm_layer(self.num_features, **dd) if preact else msnn.Identity()
         self.head = ClassifierHead(
             self.num_features,
             num_classes,
@@ -666,7 +673,7 @@             s.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         """Get the classifier head."""
         return self.head.fc
 
@@ -682,13 +689,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -748,12 +755,12 @@         take_indices, max_index = feature_take_indices(5, indices)
         self.stages = self.stages[:max_index]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers.
 
         Args:
@@ -767,7 +774,7 @@         x = self.norm(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -779,7 +786,7 @@         """
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -793,7 +800,7 @@         return x
 
 
-def _init_weights(module: nn.Module, name: str = '', zero_init_last: bool = True) -> None:
+def _init_weights(module: msnn.Cell, name: str = '', zero_init_last: bool = True) -> None:
     """Initialize module weights.
 
     Args:
@@ -802,28 +809,28 @@         zero_init_last: Zero-initialize last layer weights.
     """
     if isinstance(module, nn.Linear) or ('head.fc' in name and isinstance(module, nn.Conv2d)):
-        nn.init.normal_(module.weight, mean=0.0, std=0.01)
-        nn.init.zeros_(module.bias)
+        nn.init.normal_(module.weight, mean=0.0, std=0.01)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Conv2d):
-        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
+        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):
-        nn.init.ones_(module.weight)
-        nn.init.zeros_(module.bias)
+        nn.init.ones_(module.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif zero_init_last and hasattr(module, 'zero_init_last'):
         module.zero_init_last()
 
 
 @torch.no_grad()
-def _load_weights(model: nn.Module, checkpoint_path: str, prefix: str = 'resnet/'):
+def _load_weights(model: msnn.Cell, checkpoint_path: str, prefix: str = 'resnet/'):
     import numpy as np
 
     def t2p(conv_weights):
         """Possibly convert HWIO to OIHW."""
         if conv_weights.ndim == 4:
             conv_weights = conv_weights.transpose([3, 2, 0, 1])
-        return torch.from_numpy(conv_weights)
+        return torch.from_numpy(conv_weights)  # 'torch.from_numpy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     weights = np.load(checkpoint_path)
     stem_conv_w = adapt_input_conv(
