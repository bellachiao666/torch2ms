--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """Pre-Activation ResNet v2 with GroupNorm and Weight Standardization.
 
 A PyTorch implementation of ResNetV2 adapted from the Google Big-Transfer (BiT) source code
@@ -33,8 +38,8 @@ from functools import partial
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
 from timm.layers import GroupNormAct, BatchNormAct2d, EvoNorm2dS0, FilterResponseNormTlu2d, ClassifierHead, \
@@ -47,7 +52,7 @@ __all__ = ['ResNetV2']  # model_registry will add each entrypoint fn to this
 
 
-class PreActBasic(nn.Module):
+class PreActBasic(msnn.Cell):
     """Pre-activation basic block (not in typical 'v2' implementations)."""
 
     def __init__(
@@ -102,21 +107,21 @@                 conv_layer=conv_layer,
                 norm_layer=norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.downsample = None
 
-        self.norm1 = norm_layer(in_chs, **dd)
-        self.conv1 = conv_layer(in_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups, **dd)
-        self.norm2 = norm_layer(mid_chs, **dd)
-        self.conv2 = conv_layer(mid_chs, out_chs, 3, dilation=dilation, groups=groups, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
+        self.norm1 = norm_layer(in_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv1 = conv_layer(in_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm2 = norm_layer(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv2 = conv_layer(mid_chs, out_chs, 3, dilation=dilation, groups=groups, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
 
     def zero_init_last(self) -> None:
         """Zero-initialize the last convolution weight (not applicable to basic block)."""
-        nn.init.zeros_(self.conv2.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        nn.init.zeros_(self.conv2.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -139,7 +144,7 @@         return x + shortcut
 
 
-class PreActBottleneck(nn.Module):
+class PreActBottleneck(msnn.Cell):
     """Pre-activation (v2) bottleneck block.
 
     Follows the implementation of "Identity Mappings in Deep Residual Networks":
@@ -200,23 +205,23 @@                 conv_layer=conv_layer,
                 norm_layer=norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.downsample = None
 
-        self.norm1 = norm_layer(in_chs, **dd)
-        self.conv1 = conv_layer(in_chs, mid_chs, 1, **dd)
-        self.norm2 = norm_layer(mid_chs, **dd)
-        self.conv2 = conv_layer(mid_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups, **dd)
-        self.norm3 = norm_layer(mid_chs, **dd)
-        self.conv3 = conv_layer(mid_chs, out_chs, 1, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
+        self.norm1 = norm_layer(in_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv1 = conv_layer(in_chs, mid_chs, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm2 = norm_layer(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv2 = conv_layer(mid_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm3 = norm_layer(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv3 = conv_layer(mid_chs, out_chs, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
 
     def zero_init_last(self) -> None:
         """Zero-initialize the last convolution weight."""
-        nn.init.zeros_(self.conv3.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        nn.init.zeros_(self.conv3.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -240,7 +245,7 @@         return x + shortcut
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(msnn.Cell):
     """Non Pre-activation bottleneck block, equiv to V1.5/V1b Bottleneck. Used for ViT.
     """
     def __init__(
@@ -279,25 +284,25 @@                 conv_layer=conv_layer,
                 norm_layer=norm_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.downsample = None
 
-        self.conv1 = conv_layer(in_chs, mid_chs, 1, **dd)
-        self.norm1 = norm_layer(mid_chs, **dd)
-        self.conv2 = conv_layer(mid_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups, **dd)
-        self.norm2 = norm_layer(mid_chs, **dd)
-        self.conv3 = conv_layer(mid_chs, out_chs, 1, **dd)
-        self.norm3 = norm_layer(out_chs, apply_act=False, **dd)
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
+        self.conv1 = conv_layer(in_chs, mid_chs, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm1 = norm_layer(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv2 = conv_layer(mid_chs, mid_chs, 3, stride=stride, dilation=first_dilation, groups=groups, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm2 = norm_layer(mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv3 = conv_layer(mid_chs, out_chs, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm3 = norm_layer(out_chs, apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
         self.act3 = act_layer(inplace=True)
 
     def zero_init_last(self) -> None:
         """Zero-initialize the last batch norm weight."""
         if getattr(self.norm3, 'weight', None) is not None:
-            nn.init.zeros_(self.norm3.weight)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            nn.init.zeros_(self.norm3.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -323,7 +328,7 @@         return x
 
 
-class DownsampleConv(nn.Module):
+class DownsampleConv(msnn.Cell):
     """1x1 convolution downsampling module."""
 
     def __init__(
@@ -341,10 +346,10 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.conv = conv_layer(in_chs, out_chs, 1, stride=stride, **dd)
-        self.norm = nn.Identity() if preact else norm_layer(out_chs, apply_act=False, **dd)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.conv = conv_layer(in_chs, out_chs, 1, stride=stride, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm = msnn.Identity() if preact else norm_layer(out_chs, apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -356,7 +361,7 @@         return self.norm(self.conv(x))
 
 
-class DownsampleAvg(nn.Module):
+class DownsampleAvg(msnn.Cell):
     """AvgPool downsampling as in 'D' ResNet variants."""
 
     def __init__(
@@ -379,11 +384,11 @@             avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d
             self.pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)
         else:
-            self.pool = nn.Identity()
-        self.conv = conv_layer(in_chs, out_chs, 1, stride=1, **dd)
-        self.norm = nn.Identity() if preact else norm_layer(out_chs, apply_act=False, **dd)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            self.pool = msnn.Identity()
+        self.conv = conv_layer(in_chs, out_chs, 1, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm = msnn.Identity() if preact else norm_layer(out_chs, apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -395,7 +400,7 @@         return self.norm(self.conv(self.pool(x)))
 
 
-class ResNetStage(nn.Module):
+class ResNetStage(msnn.Cell):
     """ResNet Stage."""
     def __init__(
             self,
@@ -421,7 +426,7 @@         layer_kwargs = dict(act_layer=act_layer, conv_layer=conv_layer, norm_layer=norm_layer)
         proj_layer = DownsampleAvg if avg_down else DownsampleConv
         prev_chs = in_chs
-        self.blocks = nn.Sequential()
+        self.blocks = msnn.SequentialCell()
         for block_idx in range(depth):
             drop_path_rate = block_dpr[block_idx] if block_dpr else 0.
             stride = stride if block_idx == 0 else 1
@@ -437,12 +442,12 @@                 drop_path_rate=drop_path_rate,
                 **layer_kwargs,
                 **block_kwargs,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_chs = out_chs
             first_dilation = dilation
             proj_layer = None
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through all blocks in the stage.
 
         Args:
@@ -451,6 +456,7 @@         Returns:
             Output tensor.
         """
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
@@ -479,7 +485,7 @@         norm_layer: Callable = partial(GroupNormAct, num_groups=32),
         device=None,
         dtype=None,
-) -> nn.Sequential:
+) -> msnn.SequentialCell:
     dd = {'device': device, 'dtype': dtype}
     stem = OrderedDict()
     assert stem_type in ('', 'fixed', 'same', 'deep', 'deep_fixed', 'deep_same', 'tiered')
@@ -491,34 +497,36 @@             stem_chs = (3 * out_chs // 8, out_chs // 2)  # 'T' resnets in resnet.py
         else:
             stem_chs = (out_chs // 2, out_chs // 2)  # 'D' ResNets
-        stem['conv1'] = conv_layer(in_chs, stem_chs[0], kernel_size=3, stride=2, **dd)
-        stem['norm1'] = norm_layer(stem_chs[0], **dd)
-        stem['conv2'] = conv_layer(stem_chs[0], stem_chs[1], kernel_size=3, stride=1, **dd)
-        stem['norm2'] = norm_layer(stem_chs[1], **dd)
-        stem['conv3'] = conv_layer(stem_chs[1], out_chs, kernel_size=3, stride=1, **dd)
+        stem['conv1'] = conv_layer(in_chs, stem_chs[0], kernel_size=3, stride=2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        stem['norm1'] = norm_layer(stem_chs[0], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        stem['conv2'] = conv_layer(stem_chs[0], stem_chs[1], kernel_size=3, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        stem['norm2'] = norm_layer(stem_chs[1], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        stem['conv3'] = conv_layer(stem_chs[1], out_chs, kernel_size=3, stride=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if not preact:
-            stem['norm3'] = norm_layer(out_chs, **dd)
+            stem['norm3'] = norm_layer(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     else:
         # The usual 7x7 stem conv
-        stem['conv'] = conv_layer(in_chs, out_chs, kernel_size=7, stride=2, **dd)
+        stem['conv'] = conv_layer(in_chs, out_chs, kernel_size=7, stride=2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         if not preact:
-            stem['norm'] = norm_layer(out_chs, **dd)
+            stem['norm'] = norm_layer(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     if 'fixed' in stem_type:
         # 'fixed' SAME padding approximation that is used in BiT models
         stem['pad'] = nn.ConstantPad2d(1, 0.)
-        stem['pool'] = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)
+        stem['pool'] = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 0)
     elif 'same' in stem_type:
         # full, input size based 'SAME' padding, used in ViT Hybrid model
         stem['pool'] = create_pool2d('max', kernel_size=3, stride=2, padding='same')
     else:
         # the usual PyTorch symmetric padding
-        stem['pool'] = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
-
-    return nn.Sequential(stem)
-
-
-class ResNetV2(nn.Module):
+        stem['pool'] = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)
+
+    return msnn.SequentialCell([
+        stem
+    ])
+
+
+class ResNetV2(msnn.Cell):
     """Implementation of Pre-activation (v2) ResNet mode.
     """
 
@@ -584,7 +592,7 @@             conv_layer=conv_layer,
             norm_layer=norm_layer,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stem_feat = ('stem.conv3' if is_stem_deep(stem_type) else 'stem.conv') if preact else 'stem.norm'
         self.feature_info.append(dict(num_chs=stem_chs, reduction=2, module=stem_feat))
 
@@ -597,7 +605,7 @@         else:
             assert not basic
             block_fn = Bottleneck
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         for stage_idx, (d, c, bdpr) in enumerate(zip(layers, channels, block_dprs)):
             out_chs = make_divisible(c * wf)
             stride = 1 if stage_idx == 0 else 2
@@ -618,14 +626,14 @@                 block_dpr=bdpr,
                 block_fn=block_fn,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             prev_chs = out_chs
             curr_stride *= stride
             self.feature_info += [dict(num_chs=prev_chs, reduction=curr_stride, module=f'stages.{stage_idx}')]
             self.stages.add_module(str(stage_idx), stage)
 
         self.num_features = self.head_hidden_size = prev_chs
-        self.norm = norm_layer(self.num_features, **dd) if preact else nn.Identity()
+        self.norm = norm_layer(self.num_features, **dd) if preact else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.head = ClassifierHead(
             self.num_features,
             num_classes,
@@ -633,21 +641,21 @@             drop_rate=self.drop_rate,
             use_conv=True,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.init_weights(zero_init_last=zero_init_last)
 
-    @torch.jit.ignore
+    @ms.jit
     def init_weights(self, zero_init_last: bool = True) -> None:
         """Initialize model weights."""
         named_apply(partial(_init_weights, zero_init_last=zero_init_last), self)
 
-    @torch.jit.ignore()
+    @ms.jit()
     def load_pretrained(self, checkpoint_path: str, prefix: str = 'resnet/') -> None:
         """Load pretrained weights."""
         _load_weights(self, checkpoint_path, prefix)
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse: bool = False) -> Dict[str, Any]:
         """Group parameters for optimization."""
         matcher = dict(
@@ -659,14 +667,14 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True) -> None:
         """Enable or disable gradient checkpointing."""
         for s in self.stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         """Get the classifier head."""
         return self.head.fc
 
@@ -682,13 +690,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -715,6 +723,7 @@         if feat_idx in take_indices:
             intermediates.append(x_down)
         last_idx = len(self.stages)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             stages = self.stages
         else:
@@ -748,12 +757,12 @@         take_indices, max_index = feature_take_indices(5, indices)
         self.stages = self.stages[:max_index]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction layers.
 
         Args:
@@ -767,7 +776,7 @@         x = self.norm(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -779,7 +788,7 @@         """
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -793,7 +802,7 @@         return x
 
 
-def _init_weights(module: nn.Module, name: str = '', zero_init_last: bool = True) -> None:
+def _init_weights(module: msnn.Cell, name: str = '', zero_init_last: bool = True) -> None:
     """Initialize module weights.
 
     Args:
@@ -802,28 +811,30 @@         zero_init_last: Zero-initialize last layer weights.
     """
     if isinstance(module, nn.Linear) or ('head.fc' in name and isinstance(module, nn.Conv2d)):
-        nn.init.normal_(module.weight, mean=0.0, std=0.01)
-        nn.init.zeros_(module.bias)
+        nn.init.normal_(module.weight, mean=0.0, std=0.01)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Conv2d):
-        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
+        nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, (nn.BatchNorm2d, nn.LayerNorm, nn.GroupNorm)):
-        nn.init.ones_(module.weight)
-        nn.init.zeros_(module.bias)
+        nn.init.ones_(module.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif zero_init_last and hasattr(module, 'zero_init_last'):
         module.zero_init_last()
 
 
+# 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 装饰器 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 @torch.no_grad()
-def _load_weights(model: nn.Module, checkpoint_path: str, prefix: str = 'resnet/'):
+def _load_weights(model: msnn.Cell, checkpoint_path: str, prefix: str = 'resnet/'):
     import numpy as np
 
     def t2p(conv_weights):
         """Possibly convert HWIO to OIHW."""
         if conv_weights.ndim == 4:
             conv_weights = conv_weights.transpose([3, 2, 0, 1])
-        return torch.from_numpy(conv_weights)
+        return torch.from_numpy(conv_weights)  # 'torch.from_numpy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     weights = np.load(checkpoint_path)
     stem_conv_w = adapt_input_conv(
@@ -869,7 +880,7 @@         ResNetV2, variant, pretrained,
         feature_cfg=feature_cfg,
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _create_resnetv2_bit(variant: str, pretrained: bool = False, **kwargs: Any) -> ResNetV2:
@@ -889,7 +900,7 @@         stem_type='fixed',
         conv_layer=partial(StdConv2d, eps=1e-8),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url: str = '', **kwargs: Any) -> Dict[str, Any]:
@@ -1008,42 +1019,42 @@ def resnetv2_50x1_bit(pretrained: bool = False, **kwargs: Any) -> ResNetV2:
     """ResNetV2-50x1-BiT model."""
     return _create_resnetv2_bit(
-        'resnetv2_50x1_bit', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=1, **kwargs)
+        'resnetv2_50x1_bit', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=1, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnetv2_50x3_bit(pretrained: bool = False, **kwargs: Any) -> ResNetV2:
     """ResNetV2-50x3-BiT model."""
     return _create_resnetv2_bit(
-        'resnetv2_50x3_bit', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=3, **kwargs)
+        'resnetv2_50x3_bit', pretrained=pretrained, layers=[3, 4, 6, 3], width_factor=3, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnetv2_101x1_bit(pretrained: bool = False, **kwargs: Any) -> ResNetV2:
     """ResNetV2-101x1-BiT model."""
     return _create_resnetv2_bit(
-        'resnetv2_101x1_bit', pretrained=pretrained, layers=[3, 4, 23, 3], width_factor=1, **kwargs)
+        'resnetv2_101x1_bit', pretrained=pretrained, layers=[3, 4, 23, 3], width_factor=1, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnetv2_101x3_bit(pretrained: bool = False, **kwargs: Any) -> ResNetV2:
     """ResNetV2-101x3-BiT model."""
     return _create_resnetv2_bit(
-        'resnetv2_101x3_bit', pretrained=pretrained, layers=[3, 4, 23, 3], width_factor=3, **kwargs)
+        'resnetv2_101x3_bit', pretrained=pretrained, layers=[3, 4, 23, 3], width_factor=3, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnetv2_152x2_bit(pretrained: bool = False, **kwargs: Any) -> ResNetV2:
     """ResNetV2-152x2-BiT model."""
     return _create_resnetv2_bit(
-        'resnetv2_152x2_bit', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=2, **kwargs)
+        'resnetv2_152x2_bit', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=2, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnetv2_152x4_bit(pretrained: bool = False, **kwargs: Any) -> ResNetV2:
     """ResNetV2-152x4-BiT model."""
     return _create_resnetv2_bit(
-        'resnetv2_152x4_bit', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=4, **kwargs)
+        'resnetv2_152x4_bit', pretrained=pretrained, layers=[3, 8, 36, 3], width_factor=4, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1053,7 +1064,7 @@         layers=[2, 2, 2, 2], channels=(64, 128, 256, 512), basic=True, bottle_ratio=1.0,
         conv_layer=create_conv2d, norm_layer=BatchNormAct2d
     )
-    return _create_resnetv2('resnetv2_18', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_18', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1063,7 +1074,7 @@         layers=[2, 2, 2, 2], channels=(64, 128, 256, 512), basic=True, bottle_ratio=1.0,
         conv_layer=create_conv2d, norm_layer=BatchNormAct2d, stem_type='deep', avg_down=True
     )
-    return _create_resnetv2('resnetv2_18d', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_18d', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1073,7 +1084,7 @@         layers=(3, 4, 6, 3), channels=(64, 128, 256, 512), basic=True, bottle_ratio=1.0,
         conv_layer=create_conv2d, norm_layer=BatchNormAct2d
     )
-    return _create_resnetv2('resnetv2_34', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_34', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1083,14 +1094,14 @@         layers=(3, 4, 6, 3), channels=(64, 128, 256, 512), basic=True, bottle_ratio=1.0,
         conv_layer=create_conv2d, norm_layer=BatchNormAct2d, stem_type='deep', avg_down=True
     )
-    return _create_resnetv2('resnetv2_34d', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_34d', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnetv2_50(pretrained: bool = False, **kwargs: Any) -> ResNetV2:
     """ResNetV2-50 model."""
     model_args = dict(layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d)
-    return _create_resnetv2('resnetv2_50', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_50', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1099,7 +1110,7 @@     model_args = dict(
         layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,
         stem_type='deep', avg_down=True)
-    return _create_resnetv2('resnetv2_50d', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_50d', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1108,14 +1119,14 @@     model_args = dict(
         layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,
         stem_type='tiered', avg_down=True)
-    return _create_resnetv2('resnetv2_50t', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_50t', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnetv2_101(pretrained: bool = False, **kwargs: Any) -> ResNetV2:
     """ResNetV2-101 model."""
     model_args = dict(layers=[3, 4, 23, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d)
-    return _create_resnetv2('resnetv2_101', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_101', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1124,14 +1135,14 @@     model_args = dict(
         layers=[3, 4, 23, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,
         stem_type='deep', avg_down=True)
-    return _create_resnetv2('resnetv2_101d', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_101d', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def resnetv2_152(pretrained: bool = False, **kwargs: Any) -> ResNetV2:
     """ResNetV2-152 model."""
     model_args = dict(layers=[3, 8, 36, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d)
-    return _create_resnetv2('resnetv2_152', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_152', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1140,7 +1151,7 @@     model_args = dict(
         layers=[3, 8, 36, 3], conv_layer=create_conv2d, norm_layer=BatchNormAct2d,
         stem_type='deep', avg_down=True)
-    return _create_resnetv2('resnetv2_152d', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_152d', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 # Experimental configs (may change / be removed)
@@ -1151,7 +1162,7 @@     model_args = dict(
         layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=GroupNormAct,
         stem_type='deep', avg_down=True)
-    return _create_resnetv2('resnetv2_50d_gn', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_50d_gn', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1160,7 +1171,7 @@     model_args = dict(
         layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=EvoNorm2dS0,
         stem_type='deep', avg_down=True)
-    return _create_resnetv2('resnetv2_50d_evos', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_50d_evos', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -1169,7 +1180,7 @@     model_args = dict(
         layers=[3, 4, 6, 3], conv_layer=create_conv2d, norm_layer=FilterResponseNormTlu2d,
         stem_type='deep', avg_down=True)
-    return _create_resnetv2('resnetv2_50d_frn', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_resnetv2('resnetv2_50d_frn', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 register_model_deprecations(__name__, {
