--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """PyTorch SelecSLS Net example for ImageNet Classification
 License: CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/legalcode)
 Author: Dushyant Mehta (@mehtadushy)
@@ -11,8 +16,8 @@ """
 from typing import List, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import create_classifier
@@ -22,7 +27,7 @@ __all__ = ['SelecSls']  # model_registry will add each entrypoint fn to this
 
 
-class SequentialList(nn.Sequential):
+class SequentialList(msnn.SequentialCell):
 
     def __init__(self, *args):
         super().__init__(*args)
@@ -37,47 +42,48 @@         # type: (torch.Tensor) -> (List[torch.Tensor])
         pass
 
-    def forward(self, x) -> List[torch.Tensor]:
+    def forward(self, x) -> List[ms.Tensor]:
         for module in self:
             x = module(x)
         return x
 
 
-class SelectSeq(nn.Module):
+class SelectSeq(msnn.Cell):
     def __init__(self, mode='index', index=0):
         super().__init__()
         self.mode = mode
         self.index = index
 
     @torch.jit._overload_method  # noqa: F811
-    def forward(self, x):
+    def construct(self, x):
         # type: (List[torch.Tensor]) -> (torch.Tensor)
         pass
 
     @torch.jit._overload_method  # noqa: F811
-    def forward(self, x):
+    def construct(self, x):
         # type: (Tuple[torch.Tensor]) -> (torch.Tensor)
         pass
 
-    def forward(self, x) -> torch.Tensor:
+    def construct(self, x) -> ms.Tensor:
         if self.mode == 'index':
             return x[self.index]
         else:
-            return torch.cat(x, dim=1)
+            return mint.cat(x, dim = 1)
 
 
 def conv_bn(in_chs, out_chs, k=3, stride=1, padding=None, dilation=1, device=None, dtype=None):
     dd = {'device': device, 'dtype': dtype}
     if padding is None:
         padding = ((stride - 1) + dilation * (k - 1)) // 2
-    return nn.Sequential(
+    return msnn.SequentialCell(
+        [
         nn.Conv2d(in_chs, out_chs, k, stride, padding=padding, dilation=dilation, bias=False, **dd),
         nn.BatchNorm2d(out_chs, **dd),
-        nn.ReLU(inplace=True)
-    )
-
-
-class SelecSlsBlock(nn.Module):
+        nn.ReLU()
+    ])  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+
+class SelecSlsBlock(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -104,7 +110,7 @@         self.conv5 = conv_bn(mid_chs, mid_chs // 2, 3, **dd)
         self.conv6 = conv_bn(2 * mid_chs + (0 if is_first else skip_chs), out_chs, 1, **dd)
 
-    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:
+    def construct(self, x: List[ms.Tensor]) -> List[ms.Tensor]:
         if not isinstance(x, list):
             x = [x]
         assert len(x) in [1, 2]
@@ -113,13 +119,13 @@         d2 = self.conv3(self.conv2(d1))
         d3 = self.conv5(self.conv4(d2))
         if self.is_first:
-            out = self.conv6(torch.cat([d1, d2, d3], 1))
+            out = self.conv6(mint.cat([d1, d2, d3], 1))
             return [out, out]
         else:
-            return [self.conv6(torch.cat([d1, d2, d3, x[1]], 1)), x[1]]
-
-
-class SelecSls(nn.Module):
+            return [self.conv6(mint.cat([d1, d2, d3, x[1]], 1)), x[1]]
+
+
+class SelecSls(msnn.Cell):
     """SelecSls42 / SelecSls60 / SelecSls84
 
     Parameters
@@ -152,7 +158,9 @@         self.stem = conv_bn(in_chans, 32, stride=2, **dd)
         self.features = SequentialList(*[cfg['block'](*block_args, **dd) for block_args in cfg['features']])
         self.from_seq = SelectSeq()  # from List[tensor] -> Tensor in module compatible way
-        self.head = nn.Sequential(*[conv_bn(*conv_args, **dd) for conv_args in cfg['head']])
+        self.head = msnn.SequentialCell([
+            [conv_bn(*conv_args, **dd) for conv_args in cfg['head']]
+        ])
         self.num_features = self.head_hidden_size = cfg['num_features']
         self.feature_info = cfg['feature_info']
 
@@ -166,7 +174,7 @@ 
         for n, m in self.named_modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -181,7 +189,7 @@         assert not enable, 'gradient checkpointing not supported'
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.fc
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -205,7 +213,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.fc(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
