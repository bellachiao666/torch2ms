--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Sequencer
 
 Paper: `Sequencer: Deep LSTM for Image Classification` - https://arxiv.org/pdf/2205.01972.pdf
@@ -11,8 +16,8 @@ from itertools import accumulate
 from typing import List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, DEFAULT_CROP_PCT
 from timm.layers import lecun_normal_, DropPath, Mlp, PatchEmbed, ClassifierHead
@@ -23,48 +28,48 @@ __all__ = ['Sequencer2d']  # model_registry will add each entrypoint fn to this
 
 
-def _init_weights(module: nn.Module, name: str, head_bias: float = 0., flax=False):
+def _init_weights(module: msnn.Cell, name: str, head_bias: float = 0., flax=False):
     if isinstance(module, nn.Linear):
         if name.startswith('head'):
-            nn.init.zeros_(module.weight)
-            nn.init.constant_(module.bias, head_bias)
+            nn.init.zeros_(module.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(module.bias, head_bias)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             if flax:
                 # Flax defaults
                 lecun_normal_(module.weight)
                 if module.bias is not None:
-                    nn.init.zeros_(module.bias)
+                    nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
-                nn.init.xavier_uniform_(module.weight)
+                nn.init.xavier_uniform_(module.weight)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 if module.bias is not None:
                     if 'mlp' in name:
-                        nn.init.normal_(module.bias, std=1e-6)
+                        nn.init.normal_(module.bias, std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                     else:
-                        nn.init.zeros_(module.bias)
+                        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Conv2d):
         lecun_normal_(module.weight)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, (nn.LayerNorm, nn.BatchNorm2d, nn.GroupNorm)):
-        nn.init.ones_(module.weight)
-        nn.init.zeros_(module.bias)
+        nn.init.ones_(module.weight)  # 'torch.nn.init.ones_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, (nn.RNN, nn.GRU, nn.LSTM)):
         stdv = 1.0 / math.sqrt(module.hidden_size)
         for weight in module.parameters():
-            nn.init.uniform_(weight, -stdv, stdv)
+            nn.init.uniform_(weight, -stdv, stdv)  # 'torch.nn.init.uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
 
-class RNNIdentity(nn.Module):
+class RNNIdentity(msnn.Cell):
     def __init__(self, *args, **kwargs):
         super().__init__()
 
-    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, None]:
+    def construct(self, x: ms.Tensor) -> Tuple[ms.Tensor, None]:
         return x, None
 
 
-class RNN2dBase(nn.Module):
+class RNN2dBase(msnn.Cell):
 
     def __init__(
             self,
@@ -126,7 +131,7 @@         self.rnn_v = RNNIdentity()
         self.rnn_h = RNNIdentity()
 
-    def forward(self, x):
+    def construct(self, x):
         B, H, W, C = x.shape
 
         if self.with_vertical:
@@ -147,7 +152,7 @@ 
         if v is not None and h is not None:
             if self.union == "cat":
-                x = torch.cat([v, h], dim=-1)
+                x = mint.cat([v, h], dim = -1)
             else:
                 x = v + h
         elif v is not None:
@@ -186,7 +191,7 @@                 bias=bias,
                 bidirectional=bidirectional,
                 **dd,
-            )
+            )  # 'torch.nn.LSTM' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.with_horizontal:
             self.rnn_h = nn.LSTM(
                 input_size,
@@ -196,19 +201,19 @@                 bias=bias,
                 bidirectional=bidirectional,
                 **dd,
-            )
-
-
-class Sequencer2dBlock(nn.Module):
+            )  # 'torch.nn.LSTM' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+
+class Sequencer2dBlock(msnn.Cell):
     def __init__(
             self,
             dim: int,
             hidden_size: int,
             mlp_ratio: float = 3.0,
-            rnn_layer: Type[nn.Module] = LSTM2d,
-            mlp_layer: Type[nn.Module] = Mlp,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
+            rnn_layer: Type[msnn.Cell] = LSTM2d,
+            mlp_layer: Type[msnn.Cell] = Mlp,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
             num_layers: int = 1,
             bidirectional: bool = True,
             union: str = "cat",
@@ -231,30 +236,30 @@             with_fc=with_fc,
             **dd,
         )
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         self.norm2 = norm_layer(dim, **dd)
         self.mlp_channels = mlp_layer(dim, channels_dim, act_layer=act_layer, drop=drop, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = x + self.drop_path(self.rnn_tokens(self.norm1(x)))
         x = x + self.drop_path(self.mlp_channels(self.norm2(x)))
         return x
 
 
-class Shuffle(nn.Module):
+class Shuffle(msnn.Cell):
     def __init__(self):
         super().__init__()
 
-    def forward(self, x):
+    def construct(self, x):
         if self.training:
             B, H, W, C = x.shape
-            r = torch.randperm(H * W)
+            r = mint.randperm(H * W)  # 'torch.randperm'默认值不一致(position 3): PyTorch=torch.int64, MindSpore=mindspore.int64;
             x = x.reshape(B, -1, C)
             x = x[:, r, :].reshape(B, H, W, -1)
         return x
 
 
-class Downsample2d(nn.Module):
+class Downsample2d(msnn.Cell):
     def __init__(
             self,
             input_dim: int,
@@ -267,14 +272,14 @@         super().__init__()
         self.down = nn.Conv2d(input_dim, output_dim, kernel_size=patch_size, stride=patch_size, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = x.permute(0, 3, 1, 2)
         x = self.down(x)
         x = x.permute(0, 2, 3, 1)
         return x
 
 
-class Sequencer2dStage(nn.Module):
+class Sequencer2dStage(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -284,11 +289,11 @@             hidden_size: int,
             mlp_ratio: float,
             downsample: bool = False,
-            block_layer: Type[nn.Module] = Sequencer2dBlock,
-            rnn_layer: Type[nn.Module] = LSTM2d,
-            mlp_layer: Type[nn.Module] = Mlp,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
+            block_layer: Type[msnn.Cell] = Sequencer2dBlock,
+            rnn_layer: Type[msnn.Cell] = LSTM2d,
+            mlp_layer: Type[msnn.Cell] = Mlp,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
             num_layers: int = 1,
             bidirectional: bool = True,
             union: str = "cat",
@@ -304,7 +309,7 @@             self.downsample = Downsample2d(dim, dim_out, patch_size, **dd)
         else:
             assert dim == dim_out
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         blocks = []
         for block_idx in range(depth):
@@ -324,15 +329,17 @@                 drop_path=drop_path[block_idx] if isinstance(drop_path, (list, tuple)) else drop_path,
                 **dd,
             ))
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x):
         x = self.downsample(x)
         x = self.blocks(x)
         return x
 
 
-class Sequencer2d(nn.Module):
+class Sequencer2d(msnn.Cell):
     def __init__(
             self,
             num_classes: int = 1000,
@@ -344,11 +351,11 @@             embed_dims: Tuple[int, ...] = (192, 384, 384, 384),
             hidden_sizes: Tuple[int, ...] = (48, 96, 96, 96),
             mlp_ratios: Tuple[float, ...] = (3.0, 3.0, 3.0, 3.0),
-            block_layer: Type[nn.Module] = Sequencer2dBlock,
-            rnn_layer: Type[nn.Module] = LSTM2d,
-            mlp_layer: Type[nn.Module] = Mlp,
-            norm_layer: Type[nn.Module] = partial(nn.LayerNorm, eps=1e-6),
-            act_layer: Type[nn.Module] = nn.GELU,
+            block_layer: Type[msnn.Cell] = Sequencer2dBlock,
+            rnn_layer: Type[msnn.Cell] = LSTM2d,
+            mlp_layer: Type[msnn.Cell] = Mlp,
+            norm_layer: Type[msnn.Cell] = partial(nn.LayerNorm, eps=1e-6),
+            act_layer: Type[msnn.Cell] = nn.GELU,
             num_rnn_layers: int = 1,
             bidirectional: bool = True,
             union: str = "cat",
@@ -410,7 +417,9 @@             prev_dim = embed_dims[i]
             self.feature_info += [dict(num_chs=prev_dim, reduction=reductions[i], module=f'stages.{i}')]
 
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell([
+            stages
+        ])
         self.norm = norm_layer(embed_dims[-1], **dd)
         self.head = ClassifierHead(
             self.num_features,
@@ -446,7 +455,7 @@         assert not enable, 'gradient checkpointing not supported'
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -462,7 +471,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
