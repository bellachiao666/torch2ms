--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Selective Kernel Networks (ResNet base)
 
 Paper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)
@@ -11,7 +16,7 @@ import math
 from typing import Optional, Type
 
-from torch import nn as nn
+# from torch import nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import SelectiveKernel, ConvNormAct, create_attn
@@ -20,7 +25,7 @@ from .resnet import ResNet
 
 
-class SelectiveKernelBasic(nn.Module):
+class SelectiveKernelBasic(msnn.Cell):
     expansion = 1
 
     def __init__(
@@ -28,19 +33,19 @@             inplanes: int,
             planes: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             cardinality: int = 1,
             base_width: int = 64,
             sk_kwargs: Optional[dict] = None,
             reduce_first: int = 1,
             dilation: int = 1,
             first_dilation: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            attn_layer: Optional[Type[nn.Module]] = None,
-            aa_layer: Optional[Type[nn.Module]] = None,
-            drop_block: Optional[nn.Module] = None,
-            drop_path: Optional[nn.Module] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            attn_layer: Optional[Type[msnn.Cell]] = None,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
+            drop_block: Optional[msnn.Cell] = None,
+            drop_path: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
@@ -80,9 +85,9 @@ 
     def zero_init_last(self):
         if getattr(self.conv2.bn, 'weight', None) is not None:
-            nn.init.zeros_(self.conv2.bn.weight)
-
-    def forward(self, x):
+            nn.init.zeros_(self.conv2.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x):
         shortcut = x
         x = self.conv1(x)
         x = self.conv2(x)
@@ -97,7 +102,7 @@         return x
 
 
-class SelectiveKernelBottleneck(nn.Module):
+class SelectiveKernelBottleneck(msnn.Cell):
     expansion = 4
 
     def __init__(
@@ -105,19 +110,19 @@             inplanes: int,
             planes: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             cardinality: int = 1,
             base_width: int = 64,
             sk_kwargs: Optional[dict] = None,
             reduce_first: int = 1,
             dilation: int = 1,
             first_dilation: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
-            attn_layer: Optional[Type[nn.Module]] = None,
-            aa_layer: Optional[Type[nn.Module]] = None,
-            drop_block: Optional[nn.Module] = None,
-            drop_path: Optional[nn.Module] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
+            attn_layer: Optional[Type[msnn.Cell]] = None,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
+            drop_block: Optional[msnn.Cell] = None,
+            drop_path: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
@@ -151,9 +156,9 @@ 
     def zero_init_last(self):
         if getattr(self.conv3.bn, 'weight', None) is not None:
-            nn.init.zeros_(self.conv3.bn.weight)
-
-    def forward(self, x):
+            nn.init.zeros_(self.conv3.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x):
         shortcut = x
         x = self.conv1(x)
         x = self.conv2(x)
