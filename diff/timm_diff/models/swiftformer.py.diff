--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """SwiftFormer
 SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications
 Code: https://github.com/Amshaker/SwiftFormer
@@ -13,9 +18,8 @@ import re
 from typing import Any, Dict, List, Optional, Set, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, Linear, LayerType, to_2tuple, trunc_normal_
@@ -27,19 +31,19 @@ __all__ = ['SwiftFormer']
 
 
-class LayerScale2d(nn.Module):
+class LayerScale2d(msnn.Cell):
     def __init__(self, dim: int, init_values: float = 1e-5, inplace: bool = False, device=None, dtype=None):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.inplace = inplace
-        self.gamma = nn.Parameter(
-            init_values * torch.ones(dim, 1, 1, **dd), requires_grad=True)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.gamma = ms.Parameter(
+            init_values * mint.ones(dim, 1, 1, **dd), requires_grad=True)
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         return x.mul_(self.gamma) if self.inplace else x * self.gamma
 
 
-class Embedding(nn.Module):
+class Embedding(msnn.Cell):
     """
     Patch Embedding that is implemented by a layer of conv.
     Input: tensor in shape [B, C, H, W]
@@ -52,7 +56,7 @@             patch_size: int = 16,
             stride: int = 16,
             padding: int = 0,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -62,15 +66,15 @@         stride = to_2tuple(stride)
         padding = to_2tuple(padding)
         self.proj = nn.Conv2d(in_chans, embed_dim, patch_size, stride, padding, **dd)
-        self.norm = norm_layer(embed_dim, **dd) if norm_layer else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.norm = norm_layer(embed_dim, **dd) if norm_layer else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.proj(x)
         x = self.norm(x)
         return x
 
 
-class ConvEncoder(nn.Module):
+class ConvEncoder(msnn.Cell):
     """
     Implementation of ConvEncoder with 3*3 and 1*1 convolutions.
     Input: tensor with shape [B, C, H, W]
@@ -82,8 +86,8 @@             hidden_dim: int = 64,
             kernel_size: int = 3,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             use_layer_scale: bool = True,
             device=None,
             dtype=None,
@@ -95,10 +99,10 @@         self.pwconv1 = nn.Conv2d(dim, hidden_dim, 1, **dd)
         self.act = act_layer()
         self.pwconv2 = nn.Conv2d(hidden_dim, dim, 1, **dd)
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.layer_scale = LayerScale2d(dim, 1, **dd) if use_layer_scale else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+        self.layer_scale = LayerScale2d(dim, 1, **dd) if use_layer_scale else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         input = x
         x = self.dwconv(x)
         x = self.norm(x)
@@ -110,7 +114,7 @@         return x
 
 
-class Mlp(nn.Module):
+class Mlp(msnn.Cell):
     """
     Implementation of MLP layer with 1*1 convolutions.
     Input: tensor with shape [B, C, H, W]
@@ -121,8 +125,8 @@             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             drop: float = 0.,
             device=None,
             dtype=None,
@@ -137,7 +141,7 @@         self.fc2 = nn.Conv2d(hidden_features, out_features, 1, **dd)
         self.drop = nn.Dropout(drop)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.norm1(x)
         x = self.fc1(x)
         x = self.act(x)
@@ -147,7 +151,7 @@         return x
 
 
-class EfficientAdditiveAttention(nn.Module):
+class EfficientAdditiveAttention(msnn.Cell):
     """
     Efficient Additive Attention module for SwiftFormer.
     Input: tensor in shape [B, C, H, W]
@@ -167,27 +171,27 @@         self.to_query = nn.Linear(in_dims, token_dim * num_heads, **dd)
         self.to_key = nn.Linear(in_dims, token_dim * num_heads, **dd)
 
-        self.w_g = nn.Parameter(torch.randn(token_dim * num_heads, 1, **dd))
+        self.w_g = ms.Parameter(mint.randn(token_dim * num_heads, 1, **dd))
 
         self.proj = nn.Linear(token_dim * num_heads, token_dim * num_heads, **dd)
         self.final = nn.Linear(token_dim * num_heads, token_dim, **dd)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         B, _, H, W = x.shape
         x = x.flatten(2).permute(0, 2, 1)
 
-        query = F.normalize(self.to_query(x), dim=-1)
-        key = F.normalize(self.to_key(x), dim=-1)
-
-        attn = F.normalize(query @ self.w_g * self.scale_factor, dim=1)
-        attn = torch.sum(attn * query, dim=1, keepdim=True)
+        query = nn.functional.normalize(self.to_query(x), dim = -1)
+        key = nn.functional.normalize(self.to_key(x), dim = -1)
+
+        attn = nn.functional.normalize(query @ self.w_g * self.scale_factor, dim = 1)
+        attn = mint.sum(attn * query)
 
         out = self.proj(attn * key) + query
         out = self.final(out).permute(0, 2, 1).reshape(B, -1, H, W)
         return out
 
 
-class LocalRepresentation(nn.Module):
+class LocalRepresentation(msnn.Cell):
     """
     Local Representation module for SwiftFormer that is implemented by 3*3 depth-wise and point-wise convolutions.
     Input: tensor in shape [B, C, H, W]
@@ -199,8 +203,8 @@             kernel_size: int = 3,
             drop_path: float = 0.,
             use_layer_scale: bool = True,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -211,10 +215,10 @@         self.pwconv1 = nn.Conv2d(dim, dim, kernel_size=1, **dd)
         self.act = act_layer()
         self.pwconv2 = nn.Conv2d(dim, dim, kernel_size=1, **dd)
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-        self.layer_scale = LayerScale2d(dim, 1, **dd) if use_layer_scale else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+        self.layer_scale = LayerScale2d(dim, 1, **dd) if use_layer_scale else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         skip = x
         x = self.dwconv(x)
         x = self.norm(x)
@@ -226,7 +230,7 @@         return x
 
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     """
     SwiftFormer Encoder Block for SwiftFormer. It consists of :
     (1) Local representation module, (2) EfficientAdditiveAttention, and (3) MLP block.
@@ -239,8 +243,8 @@             mlp_ratio: float = 4.,
             drop_rate: float = 0.,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             use_layer_scale: bool = True,
             layer_scale_init_value: float = 1e-5,
             device=None,
@@ -264,20 +268,20 @@             drop=drop_rate,
             **dd,
         )
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         self.layer_scale_1 = LayerScale2d(dim, layer_scale_init_value, **dd) \
-            if use_layer_scale else nn.Identity()
+            if use_layer_scale else msnn.Identity()
         self.layer_scale_2 = LayerScale2d(dim, layer_scale_init_value, **dd) \
-            if use_layer_scale else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+            if use_layer_scale else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.local_representation(x)
         x = x + self.drop_path(self.layer_scale_1(self.attn(x)))
         x = x + self.drop_path(self.layer_scale_2(self.linear(x)))
         return x
 
 
-class Stage(nn.Module):
+class Stage(msnn.Cell):
     """
     Implementation of each SwiftFormer stages. Here, SwiftFormerEncoder used as the last block in all stages, while ConvEncoder used in the rest of the blocks.
     Input: tensor in shape [B, C, H, W]
@@ -289,20 +293,20 @@             index: int,
             layers: List[int],
             mlp_ratio: float = 4.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             drop_rate: float = 0.,
             drop_path_rate: float = 0.,
             use_layer_scale: bool = True,
             layer_scale_init_value: float = 1e-5,
-            downsample: Optional[Type[nn.Module]] = None,
+            downsample: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.grad_checkpointing = False
-        self.downsample = downsample if downsample is not None else nn.Identity()
+        self.downsample = downsample if downsample is not None else msnn.Identity()
 
         blocks = []
         for block_idx in range(layers[index]):
@@ -330,9 +334,11 @@                     use_layer_scale=use_layer_scale,
                     **dd,
                 ))
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.downsample(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
@@ -341,14 +347,14 @@         return x
 
 
-class SwiftFormer(nn.Module):
+class SwiftFormer(msnn.Cell):
     def __init__(
             self,
             layers: List[int] = [3, 3, 6, 4],
             embed_dims: List[int] = [48, 56, 112, 220],
             mlp_ratios: int = 4,
             downsamples: List[bool] = [False, True, True, True],
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             down_patch_size: int = 3,
             down_stride: int = 2,
             down_pad: int = 1,
@@ -371,14 +377,15 @@         self.global_pool = global_pool
         self.feature_info = []
 
-        self.stem = nn.Sequential(
+        self.stem = msnn.SequentialCell(
+            [
             nn.Conv2d(in_chans, embed_dims[0] // 2, 3, 2, 1, **dd),
             nn.BatchNorm2d(embed_dims[0] // 2, **dd),
             nn.ReLU(),
             nn.Conv2d(embed_dims[0] // 2, embed_dims[0], 3, 2, 1, **dd),
             nn.BatchNorm2d(embed_dims[0], **dd),
-            nn.ReLU(),
-        )
+            nn.ReLU()
+        ])
         prev_dim = embed_dims[0]
 
         stages = []
@@ -390,7 +397,7 @@                 stride=down_stride,
                 padding=down_pad,
                 **dd,
-            ) if downsamples[i] else nn.Identity()
+            ) if downsamples[i] else msnn.Identity()
             stage = Stage(
                 dim=embed_dims[i],
                 index=i,
@@ -407,15 +414,17 @@             prev_dim = embed_dims[i]
             stages.append(stage)
             self.feature_info += [dict(num_chs=embed_dims[i], reduction=2**(i+2), module=f'stages.{i}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell([
+            stages
+        ])
 
         # Classifier head
         self.num_features  = self.head_hidden_size = out_chs = embed_dims[-1]
         self.norm = nn.BatchNorm2d(out_chs, **dd)
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = Linear(out_chs, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = Linear(out_chs, num_classes, **dd) if num_classes > 0 else msnn.Identity()
         # assuming model is always distilled (valid for current checkpoints, will split def if that changes)
-        self.head_dist = Linear(out_chs, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head_dist = Linear(out_chs, num_classes, **dd) if num_classes > 0 else msnn.Identity()
         self.distilled_training = False  # must set this True to train w/ distillation token
         self._initialize_weights()
 
@@ -424,11 +433,11 @@             if isinstance(m, nn.Linear):
                 trunc_normal_(m.weight, std=.02)
                 if m.bias is not None:
-                    nn.init.constant_(m.bias, 0)
+                    nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.Conv2d):
                 trunc_normal_(m.weight, std=.02)
                 if m.bias is not None:
-                    nn.init.constant_(m.bias, 0)
+                    nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def no_weight_decay(self) -> Set:
@@ -452,7 +461,7 @@             s.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> Tuple[nn.Module, nn.Module]:
+    def get_classifier(self) -> Tuple[msnn.Cell, msnn.Cell]:
         return self.head, self.head_dist
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -460,8 +469,8 @@         if global_pool is not None:
             self.global_pool = global_pool
         device, dtype = self.head.weight.device, self.head.weight.dtype if hasattr(self.head, 'weight') else (None, None)
-        self.head = Linear(self.num_features, num_classes, device=device, dtype=dtype) if num_classes > 0 else nn.Identity()
-        self.head_dist = Linear(self.num_features, num_classes, device=device, dtype=dtype) if num_classes > 0 else nn.Identity()
+        self.head = Linear(self.num_features, num_classes, device=device, dtype=dtype) if num_classes > 0 else msnn.Identity()
+        self.head_dist = Linear(self.num_features, num_classes, device=device, dtype=dtype) if num_classes > 0 else msnn.Identity()
 
     @torch.jit.ignore
     def set_distilled_training(self, enable: bool = True):
@@ -469,13 +478,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -528,18 +537,18 @@         take_indices, max_index = feature_take_indices(len(self.stages), indices)
         self.stages = self.stages[:max_index + 1]  # truncate blocks w/ stem as idx 0
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         x = self.stem(x)
         x = self.stages(x)
         x = self.norm(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False):
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False):
         if self.global_pool == 'avg':
             x = x.mean(dim=(2, 3))
         x = self.head_drop(x)
@@ -553,13 +562,13 @@             # during standard train/finetune, inference average the classifier predictions
             return (x + x_dist) / 2
 
-    def forward(self, x: torch.Tensor):
+    def construct(self, x: ms.Tensor):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
-def checkpoint_filter_fn(state_dict: Dict[str, torch.Tensor], model: nn.Module) -> Dict[str, torch.Tensor]:
+def checkpoint_filter_fn(state_dict: Dict[str, ms.Tensor], model: msnn.Cell) -> Dict[str, ms.Tensor]:
     state_dict = state_dict.get('model', state_dict)
     if 'stem.0.weight' in state_dict:
         return state_dict
