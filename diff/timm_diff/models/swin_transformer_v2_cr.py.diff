--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Swin Transformer V2
 
 A PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`
@@ -31,9 +36,8 @@ import math
 from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, calculate_drop_path_rates, Mlp, ClassifierHead, to_2tuple, _assert, ndgrid
@@ -48,17 +52,17 @@ _logger = logging.getLogger(__name__)
 
 
-def bchw_to_bhwc(x: torch.Tensor) -> torch.Tensor:
+def bchw_to_bhwc(x: ms.Tensor) -> ms.Tensor:
     """Permutes a tensor from the shape (B, C, H, W) to (B, H, W, C)."""
     return x.permute(0, 2, 3, 1)
 
 
-def bhwc_to_bchw(x: torch.Tensor) -> torch.Tensor:
+def bhwc_to_bchw(x: ms.Tensor) -> ms.Tensor:
     """Permutes a tensor from the shape (B, H, W, C) to (B, C, H, W)."""
     return x.permute(0, 3, 1, 2)
 
 
-def window_partition(x: torch.Tensor, window_size: Tuple[int, int]) -> torch.Tensor:
+def window_partition(x: ms.Tensor, window_size: Tuple[int, int]) -> ms.Tensor:
     """Partition into non-overlapping windows.
 
     Args:
@@ -75,7 +79,7 @@ 
 
 @register_notrace_function  # reason: int argument is a Proxy
-def window_reverse(windows: torch.Tensor, window_size: Tuple[int, int], img_size: Tuple[int, int]) -> torch.Tensor:
+def window_reverse(windows: ms.Tensor, window_size: Tuple[int, int], img_size: Tuple[int, int]) -> ms.Tensor:
     """Merge windows back to feature map.
 
     Args:
@@ -93,7 +97,7 @@     return x
 
 
-class WindowMultiHeadAttention(nn.Module):
+class WindowMultiHeadAttention(msnn.Cell):
     r"""This class implements window-based Multi-Head-Attention with log-spaced continuous position bias.
 
     Args:
@@ -141,19 +145,19 @@             **dd,
         )
         # NOTE old checkpoints used inverse of logit_scale ('tau') following the paper, see conversion fn
-        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones(num_heads, **dd)))
+        self.logit_scale = ms.Parameter(mint.log(10 * mint.ones(num_heads, **dd)))
         self._make_pair_wise_relative_positions()
 
     def _make_pair_wise_relative_positions(self) -> None:
         """Initialize the pair-wise relative positions to compute the positional biases."""
         device = self.logit_scale.device
-        coordinates = torch.stack(ndgrid(
-            torch.arange(self.window_size[0], device=device, dtype=torch.float32),
-            torch.arange(self.window_size[1], device=device, dtype=torch.float32),
-        )).flatten(1)
+        coordinates = mint.stack(ndgrid(
+            mint.arange(self.window_size[0], dtype = ms.float32),
+            mint.arange(self.window_size[1], dtype = ms.float32),
+        )).flatten(1)  # 'torch.arange':没有对应的mindspore参数 'device' (position 6);
         relative_coordinates = coordinates[:, :, None] - coordinates[:, None, :]
         relative_coordinates = relative_coordinates.permute(1, 2, 0).reshape(-1, 2).float()
-        relative_coordinates_log = torch.sign(relative_coordinates) * torch.log(
+        relative_coordinates_log = mint.sign(relative_coordinates) * mint.log(
             1.0 + relative_coordinates.abs())
         self.register_buffer(
             "relative_coordinates_log",
@@ -172,7 +176,7 @@             self.window_size = window_size
             self._make_pair_wise_relative_positions()
 
-    def _relative_positional_encodings(self) -> torch.Tensor:
+    def _relative_positional_encodings(self) -> ms.Tensor:
         """Compute the relative positional encodings.
 
         Returns:
@@ -186,7 +190,7 @@         relative_position_bias = relative_position_bias.unsqueeze(0)
         return relative_position_bias
 
-    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, mask: Optional[torch.Tensor] = None) -> ms.Tensor:
         """Forward pass of window multi-head self-attention.
 
         Args:
@@ -202,8 +206,8 @@         query, key, value = qkv.unbind(0)
 
         # compute attention map with scaled cosine attention
-        attn = (F.normalize(query, dim=-1) @ F.normalize(key, dim=-1).transpose(-2, -1))
-        logit_scale = torch.clamp(self.logit_scale.reshape(1, self.num_heads, 1, 1), max=math.log(1. / 0.01)).exp()
+        attn = (nn.functional.normalize(query, dim = -1) @ mint.transpose(-2, -1))
+        logit_scale = mint.clamp(self.logit_scale.reshape(1, self.num_heads, 1, 1), max = math.log(1. / 0.01)).exp()
         attn = attn * logit_scale
         attn = attn + self._relative_positional_encodings()
 
@@ -222,7 +226,7 @@         return x
 
 
-class SwinTransformerV2CrBlock(nn.Module):
+class SwinTransformerV2CrBlock(msnn.Cell):
     r"""This class implements the Swin transformer block.
 
     Args:
@@ -282,7 +286,7 @@             **dd,
         )
         self.norm1 = norm_layer(dim, **dd)
-        self.drop_path1 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else nn.Identity()
+        self.drop_path1 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else msnn.Identity()
 
         # mlp branch
         self.mlp = Mlp(
@@ -293,11 +297,11 @@             **dd,
         )
         self.norm2 = norm_layer(dim, **dd)
-        self.drop_path2 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else nn.Identity()
+        self.drop_path2 = DropPath(drop_prob=drop_path) if drop_path > 0.0 else msnn.Identity()
 
         # Extra main branch norm layer mentioned for Huge/Giant models in V2 paper.
         # Also being used as final network norm and optional stage ending norm while still in a C-last format.
-        self.norm3 = norm_layer(dim, **dd) if extra_norm else nn.Identity()
+        self.norm3 = norm_layer(dim, **dd) if extra_norm else msnn.Identity()
 
         self.register_buffer(
             "attn_mask",
@@ -334,9 +338,9 @@         if any(self.shift_size):
             # calculate attention mask for SW-MSA
             if x is None:
-                img_mask = torch.zeros((1, *self.feat_size, 1), device=device, dtype=dtype)  # 1 H W 1
+                img_mask = mint.zeros((1, *self.feat_size, 1), dtype = dtype)  # 1 H W 1; 'torch.zeros':没有对应的mindspore参数 'device' (position 4);
             else:
-                img_mask = torch.zeros((1, x.shape[1], x.shape[2], 1), device=x.device, dtype=x.dtype)  # 1 H W 1
+                img_mask = mint.zeros((1, x.shape[1], x.shape[2], 1), dtype = x.dtype)  # 1 H W 1; 'torch.zeros':没有对应的mindspore参数 'device' (position 4);
             cnt = 0
             for h in (
                     (0, -self.window_size[0]),
@@ -361,8 +365,8 @@     def init_weights(self):
         # extra, module specific weight init
         if self.init_values is not None:
-            nn.init.constant_(self.norm1.weight, self.init_values)
-            nn.init.constant_(self.norm2.weight, self.init_values)
+            nn.init.constant_(self.norm1.weight, self.init_values)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(self.norm2.weight, self.init_values)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def set_input_size(self, feat_size: Tuple[int, int], window_size: Tuple[int, int]) -> None:
         """Method updates the image resolution to be processed and window size and so the pair-wise relative positions.
@@ -394,11 +398,11 @@             # FIXME PyTorch XLA needs cat impl, roll not lowered
             # x = torch.cat([x[:, sh:], x[:, :sh]], dim=1)
             # x = torch.cat([x[:, :, sw:], x[:, :, :sw]], dim=2)
-            x = torch.roll(x, shifts=(-sh, -sw), dims=(1, 2))
+            x = mint.roll(x, shifts = (-sh, -sw), dims = (1, 2))
 
         pad_h = (self.window_size[0] - H % self.window_size[0]) % self.window_size[0]
         pad_w = (self.window_size[1] - W % self.window_size[1]) % self.window_size[1]
-        x = torch.nn.functional.pad(x, (0, 0, 0, pad_w, 0, pad_h))
+        x = nn.functional.pad(x, (0, 0, 0, pad_w, 0, pad_h))
         _, Hp, Wp, _ = x.shape
 
         # partition windows
@@ -422,11 +426,11 @@             # FIXME PyTorch XLA needs cat impl, roll not lowered
             # x = torch.cat([x[:, -sh:], x[:, :-sh]], dim=1)
             # x = torch.cat([x[:, :, -sw:], x[:, :, :-sw]], dim=2)
-            x = torch.roll(x, shifts=(sh, sw), dims=(1, 2))
+            x = mint.roll(x, shifts = (sh, sw), dims = (1, 2))
 
         return x
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass of Swin Transformer V2 block.
 
         Args:
@@ -446,7 +450,7 @@         return x
 
 
-class PatchMerging(nn.Module):
+class PatchMerging(msnn.Cell):
     """Patch merging layer.
 
     This class implements the patch merging as a strided convolution with a normalization before.
@@ -470,7 +474,7 @@         self.norm = norm_layer(4 * dim, **dd)
         self.reduction = nn.Linear(in_features=4 * dim, out_features=2 * dim, bias=False, **dd)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass of patch merging.
 
         Args:
@@ -491,7 +495,7 @@         return x
 
 
-class PatchEmbed(nn.Module):
+class PatchEmbed(msnn.Cell):
     """2D Image to Patch Embedding."""
     def __init__(
             self,
@@ -525,7 +529,7 @@         self.strict_img_size = strict_img_size
 
         self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, **dd)
-        self.norm = norm_layer(embed_dim, **dd) if norm_layer else nn.Identity()
+        self.norm = norm_layer(embed_dim, **dd) if norm_layer else msnn.Identity()
 
     def set_input_size(self, img_size: Tuple[int, int]) -> None:
         """Update input image size.
@@ -539,7 +543,7 @@             self.grid_size = (img_size[0] // self.patch_size[0], img_size[1] // self.patch_size[1])
             self.num_patches = self.grid_size[0] * self.grid_size[1]
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass of patch embedding.
 
         Args:
@@ -557,7 +561,7 @@         return x
 
 
-class SwinTransformerV2CrStage(nn.Module):
+class SwinTransformerV2CrStage(msnn.Cell):
     r"""This class implements a stage of the Swin transformer including multiple layers.
 
     Args:
@@ -609,7 +613,7 @@             self.downsample = PatchMerging(embed_dim, norm_layer=norm_layer, **dd)
             embed_dim = embed_dim * 2
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
 
         def _extra_norm(index):
             i = index + 1
@@ -617,7 +621,8 @@                 return True
             return i == depth if extra_norm_stage else False
 
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell([
+            [
             SwinTransformerV2CrBlock(
                 dim=embed_dim,
                 num_heads=num_heads,
@@ -637,7 +642,7 @@                 **dd,
             )
             for index in range(depth)]
-        )
+        ])
 
     def set_input_size(
             self,
@@ -658,7 +663,7 @@                 window_size=window_size,
             )
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
         Args:
             x (torch.Tensor): Input tensor of the shape [B, C, H, W] or [B, L, C]
@@ -677,7 +682,7 @@         return x
 
 
-class SwinTransformerV2Cr(nn.Module):
+class SwinTransformerV2Cr(msnn.Cell):
     r""" Swin Transformer V2
         A PyTorch impl of : `Swin Transformer V2: Scaling Up Capacity and Resolution`  -
           https://arxiv.org/pdf/2111.09883
@@ -785,7 +790,9 @@                 in_dim *= 2
                 in_scale *= 2
             self.feature_info += [dict(num_chs=in_dim, reduction=4 * in_scale, module=f'stages.{stage_idx}')]
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell([
+            stages
+        ])
 
         self.head = ClassifierHead(
             self.num_features,
@@ -845,6 +852,7 @@         for s in self.stages:
             s.grad_checkpointing = enable
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore()
     def get_classifier(self) -> nn.Module:
         """Method returns the classification head of the model.
@@ -865,7 +873,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
@@ -919,7 +927,7 @@             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         x = self.patch_embed(x)
         x = self.stages(x)
         return x
@@ -927,25 +935,26 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=True) if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
 
 
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def init_weights(module: nn.Module, name: str = ''):
     # FIXME WIP determining if there's a better weight init
     if isinstance(module, nn.Linear):
         if 'qkv' in name:
             # treat the weights of Q, K, V separately
             val = math.sqrt(6. / float(module.weight.shape[0] // 3 + module.weight.shape[1]))
-            nn.init.uniform_(module.weight, -val, val)
+            nn.init.uniform_(module.weight, -val, val)  # 'torch.nn.init.uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif 'head' in name:
-            nn.init.zeros_(module.weight)
+            nn.init.zeros_(module.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
-            nn.init.xavier_uniform_(module.weight)
+            nn.init.xavier_uniform_(module.weight)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
@@ -960,7 +969,7 @@     for k, v in state_dict.items():
         if 'tau' in k:
             # convert old tau based checkpoints -> logit_scale (inverse)
-            v = torch.log(1 / v)
+            v = mint.log(1 / v)
             k = k.replace('tau', 'logit_scale')
         k = k.replace('head.', 'head.fc.')
         out_dict[k] = v
