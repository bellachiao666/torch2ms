--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ TinyViT
 
 Paper: `TinyViT: Fast Pretraining Distillation for Small Vision Transformers`
@@ -12,9 +17,9 @@ from functools import partial
 from typing import Dict, List, Optional, Tuple, Union, Type, Any
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import LayerNorm2d, NormMlpClassifierHead, DropPath,\
@@ -26,7 +31,7 @@ from ._registry import register_model, generate_default_cfgs
 
 
-class ConvNorm(torch.nn.Sequential):
+class ConvNorm(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
@@ -42,10 +47,10 @@     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
-        self.conv = nn.Conv2d(in_chs, out_chs, ks, stride, pad, dilation, groups, bias=False, **dd)
-        self.bn = nn.BatchNorm2d(out_chs, **dd)
-        torch.nn.init.constant_(self.bn.weight, bn_weight_init)
-        torch.nn.init.constant_(self.bn.bias, 0)
+        self.conv = nn.Conv2d(in_chs, out_chs, ks, stride, pad, dilation, groups, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn = nn.BatchNorm2d(out_chs, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        torch.nn.init.constant_(self.bn.weight, bn_weight_init)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch.nn.init.constant_(self.bn.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.no_grad()
     def fuse(self):
@@ -54,44 +59,43 @@         w = c.weight * w[:, None, None, None]
         b = bn.bias - bn.running_mean * bn.weight / \
             (bn.running_var + bn.eps) ** 0.5
-        m = torch.nn.Conv2d(
-            w.size(1) * self.conv.groups, w.size(0), w.shape[2:],
-            stride=self.conv.stride, padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups)
+        m = nn.Conv2d(
+            w.size(1) * self.conv.groups, w.size(0), w.shape[2:], stride = self.conv.stride, padding = self.conv.padding, dilation = self.conv.dilation, groups = self.conv.groups)
         m.weight.data.copy_(w)
         m.bias.data.copy_(b)
         return m
 
 
-class PatchEmbed(nn.Module):
+class PatchEmbed(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             device=None,
             dtype=None,
     ):
         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.stride = 4
-        self.conv1 = ConvNorm(in_chs, out_chs // 2, 3, 2, 1, **dd)
+        self.conv1 = ConvNorm(in_chs, out_chs // 2, 3, 2, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act = act_layer()
-        self.conv2 = ConvNorm(out_chs // 2, out_chs, 3, 2, 1, **dd)
-
-    def forward(self, x):
+        self.conv2 = ConvNorm(out_chs // 2, out_chs, 3, 2, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.conv1(x)
         x = self.act(x)
         x = self.conv2(x)
         return x
 
 
-class MBConv(nn.Module):
+class MBConv(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
             out_chs: int,
             expand_ratio: float,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             drop_path: float,
             device=None,
             dtype=None,
@@ -99,15 +103,15 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         mid_chs = int(in_chs * expand_ratio)
-        self.conv1 = ConvNorm(in_chs, mid_chs, ks=1, **dd)
+        self.conv1 = ConvNorm(in_chs, mid_chs, ks=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act1 = act_layer()
-        self.conv2 = ConvNorm(mid_chs, mid_chs, ks=3, stride=1, pad=1, groups=mid_chs, **dd)
+        self.conv2 = ConvNorm(mid_chs, mid_chs, ks=3, stride=1, pad=1, groups=mid_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act2 = act_layer()
-        self.conv3 = ConvNorm(mid_chs, out_chs, ks=1, bn_weight_init=0.0, **dd)
+        self.conv3 = ConvNorm(mid_chs, out_chs, ks=1, bn_weight_init=0.0, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act3 = act_layer()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x
         x = self.conv1(x)
         x = self.act1(x)
@@ -120,24 +124,24 @@         return x
 
 
-class PatchMerging(nn.Module):
+class PatchMerging(msnn.Cell):
     def __init__(
             self,
             dim: int,
             out_dim: int,
-            act_layer: Type[nn.Module],
-            device=None,
-            dtype=None,
-    ):
-        dd = {'device': device, 'dtype': dtype}
-        super().__init__()
-        self.conv1 = ConvNorm(dim, out_dim, 1, 1, 0, **dd)
+            act_layer: Type[msnn.Cell],
+            device=None,
+            dtype=None,
+    ):
+        dd = {'device': device, 'dtype': dtype}
+        super().__init__()
+        self.conv1 = ConvNorm(dim, out_dim, 1, 1, 0, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act1 = act_layer()
-        self.conv2 = ConvNorm(out_dim, out_dim, 3, 2, 1, groups=out_dim, **dd)
+        self.conv2 = ConvNorm(out_dim, out_dim, 3, 2, 1, groups=out_dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.act2 = act_layer()
-        self.conv3 = ConvNorm(out_dim, out_dim, 1, 1, 0, **dd)
-
-    def forward(self, x):
+        self.conv3 = ConvNorm(out_dim, out_dim, 1, 1, 0, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.conv1(x)
         x = self.act1(x)
         x = self.conv2(x)
@@ -146,12 +150,12 @@         return x
 
 
-class ConvLayer(nn.Module):
+class ConvLayer(msnn.Cell):
     def __init__(
             self,
             dim: int,
             depth: int,
-            act_layer: Type[nn.Module],
+            act_layer: Type[msnn.Cell],
             drop_path: Union[float, List[float]] = 0.,
             conv_expand_ratio: float = 4.,
             device=None,
@@ -161,7 +165,7 @@         super().__init__()
         self.dim = dim
         self.depth = depth
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell(*[
             MBConv(
                 dim,
                 dim,
@@ -171,21 +175,21 @@                 **dd,
             )
             for i in range(depth)
-        ])
-
-    def forward(self, x):
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.blocks(x)
         return x
 
 
-class NormMlp(nn.Module):
+class NormMlp(msnn.Cell):
     def __init__(
             self,
             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            norm_layer: Type[nn.Module] = nn.LayerNorm,
-            act_layer: Type[nn.Module] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = nn.LayerNorm,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop: float = 0.,
             device=None,
             dtype=None,
@@ -194,14 +198,14 @@         super().__init__()
         out_features = out_features or in_features
         hidden_features = hidden_features or in_features
-        self.norm = norm_layer(in_features, **dd)
-        self.fc1 = nn.Linear(in_features, hidden_features, **dd)
+        self.norm = norm_layer(in_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.fc1 = nn.Linear(in_features, hidden_features, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.act = act_layer()
         self.drop1 = nn.Dropout(drop)
-        self.fc2 = nn.Linear(hidden_features, out_features, **dd)
+        self.fc2 = nn.Linear(hidden_features, out_features, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.drop2 = nn.Dropout(drop)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.norm(x)
         x = self.fc1(x)
         x = self.act(x)
@@ -211,9 +215,9 @@         return x
 
 
-class Attention(torch.nn.Module):
-    fused_attn: torch.jit.Final[bool]
-    attention_bias_cache: Dict[str, torch.Tensor]
+class Attention(msnn.Cell):
+    fused_attn: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    attention_bias_cache: Dict[str, ms.Tensor]
 
     def __init__(
             self,
@@ -237,9 +241,9 @@         self.resolution = resolution
         self.fused_attn = use_fused_attn()
 
-        self.norm = nn.LayerNorm(dim, **dd)
-        self.qkv = nn.Linear(dim, num_heads * (self.val_dim + 2 * key_dim), **dd)
-        self.proj = nn.Linear(self.out_dim, dim, **dd)
+        self.norm = nn.LayerNorm(dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.qkv = nn.Linear(dim, num_heads * (self.val_dim + 2 * key_dim), **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.proj = nn.Linear(self.out_dim, dim, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
 
         points = list(itertools.product(range(resolution[0]), range(resolution[1])))
         N = len(points)
@@ -251,10 +255,10 @@                 if offset not in attention_offsets:
                     attention_offsets[offset] = len(attention_offsets)
                 idxs.append(attention_offsets[offset])
-        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, len(attention_offsets), **dd))
+        self.attention_biases = ms.Parameter(mint.zeros(num_heads, len(attention_offsets), **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.register_buffer(
             'attention_bias_idxs',
-            torch.tensor(idxs, device=device, dtype=torch.long).view(N, N),
+            ms.Tensor(idxs, device=device, dtype=ms.int64).view(N, N),
             persistent=False,
         )
         self.attention_bias_cache = {}
@@ -265,7 +269,7 @@         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
+    def get_attention_biases(self, device: torch.device) -> ms.Tensor:
         if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
@@ -274,7 +278,7 @@                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
-    def forward(self, x):
+    def construct(self, x):
         attn_bias = self.get_attention_biases(x.device)
         B, N, _ = x.shape
         # Normalization
@@ -288,7 +292,7 @@         v = v.permute(0, 2, 1, 3)
 
         if self.fused_attn:
-            x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_bias)
+            x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_bias)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
@@ -300,7 +304,7 @@         return x
 
 
-class TinyVitBlock(nn.Module):
+class TinyVitBlock(msnn.Cell):
     """ TinyViT Block.
 
     Args:
@@ -324,7 +328,7 @@             drop: float = 0.,
             drop_path: float = 0.,
             local_conv_size: int = 3,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
     ):
@@ -347,8 +351,8 @@             attn_ratio=1,
             resolution=window_resolution,
             **dd,
-        )
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.mlp = NormMlp(
             in_features=dim,
@@ -356,13 +360,13 @@             act_layer=act_layer,
             drop=drop,
             **dd,
-        )
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         pad = local_conv_size // 2
-        self.local_conv = ConvNorm(dim, dim, ks=local_conv_size, stride=1, pad=pad, groups=dim, **dd)
-
-    def forward(self, x):
+        self.local_conv = ConvNorm(dim, dim, ks=local_conv_size, stride=1, pad=pad, groups=dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         B, H, W, C = x.shape
         L = H * W
 
@@ -376,7 +380,7 @@             pad_r = (self.window_size - W % self.window_size) % self.window_size
             padding = pad_b > 0 or pad_r > 0
             if padding:
-                x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))
+                x = nn.functional.pad(x, (0, 0, 0, pad_r, 0, pad_b))
 
             # window partition
             pH, pW = H + pad_b, W + pad_r
@@ -410,7 +414,7 @@ register_notrace_module(TinyVitBlock)
 
 
-class TinyVitStage(nn.Module):
+class TinyVitStage(msnn.Cell):
     """ A basic TinyViT layer for one stage.
 
     Args:
@@ -437,9 +441,9 @@             mlp_ratio: float = 4.,
             drop: float = 0.,
             drop_path: Union[float, List[float]] = 0.,
-            downsample: Optional[Type[nn.Module]] = None,
+            downsample: Optional[Type[msnn.Cell]] = None,
             local_conv_size: int = 3,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
     ):
@@ -455,13 +459,13 @@                 out_dim=out_dim,
                 act_layer=act_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
             assert dim == out_dim
 
         # build blocks
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell(*[
             TinyVitBlock(
                 dim=out_dim,
                 num_heads=num_heads,
@@ -473,9 +477,9 @@                 act_layer=act_layer,
                 **dd,
             )
-            for i in range(depth)])
-
-    def forward(self, x):
+            for i in range(depth)])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.downsample(x)
         x = x.permute(0, 2, 3, 1)  # BCHW -> BHWC
         x = self.blocks(x)
@@ -486,7 +490,7 @@         return f"dim={self.out_dim}, depth={self.depth}"
 
 
-class TinyVit(nn.Module):
+class TinyVit(msnn.Cell):
     def __init__(
             self,
             in_chans: int = 3,
@@ -502,7 +506,7 @@             use_checkpoint: bool = False,
             mbconv_expand_ratio: float = 4.0,
             local_conv_size: int = 3,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             device=None,
             dtype=None,
     ):
@@ -520,13 +524,13 @@             out_chs=embed_dims[0],
             act_layer=act_layer,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # stochastic depth rate rule
         dpr = calculate_drop_path_rates(drop_path_rate, sum(depths))
 
         # build stages
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         stride = self.patch_embed.stride
         prev_dim = embed_dims[0]
         self.feature_info = []
@@ -539,7 +543,7 @@                     drop_path=dpr[:depths[stage_idx]],
                     conv_expand_ratio=mbconv_expand_ratio,
                     **dd,
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
                 out_dim = embed_dims[stage_idx]
                 drop_path_rate = dpr[sum(depths[:stage_idx]):sum(depths[:stage_idx + 1])]
@@ -556,7 +560,7 @@                     downsample=PatchMerging,
                     act_layer=act_layer,
                     **dd,
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
                 prev_dim = out_dim
                 stride *= 2
             self.stages.append(stage)
@@ -572,7 +576,7 @@             pool_type=global_pool,
             norm_layer=norm_layer_cf,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # init weights
         self.apply(self._init_weights)
@@ -581,17 +585,17 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
-
-    @torch.jit.ignore
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def no_weight_decay_keywords(self):
         return {'attention_biases'}
 
-    @torch.jit.ignore
+    @ms.jit
     def no_weight_decay(self):
         return {x for x in self.state_dict().keys() if 'attention_biases' in x}
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(
             stem=r'^patch_embed',
@@ -602,12 +606,12 @@         )
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -616,13 +620,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -685,7 +689,7 @@         x = self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
         return x
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -784,7 +788,7 @@         feature_cfg=dict(flatten_sequential=True, out_indices=out_indices),
         pretrained_filter_fn=checkpoint_filter_fn,
         **kwargs
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -798,7 +802,7 @@         drop_path_rate=0.0,
     )
     model_kwargs.update(kwargs)
-    return _create_tiny_vit('tiny_vit_5m_224', pretrained, **model_kwargs)
+    return _create_tiny_vit('tiny_vit_5m_224', pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -811,7 +815,7 @@         drop_path_rate=0.1,
     )
     model_kwargs.update(kwargs)
-    return _create_tiny_vit('tiny_vit_11m_224', pretrained, **model_kwargs)
+    return _create_tiny_vit('tiny_vit_11m_224', pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -824,7 +828,7 @@         drop_path_rate=0.2,
     )
     model_kwargs.update(kwargs)
-    return _create_tiny_vit('tiny_vit_21m_224', pretrained, **model_kwargs)
+    return _create_tiny_vit('tiny_vit_21m_224', pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -837,7 +841,7 @@         drop_path_rate=0.1,
     )
     model_kwargs.update(kwargs)
-    return _create_tiny_vit('tiny_vit_21m_384', pretrained, **model_kwargs)
+    return _create_tiny_vit('tiny_vit_21m_384', pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -850,4 +854,4 @@         drop_path_rate=0.1,
     )
     model_kwargs.update(kwargs)
-    return _create_tiny_vit('tiny_vit_21m_512', pretrained, **model_kwargs)
+    return _create_tiny_vit('tiny_vit_21m_512', pretrained, **model_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
