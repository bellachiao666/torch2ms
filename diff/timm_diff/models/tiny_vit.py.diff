--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ TinyViT
 
 Paper: `TinyViT: Fast Pretraining Distillation for Small Vision Transformers`
@@ -12,9 +17,9 @@ from functools import partial
 from typing import Dict, List, Optional, Tuple, Union, Type, Any
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import LayerNorm2d, NormMlpClassifierHead, DropPath,\
@@ -26,7 +31,7 @@ from ._registry import register_model, generate_default_cfgs
 
 
-class ConvNorm(torch.nn.Sequential):
+class ConvNorm(msnn.SequentialCell):
     def __init__(
             self,
             in_chs: int,
@@ -44,8 +49,8 @@         super().__init__()
         self.conv = nn.Conv2d(in_chs, out_chs, ks, stride, pad, dilation, groups, bias=False, **dd)
         self.bn = nn.BatchNorm2d(out_chs, **dd)
-        torch.nn.init.constant_(self.bn.weight, bn_weight_init)
-        torch.nn.init.constant_(self.bn.bias, 0)
+        torch.nn.init.constant_(self.bn.weight, bn_weight_init)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch.nn.init.constant_(self.bn.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.no_grad()
     def fuse(self):
@@ -54,15 +59,14 @@         w = c.weight * w[:, None, None, None]
         b = bn.bias - bn.running_mean * bn.weight / \
             (bn.running_var + bn.eps) ** 0.5
-        m = torch.nn.Conv2d(
-            w.size(1) * self.conv.groups, w.size(0), w.shape[2:],
-            stride=self.conv.stride, padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups)
+        m = nn.Conv2d(
+            w.size(1) * self.conv.groups, w.size(0), w.shape[2:], stride = self.conv.stride, padding = self.conv.padding, dilation = self.conv.dilation, groups = self.conv.groups)
         m.weight.data.copy_(w)
         m.bias.data.copy_(b)
         return m
 
 
-class PatchEmbed(nn.Module):
+class PatchEmbed(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -78,14 +82,14 @@         self.act = act_layer()
         self.conv2 = ConvNorm(out_chs // 2, out_chs, 3, 2, 1, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         x = self.act(x)
         x = self.conv2(x)
         return x
 
 
-class MBConv(nn.Module):
+class MBConv(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -105,9 +109,9 @@         self.act2 = act_layer()
         self.conv3 = ConvNorm(mid_chs, out_chs, ks=1, bn_weight_init=0.0, **dd)
         self.act3 = act_layer()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x):
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x):
         shortcut = x
         x = self.conv1(x)
         x = self.act1(x)
@@ -120,7 +124,7 @@         return x
 
 
-class PatchMerging(nn.Module):
+class PatchMerging(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -137,7 +141,7 @@         self.act2 = act_layer()
         self.conv3 = ConvNorm(out_dim, out_dim, 1, 1, 0, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         x = self.act1(x)
         x = self.conv2(x)
@@ -146,7 +150,7 @@         return x
 
 
-class ConvLayer(nn.Module):
+class ConvLayer(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -161,7 +165,8 @@         super().__init__()
         self.dim = dim
         self.depth = depth
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell([
+            [
             MBConv(
                 dim,
                 dim,
@@ -171,14 +176,15 @@                 **dd,
             )
             for i in range(depth)
+        ]
         ])
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.blocks(x)
         return x
 
 
-class NormMlp(nn.Module):
+class NormMlp(msnn.Cell):
     def __init__(
             self,
             in_features: int,
@@ -201,7 +207,7 @@         self.fc2 = nn.Linear(hidden_features, out_features, **dd)
         self.drop2 = nn.Dropout(drop)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.norm(x)
         x = self.fc1(x)
         x = self.act(x)
@@ -211,7 +217,7 @@         return x
 
 
-class Attention(torch.nn.Module):
+class Attention(msnn.Cell):
     fused_attn: torch.jit.Final[bool]
     attention_bias_cache: Dict[str, torch.Tensor]
 
@@ -251,12 +257,12 @@                 if offset not in attention_offsets:
                     attention_offsets[offset] = len(attention_offsets)
                 idxs.append(attention_offsets[offset])
-        self.attention_biases = torch.nn.Parameter(torch.zeros(num_heads, len(attention_offsets), **dd))
+        self.attention_biases = ms.Parameter(mint.zeros(num_heads, len(attention_offsets), **dd))
         self.register_buffer(
             'attention_bias_idxs',
-            torch.tensor(idxs, device=device, dtype=torch.long).view(N, N),
+            ms.Tensor(idxs, dtype = torch.long).view(N, N),
             persistent=False,
-        )
+        )  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;; 'torch.tensor':没有对应的mindspore参数 'device' (position 2);
         self.attention_bias_cache = {}
 
     @torch.no_grad()
@@ -265,7 +271,8 @@         if mode and self.attention_bias_cache:
             self.attention_bias_cache = {}  # clear ab cache
 
-    def get_attention_biases(self, device: torch.device) -> torch.Tensor:
+    # 类型标注 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    def get_attention_biases(self, device: torch.device) -> ms.Tensor:
         if torch.jit.is_tracing() or self.training:
             return self.attention_biases[:, self.attention_bias_idxs]
         else:
@@ -274,7 +281,7 @@                 self.attention_bias_cache[device_key] = self.attention_biases[:, self.attention_bias_idxs]
             return self.attention_bias_cache[device_key]
 
-    def forward(self, x):
+    def construct(self, x):
         attn_bias = self.get_attention_biases(x.device)
         B, N, _ = x.shape
         # Normalization
@@ -288,7 +295,7 @@         v = v.permute(0, 2, 1, 3)
 
         if self.fused_attn:
-            x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_bias)
+            x = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_bias)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
@@ -300,7 +307,7 @@         return x
 
 
-class TinyVitBlock(nn.Module):
+class TinyVitBlock(msnn.Cell):
     """ TinyViT Block.
 
     Args:
@@ -348,7 +355,7 @@             resolution=window_resolution,
             **dd,
         )
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.mlp = NormMlp(
             in_features=dim,
@@ -357,12 +364,12 @@             drop=drop,
             **dd,
         )
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         pad = local_conv_size // 2
         self.local_conv = ConvNorm(dim, dim, ks=local_conv_size, stride=1, pad=pad, groups=dim, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         B, H, W, C = x.shape
         L = H * W
 
@@ -376,7 +383,7 @@             pad_r = (self.window_size - W % self.window_size) % self.window_size
             padding = pad_b > 0 or pad_r > 0
             if padding:
-                x = F.pad(x, (0, 0, 0, pad_r, 0, pad_b))
+                x = nn.functional.pad(x, (0, 0, 0, pad_r, 0, pad_b))
 
             # window partition
             pH, pW = H + pad_b, W + pad_r
@@ -410,7 +417,7 @@ register_notrace_module(TinyVitBlock)
 
 
-class TinyVitStage(nn.Module):
+class TinyVitStage(msnn.Cell):
     """ A basic TinyViT layer for one stage.
 
     Args:
@@ -457,11 +464,12 @@                 **dd,
             )
         else:
-            self.downsample = nn.Identity()
+            self.downsample = msnn.Identity()
             assert dim == out_dim
 
         # build blocks
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell([
+            [
             TinyVitBlock(
                 dim=out_dim,
                 num_heads=num_heads,
@@ -473,9 +481,10 @@                 act_layer=act_layer,
                 **dd,
             )
-            for i in range(depth)])
-
-    def forward(self, x):
+            for i in range(depth)]
+        ])
+
+    def construct(self, x):
         x = self.downsample(x)
         x = x.permute(0, 2, 3, 1)  # BCHW -> BHWC
         x = self.blocks(x)
@@ -486,7 +495,7 @@         return f"dim={self.out_dim}, depth={self.depth}"
 
 
-class TinyVit(nn.Module):
+class TinyVit(msnn.Cell):
     def __init__(
             self,
             in_chans: int = 3,
@@ -526,7 +535,7 @@         dpr = calculate_drop_path_rates(drop_path_rate, sum(depths))
 
         # build stages
-        self.stages = nn.Sequential()
+        self.stages = msnn.SequentialCell()
         stride = self.patch_embed.stride
         prev_dim = embed_dims[0]
         self.feature_info = []
@@ -581,7 +590,7 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def no_weight_decay_keywords(self):
@@ -606,6 +615,7 @@     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         return self.head.fc
@@ -616,7 +626,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
@@ -685,7 +695,7 @@         x = self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
         return x
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
