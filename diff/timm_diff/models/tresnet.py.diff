--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 TResNet: High Performance GPU-Dedicated Architecture
 https://arxiv.org/pdf/2003.13630.pdf
@@ -9,8 +14,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.layers import SpaceToDepth, BlurPool2d, ClassifierHead, SEModule, ConvNormAct, DropPath, calculate_drop_path_rates
 from ._builder import build_model_with_cfg
@@ -21,7 +26,7 @@ __all__ = ['TResNet']  # model_registry will add each entrypoint fn to this
 
 
-class BasicBlock(nn.Module):
+class BasicBlock(msnn.Cell):
     expansion = 1
 
     def __init__(
@@ -29,9 +34,9 @@             inplanes: int,
             planes: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             use_se: bool = True,
-            aa_layer: Optional[Type[nn.Module]] = None,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
             drop_path_rate: float = 0.,
             device=None,
             dtype=None,
@@ -50,15 +55,15 @@             act_layer=act_layer,
             aa_layer=aa_layer,
             **dd,
-        )
-        self.conv2 = ConvNormAct(planes, planes, kernel_size=3, stride=1, apply_act=False, **dd)
-        self.act = nn.ReLU(inplace=True)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.conv2 = ConvNormAct(planes, planes, kernel_size=3, stride=1, apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.act = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         rd_chs = max(planes * self.expansion // 4, 64)
-        self.se = SEModule(planes * self.expansion, rd_channels=rd_chs, **dd) if use_se else None
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
-
-    def forward(self, x):
+        self.se = SEModule(planes * self.expansion, rd_channels=rd_chs, **dd) if use_se else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
+
+    def construct(self, x):
         if self.downsample is not None:
             shortcut = self.downsample(x)
         else:
@@ -72,7 +77,7 @@         return out
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(msnn.Cell):
     expansion = 4
 
     def __init__(
@@ -80,10 +85,10 @@             inplanes: int,
             planes: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             use_se: bool = True,
-            act_layer: Optional[Type[nn.Module]] = None,
-            aa_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Optional[Type[msnn.Cell]] = None,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
             drop_path_rate: float = 0.,
             device=None,
             dtype=None,
@@ -94,7 +99,7 @@         self.stride = stride
         act_layer = act_layer or partial(nn.LeakyReLU, negative_slope=1e-3)
 
-        self.conv1 = ConvNormAct(inplanes, planes, kernel_size=1, stride=1, act_layer=act_layer, **dd)
+        self.conv1 = ConvNormAct(inplanes, planes, kernel_size=1, stride=1, act_layer=act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.conv2 = ConvNormAct(
             planes,
             planes,
@@ -103,17 +108,17 @@             act_layer=act_layer,
             aa_layer=aa_layer,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         reduction_chs = max(planes * self.expansion // 8, 64)
-        self.se = SEModule(planes, rd_channels=reduction_chs, **dd) if use_se else None
-
-        self.conv3 = ConvNormAct(planes, planes * self.expansion, kernel_size=1, stride=1, apply_act=False, **dd)
-
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
-        self.act = nn.ReLU(inplace=True)
-
-    def forward(self, x):
+        self.se = SEModule(planes, rd_channels=reduction_chs, **dd) if use_se else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.conv3 = ConvNormAct(planes, planes * self.expansion, kernel_size=1, stride=1, apply_act=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
+        self.act = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+    def construct(self, x):
         if self.downsample is not None:
             shortcut = self.downsample(x)
         else:
@@ -128,7 +133,7 @@         return out
 
 
-class TResNet(nn.Module):
+class TResNet(msnn.Cell):
     def __init__(
             self,
             layers: List[int],
@@ -159,22 +164,22 @@             self.planes = self.planes // 8 * 8
 
         dpr = calculate_drop_path_rates(drop_path_rate, layers, stagewise=True)
-        conv1 = ConvNormAct(in_chans * 16, self.planes, stride=1, kernel_size=3, act_layer=act_layer, **dd)
+        conv1 = ConvNormAct(in_chans * 16, self.planes, stride=1, kernel_size=3, act_layer=act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         layer1 = self._make_layer(
             Bottleneck if v2 else BasicBlock,
-            self.planes, layers[0], stride=1, use_se=True, aa_layer=aa_layer, drop_path_rate=dpr[0], **dd)
+            self.planes, layers[0], stride=1, use_se=True, aa_layer=aa_layer, drop_path_rate=dpr[0], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         layer2 = self._make_layer(
             Bottleneck if v2 else BasicBlock,
-            self.planes * 2, layers[1], stride=2, use_se=True, aa_layer=aa_layer, drop_path_rate=dpr[1], **dd)
+            self.planes * 2, layers[1], stride=2, use_se=True, aa_layer=aa_layer, drop_path_rate=dpr[1], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         layer3 = self._make_layer(
             Bottleneck,
-            self.planes * 4, layers[2], stride=2, use_se=True, aa_layer=aa_layer, drop_path_rate=dpr[2], **dd)
+            self.planes * 4, layers[2], stride=2, use_se=True, aa_layer=aa_layer, drop_path_rate=dpr[2], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         layer4 = self._make_layer(
             Bottleneck,
-            self.planes * 8, layers[3], stride=2, use_se=False, aa_layer=aa_layer, drop_path_rate=dpr[3], **dd)
+            self.planes * 8, layers[3], stride=2, use_se=False, aa_layer=aa_layer, drop_path_rate=dpr[3], **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # body
-        self.body = nn.Sequential(OrderedDict([
+        self.body = msnn.SequentialCell(OrderedDict([
             ('s2d', SpaceToDepth()),
             ('conv1', conv1),
             ('layer1', layer1),
@@ -193,21 +198,21 @@ 
         # head
         self.num_features = self.head_hidden_size = (self.planes * 8) * Bottleneck.expansion
-        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate, **dd)
+        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # model initialization
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if isinstance(m, nn.Linear):
                 m.weight.data.normal_(0, 0.01)
 
         # residual connections special initialization
         for m in self.modules():
             if isinstance(m, BasicBlock):
-                nn.init.zeros_(m.conv2.bn.weight)
+                nn.init.zeros_(m.conv2.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if isinstance(m, Bottleneck):
-                nn.init.zeros_(m.conv3.bn.weight)
+                nn.init.zeros_(m.conv3.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def _make_layer(
             self,
@@ -228,10 +233,10 @@             layers = []
             if stride == 2:
                 # avg pooling before 1x1 conv
-                layers.append(nn.AvgPool2d(kernel_size=2, stride=2, ceil_mode=True, count_include_pad=False))
+                layers.append(nn.AvgPool2d(kernel_size = 2, stride = 2, ceil_mode = True, count_include_pad = False))
             layers += [ConvNormAct(
-                self.inplanes, planes * block.expansion, kernel_size=1, stride=1, apply_act=False, **dd)]
-            downsample = nn.Sequential(*layers)
+                self.inplanes, planes * block.expansion, kernel_size=1, stride=1, apply_act=False, **dd)]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            downsample = msnn.SequentialCell(*layers)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         layers = []
         for i in range(blocks):
@@ -244,21 +249,21 @@                 aa_layer=aa_layer,
                 drop_path_rate=drop_path_rate[i] if isinstance(drop_path_rate, list) else drop_path_rate,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.inplanes = planes * block.expansion
-        return nn.Sequential(*layers)
-
-    @torch.jit.ignore
+        return msnn.SequentialCell(*layers)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         matcher = dict(stem=r'^body\.conv1', blocks=r'^body\.layer(\d+)' if coarse else r'^body\.layer(\d+)\.(\d+)')
         return matcher
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -267,13 +272,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -344,7 +349,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -380,7 +385,7 @@         pretrained_filter_fn=checkpoint_filter_fn,
         feature_cfg=dict(out_indices=(1, 2, 3, 4), flatten_sequential=True),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url='', **kwargs):
@@ -418,25 +423,25 @@ @register_model
 def tresnet_m(pretrained=False, **kwargs) -> TResNet:
     model_args = dict(layers=[3, 4, 11, 3])
-    return _create_tresnet('tresnet_m', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_tresnet('tresnet_m', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def tresnet_l(pretrained=False, **kwargs) -> TResNet:
     model_args = dict(layers=[4, 5, 18, 3], width_factor=1.2)
-    return _create_tresnet('tresnet_l', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_tresnet('tresnet_l', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def tresnet_xl(pretrained=False, **kwargs) -> TResNet:
     model_args = dict(layers=[4, 5, 24, 3], width_factor=1.3)
-    return _create_tresnet('tresnet_xl', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_tresnet('tresnet_xl', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def tresnet_v2_l(pretrained=False, **kwargs) -> TResNet:
     model_args = dict(layers=[3, 4, 23, 3], width_factor=1.0, v2=True)
-    return _create_tresnet('tresnet_v2_l', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _create_tresnet('tresnet_v2_l', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 register_model_deprecations(__name__, {
