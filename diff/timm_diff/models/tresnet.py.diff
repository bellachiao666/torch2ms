--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 TResNet: High Performance GPU-Dedicated Architecture
 https://arxiv.org/pdf/2003.13630.pdf
@@ -9,8 +14,8 @@ from functools import partial
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.layers import SpaceToDepth, BlurPool2d, ClassifierHead, SEModule, ConvNormAct, DropPath, calculate_drop_path_rates
 from ._builder import build_model_with_cfg
@@ -21,7 +26,7 @@ __all__ = ['TResNet']  # model_registry will add each entrypoint fn to this
 
 
-class BasicBlock(nn.Module):
+class BasicBlock(msnn.Cell):
     expansion = 1
 
     def __init__(
@@ -29,9 +34,9 @@             inplanes: int,
             planes: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             use_se: bool = True,
-            aa_layer: Optional[Type[nn.Module]] = None,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
             drop_path_rate: float = 0.,
             device=None,
             dtype=None,
@@ -52,13 +57,13 @@             **dd,
         )
         self.conv2 = ConvNormAct(planes, planes, kernel_size=3, stride=1, apply_act=False, **dd)
-        self.act = nn.ReLU(inplace=True)
+        self.act = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         rd_chs = max(planes * self.expansion // 4, 64)
         self.se = SEModule(planes * self.expansion, rd_channels=rd_chs, **dd) if use_se else None
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
-
-    def forward(self, x):
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
+
+    def construct(self, x):
         if self.downsample is not None:
             shortcut = self.downsample(x)
         else:
@@ -72,7 +77,7 @@         return out
 
 
-class Bottleneck(nn.Module):
+class Bottleneck(msnn.Cell):
     expansion = 4
 
     def __init__(
@@ -80,10 +85,10 @@             inplanes: int,
             planes: int,
             stride: int = 1,
-            downsample: Optional[nn.Module] = None,
+            downsample: Optional[msnn.Cell] = None,
             use_se: bool = True,
-            act_layer: Optional[Type[nn.Module]] = None,
-            aa_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Optional[Type[msnn.Cell]] = None,
+            aa_layer: Optional[Type[msnn.Cell]] = None,
             drop_path_rate: float = 0.,
             device=None,
             dtype=None,
@@ -110,10 +115,10 @@ 
         self.conv3 = ConvNormAct(planes, planes * self.expansion, kernel_size=1, stride=1, apply_act=False, **dd)
 
-        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()
-        self.act = nn.ReLU(inplace=True)
-
-    def forward(self, x):
+        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else msnn.Identity()
+        self.act = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+    def construct(self, x):
         if self.downsample is not None:
             shortcut = self.downsample(x)
         else:
@@ -128,7 +133,7 @@         return out
 
 
-class TResNet(nn.Module):
+class TResNet(msnn.Cell):
     def __init__(
             self,
             layers: List[int],
@@ -174,14 +179,16 @@             self.planes * 8, layers[3], stride=2, use_se=False, aa_layer=aa_layer, drop_path_rate=dpr[3], **dd)
 
         # body
-        self.body = nn.Sequential(OrderedDict([
+        self.body = msnn.SequentialCell([
+            OrderedDict([
             ('s2d', SpaceToDepth()),
             ('conv1', conv1),
             ('layer1', layer1),
             ('layer2', layer2),
             ('layer3', layer3),
             ('layer4', layer4),
-        ]))
+        ])
+        ])
 
         self.feature_info = [
             dict(num_chs=self.planes, reduction=2, module=''),  # Not with S2D?
@@ -198,16 +205,16 @@         # model initialization
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if isinstance(m, nn.Linear):
                 m.weight.data.normal_(0, 0.01)
 
         # residual connections special initialization
         for m in self.modules():
             if isinstance(m, BasicBlock):
-                nn.init.zeros_(m.conv2.bn.weight)
+                nn.init.zeros_(m.conv2.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if isinstance(m, Bottleneck):
-                nn.init.zeros_(m.conv3.bn.weight)
+                nn.init.zeros_(m.conv3.bn.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def _make_layer(
             self,
@@ -228,10 +235,12 @@             layers = []
             if stride == 2:
                 # avg pooling before 1x1 conv
-                layers.append(nn.AvgPool2d(kernel_size=2, stride=2, ceil_mode=True, count_include_pad=False))
+                layers.append(nn.AvgPool2d(kernel_size = 2, stride = 2, ceil_mode = True, count_include_pad = False))
             layers += [ConvNormAct(
                 self.inplanes, planes * block.expansion, kernel_size=1, stride=1, apply_act=False, **dd)]
-            downsample = nn.Sequential(*layers)
+            downsample = msnn.SequentialCell([
+                layers
+            ])
 
         layers = []
         for i in range(blocks):
@@ -246,7 +255,9 @@                 **dd,
             ))
             self.inplanes = planes * block.expansion
-        return nn.Sequential(*layers)
+        return msnn.SequentialCell([
+            layers
+        ])
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -258,7 +269,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -267,13 +278,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -344,7 +355,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
