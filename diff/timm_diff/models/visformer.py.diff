--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Visformer
 
 Paper: Visformer: The Vision-friendly Transformer - https://arxiv.org/abs/2104.12533
@@ -8,8 +13,8 @@ """
 
 from typing import Optional, Union, Type, Any
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import to_2tuple, trunc_normal_, DropPath, calculate_drop_path_rates, PatchEmbed, LayerNorm2d, create_classifier, use_fused_attn
@@ -21,13 +26,13 @@ __all__ = ['Visformer']
 
 
-class SpatialMlp(nn.Module):
+class SpatialMlp(msnn.Cell):
     def __init__(
             self,
             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop: float = 0.,
             group: int = 8,
             spatial_conv: bool = False,
@@ -50,20 +55,20 @@                 hidden_features = in_features * 2
         self.hidden_features = hidden_features
         self.group = group
-        self.conv1 = nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0, bias=False, **dd)
+        self.conv1 = nn.Conv2d(in_features, hidden_features, 1, stride=1, padding=0, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.act1 = act_layer()
         self.drop1 = nn.Dropout(drop_probs[0])
         if self.spatial_conv:
             self.conv2 = nn.Conv2d(
-                hidden_features, hidden_features, 3, stride=1, padding=1, groups=self.group, bias=False, **dd)
+                hidden_features, hidden_features, 3, stride=1, padding=1, groups=self.group, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
             self.act2 = act_layer()
         else:
             self.conv2 = None
             self.act2 = None
-        self.conv3 = nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0, bias=False, **dd)
+        self.conv3 = nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.drop3 = nn.Dropout(drop_probs[1])
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         x = self.act1(x)
         x = self.drop1(x)
@@ -75,8 +80,8 @@         return x
 
 
-class Attention(nn.Module):
-    fused_attn: torch.jit.Final[bool]
+class Attention(msnn.Cell):
+    fused_attn: torch.jit.Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -97,12 +102,12 @@         self.scale = head_dim ** -0.5
         self.fused_attn = use_fused_attn(experimental=True)
 
-        self.qkv = nn.Conv2d(dim, head_dim * num_heads * 3, 1, stride=1, padding=0, bias=False, **dd)
+        self.qkv = nn.Conv2d(dim, head_dim * num_heads * 3, 1, stride=1, padding=0, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.attn_drop = nn.Dropout(attn_drop)
-        self.proj = nn.Conv2d(self.head_dim * self.num_heads, dim, 1, stride=1, padding=0, bias=False, **dd)
+        self.proj = nn.Conv2d(self.head_dim * self.num_heads, dim, 1, stride=1, padding=0, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x):
+    def construct(self, x):
         B, C, H, W = x.shape
         x = self.qkv(x).reshape(B, 3, self.num_heads, self.head_dim, -1).permute(1, 0, 2, 4, 3)
         q, k, v = x.unbind(0)
@@ -111,7 +116,7 @@             x = torch.nn.functional.scaled_dot_product_attention(
                 q.contiguous(), k.contiguous(), v.contiguous(),
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             attn = (q @ k.transpose(-2, -1)) * self.scale
             attn = attn.softmax(dim=-1)
@@ -124,7 +129,7 @@         return x
 
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -134,8 +139,8 @@             proj_drop: float = 0.,
             attn_drop: float = 0.,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             group: int = 8,
             attn_disabled: bool = False,
             spatial_conv: bool = False,
@@ -145,12 +150,12 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.spatial_conv = spatial_conv
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         if attn_disabled:
             self.norm1 = None
             self.attn = None
         else:
-            self.norm1 = norm_layer(dim, **dd)
+            self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.attn = Attention(
                 dim,
                 num_heads=num_heads,
@@ -158,9 +163,9 @@                 attn_drop=attn_drop,
                 proj_drop=proj_drop,
                 **dd,
-            )
-
-        self.norm2 = norm_layer(dim, **dd)
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.mlp = SpatialMlp(
             in_features=dim,
             hidden_features=int(dim * mlp_ratio),
@@ -169,16 +174,16 @@             group=group,
             spatial_conv=spatial_conv,
             **dd,
-        )
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         if self.attn is not None:
             x = x + self.drop_path(self.attn(self.norm1(x)))
         x = x + self.drop_path(self.mlp(self.norm2(x)))
         return x
 
 
-class Visformer(nn.Module):
+class Visformer(msnn.Cell):
     def __init__(
             self,
             img_size: int = 224,
@@ -195,7 +200,7 @@             proj_drop_rate: float = 0.,
             attn_drop_rate: float = 0.,
             drop_path_rate: float = 0.,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             attn_stage: str = '111',
             use_pos_embed: bool = True,
             spatial_conv: str = '111',
@@ -203,7 +208,7 @@             group: int = 8,
             global_pool: str = 'avg',
             conv_init: bool = False,
-            embed_norm: Optional[Type[nn.Module]] = None,
+            embed_norm: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -237,7 +242,7 @@                 norm_layer=embed_norm,
                 flatten=False,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             img_size = [x // patch_size for x in img_size]
         else:
             if self.init_channels is None:
@@ -250,14 +255,15 @@                     norm_layer=embed_norm,
                     flatten=False,
                     **dd,
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
                 img_size = [x // (patch_size // 2) for x in img_size]
             else:
-                self.stem = nn.Sequential(
+                self.stem = msnn.SequentialCell(
+                    [
                     nn.Conv2d(in_chans, self.init_channels, 7, stride=2, padding=3, bias=False, **dd),
                     nn.BatchNorm2d(self.init_channels, **dd),
-                    nn.ReLU(inplace=True)
-                )
+                    nn.ReLU()
+                ])  # 存在 *args/**kwargs，需手动确认参数映射;; 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
                 img_size = [x // 2 for x in img_size]
                 self.patch_embed1 = PatchEmbed(
                     img_size=img_size,
@@ -267,19 +273,19 @@                     norm_layer=embed_norm,
                     flatten=False,
                     **dd,
-                )
+                )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
                 img_size = [x // (patch_size // 4) for x in img_size]
 
         if self.use_pos_embed:
             if self.vit_stem:
-                self.pos_embed1 = nn.Parameter(torch.zeros(1, embed_dim, *img_size, **dd))
+                self.pos_embed1 = ms.Parameter(mint.zeros(1, embed_dim, *img_size, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
-                self.pos_embed1 = nn.Parameter(torch.zeros(1, embed_dim//2, *img_size, **dd))
-            self.pos_drop = nn.Dropout(p=pos_drop_rate)
+                self.pos_embed1 = ms.Parameter(mint.zeros(1, embed_dim//2, *img_size, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.pos_drop = nn.Dropout(p = pos_drop_rate)
         else:
             self.pos_embed1 = None
 
-        self.stage1 = nn.Sequential(*[
+        self.stage1 = msnn.SequentialCell(*[
             Block(
                 dim=embed_dim//2,
                 num_heads=num_heads,
@@ -295,7 +301,7 @@                 **dd,
             )
             for i in range(self.stage_num1)
-        ])
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # stage2
         if not self.vit_stem:
@@ -307,15 +313,15 @@                 norm_layer=embed_norm,
                 flatten=False,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             img_size = [x // (patch_size // 8) for x in img_size]
             if self.use_pos_embed:
-                self.pos_embed2 = nn.Parameter(torch.zeros(1, embed_dim, *img_size, **dd))
+                self.pos_embed2 = ms.Parameter(mint.zeros(1, embed_dim, *img_size, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
                 self.pos_embed2 = None
         else:
             self.patch_embed2 = None
-        self.stage2 = nn.Sequential(*[
+        self.stage2 = msnn.SequentialCell(*[
             Block(
                 dim=embed_dim,
                 num_heads=num_heads,
@@ -331,7 +337,7 @@                 **dd,
             )
             for i in range(self.stage_num1, self.stage_num1+self.stage_num2)
-        ])
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # stage 3
         if not self.vit_stem:
@@ -343,15 +349,15 @@                 norm_layer=embed_norm,
                 flatten=False,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             img_size = [x // (patch_size // 8) for x in img_size]
             if self.use_pos_embed:
-                self.pos_embed3 = nn.Parameter(torch.zeros(1, embed_dim*2, *img_size, **dd))
+                self.pos_embed3 = ms.Parameter(mint.zeros(1, embed_dim*2, *img_size, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
                 self.pos_embed3 = None
         else:
             self.patch_embed3 = None
-        self.stage3 = nn.Sequential(*[
+        self.stage3 = msnn.SequentialCell(*[
             Block(
                 dim=embed_dim * 2,
                 num_heads=num_heads,
@@ -367,10 +373,10 @@                 **dd,
             )
             for i in range(self.stage_num1+self.stage_num2, depth)
-        ])
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.num_features = self.head_hidden_size = embed_dim if self.vit_stem else embed_dim * 2
-        self.norm = norm_layer(self.num_features, **dd)
+        self.norm = norm_layer(self.num_features, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # head
         global_pool, head = create_classifier(
@@ -396,16 +402,16 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=0.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif isinstance(m, nn.Conv2d):
             if self.conv_init:
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
                 trunc_normal_(m.weight, std=0.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0.)
-
-    @torch.jit.ignore
+                nn.init.constant_(m.bias, 0.)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^patch_embed1|pos_embed1|stem',  # stem and embed
@@ -416,12 +422,12 @@             ]
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -439,6 +445,7 @@         x = self.patch_embed1(x)
         if self.pos_embed1 is not None:
             x = self.pos_drop(x + self.pos_embed1)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stage1, x)
         else:
@@ -449,6 +456,7 @@             x = self.patch_embed2(x)
             if self.pos_embed2 is not None:
                 x = self.pos_drop(x + self.pos_embed2)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stage2, x)
         else:
@@ -459,6 +467,7 @@             x = self.patch_embed3(x)
             if self.pos_embed3 is not None:
                 x = self.pos_drop(x + self.pos_embed3)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.stage3, x)
         else:
@@ -472,7 +481,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -481,7 +490,7 @@ def _create_visformer(variant, pretrained=False, default_cfg=None, **kwargs):
     if kwargs.get('features_only', None):
         raise RuntimeError('features_only not implemented for Vision Transformer models.')
-    model = build_model_with_cfg(Visformer, variant, pretrained, **kwargs)
+    model = build_model_with_cfg(Visformer, variant, pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -509,7 +518,7 @@         init_channels=16, embed_dim=192, depth=(7, 4, 4), num_heads=3, mlp_ratio=4., group=8,
         attn_stage='011', spatial_conv='100', norm_layer=nn.BatchNorm2d, conv_init=True,
         embed_norm=nn.BatchNorm2d)
-    model = _create_visformer('visformer_tiny', pretrained=pretrained, **dict(model_cfg, **kwargs))
+    model = _create_visformer('visformer_tiny', pretrained=pretrained, **dict(model_cfg, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -519,7 +528,7 @@         init_channels=32, embed_dim=384, depth=(7, 4, 4), num_heads=6, mlp_ratio=4., group=8,
         attn_stage='011', spatial_conv='100', norm_layer=nn.BatchNorm2d, conv_init=True,
         embed_norm=nn.BatchNorm2d)
-    model = _create_visformer('visformer_small', pretrained=pretrained, **dict(model_cfg, **kwargs))
+    model = _create_visformer('visformer_small', pretrained=pretrained, **dict(model_cfg, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
