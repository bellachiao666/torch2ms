--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Visformer
 
 Paper: Visformer: The Vision-friendly Transformer - https://arxiv.org/abs/2104.12533
@@ -8,8 +13,8 @@ """
 
 from typing import Optional, Union, Type, Any
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import to_2tuple, trunc_normal_, DropPath, calculate_drop_path_rates, PatchEmbed, LayerNorm2d, create_classifier, use_fused_attn
@@ -21,13 +26,13 @@ __all__ = ['Visformer']
 
 
-class SpatialMlp(nn.Module):
+class SpatialMlp(msnn.Cell):
     def __init__(
             self,
             in_features: int,
             hidden_features: Optional[int] = None,
             out_features: Optional[int] = None,
-            act_layer: Type[nn.Module] = nn.GELU,
+            act_layer: Type[msnn.Cell] = nn.GELU,
             drop: float = 0.,
             group: int = 8,
             spatial_conv: bool = False,
@@ -63,7 +68,7 @@         self.conv3 = nn.Conv2d(hidden_features, out_features, 1, stride=1, padding=0, bias=False, **dd)
         self.drop3 = nn.Dropout(drop_probs[1])
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         x = self.act1(x)
         x = self.drop1(x)
@@ -75,7 +80,7 @@         return x
 
 
-class Attention(nn.Module):
+class Attention(msnn.Cell):
     fused_attn: torch.jit.Final[bool]
 
     def __init__(
@@ -102,7 +107,7 @@         self.proj = nn.Conv2d(self.head_dim * self.num_heads, dim, 1, stride=1, padding=0, bias=False, **dd)
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x):
+    def construct(self, x):
         B, C, H, W = x.shape
         x = self.qkv(x).reshape(B, 3, self.num_heads, self.head_dim, -1).permute(1, 0, 2, 4, 3)
         q, k, v = x.unbind(0)
@@ -111,7 +116,7 @@             x = torch.nn.functional.scaled_dot_product_attention(
                 q.contiguous(), k.contiguous(), v.contiguous(),
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             attn = (q @ k.transpose(-2, -1)) * self.scale
             attn = attn.softmax(dim=-1)
@@ -124,7 +129,7 @@         return x
 
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -134,8 +139,8 @@             proj_drop: float = 0.,
             attn_drop: float = 0.,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             group: int = 8,
             attn_disabled: bool = False,
             spatial_conv: bool = False,
@@ -145,7 +150,7 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.spatial_conv = spatial_conv
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
         if attn_disabled:
             self.norm1 = None
             self.attn = None
@@ -171,14 +176,14 @@             **dd,
         )
 
-    def forward(self, x):
+    def construct(self, x):
         if self.attn is not None:
             x = x + self.drop_path(self.attn(self.norm1(x)))
         x = x + self.drop_path(self.mlp(self.norm2(x)))
         return x
 
 
-class Visformer(nn.Module):
+class Visformer(msnn.Cell):
     def __init__(
             self,
             img_size: int = 224,
@@ -195,7 +200,7 @@             proj_drop_rate: float = 0.,
             attn_drop_rate: float = 0.,
             drop_path_rate: float = 0.,
-            norm_layer: Type[nn.Module] = LayerNorm2d,
+            norm_layer: Type[msnn.Cell] = LayerNorm2d,
             attn_stage: str = '111',
             use_pos_embed: bool = True,
             spatial_conv: str = '111',
@@ -203,7 +208,7 @@             group: int = 8,
             global_pool: str = 'avg',
             conv_init: bool = False,
-            embed_norm: Optional[Type[nn.Module]] = None,
+            embed_norm: Optional[Type[msnn.Cell]] = None,
             device=None,
             dtype=None,
     ):
@@ -253,11 +258,12 @@                 )
                 img_size = [x // (patch_size // 2) for x in img_size]
             else:
-                self.stem = nn.Sequential(
+                self.stem = msnn.SequentialCell(
+                    [
                     nn.Conv2d(in_chans, self.init_channels, 7, stride=2, padding=3, bias=False, **dd),
                     nn.BatchNorm2d(self.init_channels, **dd),
-                    nn.ReLU(inplace=True)
-                )
+                    nn.ReLU()
+                ])  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
                 img_size = [x // 2 for x in img_size]
                 self.patch_embed1 = PatchEmbed(
                     img_size=img_size,
@@ -272,14 +278,15 @@ 
         if self.use_pos_embed:
             if self.vit_stem:
-                self.pos_embed1 = nn.Parameter(torch.zeros(1, embed_dim, *img_size, **dd))
+                self.pos_embed1 = ms.Parameter(mint.zeros(1, embed_dim, *img_size, **dd))
             else:
-                self.pos_embed1 = nn.Parameter(torch.zeros(1, embed_dim//2, *img_size, **dd))
-            self.pos_drop = nn.Dropout(p=pos_drop_rate)
+                self.pos_embed1 = ms.Parameter(mint.zeros(1, embed_dim//2, *img_size, **dd))
+            self.pos_drop = nn.Dropout(p = pos_drop_rate)
         else:
             self.pos_embed1 = None
 
-        self.stage1 = nn.Sequential(*[
+        self.stage1 = msnn.SequentialCell([
+            [
             Block(
                 dim=embed_dim//2,
                 num_heads=num_heads,
@@ -295,6 +302,7 @@                 **dd,
             )
             for i in range(self.stage_num1)
+        ]
         ])
 
         # stage2
@@ -310,12 +318,13 @@             )
             img_size = [x // (patch_size // 8) for x in img_size]
             if self.use_pos_embed:
-                self.pos_embed2 = nn.Parameter(torch.zeros(1, embed_dim, *img_size, **dd))
+                self.pos_embed2 = ms.Parameter(mint.zeros(1, embed_dim, *img_size, **dd))
             else:
                 self.pos_embed2 = None
         else:
             self.patch_embed2 = None
-        self.stage2 = nn.Sequential(*[
+        self.stage2 = msnn.SequentialCell([
+            [
             Block(
                 dim=embed_dim,
                 num_heads=num_heads,
@@ -331,6 +340,7 @@                 **dd,
             )
             for i in range(self.stage_num1, self.stage_num1+self.stage_num2)
+        ]
         ])
 
         # stage 3
@@ -346,12 +356,13 @@             )
             img_size = [x // (patch_size // 8) for x in img_size]
             if self.use_pos_embed:
-                self.pos_embed3 = nn.Parameter(torch.zeros(1, embed_dim*2, *img_size, **dd))
+                self.pos_embed3 = ms.Parameter(mint.zeros(1, embed_dim*2, *img_size, **dd))
             else:
                 self.pos_embed3 = None
         else:
             self.patch_embed3 = None
-        self.stage3 = nn.Sequential(*[
+        self.stage3 = msnn.SequentialCell([
+            [
             Block(
                 dim=embed_dim * 2,
                 num_heads=num_heads,
@@ -367,6 +378,7 @@                 **dd,
             )
             for i in range(self.stage_num1+self.stage_num2, depth)
+        ]
         ])
 
         self.num_features = self.head_hidden_size = embed_dim if self.vit_stem else embed_dim * 2
@@ -396,14 +408,14 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=0.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif isinstance(m, nn.Conv2d):
             if self.conv_init:
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
                 trunc_normal_(m.weight, std=0.02)
             if m.bias is not None:
-                nn.init.constant_(m.bias, 0.)
+                nn.init.constant_(m.bias, 0.)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -421,7 +433,7 @@         self.grad_checkpointing = enable
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.head
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -472,7 +484,7 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
