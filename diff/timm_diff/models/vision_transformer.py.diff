--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Vision Transformer (ViT) in PyTorch
 
 A PyTorch implement of Vision Transformers as described in:
@@ -35,10 +40,10 @@ except ImportError:
     from typing_extensions import Literal
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.jit import Final
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
+# from torch.jit import Final
 
 from timm.data import (
     IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD,
@@ -87,6 +92,7 @@ }
 
 
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def _create_attn(
         attn_layer: LayerType,
         dim: int,
@@ -123,7 +129,7 @@     )
 
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     """Transformer block with pre-normalization."""
 
     def __init__(
@@ -185,8 +191,8 @@             depth=depth,
             **dd,
         )
-        self.ls1 = LayerScale(dim, init_values=init_values, **dd) if init_values else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.ls1 = LayerScale(dim, init_values=init_values, **dd) if init_values else msnn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.norm2 = norm_layer(dim, **dd)
         self.mlp = mlp_layer(
@@ -198,16 +204,16 @@             drop=proj_drop,
             **dd,
         )
-        self.ls2 = LayerScale(dim, init_values=init_values, **dd) if init_values else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+        self.ls2 = LayerScale(dim, init_values=init_values, **dd) if init_values else msnn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor, attn_mask: Optional[torch.Tensor] = None) -> ms.Tensor:
         x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-class ResPostBlock(nn.Module):
+class ResPostBlock(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -249,7 +255,7 @@             **dd,
         )
         self.norm1 = norm_layer(dim, **dd)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.mlp = mlp_layer(
             in_features=dim,
@@ -261,23 +267,23 @@             **dd,
         )
         self.norm2 = norm_layer(dim, **dd)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.init_weights()
 
     def init_weights(self) -> None:
         # NOTE this init overrides that base model init with specific changes for the block type
         if self.init_values is not None:
-            nn.init.constant_(self.norm1.weight, self.init_values)
-            nn.init.constant_(self.norm2.weight, self.init_values)
-
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+            nn.init.constant_(self.norm1.weight, self.init_values)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(self.norm2.weight, self.init_values)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor, attn_mask: Optional[torch.Tensor] = None) -> ms.Tensor:
         x = x + self.drop_path1(self.norm1(self.attn(x, attn_mask=attn_mask)))
         x = x + self.drop_path2(self.norm2(self.mlp(x)))
         return x
 
 
-class ParallelScalingBlock(nn.Module):
+class ParallelScalingBlock(msnn.Cell):
     """ Parallel ViT block (MLP & Attention in parallel)
     Based on:
       'Scaling Vision Transformers to 22 Billion Parameters` - https://arxiv.org/abs/2302.05442
@@ -325,11 +331,11 @@             self.register_buffer('qkv_bias', None)
             self.register_parameter('mlp_bias', None)
         else:
-            self.register_buffer('qkv_bias', torch.zeros(3 * dim, **dd), persistent=False)
-            self.mlp_bias = nn.Parameter(torch.zeros(mlp_hidden_dim, **dd))
-
-        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()
-        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()
+            self.register_buffer('qkv_bias', mint.zeros(3 * dim, **dd), persistent=False)
+            self.mlp_bias = ms.Parameter(mint.zeros(mlp_hidden_dim, **dd))
+
+        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else msnn.Identity()
+        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else msnn.Identity()
         self.attn_drop = nn.Dropout(attn_drop)
 
         self.mlp_drop = nn.Dropout(proj_drop)
@@ -346,10 +352,10 @@             self.attn_out_proj = nn.Linear(dim, dim, bias=proj_bias, **dd)
             self.mlp_out_proj = nn.Linear(mlp_hidden_dim, dim, bias=proj_bias, **dd)
 
-        self.ls = LayerScale(dim, init_values=init_values, **dd) if init_values is not None else nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+        self.ls = LayerScale(dim, init_values=init_values, **dd) if init_values is not None else msnn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor, attn_mask: Optional[torch.Tensor] = None) -> ms.Tensor:
         B, N, C = x.shape
 
         # Combined MLP fc1 & qkv projections
@@ -357,10 +363,10 @@         if self.mlp_bias is not None:
             # Concat constant zero-bias for qkv w/ trainable mlp_bias.
             # Appears faster than adding to x_mlp separately
-            y = F.linear(y, self.in_proj.weight, torch.cat((self.qkv_bias, self.mlp_bias)))
+            y = nn.functional.linear(y, self.in_proj.weight, mint.cat((self.qkv_bias, self.mlp_bias)))
         else:
             y = self.in_proj(y)
-        x_mlp, q, k, v = torch.split(y, self.in_split, dim=-1)
+        x_mlp, q, k, v = mint.split(y, self.in_split, dim = -1)
 
         # Dot product attention w/ qk norm
         q = self.q_norm(q.view(B, N, self.num_heads, self.head_dim)).transpose(1, 2)
@@ -371,7 +377,7 @@                 q, k, v,
                 attn_mask=attn_mask,
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
@@ -388,7 +394,7 @@ 
         # Output projection (fused or separate)
         if self.out_proj is not None:
-            y = self.out_proj(torch.cat((x_attn, x_mlp), dim=-1))
+            y = self.out_proj(mint.cat((x_attn, x_mlp), dim = -1))
         else:
             y = self.attn_out_proj(x_attn) + self.mlp_out_proj(x_mlp)
 
@@ -397,7 +403,7 @@         return x
 
 
-class DiffParallelScalingBlock(nn.Module):
+class DiffParallelScalingBlock(msnn.Cell):
     """ Parallel ViT block with Differential Attention (MLP & Attention in parallel).
 
     Combines the parallel MLP+Attention structure from 'Scaling Vision Transformers to
@@ -447,11 +453,11 @@             self.register_buffer('qkv_bias', None)
             self.register_parameter('mlp_bias', None)
         else:
-            self.register_buffer('qkv_bias', torch.zeros(3 * dim, **dd), persistent=False)
-            self.mlp_bias = nn.Parameter(torch.zeros(mlp_hidden_dim, **dd))
-
-        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()
-        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()
+            self.register_buffer('qkv_bias', mint.zeros(3 * dim, **dd), persistent=False)
+            self.mlp_bias = ms.Parameter(mint.zeros(mlp_hidden_dim, **dd))
+
+        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else msnn.Identity()
+        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else msnn.Identity()
         self.attn_drop = nn.Dropout(attn_drop)
         self.attn_drop_p = attn_drop
 
@@ -459,15 +465,15 @@         self.sub_norm = RmsNorm(2 * self.head_dim, eps=1e-5, **dd)
         self.dual_lambda = dual_lambda
         if dual_lambda:
-            self.lambda_a = nn.Parameter(torch.empty((), dtype=torch.float32, device=device))
-            self.lambda_b = nn.Parameter(torch.empty((), dtype=torch.float32, device=device))
+            self.lambda_a = ms.Parameter(mint.empty((), dtype = ms.float32, device = device))
+            self.lambda_b = ms.Parameter(mint.empty((), dtype = ms.float32, device = device))
             self.lambda_q1 = self.lambda_k1 = self.lambda_q2 = self.lambda_k2 = None
         else:
             self.lambda_a = self.lambda_b = None
-            self.lambda_q1 = nn.Parameter(torch.empty(self.head_dim, dtype=torch.float32, device=device))
-            self.lambda_k1 = nn.Parameter(torch.empty(self.head_dim, dtype=torch.float32, device=device))
-            self.lambda_q2 = nn.Parameter(torch.empty(self.head_dim, dtype=torch.float32, device=device))
-            self.lambda_k2 = nn.Parameter(torch.empty(self.head_dim, dtype=torch.float32, device=device))
+            self.lambda_q1 = ms.Parameter(mint.empty(self.head_dim, dtype = ms.float32, device = device))
+            self.lambda_k1 = ms.Parameter(mint.empty(self.head_dim, dtype = ms.float32, device = device))
+            self.lambda_q2 = ms.Parameter(mint.empty(self.head_dim, dtype = ms.float32, device = device))
+            self.lambda_k2 = ms.Parameter(mint.empty(self.head_dim, dtype = ms.float32, device = device))
 
         self.mlp_drop = nn.Dropout(proj_drop)
         self.mlp_act = act_layer()
@@ -475,8 +481,8 @@         # Fused output projection for both attention and MLP
         self.out_proj = nn.Linear(dim + mlp_hidden_dim, dim, bias=proj_bias, **dd)
 
-        self.ls = LayerScale(dim, init_values=init_values, **dd) if init_values is not None else nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.ls = LayerScale(dim, init_values=init_values, **dd) if init_values is not None else msnn.Identity()
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.lambda_init = 0.8
         self.set_lambda_init(depth)
@@ -487,33 +493,33 @@ 
     def reset_parameters(self):
         if self.dual_lambda:
-            nn.init.zeros_(self.lambda_a)
-            nn.init.zeros_(self.lambda_b)
+            nn.init.zeros_(self.lambda_a)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.zeros_(self.lambda_b)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
-            nn.init.normal_(self.lambda_q1, mean=0, std=0.1)
-            nn.init.normal_(self.lambda_k1, mean=0, std=0.1)
-            nn.init.normal_(self.lambda_q2, mean=0, std=0.1)
-            nn.init.normal_(self.lambda_k2, mean=0, std=0.1)
-
-    def _compute_lambda(self) -> torch.Tensor:
+            nn.init.normal_(self.lambda_q1, mean=0, std=0.1)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.normal_(self.lambda_k1, mean=0, std=0.1)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.normal_(self.lambda_q2, mean=0, std=0.1)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.normal_(self.lambda_k2, mean=0, std=0.1)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def _compute_lambda(self) -> ms.Tensor:
         if self.lambda_a is not None:
-            lambda_1 = torch.exp(self.lambda_a)
-            lambda_2 = torch.exp(self.lambda_b)
+            lambda_1 = mint.exp(self.lambda_a)
+            lambda_2 = mint.exp(self.lambda_b)
         else:
-            lambda_1 = torch.exp(torch.sum(self.lambda_q1 * self.lambda_k1, dim=-1).float())
-            lambda_2 = torch.exp(torch.sum(self.lambda_q2 * self.lambda_k2, dim=-1).float())
+            lambda_1 = mint.exp(mint.sum(self.lambda_q1 * self.lambda_k1).float())
+            lambda_2 = mint.exp(mint.sum(self.lambda_q2 * self.lambda_k2).float())
         return lambda_1 - lambda_2 + self.lambda_init
 
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, attn_mask: Optional[torch.Tensor] = None) -> ms.Tensor:
         B, N, C = x.shape
 
         # Combined MLP fc1 & qkv projections
         y = self.in_norm(x)
         if self.mlp_bias is not None:
-            y = F.linear(y, self.in_proj.weight, torch.cat((self.qkv_bias, self.mlp_bias)))
+            y = nn.functional.linear(y, self.in_proj.weight, mint.cat((self.qkv_bias, self.mlp_bias)))
         else:
             y = self.in_proj(y)
-        x_mlp, q, k, v = torch.split(y, self.in_split, dim=-1)
+        x_mlp, q, k, v = mint.split(y, self.in_split, dim = -1)
 
         # Reshape for differential attention (2x heads with half head_dim for q/k)
         q = q.reshape(B, N, 2 * self.num_heads, self.head_dim).transpose(1, 2)
@@ -531,8 +537,8 @@             k1, k2 = k.unbind(2)
 
             dropout_p = self.attn_drop_p if self.training else 0.0
-            attn1 = F.scaled_dot_product_attention(q1, k1, v, attn_mask=attn_mask, dropout_p=dropout_p)
-            attn2 = F.scaled_dot_product_attention(q2, k2, v, attn_mask=attn_mask, dropout_p=dropout_p)
+            attn1 = F.scaled_dot_product_attention(q1, k1, v, attn_mask=attn_mask, dropout_p=dropout_p)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            attn2 = F.scaled_dot_product_attention(q2, k2, v, attn_mask=attn_mask, dropout_p=dropout_p)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
             x_attn = attn1 - lambda_full * attn2
         else:
@@ -555,14 +561,14 @@         x_mlp = self.mlp_drop(x_mlp)
 
         # Fused output projection
-        y = self.out_proj(torch.cat((x_attn, x_mlp), dim=-1))
+        y = self.out_proj(mint.cat((x_attn, x_mlp), dim = -1))
 
         # Add residual w/ drop path & layer scale applied
         x = x + self.drop_path(self.ls(y))
         return x
 
 
-class ParallelThingsBlock(nn.Module):
+class ParallelThingsBlock(msnn.Cell):
     """ Parallel ViT block (N parallel attention followed by N parallel MLP)
     Based on:
       `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795
@@ -593,10 +599,11 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.num_parallel = num_parallel
-        self.attns = nn.ModuleList()
-        self.ffns = nn.ModuleList()
+        self.attns = msnn.CellList()
+        self.ffns = msnn.CellList()
         for _ in range(num_parallel):
-            self.attns.append(nn.Sequential(OrderedDict([
+            self.attns.append(msnn.SequentialCell([
+                OrderedDict([
                 ('norm', norm_layer(dim, **dd)),
                 ('attn', _create_attn(
                     attn_layer,
@@ -612,10 +619,12 @@                     depth=depth,
                     **dd,
                 )),
-                ('ls', LayerScale(dim, init_values=init_values, **dd) if init_values else nn.Identity()),
-                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())
-            ])))
-            self.ffns.append(nn.Sequential(OrderedDict([
+                ('ls', LayerScale(dim, init_values=init_values, **dd) if init_values else msnn.Identity()),
+                ('drop_path', DropPath(drop_path) if drop_path > 0. else msnn.Identity())
+            ])
+            ]))
+            self.ffns.append(msnn.SequentialCell([
+                OrderedDict([
                 ('norm', norm_layer(dim, **dd)),
                 ('mlp', mlp_layer(
                     dim,
@@ -626,11 +635,12 @@                     drop=proj_drop,
                     **dd,
                 )),
-                ('ls', LayerScale(dim, init_values=init_values, **dd) if init_values else nn.Identity()),
-                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())
-            ])))
-
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+                ('ls', LayerScale(dim, init_values=init_values, **dd) if init_values else msnn.Identity()),
+                ('drop_path', DropPath(drop_path) if drop_path > 0. else msnn.Identity())
+            ])
+            ]))
+
+    def construct(self, x: ms.Tensor, attn_mask: Optional[torch.Tensor] = None) -> ms.Tensor:
         if attn_mask is not None:
             attn_out = []
             for attn in self.attns:
@@ -639,15 +649,15 @@                 x_attn = attn.ls(x_attn)
                 x_attn = attn.drop_path(x_attn)
                 attn_out.append(x_attn)
-            x = x + torch.stack(attn_out).sum(dim=0)
+            x = x + mint.stack(attn_out).sum(dim=0)
         else:
-            x = x + torch.stack([attn(x) for attn in self.attns]).sum(dim=0)
-        x = x + torch.stack([ffn(x) for ffn in self.ffns]).sum(dim=0)
+            x = x + mint.stack([attn(x) for attn in self.attns]).sum(dim=0)
+        x = x + mint.stack([ffn(x) for ffn in self.ffns]).sum(dim=0)
         return x
 
 
 def global_pool_nlc(
-        x: torch.Tensor,
+        x: ms.Tensor,
         pool_type: str = 'token',
         num_prefix_tokens: int = 1,
         reduce_include_prefix: bool = False,
@@ -671,7 +681,7 @@     return x
 
 
-class VisionTransformer(nn.Module):
+class VisionTransformer(msnn.Cell):
     """ Vision Transformer
 
     A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`
@@ -796,25 +806,26 @@         num_patches = self.patch_embed.num_patches
         reduction = self.patch_embed.feat_ratio() if hasattr(self.patch_embed, 'feat_ratio') else patch_size
 
-        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim, **dd)) if class_token else None
-        self.reg_token = nn.Parameter(torch.zeros(1, reg_tokens, embed_dim, **dd)) if reg_tokens else None
+        self.cls_token = ms.Parameter(mint.zeros(1, 1, embed_dim, **dd)) if class_token else None
+        self.reg_token = ms.Parameter(mint.zeros(1, reg_tokens, embed_dim, **dd)) if reg_tokens else None
         embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens
         if not pos_embed or pos_embed == 'none':
             self.pos_embed = None
         else:
-            self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim, **dd) * .02)
-        self.pos_drop = nn.Dropout(p=pos_drop_rate)
+            self.pos_embed = ms.Parameter(mint.randn(1, embed_len, embed_dim, **dd) * .02)
+        self.pos_drop = nn.Dropout(p = pos_drop_rate)
         if patch_drop_rate > 0:
             self.patch_drop = PatchDropout(
                 patch_drop_rate,
                 num_prefix_tokens=self.num_prefix_tokens,
             )
         else:
-            self.patch_drop = nn.Identity()
-        self.norm_pre = norm_layer(embed_dim, **dd) if pre_norm else nn.Identity()
+            self.patch_drop = msnn.Identity()
+        self.norm_pre = norm_layer(embed_dim, **dd) if pre_norm else msnn.Identity()
 
         dpr = calculate_drop_path_rates(drop_path_rate, depth)  # stochastic depth decay rule
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell([
+            [
             block_fn(
                 dim=embed_dim,
                 num_heads=num_heads,
@@ -835,10 +846,11 @@                 depth=i,
                 **dd,
             )
-            for i in range(depth)])
+            for i in range(depth)]
+        ])
         self.feature_info = [
             dict(module=f'blocks.{i}', num_chs=embed_dim, reduction=reduction) for i in range(depth)]
-        self.norm = norm_layer(embed_dim, **dd) if final_norm and not use_fc_norm else nn.Identity()
+        self.norm = norm_layer(embed_dim, **dd) if final_norm and not use_fc_norm else msnn.Identity()
 
         # Classifier Head
         if global_pool == 'map':
@@ -852,9 +864,9 @@             )
         else:
             self.attn_pool = None
-        self.fc_norm = norm_layer(embed_dim, **dd) if final_norm and use_fc_norm else nn.Identity()
+        self.fc_norm = norm_layer(embed_dim, **dd) if final_norm and use_fc_norm else msnn.Identity()
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(self.embed_dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.embed_dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()
 
         if weight_init != 'skip':
             self.init_weights(weight_init)
@@ -881,12 +893,13 @@         if self.pos_embed is not None:
             trunc_normal_(self.pos_embed, std=.02)
         if self.cls_token is not None:
-            nn.init.normal_(self.cls_token, std=1e-6)
+            nn.init.normal_(self.cls_token, std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.reg_token is not None:
-            nn.init.normal_(self.reg_token, std=1e-6)
+            nn.init.normal_(self.reg_token, std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         named_apply(get_init_weights_vit(mode, head_bias), self)
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def _init_weights(self, m: nn.Module) -> None:
         """Initialize weights for a single module (compatibility method)."""
         # this fn left here for compat with downstream users
@@ -933,6 +946,7 @@         if hasattr(self.patch_embed, 'set_grad_checkpointing'):
             self.patch_embed.set_grad_checkpointing(enable)
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         """Get the classifier head."""
@@ -953,7 +967,7 @@             elif global_pool != 'map' and self.attn_pool is not None:
                 self.attn_pool = None  # remove attention pooling
             self.global_pool = global_pool
-        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else msnn.Identity()
 
     def set_input_size(
             self,
@@ -972,7 +986,7 @@             num_prefix_tokens = 0 if self.no_embed_class else self.num_prefix_tokens
             num_new_tokens = self.patch_embed.num_patches + num_prefix_tokens
             if num_new_tokens != self.pos_embed.shape[1]:
-                self.pos_embed = nn.Parameter(resample_abs_pos_embed(
+                self.pos_embed = ms.Parameter(resample_abs_pos_embed(
                     self.pos_embed,
                     new_size=self.patch_embed.grid_size,
                     old_size=prev_grid_size,
@@ -980,7 +994,7 @@                     verbose=True,
                 ))
 
-    def _pos_embed(self, x: torch.Tensor) -> torch.Tensor:
+    def _pos_embed(self, x: ms.Tensor) -> ms.Tensor:
         """Apply positional embedding to input."""
         if self.pos_embed is None:
             return x.view(x.shape[0], -1, x.shape[-1])
@@ -1009,19 +1023,19 @@             # position embedding does not overlap with class token, add then concat
             x = x + pos_embed
             if to_cat:
-                x = torch.cat(to_cat + [x], dim=1)
+                x = mint.cat(to_cat + [x], dim = 1)
         else:
             # original timm, JAX, and deit vit impl
             # pos_embed has entry for class token, concat then add
             if to_cat:
-                x = torch.cat(to_cat + [x], dim=1)
+                x = mint.cat(to_cat + [x], dim = 1)
             x = x + pos_embed
 
         return self.pos_drop(x)
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             return_prefix_tokens: bool = False,
             norm: bool = False,
@@ -1133,15 +1147,15 @@         take_indices, max_index = feature_take_indices(len(self.blocks), indices)
         self.blocks = self.blocks[:max_index + 1]  # truncate blocks
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
-            self.fc_norm = nn.Identity()
+            self.fc_norm = msnn.Identity()
             self.reset_classifier(0, '')
         return take_indices
 
     def get_intermediate_layers(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             n: Union[int, List[int], Tuple[int]] = 1,
             reshape: bool = False,
             return_prefix_tokens: bool = False,
@@ -1171,7 +1185,7 @@             attn_mask=attn_mask,
         )
 
-    def forward_features(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor, attn_mask: Optional[torch.Tensor] = None) -> ms.Tensor:
         """Forward pass through feature layers (embeddings, transformer blocks, post-transformer norm)."""
         x = self.patch_embed(x)
         x = self._pos_embed(x)
@@ -1190,7 +1204,7 @@         x = self.norm(x)
         return x
 
-    def pool(self, x: torch.Tensor, pool_type: Optional[str] = None) -> torch.Tensor:
+    def pool(self, x: ms.Tensor, pool_type: Optional[str] = None) -> ms.Tensor:
         """Apply pooling to feature tokens.
 
         Args:
@@ -1214,7 +1228,7 @@         )
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -1229,12 +1243,13 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, attn_mask: Optional[torch.Tensor] = None) -> ms.Tensor:
         x = self.forward_features(x, attn_mask=attn_mask)
         x = self.forward_head(x)
         return x
 
 
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def init_weights_vit_timm(module: nn.Module, name: str = '') -> None:
     """ViT weight initialization, original timm impl (for reproducibility).
 
@@ -1245,11 +1260,12 @@     if isinstance(module, nn.Linear):
         trunc_normal_(module.weight, std=.02)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
 
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def init_weights_vit_jax(module: nn.Module, name: str = '', head_bias: float = 0.0) -> None:
     """ViT weight initialization, matching JAX (Flax) impl.
 
@@ -1260,20 +1276,21 @@     """
     if isinstance(module, nn.Linear):
         if name.startswith('head'):
-            nn.init.zeros_(module.weight)
-            nn.init.constant_(module.bias, head_bias)
+            nn.init.zeros_(module.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(module.bias, head_bias)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
-            nn.init.xavier_uniform_(module.weight)
+            nn.init.xavier_uniform_(module.weight)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.normal_(module.bias, std=1e-6) if 'mlp' in name else nn.init.zeros_(module.bias)
+                nn.init.normal_(module.bias, std=1e-6) if 'mlp' in name else nn.init.zeros_(module.bias)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Conv2d):
         lecun_normal_(module.weight)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
 
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def init_weights_vit_moco(module: nn.Module, name: str = '') -> None:
     """ViT weight initialization, matching moco-v3 impl minus fixed PatchEmbed.
 
@@ -1285,11 +1302,11 @@         if 'qkv' in name:
             # treat the weights of Q, K, V separately
             val = math.sqrt(6. / float(module.weight.shape[0] // 3 + module.weight.shape[1]))
-            nn.init.uniform_(module.weight, -val, val)
+            nn.init.uniform_(module.weight, -val, val)  # 'torch.nn.init.uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
-            nn.init.xavier_uniform_(module.weight)
+            nn.init.xavier_uniform_(module.weight)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
@@ -1304,13 +1321,13 @@ 
 
 def resize_pos_embed(
-        posemb: torch.Tensor,
-        posemb_new: torch.Tensor,
+        posemb: ms.Tensor,
+        posemb_new: ms.Tensor,
         num_prefix_tokens: int = 1,
         gs_new: Tuple[int, int] = (),
         interpolation: str = 'bicubic',
         antialias: bool = False,
-) -> torch.Tensor:
+) -> ms.Tensor:
     """ Rescale the grid of position embeddings when loading from state_dict.
     *DEPRECATED* This function is being deprecated in favour of using resample_abs_pos_embed
     """
@@ -1355,7 +1372,7 @@             elif _w.ndim == 2:
                 _w = _w.transpose([1, 0])
 
-        _w = torch.from_numpy(_w)
+        _w = torch.from_numpy(_w)  # 'torch.from_numpy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return _w
 
     if load_bfloat16:
@@ -1443,9 +1460,9 @@         block_prefix = f'{prefix}MAPHead_0/'
         mha_prefix = block_prefix + f'MultiHeadDotProductAttention_0/'
         model.attn_pool.latent.copy_(_n2p(w[f'{block_prefix}probe'], t=False))
-        model.attn_pool.kv.weight.copy_(torch.cat([
+        model.attn_pool.kv.weight.copy_(mint.cat([
             _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('key', 'value')]))
-        model.attn_pool.kv.bias.copy_(torch.cat([
+        model.attn_pool.kv.bias.copy_(mint.cat([
             _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('key', 'value')]))
         model.attn_pool.q.weight.copy_(_n2p(w[f'{mha_prefix}query/kernel'], t=False).flatten(1).T)
         model.attn_pool.q.bias.copy_(_n2p(w[f'{mha_prefix}query/bias'], t=False).reshape(-1))
@@ -1468,9 +1485,9 @@         mha_prefix = block_prefix + f'MultiHeadDotProductAttention_{mha_sub}/'
         block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale'], idx=idx))
         block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias'], idx=idx))
-        block.attn.qkv.weight.copy_(torch.cat([
+        block.attn.qkv.weight.copy_(mint.cat([
             _n2p(w[f'{mha_prefix}{n}/kernel'], t=False, idx=idx).flatten(1).T for n in ('query', 'key', 'value')]))
-        block.attn.qkv.bias.copy_(torch.cat([
+        block.attn.qkv.bias.copy_(mint.cat([
             _n2p(w[f'{mha_prefix}{n}/bias'], t=False, idx=idx).reshape(-1) for n in ('query', 'key', 'value')]))
         block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel'], idx=idx).flatten(1))
         block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias'], idx=idx))
@@ -1511,7 +1528,7 @@         if k == 'proj':
             k = 'head.weight'
             v = v.transpose(0, 1)
-            out_dict['head.bias'] = torch.zeros(v.shape[0])
+            out_dict['head.bias'] = mint.zeros(v.shape[0])
         elif k == 'class_embedding':
             k = 'cls_token'
             v = v.unsqueeze(0).unsqueeze(1)
@@ -1615,9 +1632,8 @@         stash = buf.setdefault((blk, kind), {})  # Gather by block & param type
         stash[which] = v
         if len(stash) == 3:  # Have q, k, v -> concatenate
-            out[f"blocks.{blk}.attn.qkv.{kind}"] = torch.cat(
-                [stash['q'], stash['k'], stash['v']], dim=0
-            )
+            out[f"blocks.{blk}.attn.qkv.{kind}"] = mint.cat(
+                [stash['q'], stash['k'], stash['v']], dim = 0)
 
     return out
 
@@ -1655,7 +1671,7 @@         if 'visual.head.proj.weight' in state_dict and isinstance(model.head, nn.Linear):
             # remap final nn.Linear if it exists outside of the timm .trunk (ie in visual.head.proj)
             out_dict['head.weight'] = state_dict['visual.head.proj.weight']
-            out_dict['head.bias'] = torch.zeros(state_dict['visual.head.proj.weight'].shape[0])
+            out_dict['head.bias'] = mint.zeros(state_dict['visual.head.proj.weight'].shape[0])
     elif 'module.visual.trunk.pos_embed' in state_dict:
         prefix = 'module.visual.trunk.'
     elif 'preprocessor.patchifier.proj.weight' in state_dict:
