--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Vision Transformer (ViT) in PyTorch
 
 A PyTorch implement of Vision Transformers as described in:
@@ -35,10 +40,10 @@ except ImportError:
     from typing_extensions import Literal
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-from torch.jit import Final
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
+# from torch.jit import Final
 
 from timm.data import (
     IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD,
@@ -97,10 +102,10 @@         proj_bias: bool = True,
         attn_drop: float = 0.,
         proj_drop: float = 0.,
-        norm_layer: Optional[Type[nn.Module]] = None,
+        norm_layer: Optional[Type[msnn.Cell]] = None,
         depth: int = 0,
         **kwargs,
-) -> nn.Module:
+) -> msnn.Cell:
     if isinstance(attn_layer, str):
         attn_layer = ATTN_LAYERS.get(attn_layer, None)
         assert attn_layer is not None, f'Unknown attn_layer: {attn_layer}'
@@ -120,10 +125,10 @@         proj_drop=proj_drop,
         norm_layer=norm_layer,
         **kwargs,
-    )
-
-
-class Block(nn.Module):
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+
+class Block(msnn.Cell):
     """Transformer block with pre-normalization."""
 
     def __init__(
@@ -140,9 +145,9 @@             attn_drop: float = 0.,
             init_values: Optional[float] = None,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = LayerNorm,
-            mlp_layer: Type[nn.Module] = Mlp,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            mlp_layer: Type[msnn.Cell] = Mlp,
             attn_layer: LayerType = Attention,
             depth: int = 0,
             device=None,
@@ -170,7 +175,7 @@         super().__init__()
         dd = {'device': device, 'dtype': dtype}
 
-        self.norm1 = norm_layer(dim, **dd)
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.attn = _create_attn(
             attn_layer,
             dim,
@@ -184,11 +189,11 @@             norm_layer=norm_layer,
             depth=depth,
             **dd,
-        )
-        self.ls1 = LayerScale(dim, init_values=init_values, **dd) if init_values else nn.Identity()
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-        self.norm2 = norm_layer(dim, **dd)
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls1 = LayerScale(dim, init_values=init_values, **dd) if init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.mlp = mlp_layer(
             in_features=dim,
             hidden_features=int(dim * mlp_ratio),
@@ -197,17 +202,17 @@             bias=proj_bias,
             drop=proj_drop,
             **dd,
-        )
-        self.ls2 = LayerScale(dim, init_values=init_values, **dd) if init_values else nn.Identity()
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.ls2 = LayerScale(dim, init_values=init_values, **dd) if init_values else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor, attn_mask: Optional[ms.Tensor] = None) -> ms.Tensor:
         x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x), attn_mask=attn_mask)))
         x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
         return x
 
 
-class ResPostBlock(nn.Module):
+class ResPostBlock(msnn.Cell):
     def __init__(
             self,
             dim: int,
@@ -222,9 +227,9 @@             attn_drop: float = 0.,
             init_values: Optional[float] = None,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = LayerNorm,
-            mlp_layer: Type[nn.Module] = Mlp,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            mlp_layer: Type[msnn.Cell] = Mlp,
             attn_layer: LayerType = Attention,
             depth: int = 0,
             device=None,
@@ -247,9 +252,9 @@             norm_layer=norm_layer,
             depth=depth,
             **dd,
-        )
-        self.norm1 = norm_layer(dim, **dd)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm1 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.mlp = mlp_layer(
             in_features=dim,
@@ -259,30 +264,30 @@             bias=proj_bias,
             drop=proj_drop,
             **dd,
-        )
-        self.norm2 = norm_layer(dim, **dd)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.norm2 = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.init_weights()
 
     def init_weights(self) -> None:
         # NOTE this init overrides that base model init with specific changes for the block type
         if self.init_values is not None:
-            nn.init.constant_(self.norm1.weight, self.init_values)
-            nn.init.constant_(self.norm2.weight, self.init_values)
-
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+            nn.init.constant_(self.norm1.weight, self.init_values)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(self.norm2.weight, self.init_values)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x: ms.Tensor, attn_mask: Optional[ms.Tensor] = None) -> ms.Tensor:
         x = x + self.drop_path1(self.norm1(self.attn(x, attn_mask=attn_mask)))
         x = x + self.drop_path2(self.norm2(self.mlp(x)))
         return x
 
 
-class ParallelScalingBlock(nn.Module):
+class ParallelScalingBlock(msnn.Cell):
     """ Parallel ViT block (MLP & Attention in parallel)
     Based on:
       'Scaling Vision Transformers to 22 Billion Parameters` - https://arxiv.org/abs/2302.05442
     """
-    fused_attn: Final[bool]
+    fused_attn: Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -298,9 +303,9 @@             attn_drop: float = 0.,
             init_values: Optional[float] = None,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = LayerNorm,
-            mlp_layer: Optional[Type[nn.Module]] = None,  # not used
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            mlp_layer: Optional[Type[msnn.Cell]] = None,  # not used
             attn_layer: Optional[LayerType] = None,  # not used
             depth: int = 0,  # not used
             fuse_out_proj: bool = False,
@@ -318,18 +323,18 @@         mlp_hidden_dim = int(mlp_ratio * dim)
         in_proj_out_dim = mlp_hidden_dim + 3 * dim
 
-        self.in_norm = norm_layer(dim, **dd)
-        self.in_proj = nn.Linear(dim, in_proj_out_dim, bias=qkv_bias, **dd)
+        self.in_norm = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.in_proj = nn.Linear(dim, in_proj_out_dim, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.in_split = [mlp_hidden_dim] + [dim] * 3
         if qkv_bias:
             self.register_buffer('qkv_bias', None)
             self.register_parameter('mlp_bias', None)
         else:
-            self.register_buffer('qkv_bias', torch.zeros(3 * dim, **dd), persistent=False)
-            self.mlp_bias = nn.Parameter(torch.zeros(mlp_hidden_dim, **dd))
-
-        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()
-        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()
+            self.register_buffer('qkv_bias', mint.zeros(3 * dim, **dd), persistent=False)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.mlp_bias = ms.Parameter(mint.zeros(mlp_hidden_dim, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.attn_drop = nn.Dropout(attn_drop)
 
         self.mlp_drop = nn.Dropout(proj_drop)
@@ -337,19 +342,19 @@ 
         if fuse_out_proj:
             # Fused output projection for both attention and MLP
-            self.out_proj = nn.Linear(dim + mlp_hidden_dim, dim, bias=proj_bias, **dd)
+            self.out_proj = nn.Linear(dim + mlp_hidden_dim, dim, bias=proj_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
             self.attn_out_proj = None
             self.mlp_out_proj = None
         else:
             # Separate output projections
             self.out_proj = None
-            self.attn_out_proj = nn.Linear(dim, dim, bias=proj_bias, **dd)
-            self.mlp_out_proj = nn.Linear(mlp_hidden_dim, dim, bias=proj_bias, **dd)
-
-        self.ls = LayerScale(dim, init_values=init_values, **dd) if init_values is not None else nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+            self.attn_out_proj = nn.Linear(dim, dim, bias=proj_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+            self.mlp_out_proj = nn.Linear(mlp_hidden_dim, dim, bias=proj_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+        self.ls = LayerScale(dim, init_values=init_values, **dd) if init_values is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor, attn_mask: Optional[ms.Tensor] = None) -> ms.Tensor:
         B, N, C = x.shape
 
         # Combined MLP fc1 & qkv projections
@@ -357,10 +362,10 @@         if self.mlp_bias is not None:
             # Concat constant zero-bias for qkv w/ trainable mlp_bias.
             # Appears faster than adding to x_mlp separately
-            y = F.linear(y, self.in_proj.weight, torch.cat((self.qkv_bias, self.mlp_bias)))
+            y = nn.functional.linear(y, self.in_proj.weight, mint.cat((self.qkv_bias, self.mlp_bias)))
         else:
             y = self.in_proj(y)
-        x_mlp, q, k, v = torch.split(y, self.in_split, dim=-1)
+        x_mlp, q, k, v = mint.split(y, self.in_split, dim=-1)
 
         # Dot product attention w/ qk norm
         q = self.q_norm(q.view(B, N, self.num_heads, self.head_dim)).transpose(1, 2)
@@ -371,7 +376,7 @@                 q, k, v,
                 attn_mask=attn_mask,
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
@@ -388,7 +393,7 @@ 
         # Output projection (fused or separate)
         if self.out_proj is not None:
-            y = self.out_proj(torch.cat((x_attn, x_mlp), dim=-1))
+            y = self.out_proj(mint.cat((x_attn, x_mlp), dim=-1))
         else:
             y = self.attn_out_proj(x_attn) + self.mlp_out_proj(x_mlp)
 
@@ -397,14 +402,14 @@         return x
 
 
-class DiffParallelScalingBlock(nn.Module):
+class DiffParallelScalingBlock(msnn.Cell):
     """ Parallel ViT block with Differential Attention (MLP & Attention in parallel).
 
     Combines the parallel MLP+Attention structure from 'Scaling Vision Transformers to
     22 Billion Parameters' (https://arxiv.org/abs/2302.05442) with differential attention
     from 'Differential Transformer' (https://arxiv.org/abs/2410.05258).
     """
-    fused_attn: Final[bool]
+    fused_attn: Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -420,9 +425,9 @@             attn_drop: float = 0.,
             init_values: Optional[float] = None,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = LayerNorm,
-            mlp_layer: Optional[Type[nn.Module]] = None,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            mlp_layer: Optional[Type[msnn.Cell]] = None,
             attn_layer: Optional[LayerType] = None,
             depth: int = 0,
             dual_lambda: bool = False,
@@ -440,43 +445,43 @@         mlp_hidden_dim = int(mlp_ratio * dim)
         in_proj_out_dim = mlp_hidden_dim + 3 * dim
 
-        self.in_norm = norm_layer(dim, **dd)
-        self.in_proj = nn.Linear(dim, in_proj_out_dim, bias=qkv_bias, **dd)
+        self.in_norm = norm_layer(dim, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.in_proj = nn.Linear(dim, in_proj_out_dim, bias=qkv_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         self.in_split = [mlp_hidden_dim] + [dim] * 3
         if qkv_bias:
             self.register_buffer('qkv_bias', None)
             self.register_parameter('mlp_bias', None)
         else:
-            self.register_buffer('qkv_bias', torch.zeros(3 * dim, **dd), persistent=False)
-            self.mlp_bias = nn.Parameter(torch.zeros(mlp_hidden_dim, **dd))
-
-        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()
-        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else nn.Identity()
+            self.register_buffer('qkv_bias', mint.zeros(3 * dim, **dd), persistent=False)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.mlp_bias = ms.Parameter(mint.zeros(mlp_hidden_dim, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.q_norm = norm_layer(self.head_dim, **dd) if qk_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.k_norm = norm_layer(self.head_dim, **dd) if qk_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.attn_drop = nn.Dropout(attn_drop)
         self.attn_drop_p = attn_drop
 
         # Differential attention specific
-        self.sub_norm = RmsNorm(2 * self.head_dim, eps=1e-5, **dd)
+        self.sub_norm = RmsNorm(2 * self.head_dim, eps=1e-5, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.dual_lambda = dual_lambda
         if dual_lambda:
-            self.lambda_a = nn.Parameter(torch.empty((), dtype=torch.float32, device=device))
-            self.lambda_b = nn.Parameter(torch.empty((), dtype=torch.float32, device=device))
+            self.lambda_a = ms.Parameter(mint.empty((), dtype=ms.float32, device=device))
+            self.lambda_b = ms.Parameter(mint.empty((), dtype=ms.float32, device=device))
             self.lambda_q1 = self.lambda_k1 = self.lambda_q2 = self.lambda_k2 = None
         else:
             self.lambda_a = self.lambda_b = None
-            self.lambda_q1 = nn.Parameter(torch.empty(self.head_dim, dtype=torch.float32, device=device))
-            self.lambda_k1 = nn.Parameter(torch.empty(self.head_dim, dtype=torch.float32, device=device))
-            self.lambda_q2 = nn.Parameter(torch.empty(self.head_dim, dtype=torch.float32, device=device))
-            self.lambda_k2 = nn.Parameter(torch.empty(self.head_dim, dtype=torch.float32, device=device))
+            self.lambda_q1 = ms.Parameter(mint.empty(self.head_dim, dtype=ms.float32, device=device))
+            self.lambda_k1 = ms.Parameter(mint.empty(self.head_dim, dtype=ms.float32, device=device))
+            self.lambda_q2 = ms.Parameter(mint.empty(self.head_dim, dtype=ms.float32, device=device))
+            self.lambda_k2 = ms.Parameter(mint.empty(self.head_dim, dtype=ms.float32, device=device))
 
         self.mlp_drop = nn.Dropout(proj_drop)
         self.mlp_act = act_layer()
 
         # Fused output projection for both attention and MLP
-        self.out_proj = nn.Linear(dim + mlp_hidden_dim, dim, bias=proj_bias, **dd)
-
-        self.ls = LayerScale(dim, init_values=init_values, **dd) if init_values is not None else nn.Identity()
-        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.out_proj = nn.Linear(dim + mlp_hidden_dim, dim, bias=proj_bias, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+        self.ls = LayerScale(dim, init_values=init_values, **dd) if init_values is not None else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.lambda_init = 0.8
         self.set_lambda_init(depth)
@@ -487,33 +492,33 @@ 
     def reset_parameters(self):
         if self.dual_lambda:
-            nn.init.zeros_(self.lambda_a)
-            nn.init.zeros_(self.lambda_b)
+            nn.init.zeros_(self.lambda_a)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.zeros_(self.lambda_b)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
-            nn.init.normal_(self.lambda_q1, mean=0, std=0.1)
-            nn.init.normal_(self.lambda_k1, mean=0, std=0.1)
-            nn.init.normal_(self.lambda_q2, mean=0, std=0.1)
-            nn.init.normal_(self.lambda_k2, mean=0, std=0.1)
-
-    def _compute_lambda(self) -> torch.Tensor:
+            nn.init.normal_(self.lambda_q1, mean=0, std=0.1)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.normal_(self.lambda_k1, mean=0, std=0.1)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.normal_(self.lambda_q2, mean=0, std=0.1)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.normal_(self.lambda_k2, mean=0, std=0.1)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def _compute_lambda(self) -> ms.Tensor:
         if self.lambda_a is not None:
-            lambda_1 = torch.exp(self.lambda_a)
-            lambda_2 = torch.exp(self.lambda_b)
+            lambda_1 = mint.exp(self.lambda_a)
+            lambda_2 = mint.exp(self.lambda_b)
         else:
-            lambda_1 = torch.exp(torch.sum(self.lambda_q1 * self.lambda_k1, dim=-1).float())
-            lambda_2 = torch.exp(torch.sum(self.lambda_q2 * self.lambda_k2, dim=-1).float())
+            lambda_1 = mint.exp(mint.sum(self.lambda_q1 * self.lambda_k1, dim=-1).float())
+            lambda_2 = mint.exp(mint.sum(self.lambda_q2 * self.lambda_k2, dim=-1).float())
         return lambda_1 - lambda_2 + self.lambda_init
 
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, attn_mask: Optional[ms.Tensor] = None) -> ms.Tensor:
         B, N, C = x.shape
 
         # Combined MLP fc1 & qkv projections
         y = self.in_norm(x)
         if self.mlp_bias is not None:
-            y = F.linear(y, self.in_proj.weight, torch.cat((self.qkv_bias, self.mlp_bias)))
+            y = nn.functional.linear(y, self.in_proj.weight, mint.cat((self.qkv_bias, self.mlp_bias)))
         else:
             y = self.in_proj(y)
-        x_mlp, q, k, v = torch.split(y, self.in_split, dim=-1)
+        x_mlp, q, k, v = mint.split(y, self.in_split, dim=-1)
 
         # Reshape for differential attention (2x heads with half head_dim for q/k)
         q = q.reshape(B, N, 2 * self.num_heads, self.head_dim).transpose(1, 2)
@@ -531,8 +536,8 @@             k1, k2 = k.unbind(2)
 
             dropout_p = self.attn_drop_p if self.training else 0.0
-            attn1 = F.scaled_dot_product_attention(q1, k1, v, attn_mask=attn_mask, dropout_p=dropout_p)
-            attn2 = F.scaled_dot_product_attention(q2, k2, v, attn_mask=attn_mask, dropout_p=dropout_p)
+            attn1 = F.scaled_dot_product_attention(q1, k1, v, attn_mask=attn_mask, dropout_p=dropout_p)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            attn2 = F.scaled_dot_product_attention(q2, k2, v, attn_mask=attn_mask, dropout_p=dropout_p)  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
             x_attn = attn1 - lambda_full * attn2
         else:
@@ -555,14 +560,14 @@         x_mlp = self.mlp_drop(x_mlp)
 
         # Fused output projection
-        y = self.out_proj(torch.cat((x_attn, x_mlp), dim=-1))
+        y = self.out_proj(mint.cat((x_attn, x_mlp), dim=-1))
 
         # Add residual w/ drop path & layer scale applied
         x = x + self.drop_path(self.ls(y))
         return x
 
 
-class ParallelThingsBlock(nn.Module):
+class ParallelThingsBlock(msnn.Cell):
     """ Parallel ViT block (N parallel attention followed by N parallel MLP)
     Based on:
       `Three things everyone should know about Vision Transformers` - https://arxiv.org/abs/2203.09795
@@ -582,9 +587,9 @@             proj_drop: float = 0.,
             attn_drop: float = 0.,
             drop_path: float = 0.,
-            act_layer: Type[nn.Module] = nn.GELU,
-            norm_layer: Type[nn.Module] = LayerNorm,
-            mlp_layer: Type[nn.Module] = Mlp,
+            act_layer: Type[msnn.Cell] = nn.GELU,
+            norm_layer: Type[msnn.Cell] = LayerNorm,
+            mlp_layer: Type[msnn.Cell] = Mlp,
             attn_layer: LayerType = Attention,
             depth: int = 0,
             device=None,
@@ -593,10 +598,11 @@         dd = {'device': device, 'dtype': dtype}
         super().__init__()
         self.num_parallel = num_parallel
-        self.attns = nn.ModuleList()
-        self.ffns = nn.ModuleList()
+        self.attns = msnn.CellList()
+        self.ffns = msnn.CellList()
         for _ in range(num_parallel):
-            self.attns.append(nn.Sequential(OrderedDict([
+            self.attns.append(msnn.SequentialCell([
+                OrderedDict([
                 ('norm', norm_layer(dim, **dd)),
                 ('attn', _create_attn(
                     attn_layer,
@@ -612,10 +618,12 @@                     depth=depth,
                     **dd,
                 )),
-                ('ls', LayerScale(dim, init_values=init_values, **dd) if init_values else nn.Identity()),
-                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())
-            ])))
-            self.ffns.append(nn.Sequential(OrderedDict([
+                ('ls', LayerScale(dim, init_values=init_values, **dd) if init_values else msnn.Identity()),
+                ('drop_path', DropPath(drop_path) if drop_path > 0. else msnn.Identity())
+            ])
+            ]))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            self.ffns.append(msnn.SequentialCell([
+                OrderedDict([
                 ('norm', norm_layer(dim, **dd)),
                 ('mlp', mlp_layer(
                     dim,
@@ -626,11 +634,12 @@                     drop=proj_drop,
                     **dd,
                 )),
-                ('ls', LayerScale(dim, init_values=init_values, **dd) if init_values else nn.Identity()),
-                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())
-            ])))
-
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+                ('ls', LayerScale(dim, init_values=init_values, **dd) if init_values else msnn.Identity()),
+                ('drop_path', DropPath(drop_path) if drop_path > 0. else msnn.Identity())
+            ])
+            ]))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x: ms.Tensor, attn_mask: Optional[ms.Tensor] = None) -> ms.Tensor:
         if attn_mask is not None:
             attn_out = []
             for attn in self.attns:
@@ -639,15 +648,15 @@                 x_attn = attn.ls(x_attn)
                 x_attn = attn.drop_path(x_attn)
                 attn_out.append(x_attn)
-            x = x + torch.stack(attn_out).sum(dim=0)
+            x = x + mint.stack(attn_out).sum(dim=0)
         else:
-            x = x + torch.stack([attn(x) for attn in self.attns]).sum(dim=0)
-        x = x + torch.stack([ffn(x) for ffn in self.ffns]).sum(dim=0)
+            x = x + mint.stack([attn(x) for attn in self.attns]).sum(dim=0)
+        x = x + mint.stack([ffn(x) for ffn in self.ffns]).sum(dim=0)
         return x
 
 
 def global_pool_nlc(
-        x: torch.Tensor,
+        x: ms.Tensor,
         pool_type: str = 'token',
         num_prefix_tokens: int = 1,
         reduce_include_prefix: bool = False,
@@ -671,13 +680,13 @@     return x
 
 
-class VisionTransformer(nn.Module):
+class VisionTransformer(msnn.Cell):
     """ Vision Transformer
 
     A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`
         - https://arxiv.org/abs/2010.11929
     """
-    dynamic_img_size: Final[bool]
+    dynamic_img_size: Final[bool]  # 'torch.jit.Final' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     def __init__(
             self,
@@ -718,8 +727,8 @@             embed_norm_layer: Optional[LayerType] = None,
             norm_layer: Optional[LayerType] = None,
             act_layer: Optional[LayerType] = None,
-            block_fn: Type[nn.Module] = Block,
-            mlp_layer: Type[nn.Module] = Mlp,
+            block_fn: Type[msnn.Cell] = Block,
+            mlp_layer: Type[msnn.Cell] = Mlp,
             attn_layer: LayerType = Attention,
             device=None,
             dtype=None,
@@ -792,29 +801,29 @@             dynamic_img_pad=dynamic_img_pad,
             **embed_args,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         num_patches = self.patch_embed.num_patches
         reduction = self.patch_embed.feat_ratio() if hasattr(self.patch_embed, 'feat_ratio') else patch_size
 
-        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim, **dd)) if class_token else None
-        self.reg_token = nn.Parameter(torch.zeros(1, reg_tokens, embed_dim, **dd)) if reg_tokens else None
+        self.cls_token = ms.Parameter(mint.zeros(1, 1, embed_dim, **dd)) if class_token else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.reg_token = ms.Parameter(mint.zeros(1, reg_tokens, embed_dim, **dd)) if reg_tokens else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens
         if not pos_embed or pos_embed == 'none':
             self.pos_embed = None
         else:
-            self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim, **dd) * .02)
-        self.pos_drop = nn.Dropout(p=pos_drop_rate)
+            self.pos_embed = ms.Parameter(mint.randn(1, embed_len, embed_dim, **dd) * .02)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.pos_drop = nn.Dropout(p = pos_drop_rate)
         if patch_drop_rate > 0:
             self.patch_drop = PatchDropout(
                 patch_drop_rate,
                 num_prefix_tokens=self.num_prefix_tokens,
             )
         else:
-            self.patch_drop = nn.Identity()
-        self.norm_pre = norm_layer(embed_dim, **dd) if pre_norm else nn.Identity()
+            self.patch_drop = msnn.Identity()
+        self.norm_pre = norm_layer(embed_dim, **dd) if pre_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         dpr = calculate_drop_path_rates(drop_path_rate, depth)  # stochastic depth decay rule
-        self.blocks = nn.Sequential(*[
+        self.blocks = msnn.SequentialCell(*[
             block_fn(
                 dim=embed_dim,
                 num_heads=num_heads,
@@ -835,10 +844,10 @@                 depth=i,
                 **dd,
             )
-            for i in range(depth)])
+            for i in range(depth)])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info = [
             dict(module=f'blocks.{i}', num_chs=embed_dim, reduction=reduction) for i in range(depth)]
-        self.norm = norm_layer(embed_dim, **dd) if final_norm and not use_fc_norm else nn.Identity()
+        self.norm = norm_layer(embed_dim, **dd) if final_norm and not use_fc_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # Classifier Head
         if global_pool == 'map':
@@ -849,12 +858,12 @@                 norm_layer=norm_layer,
                 act_layer=act_layer,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.attn_pool = None
-        self.fc_norm = norm_layer(embed_dim, **dd) if final_norm and use_fc_norm else nn.Identity()
+        self.fc_norm = norm_layer(embed_dim, **dd) if final_norm and use_fc_norm else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(self.embed_dim, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.embed_dim, num_classes, **dd) if num_classes > 0 else msnn.Identity()  # 存在 *args/**kwargs，需手动确认参数映射;
 
         if weight_init != 'skip':
             self.init_weights(weight_init)
@@ -881,18 +890,18 @@         if self.pos_embed is not None:
             trunc_normal_(self.pos_embed, std=.02)
         if self.cls_token is not None:
-            nn.init.normal_(self.cls_token, std=1e-6)
+            nn.init.normal_(self.cls_token, std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.reg_token is not None:
-            nn.init.normal_(self.reg_token, std=1e-6)
+            nn.init.normal_(self.reg_token, std=1e-6)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         named_apply(get_init_weights_vit(mode, head_bias), self)
 
-    def _init_weights(self, m: nn.Module) -> None:
+    def _init_weights(self, m: msnn.Cell) -> None:
         """Initialize weights for a single module (compatibility method)."""
         # this fn left here for compat with downstream users
         init_weights_vit_timm(m)
 
-    @torch.jit.ignore()
+    @ms.jit()
     def load_pretrained(self, checkpoint_path: str, prefix: str = '') -> None:
         """Load pretrained weights.
 
@@ -902,12 +911,12 @@         """
         _load_weights(self, checkpoint_path, prefix)
 
-    @torch.jit.ignore
+    @ms.jit
     def no_weight_decay(self) -> Set[str]:
         """Set of parameters that should not use weight decay."""
         return {'pos_embed', 'cls_token', 'dist_token'}
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse: bool = False) -> Dict[str, Union[str, List]]:
         """Create regex patterns for parameter grouping.
 
@@ -922,7 +931,7 @@             blocks=[(r'^blocks\.(\d+)', None), (r'^norm', (99999,))]
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable: bool = True) -> None:
         """Enable or disable gradient checkpointing.
 
@@ -933,8 +942,8 @@         if hasattr(self.patch_embed, 'set_grad_checkpointing'):
             self.patch_embed.set_grad_checkpointing(enable)
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         """Get the classifier head."""
         return self.head
 
@@ -953,7 +962,7 @@             elif global_pool != 'map' and self.attn_pool is not None:
                 self.attn_pool = None  # remove attention pooling
             self.global_pool = global_pool
-        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else msnn.Identity()
 
     def set_input_size(
             self,
@@ -972,7 +981,7 @@             num_prefix_tokens = 0 if self.no_embed_class else self.num_prefix_tokens
             num_new_tokens = self.patch_embed.num_patches + num_prefix_tokens
             if num_new_tokens != self.pos_embed.shape[1]:
-                self.pos_embed = nn.Parameter(resample_abs_pos_embed(
+                self.pos_embed = ms.Parameter(resample_abs_pos_embed(
                     self.pos_embed,
                     new_size=self.patch_embed.grid_size,
                     old_size=prev_grid_size,
@@ -980,7 +989,7 @@                     verbose=True,
                 ))
 
-    def _pos_embed(self, x: torch.Tensor) -> torch.Tensor:
+    def _pos_embed(self, x: ms.Tensor) -> ms.Tensor:
         """Apply positional embedding to input."""
         if self.pos_embed is None:
             return x.view(x.shape[0], -1, x.shape[-1])
@@ -1009,19 +1018,19 @@             # position embedding does not overlap with class token, add then concat
             x = x + pos_embed
             if to_cat:
-                x = torch.cat(to_cat + [x], dim=1)
+                x = mint.cat(to_cat + [x], dim=1)
         else:
             # original timm, JAX, and deit vit impl
             # pos_embed has entry for class token, concat then add
             if to_cat:
-                x = torch.cat(to_cat + [x], dim=1)
+                x = mint.cat(to_cat + [x], dim=1)
             x = x + pos_embed
 
         return self.pos_drop(x)
 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             return_prefix_tokens: bool = False,
             norm: bool = False,
@@ -1029,8 +1038,8 @@             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
             output_dict: bool = False,
-            attn_mask: Optional[torch.Tensor] = None,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]], Dict[str, Any]]:
+            attn_mask: Optional[ms.Tensor] = None,
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]], Dict[str, Any]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -1059,6 +1068,7 @@         x = self.patch_drop(x)
         x = self.norm_pre(x)
 
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             blocks = self.blocks
         else:
@@ -1066,6 +1076,7 @@         for i, blk in enumerate(blocks):
             if attn_mask is not None:
                 x = blk(x, attn_mask=attn_mask)
+            # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif self.grad_checkpointing and not torch.jit.is_scripting():
                 x = checkpoint(blk, x)
             else:
@@ -1103,6 +1114,7 @@             return result_dict
 
         # For non-dictionary output, maintain the original behavior
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if not torch.jit.is_scripting() and return_prefix_tokens and prefix_tokens is not None:
             # return_prefix not support in torchscript due to poor type handling
             intermediates = list(zip(intermediates, prefix_tokens))
@@ -1133,21 +1145,21 @@         take_indices, max_index = feature_take_indices(len(self.blocks), indices)
         self.blocks = self.blocks[:max_index + 1]  # truncate blocks
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
-            self.fc_norm = nn.Identity()
+            self.fc_norm = msnn.Identity()
             self.reset_classifier(0, '')
         return take_indices
 
     def get_intermediate_layers(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             n: Union[int, List[int], Tuple[int]] = 1,
             reshape: bool = False,
             return_prefix_tokens: bool = False,
             norm: bool = False,
-            attn_mask: Optional[torch.Tensor] = None,
-    ) -> List[torch.Tensor]:
+            attn_mask: Optional[ms.Tensor] = None,
+    ) -> List[ms.Tensor]:
         """Get intermediate layer outputs (DINO interface compatibility).
 
         NOTE: This API is for backwards compat, favour using forward_intermediates() directly.
@@ -1171,7 +1183,7 @@             attn_mask=attn_mask,
         )
 
-    def forward_features(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor, attn_mask: Optional[ms.Tensor] = None) -> ms.Tensor:
         """Forward pass through feature layers (embeddings, transformer blocks, post-transformer norm)."""
         x = self.patch_embed(x)
         x = self._pos_embed(x)
@@ -1182,6 +1194,7 @@             # If mask provided, we need to apply blocks one by one
             for blk in self.blocks:
                 x = blk(x, attn_mask=attn_mask)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
@@ -1190,7 +1203,7 @@         x = self.norm(x)
         return x
 
-    def pool(self, x: torch.Tensor, pool_type: Optional[str] = None) -> torch.Tensor:
+    def pool(self, x: ms.Tensor, pool_type: Optional[str] = None) -> ms.Tensor:
         """Apply pooling to feature tokens.
 
         Args:
@@ -1214,7 +1227,7 @@         )
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classifier head.
 
         Args:
@@ -1229,13 +1242,13 @@         x = self.head_drop(x)
         return x if pre_logits else self.head(x)
 
-    def forward(self, x: torch.Tensor, attn_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
+    def construct(self, x: ms.Tensor, attn_mask: Optional[ms.Tensor] = None) -> ms.Tensor:
         x = self.forward_features(x, attn_mask=attn_mask)
         x = self.forward_head(x)
         return x
 
 
-def init_weights_vit_timm(module: nn.Module, name: str = '') -> None:
+def init_weights_vit_timm(module: msnn.Cell, name: str = '') -> None:
     """ViT weight initialization, original timm impl (for reproducibility).
 
     Args:
@@ -1245,12 +1258,12 @@     if isinstance(module, nn.Linear):
         trunc_normal_(module.weight, std=.02)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
 
-def init_weights_vit_jax(module: nn.Module, name: str = '', head_bias: float = 0.0) -> None:
+def init_weights_vit_jax(module: msnn.Cell, name: str = '', head_bias: float = 0.0) -> None:
     """ViT weight initialization, matching JAX (Flax) impl.
 
     Args:
@@ -1260,21 +1273,21 @@     """
     if isinstance(module, nn.Linear):
         if name.startswith('head'):
-            nn.init.zeros_(module.weight)
-            nn.init.constant_(module.bias, head_bias)
+            nn.init.zeros_(module.weight)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(module.bias, head_bias)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
-            nn.init.xavier_uniform_(module.weight)
+            nn.init.xavier_uniform_(module.weight)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if module.bias is not None:
-                nn.init.normal_(module.bias, std=1e-6) if 'mlp' in name else nn.init.zeros_(module.bias)
+                nn.init.normal_(module.bias, std=1e-6) if 'mlp' in name else nn.init.zeros_(module.bias)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif isinstance(module, nn.Conv2d):
         lecun_normal_(module.weight)
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
 
-def init_weights_vit_moco(module: nn.Module, name: str = '') -> None:
+def init_weights_vit_moco(module: msnn.Cell, name: str = '') -> None:
     """ViT weight initialization, matching moco-v3 impl minus fixed PatchEmbed.
 
     Args:
@@ -1285,11 +1298,11 @@         if 'qkv' in name:
             # treat the weights of Q, K, V separately
             val = math.sqrt(6. / float(module.weight.shape[0] // 3 + module.weight.shape[1]))
-            nn.init.uniform_(module.weight, -val, val)
+            nn.init.uniform_(module.weight, -val, val)  # 'torch.nn.init.uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
-            nn.init.xavier_uniform_(module.weight)
+            nn.init.xavier_uniform_(module.weight)  # 'torch.nn.init.xavier_uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if module.bias is not None:
-            nn.init.zeros_(module.bias)
+            nn.init.zeros_(module.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     elif hasattr(module, 'init_weights'):
         module.init_weights()
 
@@ -1304,13 +1317,13 @@ 
 
 def resize_pos_embed(
-        posemb: torch.Tensor,
-        posemb_new: torch.Tensor,
+        posemb: ms.Tensor,
+        posemb_new: ms.Tensor,
         num_prefix_tokens: int = 1,
         gs_new: Tuple[int, int] = (),
         interpolation: str = 'bicubic',
         antialias: bool = False,
-) -> torch.Tensor:
+) -> ms.Tensor:
     """ Rescale the grid of position embeddings when loading from state_dict.
     *DEPRECATED* This function is being deprecated in favour of using resample_abs_pos_embed
     """
@@ -1328,6 +1341,8 @@     )
 
 
+# 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 装饰器 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 @torch.no_grad()
 def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str = '', load_bfloat16: bool = False) -> None:
     """ Load weights from .npz checkpoints for official Google Brain Flax implementation
@@ -1355,7 +1370,7 @@             elif _w.ndim == 2:
                 _w = _w.transpose([1, 0])
 
-        _w = torch.from_numpy(_w)
+        _w = torch.from_numpy(_w)  # 'torch.from_numpy' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return _w
 
     if load_bfloat16:
@@ -1443,9 +1458,9 @@         block_prefix = f'{prefix}MAPHead_0/'
         mha_prefix = block_prefix + f'MultiHeadDotProductAttention_0/'
         model.attn_pool.latent.copy_(_n2p(w[f'{block_prefix}probe'], t=False))
-        model.attn_pool.kv.weight.copy_(torch.cat([
+        model.attn_pool.kv.weight.copy_(mint.cat([
             _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('key', 'value')]))
-        model.attn_pool.kv.bias.copy_(torch.cat([
+        model.attn_pool.kv.bias.copy_(mint.cat([
             _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('key', 'value')]))
         model.attn_pool.q.weight.copy_(_n2p(w[f'{mha_prefix}query/kernel'], t=False).flatten(1).T)
         model.attn_pool.q.bias.copy_(_n2p(w[f'{mha_prefix}query/bias'], t=False).reshape(-1))
@@ -1468,9 +1483,9 @@         mha_prefix = block_prefix + f'MultiHeadDotProductAttention_{mha_sub}/'
         block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale'], idx=idx))
         block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias'], idx=idx))
-        block.attn.qkv.weight.copy_(torch.cat([
+        block.attn.qkv.weight.copy_(mint.cat([
             _n2p(w[f'{mha_prefix}{n}/kernel'], t=False, idx=idx).flatten(1).T for n in ('query', 'key', 'value')]))
-        block.attn.qkv.bias.copy_(torch.cat([
+        block.attn.qkv.bias.copy_(mint.cat([
             _n2p(w[f'{mha_prefix}{n}/bias'], t=False, idx=idx).reshape(-1) for n in ('query', 'key', 'value')]))
         block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel'], idx=idx).flatten(1))
         block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias'], idx=idx))
@@ -1484,10 +1499,10 @@ 
 
 def _convert_openai_clip(
-        state_dict: Dict[str, torch.Tensor],
+        state_dict: Dict[str, ms.Tensor],
         model: VisionTransformer,
         prefix: str = 'visual.',
-) -> Dict[str, torch.Tensor]:
+) -> Dict[str, ms.Tensor]:
     out_dict = {}
     swaps = [
         ('conv1', 'patch_embed.proj'),
@@ -1511,7 +1526,7 @@         if k == 'proj':
             k = 'head.weight'
             v = v.transpose(0, 1)
-            out_dict['head.bias'] = torch.zeros(v.shape[0])
+            out_dict['head.bias'] = mint.zeros(v.shape[0])
         elif k == 'class_embedding':
             k = 'cls_token'
             v = v.unsqueeze(0).unsqueeze(1)
@@ -1522,9 +1537,9 @@ 
 
 def _convert_dinov2(
-        state_dict: Dict[str, torch.Tensor],
+        state_dict: Dict[str, ms.Tensor],
         model: VisionTransformer,
-) -> Dict[str, torch.Tensor]:
+) -> Dict[str, ms.Tensor]:
     import re
     out_dict = {}
     state_dict.pop("mask_token", None)
@@ -1545,9 +1560,9 @@ 
 
 def _convert_aimv2(
-        state_dict: Dict[str, torch.Tensor],
+        state_dict: Dict[str, ms.Tensor],
         model: VisionTransformer,
-) -> Dict[str, torch.Tensor]:
+) -> Dict[str, ms.Tensor]:
     out_dict = {}
     for k, v in state_dict.items():
         k = k.replace('norm_1', 'norm1')
@@ -1615,7 +1630,7 @@         stash = buf.setdefault((blk, kind), {})  # Gather by block & param type
         stash[which] = v
         if len(stash) == 3:  # Have q, k, v -> concatenate
-            out[f"blocks.{blk}.attn.qkv.{kind}"] = torch.cat(
+            out[f"blocks.{blk}.attn.qkv.{kind}"] = mint.cat(
                 [stash['q'], stash['k'], stash['v']], dim=0
             )
 
@@ -1623,12 +1638,12 @@ 
 
 def checkpoint_filter_fn(
-        state_dict: Dict[str, torch.Tensor],
+        state_dict: Dict[str, ms.Tensor],
         model: VisionTransformer,
         adapt_layer_scale: bool = False,
         interpolation: str = 'bicubic',
         antialias: bool = True,
-) -> Dict[str, torch.Tensor]:
+) -> Dict[str, ms.Tensor]:
     """ convert patch embedding weight from manual patchify + linear proj to conv"""
     import re
     out_dict = {}
@@ -1655,7 +1670,7 @@         if 'visual.head.proj.weight' in state_dict and isinstance(model.head, nn.Linear):
             # remap final nn.Linear if it exists outside of the timm .trunk (ie in visual.head.proj)
             out_dict['head.weight'] = state_dict['visual.head.proj.weight']
-            out_dict['head.bias'] = torch.zeros(state_dict['visual.head.proj.weight'].shape[0])
+            out_dict['head.bias'] = mint.zeros(state_dict['visual.head.proj.weight'].shape[0])
     elif 'module.visual.trunk.pos_embed' in state_dict:
         prefix = 'module.visual.trunk.'
     elif 'preprocessor.patchifier.proj.weight' in state_dict:
@@ -2955,7 +2970,7 @@     if use_naflex:
         # Import here to avoid circular imports
         from .naflexvit import _create_naflexvit_from_classic
-        return _create_naflexvit_from_classic(variant, pretrained, **kwargs)
+        return _create_naflexvit_from_classic(variant, pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     out_indices = kwargs.pop('out_indices', 3)
     if 'flexi' in variant:
@@ -2978,7 +2993,7 @@         pretrained_strict=strict,
         feature_cfg=dict(out_indices=out_indices, feature_cls='getter'),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -2986,7 +3001,7 @@     """ ViT-Tiny (Vit-Ti/16)
     """
     model_args = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)
-    model = _create_vision_transformer('vit_tiny_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_tiny_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -2995,7 +3010,7 @@     """ ViT-Tiny (Vit-Ti/16) @ 384x384.
     """
     model_args = dict(patch_size=16, embed_dim=192, depth=12, num_heads=3)
-    model = _create_vision_transformer('vit_tiny_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_tiny_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3004,7 +3019,7 @@     """ ViT-Small (ViT-S/32)
     """
     model_args = dict(patch_size=32, embed_dim=384, depth=12, num_heads=6)
-    model = _create_vision_transformer('vit_small_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_small_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3013,7 +3028,7 @@     """ ViT-Small (ViT-S/32) at 384x384.
     """
     model_args = dict(patch_size=32, embed_dim=384, depth=12, num_heads=6)
-    model = _create_vision_transformer('vit_small_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_small_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3022,7 +3037,7 @@     """ ViT-Small (ViT-S/16)
     """
     model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)
-    model = _create_vision_transformer('vit_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_small_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3031,7 +3046,7 @@     """ ViT-Small (ViT-S/16)
     """
     model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6)
-    model = _create_vision_transformer('vit_small_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_small_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3040,7 +3055,7 @@     """ ViT-Small (ViT-S/8)
     """
     model_args = dict(patch_size=8, embed_dim=384, depth=12, num_heads=6)
-    model = _create_vision_transformer('vit_small_patch8_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_small_patch8_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3050,7 +3065,7 @@     ImageNet-1k weights fine-tuned from in21k, source https://github.com/google-research/vision_transformer.
     """
     model_args = dict(patch_size=32, embed_dim=768, depth=12, num_heads=12)
-    model = _create_vision_transformer('vit_base_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_base_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3060,7 +3075,7 @@     ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.
     """
     model_args = dict(patch_size=32, embed_dim=768, depth=12, num_heads=12)
-    model = _create_vision_transformer('vit_base_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_base_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3070,7 +3085,7 @@     ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.
     """
     model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)
-    model = _create_vision_transformer('vit_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3080,7 +3095,7 @@     ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.
     """
     model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12)
-    model = _create_vision_transformer('vit_base_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_base_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3090,7 +3105,7 @@     ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.
     """
     model_args = dict(patch_size=8, embed_dim=768, depth=12, num_heads=12)
-    model = _create_vision_transformer('vit_base_patch8_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_base_patch8_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3099,7 +3114,7 @@     """ ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929). No pretrained weights.
     """
     model_args = dict(patch_size=32, embed_dim=1024, depth=24, num_heads=16)
-    model = _create_vision_transformer('vit_large_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_large_patch32_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3109,7 +3124,7 @@     ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.
     """
     model_args = dict(patch_size=32, embed_dim=1024, depth=24, num_heads=16)
-    model = _create_vision_transformer('vit_large_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_large_patch32_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3119,7 +3134,7 @@     ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.
     """
     model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16)
-    model = _create_vision_transformer('vit_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3129,7 +3144,7 @@     ImageNet-1k weights fine-tuned from in21k @ 384x384, source https://github.com/google-research/vision_transformer.
     """
     model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16)
-    model = _create_vision_transformer('vit_large_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_large_patch16_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3138,7 +3153,7 @@     """ ViT-Large model (ViT-L/14)
     """
     model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16)
-    model = _create_vision_transformer('vit_large_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_large_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3147,7 +3162,7 @@     """ ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).
     """
     model_args = dict(patch_size=14, embed_dim=1280, depth=32, num_heads=16)
-    model = _create_vision_transformer('vit_huge_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_huge_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3156,7 +3171,7 @@     """ ViT-Giant (little-g) model (ViT-g/14) from `Scaling Vision Transformers` - https://arxiv.org/abs/2106.04560
     """
     model_args = dict(patch_size=14, embed_dim=1408, mlp_ratio=48/11, depth=40, num_heads=16)
-    model = _create_vision_transformer('vit_giant_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('vit_giant_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3166,7 +3181,7 @@     """
     model_args = dict(patch_size=14, embed_dim=1664, mlp_ratio=64/13, depth=48, num_heads=16)
     model = _create_vision_transformer(
-        'vit_gigantic_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_gigantic_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3177,7 +3192,7 @@     """
     model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False)
     model = _create_vision_transformer(
-        'vit_base_patch16_224_miil', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_224_miil', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3189,7 +3204,7 @@         patch_size=16, embed_dim=512, depth=12, num_heads=8, class_token=False,
         global_pool='avg', qkv_bias=False, init_values=1e-6, fc_norm=False)
     model = _create_vision_transformer(
-        'vit_medium_patch16_gap_240', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_medium_patch16_gap_240', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3201,7 +3216,7 @@         patch_size=16, embed_dim=512, depth=12, num_heads=8, class_token=False,
         global_pool='avg', qkv_bias=False, init_values=1e-6, fc_norm=False)
     model = _create_vision_transformer(
-        'vit_medium_patch16_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_medium_patch16_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3213,7 +3228,7 @@         patch_size=16, embed_dim=512, depth=12, num_heads=8, class_token=False,
         global_pool='avg', qkv_bias=False, init_values=1e-6, fc_norm=False)
     model = _create_vision_transformer(
-        'vit_medium_patch16_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_medium_patch16_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3225,7 +3240,7 @@         patch_size=16, embed_dim=640, depth=12, num_heads=10, class_token=False,
         global_pool='avg', qkv_bias=False, init_values=1e-6, fc_norm=False)
     model = _create_vision_transformer(
-        'vit_betwixt_patch16_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_betwixt_patch16_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3236,7 +3251,7 @@     model_args = dict(
         patch_size=16, embed_dim=768, depth=12, num_heads=16, class_token=False, global_pool='avg', fc_norm=False)
     model = _create_vision_transformer(
-        'vit_base_patch16_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3247,7 +3262,7 @@     model_args = dict(
         patch_size=14, embed_dim=1280, depth=32, num_heads=16, class_token=False, global_pool='avg', fc_norm=False)
     model = _create_vision_transformer(
-        'vit_huge_patch14_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_huge_patch14_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3258,7 +3273,7 @@     model_args = dict(
         patch_size=16, embed_dim=1280, depth=32, num_heads=16, class_token=False, global_pool='avg', fc_norm=False)
     model = _create_vision_transformer(
-        'vit_huge_patch16_gap_448', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_huge_patch16_gap_448', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3270,7 +3285,7 @@         patch_size=16, embed_dim=1408, depth=40, num_heads=16, mlp_ratio=48/11,
         class_token=False, global_pool='avg', fc_norm=False)
     model = _create_vision_transformer(
-        'vit_giant_patch16_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_giant_patch16_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3279,7 +3294,7 @@     # TinyCLIP 8M
     model_args = dict(embed_dim=256, depth=10, num_heads=4, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_xsmall_patch16_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_xsmall_patch16_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3289,7 +3304,7 @@     model_args = dict(
         patch_size=32, embed_dim=512, depth=12, num_heads=8, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_medium_patch32_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_medium_patch32_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3298,7 +3313,7 @@     # TinyCLIP 39M
     model_args = dict(embed_dim=512, depth=12, num_heads=8, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_medium_patch16_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_medium_patch16_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3308,7 +3323,7 @@     model_args = dict(
         patch_size=32, embed_dim=640, depth=12, num_heads=10, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_betwixt_patch32_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_betwixt_patch32_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3319,7 +3334,7 @@     model_args = dict(
         patch_size=32, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_base_patch32_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch32_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3330,7 +3345,7 @@     model_args = dict(
         patch_size=32, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_base_patch32_clip_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch32_clip_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3341,7 +3356,7 @@     model_args = dict(
         patch_size=32, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_base_patch32_clip_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch32_clip_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3352,7 +3367,7 @@     model_args = dict(
         patch_size=32, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_base_patch32_clip_448', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch32_clip_448', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3363,7 +3378,7 @@     model_args = dict(
         patch_size=16, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_base_patch16_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3374,7 +3389,7 @@     model_args = dict(
         patch_size=16, embed_dim=768, depth=12, num_heads=12, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_base_patch16_clip_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_clip_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3385,7 +3400,7 @@     model_args = dict(
         patch_size=16, embed_dim=896, depth=12, num_heads=14, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_base_patch16_plus_clip_240', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_plus_clip_240', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3396,7 +3411,7 @@     model_args = dict(
         patch_size=14, embed_dim=1024, depth=24, num_heads=16, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_large_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3407,7 +3422,7 @@     model_args = dict(
         patch_size=14, embed_dim=1024, depth=24, num_heads=16, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_large_patch14_clip_336', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch14_clip_336', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3418,7 +3433,7 @@     model_args = dict(
         patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_huge_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_huge_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3429,7 +3444,7 @@     model_args = dict(
         patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_huge_patch14_clip_336', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_huge_patch14_clip_336', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3440,7 +3455,7 @@     model_args = dict(
         patch_size=14, embed_dim=1280, depth=32, num_heads=16, pre_norm=True, norm_layer=partial(LayerNorm, eps=1e-5))
     model = _create_vision_transformer(
-        'vit_huge_patch14_clip_378', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_huge_patch14_clip_378', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3454,7 +3469,7 @@         norm_layer=partial(LayerNorm, eps=1e-5),
     )
     model = _create_vision_transformer(
-        'vit_giant_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_giant_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3468,7 +3483,7 @@         norm_layer=partial(LayerNorm, eps=1e-5),
     )
     model = _create_vision_transformer(
-        'vit_gigantic_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_gigantic_patch14_clip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3482,7 +3497,7 @@         norm_layer=partial(LayerNorm, eps=1e-5),
     )
     model = _create_vision_transformer(
-        'vit_gigantic_patch14_clip_378', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_gigantic_patch14_clip_378', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3495,7 +3510,7 @@         norm_layer=partial(LayerNorm, eps=1e-5), act_layer='quick_gelu'
     )
     model = _create_vision_transformer(
-        'vit_base_patch32_clip_quickgelu_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch32_clip_quickgelu_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3508,7 +3523,7 @@         norm_layer=partial(LayerNorm, eps=1e-5), act_layer='quick_gelu'
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_clip_quickgelu_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_clip_quickgelu_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3521,7 +3536,7 @@         norm_layer=partial(LayerNorm, eps=1e-5), act_layer='quick_gelu'
     )
     model = _create_vision_transformer(
-        'vit_large_patch14_clip_quickgelu_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch14_clip_quickgelu_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3534,7 +3549,7 @@         norm_layer=partial(LayerNorm, eps=1e-5), act_layer='quick_gelu'
     )
     model = _create_vision_transformer(
-        'vit_large_patch14_clip_quickgelu_336', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch14_clip_quickgelu_336', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3547,7 +3562,7 @@         norm_layer=partial(LayerNorm, eps=1e-5), act_layer='quick_gelu'
     )
     model = _create_vision_transformer(
-        'vit_huge_patch14_clip_quickgelu_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_huge_patch14_clip_quickgelu_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3560,7 +3575,7 @@         norm_layer=partial(LayerNorm, eps=1e-5), act_layer='quick_gelu'
     )
     model = _create_vision_transformer(
-        'vit_huge_patch14_clip_quickgelu_378', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_huge_patch14_clip_quickgelu_378', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3573,7 +3588,7 @@         norm_layer=partial(LayerNorm, eps=1e-5), act_layer='quick_gelu'
     )
     model = _create_vision_transformer(
-        'vit_gigantic_patch14_clip_quickgelu_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_gigantic_patch14_clip_quickgelu_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3585,7 +3600,7 @@     """
     model_args = dict(patch_size=32, embed_dim=896, depth=12, num_heads=14, init_values=1e-5)
     model = _create_vision_transformer(
-        'vit_base_patch32_plus_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch32_plus_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3595,7 +3610,7 @@     """
     model_args = dict(patch_size=16, embed_dim=896, depth=12, num_heads=14, init_values=1e-5)
     model = _create_vision_transformer(
-        'vit_base_patch16_plus_240', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_plus_240', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3607,7 +3622,7 @@         patch_size=16, embed_dim=768, depth=12, num_heads=12, qkv_bias=False, init_values=1e-5,
         class_token=False, block_fn=ResPostBlock, global_pool='avg')
     model = _create_vision_transformer(
-        'vit_base_patch16_rpn_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_rpn_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3619,7 +3634,7 @@     """
     model_args = dict(patch_size=16, embed_dim=384, depth=36, num_heads=6, init_values=1e-5)
     model = _create_vision_transformer(
-        'vit_small_patch16_36x1_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_small_patch16_36x1_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3632,7 +3647,7 @@     model_args = dict(
         patch_size=16, embed_dim=384, depth=18, num_heads=6, init_values=1e-5, block_fn=ParallelThingsBlock)
     model = _create_vision_transformer(
-        'vit_small_patch16_18x2_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_small_patch16_18x2_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3644,7 +3659,7 @@     model_args = dict(
         patch_size=16, embed_dim=768, depth=18, num_heads=12, init_values=1e-5, block_fn=ParallelThingsBlock)
     model = _create_vision_transformer(
-        'vit_base_patch16_18x2_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_18x2_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3653,7 +3668,7 @@     """ EVA-large model https://arxiv.org/abs/2211.07636 /via MAE MIM pretrain"""
     model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, global_pool='avg')
     model = _create_vision_transformer(
-        'eva_large_patch14_196', pretrained=pretrained, **dict(model_args, **kwargs))
+        'eva_large_patch14_196', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3661,7 +3676,7 @@ def eva_large_patch14_336(pretrained: bool = False, **kwargs) -> VisionTransformer:
     """ EVA-large model https://arxiv.org/abs/2211.07636 via MAE MIM pretrain"""
     model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, global_pool='avg')
-    model = _create_vision_transformer('eva_large_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('eva_large_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3670,7 +3685,7 @@     """ FlexiViT-Small
     """
     model_args = dict(patch_size=16, embed_dim=384, depth=12, num_heads=6, no_embed_class=True)
-    model = _create_vision_transformer('flexivit_small', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('flexivit_small', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3679,7 +3694,7 @@     """ FlexiViT-Base
     """
     model_args = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, no_embed_class=True)
-    model = _create_vision_transformer('flexivit_base', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('flexivit_base', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3688,7 +3703,7 @@     """ FlexiViT-Large
     """
     model_args = dict(patch_size=16, embed_dim=1024, depth=24, num_heads=16, no_embed_class=True)
-    model = _create_vision_transformer('flexivit_large', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('flexivit_large', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3701,7 +3716,7 @@         norm_layer=RmsNorm, block_fn=ParallelScalingBlock, qkv_bias=False, qk_norm=True,
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3714,7 +3729,7 @@         norm_layer=RmsNorm, block_fn=ParallelScalingBlock, qkv_bias=False, qk_norm=True,
     )
     model = _create_vision_transformer(
-        'vit_large_patch14_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch14_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3727,7 +3742,7 @@         norm_layer=RmsNorm, block_fn=ParallelScalingBlock, qkv_bias=False, qk_norm=True,
     )
     model = _create_vision_transformer(
-        'vit_huge_patch14_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_huge_patch14_xp_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3737,7 +3752,7 @@     """
     model_args = dict(patch_size=14, embed_dim=384, depth=12, num_heads=6, init_values=1e-5)
     model = _create_vision_transformer(
-        'vit_small_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_small_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3747,7 +3762,7 @@     """
     model_args = dict(patch_size=14, embed_dim=768, depth=12, num_heads=12, init_values=1e-5)
     model = _create_vision_transformer(
-        'vit_base_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3757,7 +3772,7 @@     """
     model_args = dict(patch_size=14, embed_dim=1024, depth=24, num_heads=16, init_values=1e-5)
     model = _create_vision_transformer(
-        'vit_large_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3774,7 +3789,7 @@         mlp_ratio=2.66667 * 2, mlp_layer=SwiGLUPacked, act_layer=nn.SiLU
     )
     model = _create_vision_transformer(
-        'vit_giant_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_giant_patch14_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3787,7 +3802,7 @@         reg_tokens=4, no_embed_class=True,
     )
     model = _create_vision_transformer(
-        'vit_small_patch14_reg4_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_small_patch14_reg4_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3800,7 +3815,7 @@         reg_tokens=4, no_embed_class=True,
     )
     model = _create_vision_transformer(
-        'vit_base_patch14_reg4_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch14_reg4_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3813,7 +3828,7 @@         reg_tokens=4, no_embed_class=True,
     )
     model = _create_vision_transformer(
-        'vit_large_patch14_reg4_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch14_reg4_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3830,7 +3845,7 @@         mlp_layer=SwiGLUPacked, act_layer=nn.SiLU, reg_tokens=4, no_embed_class=True,
     )
     model = _create_vision_transformer(
-        'vit_giant_patch14_reg4_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_giant_patch14_reg4_dinov2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3841,7 +3856,7 @@         act_layer='gelu_tanh',
     )
     model = _create_vision_transformer(
-        'vit_base_patch32_siglip_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch32_siglip_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3851,7 +3866,7 @@         patch_size=16, embed_dim=768, depth=12, num_heads=12, class_token=False, global_pool='map',
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_siglip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_siglip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3861,7 +3876,7 @@         patch_size=16, embed_dim=768, depth=12, num_heads=12, class_token=False, global_pool='map',
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_siglip_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_siglip_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3871,7 +3886,7 @@         patch_size=16, embed_dim=768, depth=12, num_heads=12, class_token=False, global_pool='map',
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_siglip_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_siglip_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3881,7 +3896,7 @@         patch_size=16, embed_dim=768, depth=12, num_heads=12, class_token=False, global_pool='map',
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_siglip_512', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_siglip_512', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3891,7 +3906,7 @@         patch_size=16, embed_dim=1024, depth=24, num_heads=16, class_token=False, global_pool='map',
     )
     model = _create_vision_transformer(
-        'vit_large_patch16_siglip_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch16_siglip_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3901,7 +3916,7 @@         patch_size=16, embed_dim=1024, depth=24, num_heads=16, class_token=False, global_pool='map',
     )
     model = _create_vision_transformer(
-        'vit_large_patch16_siglip_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch16_siglip_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3912,7 +3927,7 @@         act_layer='gelu_tanh'
     )
     model = _create_vision_transformer(
-        'vit_large_patch16_siglip_512', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch16_siglip_512', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3922,7 +3937,7 @@         patch_size=14, embed_dim=1152, depth=27, num_heads=16, mlp_ratio=3.7362, class_token=False, global_pool='map',
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch14_siglip_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch14_siglip_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3933,7 +3948,7 @@         patch_size=14, embed_dim=1152, depth=27, num_heads=16, mlp_ratio=3.7362, class_token=False, global_pool='map',
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch14_siglip_378', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch14_siglip_378', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3943,7 +3958,7 @@         patch_size=14, embed_dim=1152, depth=27, num_heads=16, mlp_ratio=3.7362, class_token=False, global_pool='map',
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch14_siglip_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch14_siglip_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3954,7 +3969,7 @@         act_layer='gelu_tanh',
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch16_siglip_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch16_siglip_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3965,7 +3980,7 @@         act_layer='gelu_tanh',
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch16_siglip_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch16_siglip_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3976,7 +3991,7 @@         act_layer='gelu_tanh',
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch16_siglip_512', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch16_siglip_512', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3987,7 +4002,7 @@         act_layer='gelu_tanh',
     )
     model = _create_vision_transformer(
-        'vit_giantopt_patch16_siglip_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_giantopt_patch16_siglip_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -3998,7 +4013,7 @@         act_layer='gelu_tanh',
     )
     model = _create_vision_transformer(
-        'vit_giantopt_patch16_siglip_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_giantopt_patch16_siglip_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4009,7 +4024,7 @@         act_layer='gelu_tanh',
     )
     model = _create_vision_transformer(
-        'vit_base_patch32_siglip_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch32_siglip_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4020,7 +4035,7 @@         patch_size=16, embed_dim=768, depth=12, num_heads=12, class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_siglip_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_siglip_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4031,7 +4046,7 @@         patch_size=16, embed_dim=768, depth=12, num_heads=12, class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_siglip_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_siglip_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4042,7 +4057,7 @@         patch_size=16, embed_dim=768, depth=12, num_heads=12, class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_siglip_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_siglip_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4053,7 +4068,7 @@         patch_size=16, embed_dim=768, depth=12, num_heads=12, class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_siglip_gap_512', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_siglip_gap_512', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4064,7 +4079,7 @@         patch_size=16, embed_dim=1024, depth=24, num_heads=16, class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_large_patch16_siglip_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch16_siglip_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4075,7 +4090,7 @@         patch_size=16, embed_dim=1024, depth=24, num_heads=16, class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_large_patch16_siglip_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch16_siglip_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4086,7 +4101,7 @@         global_pool='avg', fc_norm=False, act_layer='gelu_tanh'
     )
     model = _create_vision_transformer(
-        'vit_large_patch16_siglip_gap_512', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_large_patch16_siglip_gap_512', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4098,7 +4113,7 @@         class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch14_siglip_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch14_siglip_gap_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4110,7 +4125,7 @@         class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch14_siglip_gap_378', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch14_siglip_gap_378', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4122,7 +4137,7 @@         class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch14_siglip_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch14_siglip_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4134,7 +4149,7 @@         class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch14_siglip_gap_448', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch14_siglip_gap_448', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4146,7 +4161,7 @@         class_token=False, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch14_siglip_gap_896', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch14_siglip_gap_896', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4158,7 +4173,7 @@         class_token=False, global_pool='avg', fc_norm=False, act_layer='gelu_tanh',
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch16_siglip_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch16_siglip_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4169,7 +4184,7 @@         global_pool='avg', fc_norm=False, act_layer='gelu_tanh'
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch16_siglip_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch16_siglip_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4180,7 +4195,7 @@         global_pool='avg', fc_norm=False, act_layer='gelu_tanh'
     )
     model = _create_vision_transformer(
-        'vit_so400m_patch16_siglip_gap_512', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so400m_patch16_siglip_gap_512', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4191,7 +4206,7 @@         global_pool='avg', fc_norm=False, act_layer='gelu_tanh'
     )
     model = _create_vision_transformer(
-        'vit_giantopt_patch16_siglip_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_giantopt_patch16_siglip_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4202,7 +4217,7 @@         global_pool='avg', fc_norm=False, act_layer='gelu_tanh'
     )
     model = _create_vision_transformer(
-        'vit_giantopt_patch16_siglip_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_giantopt_patch16_siglip_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4213,7 +4228,7 @@         class_token=False, no_embed_class=True, reg_tokens=1, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_wee_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_wee_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4224,7 +4239,7 @@         class_token=False, no_embed_class=True, reg_tokens=1, global_pool='avg', attn_layer='diff',
     )
     model = _create_vision_transformer(
-        'vit_dwee_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_dwee_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4235,7 +4250,7 @@         class_token=False, no_embed_class=True, reg_tokens=1, global_pool='avg', block_fn=ParallelScalingBlock,
     )
     model = _create_vision_transformer(
-        'vit_pwee_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_pwee_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4246,7 +4261,7 @@         class_token=False, no_embed_class=True, reg_tokens=1, global_pool='avg', block_fn=DiffParallelScalingBlock,
     )
     model = _create_vision_transformer(
-        'vit_dpwee_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_dpwee_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4257,7 +4272,7 @@         class_token=False, no_embed_class=True, reg_tokens=1, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_little_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_little_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4268,7 +4283,7 @@         class_token=False, no_embed_class=True, reg_tokens=4, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_little_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_little_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4279,7 +4294,7 @@         class_token=False, no_embed_class=True, reg_tokens=1, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_medium_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_medium_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4290,7 +4305,7 @@         class_token=False, no_embed_class=True, reg_tokens=4, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_medium_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_medium_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4301,7 +4316,7 @@         class_token=False, no_embed_class=True, reg_tokens=4, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_mediumd_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_mediumd_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4312,7 +4327,7 @@         class_token=False, no_embed_class=True, reg_tokens=4, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_mediumd_patch16_reg4_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_mediumd_patch16_reg4_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4323,7 +4338,7 @@         class_token=False, no_embed_class=True, reg_tokens=1, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_betwixt_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_betwixt_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4334,7 +4349,7 @@         class_token=False, no_embed_class=True, reg_tokens=4, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_betwixt_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_betwixt_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4345,7 +4360,7 @@         class_token=False, no_embed_class=True, reg_tokens=4, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_betwixt_patch16_reg4_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_betwixt_patch16_reg4_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4356,7 +4371,7 @@         no_embed_class=True, global_pool='avg', reg_tokens=4,
     )
     model = _create_vision_transformer(
-        'vit_base_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_base_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4368,7 +4383,7 @@         class_token=False, reg_tokens=4, global_pool='map',
     )
     model = _create_vision_transformer(
-        'vit_so150m_patch16_reg4_map_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so150m_patch16_reg4_map_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4380,7 +4395,7 @@         class_token=False, reg_tokens=4, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_so150m_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so150m_patch16_reg4_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4392,7 +4407,7 @@         class_token=False, reg_tokens=4, global_pool='avg', fc_norm=False,
     )
     model = _create_vision_transformer(
-        'vit_so150m_patch16_reg4_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so150m_patch16_reg4_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4404,7 +4419,7 @@         qkv_bias=False, class_token=False, reg_tokens=1, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_so150m2_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so150m2_patch16_reg1_gap_256', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4416,7 +4431,7 @@         qkv_bias=False, class_token=False, reg_tokens=1, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_so150m2_patch16_reg1_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so150m2_patch16_reg1_gap_384', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4428,7 +4443,7 @@         qkv_bias=False, class_token=False, reg_tokens=1, global_pool='avg',
     )
     model = _create_vision_transformer(
-        'vit_so150m2_patch16_reg1_gap_448', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_so150m2_patch16_reg1_gap_448', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4439,7 +4454,7 @@         init_values=0.1, final_norm=False, dynamic_img_size=True,
     )
     model = _create_vision_transformer(
-        'vit_intern300m_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))
+        'vit_intern300m_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4453,7 +4468,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_large_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_large_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4468,7 +4483,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_huge_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_huge_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4482,7 +4497,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_1b_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_1b_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4496,7 +4511,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_3b_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_3b_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4510,7 +4525,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_large_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_large_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4524,7 +4539,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_huge_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_huge_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4538,7 +4553,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_1b_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_1b_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4552,7 +4567,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_3b_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_3b_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4566,7 +4581,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_large_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_large_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4580,7 +4595,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_huge_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_huge_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4594,7 +4609,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_1b_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_1b_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4608,7 +4623,7 @@         norm_layer=partial(RmsNorm, eps=1e-5), embed_norm_layer=partial(RmsNorm, eps=1e-5), mlp_layer=SwiGLU,
     )
     model = _create_vision_transformer(
-        'aimv2_3b_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))
+        'aimv2_3b_patch14_448', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4617,7 +4632,7 @@     """ ViT Test
     """
     model_args = dict(patch_size=16, embed_dim=64, depth=6, num_heads=2, mlp_ratio=3, dynamic_img_size=True)
-    model = _create_vision_transformer('test_vit', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('test_vit', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4628,7 +4643,7 @@     model_args = dict(
         patch_size=16, embed_dim=64, depth=8, num_heads=2, mlp_ratio=3,
         class_token=False, reg_tokens=1, global_pool='avg', init_values=1e-5, dynamic_img_size=True)
-    model = _create_vision_transformer('test_vit2', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('test_vit2', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4639,7 +4654,7 @@     model_args = dict(
         patch_size=16, embed_dim=96, depth=9, num_heads=3, mlp_ratio=2,
         class_token=False, reg_tokens=1, global_pool='map', pool_include_prefix=True, init_values=1e-5)
-    model = _create_vision_transformer('test_vit3', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('test_vit3', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4652,7 +4667,7 @@         class_token=False, reg_tokens=1, global_pool='avg', init_values=1e-5, dynamic_img_size=True,
         norm_layer='rmsnorm',
     )
-    model = _create_vision_transformer('test_vit4', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('test_vit4', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4666,7 +4681,7 @@         scale_attn_norm=True, scale_mlp_norm=True, class_token=True, global_pool='avg',
         norm_layer=partial(LayerNorm, eps=1e-5)
     )
-    model = _create_vision_transformer('beit3_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('beit3_base_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4680,7 +4695,7 @@         scale_attn_norm=True, scale_mlp_norm=True, class_token=True, global_pool='avg',
         norm_layer=partial(LayerNorm, eps=1e-5),
     )
-    model = _create_vision_transformer('beit3_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('beit3_large_patch16_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4694,7 +4709,7 @@         scale_attn_norm=True, scale_mlp_norm=True, class_token=True, global_pool='avg',
         norm_layer=partial(LayerNorm, eps=1e-5),
     )
-    model = _create_vision_transformer('beit3_giant_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('beit3_giant_patch14_224', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
@@ -4708,7 +4723,7 @@         scale_attn_norm=True, scale_mlp_norm=True, class_token=True, global_pool='avg',
         norm_layer=partial(LayerNorm, eps=1e-5),
     )
-    model = _create_vision_transformer('beit3_giant_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))
+    model = _create_vision_transformer('beit3_giant_patch14_336', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     return model
 
 
