--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Vision OutLOoker (VOLO) implementation
 
 Paper: `VOLO: Vision Outlooker for Visual Recognition` - https://arxiv.org/abs/2106.13112
@@ -22,9 +27,9 @@ import math
 from typing import Any, Callable, Dict, List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import DropPath, Mlp, to_2tuple, to_ntuple, trunc_normal_, use_fused_attn
@@ -36,7 +41,7 @@ __all__ = ['VOLO']  # model_registry will add each entrypoint fn to this
 
 
-class OutlookAttention(nn.Module):
+class OutlookAttention(msnn.Cell):
     """Outlook attention mechanism for VOLO models."""
 
     def __init__(
@@ -80,10 +85,10 @@         self.proj = nn.Linear(dim, dim, **dd)
         self.proj_drop = nn.Dropout(proj_drop)
 
-        self.unfold = nn.Unfold(kernel_size=kernel_size, padding=padding, stride=stride)
-        self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True)
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.unfold = nn.Unfold(kernel_size = kernel_size, padding = padding, stride = stride)
+        self.pool = nn.AvgPool2d(kernel_size = stride, stride = stride, ceil_mode = True)
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -110,7 +115,7 @@         attn = self.attn_drop(attn)
 
         x = (attn @ v).permute(0, 1, 4, 3, 2).reshape(B, C * self.kernel_size * self.kernel_size, h * w)
-        x = F.fold(x, output_size=(H, W), kernel_size=self.kernel_size, padding=self.padding, stride=self.stride)
+        x = nn.functional.fold(x, output_size = (H, W), kernel_size = self.kernel_size, padding = self.padding, stride = self.stride)
 
         x = self.proj(x.permute(0, 2, 3, 1))
         x = self.proj_drop(x)
@@ -118,7 +123,7 @@         return x
 
 
-class Outlooker(nn.Module):
+class Outlooker(msnn.Cell):
     """Outlooker block that combines outlook attention with MLP."""
 
     def __init__(
@@ -165,7 +170,7 @@             attn_drop=attn_drop,
             **dd,
         )
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.norm2 = norm_layer(dim, **dd)
         self.mlp = Mlp(
@@ -174,9 +179,9 @@             act_layer=act_layer,
             **dd,
         )
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -190,7 +195,7 @@         return x
 
 
-class Attention(nn.Module):
+class Attention(msnn.Cell):
     """Multi-head self-attention module."""
     fused_attn: torch.jit.Final[bool]
 
@@ -225,7 +230,7 @@         self.proj = nn.Linear(dim, dim, **dd)
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -243,7 +248,7 @@             x = F.scaled_dot_product_attention(
                 q, k, v,
                 dropout_p=self.attn_drop.p if self.training else 0.,
-            )
+            )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             q = q * self.scale
             attn = q @ k.transpose(-2, -1)
@@ -258,7 +263,7 @@         return x
 
 
-class Transformer(nn.Module):
+class Transformer(msnn.Cell):
     """Transformer block with multi-head self-attention and MLP."""
 
     def __init__(
@@ -290,13 +295,13 @@         super().__init__()
         self.norm1 = norm_layer(dim, **dd)
         self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, **dd)
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.norm2 = norm_layer(dim, **dd)
         self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, **dd)
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -310,7 +315,7 @@         return x
 
 
-class ClassAttention(nn.Module):
+class ClassAttention(msnn.Cell):
     """Class attention mechanism for class token interaction."""
 
     def __init__(
@@ -350,7 +355,7 @@         self.proj = nn.Linear(self.head_dim * self.num_heads, dim, **dd)
         self.proj_drop = nn.Dropout(proj_drop)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -375,7 +380,7 @@         return cls_embed
 
 
-class ClassBlock(nn.Module):
+class ClassBlock(msnn.Cell):
     """Class block that combines class attention with MLP."""
 
     def __init__(
@@ -419,7 +424,7 @@             proj_drop=drop,
             **dd,
         )
-        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
 
         self.norm2 = norm_layer(dim, **dd)
         self.mlp = Mlp(
@@ -429,9 +434,9 @@             drop=drop,
             **dd,
         )
-        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else msnn.Identity()
+
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -443,9 +448,10 @@         cls_embed = x[:, :1]
         cls_embed = cls_embed + self.drop_path1(self.attn(self.norm1(x)))
         cls_embed = cls_embed + self.drop_path2(self.mlp(self.norm2(cls_embed)))
-        return torch.cat([cls_embed, x[:, 1:]], dim=1)
-
-
+        return mint.cat([cls_embed, x[:, 1:]], dim = 1)
+
+
+# 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def get_block(block_type: str, **kwargs: Any) -> nn.Module:
     """Get block based on type.
 
@@ -477,25 +483,25 @@     """
     W = size[1] // scale
     H = size[2] // scale
-    W_t = torch.tensor(W, dtype=torch.float32)
-    H_t = torch.tensor(H, dtype=torch.float32)
-    cut_rat = torch.sqrt(1. - lam)
+    W_t = ms.Tensor(W, dtype = ms.float32)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;
+    H_t = ms.Tensor(H, dtype = ms.float32)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;
+    cut_rat = mint.sqrt(1. - lam)
     cut_w = (W_t * cut_rat).int()
     cut_h = (H_t * cut_rat).int()
 
     # uniform
-    cx = torch.randint(0, W, (1,))
-    cy = torch.randint(0, H, (1,))
-
-    bbx1 = torch.clamp(cx - cut_w // 2, 0, W)
-    bby1 = torch.clamp(cy - cut_h // 2, 0, H)
-    bbx2 = torch.clamp(cx + cut_w // 2, 0, W)
-    bby2 = torch.clamp(cy + cut_h // 2, 0, H)
+    cx = mint.randint(0, W, (1,))
+    cy = mint.randint(0, H, (1,))
+
+    bbx1 = mint.clamp(cx - cut_w // 2, 0, W)
+    bby1 = mint.clamp(cy - cut_h // 2, 0, H)
+    bbx2 = mint.clamp(cx + cut_w // 2, 0, W)
+    bby2 = mint.clamp(cy + cut_h // 2, 0, H)
 
     return bbx1.item(), bby1.item(), bbx2.item(), bby2.item()
 
 
-class PatchEmbed(nn.Module):
+class PatchEmbed(msnn.Cell):
     """Image to patch embedding with multi-layer convolution."""
 
     def __init__(
@@ -527,17 +533,18 @@         super().__init__()
         assert patch_size in [4, 8, 16]
         if stem_conv:
-            self.conv = nn.Sequential(
+            self.conv = msnn.SequentialCell(
+                [
                 nn.Conv2d(in_chans, hidden_dim, kernel_size=7, stride=stem_stride, padding=3, bias=False, **dd),
                 nn.BatchNorm2d(hidden_dim, **dd),
-                nn.ReLU(inplace=True),
+                nn.ReLU(),
                 nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1, bias=False, **dd),
                 nn.BatchNorm2d(hidden_dim, **dd),
-                nn.ReLU(inplace=True),
+                nn.ReLU(),
                 nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1, bias=False, **dd),
                 nn.BatchNorm2d(hidden_dim, **dd),
-                nn.ReLU(inplace=True),
-            )
+                nn.ReLU()
+            ])  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         else:
             self.conv = None
 
@@ -550,7 +557,7 @@         )
         self.num_patches = (img_size // patch_size) * (img_size // patch_size)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -565,7 +572,7 @@         return x
 
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
     """Downsampling module between stages."""
 
     def __init__(
@@ -587,7 +594,7 @@         dd = {'device': device, 'dtype': dtype}
         self.proj = nn.Conv2d(in_embed_dim, out_embed_dim, kernel_size=patch_size, stride=patch_size, **dd)
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass.
 
         Args:
@@ -618,7 +625,7 @@         device=None,
         dtype=None,
         **kwargs: Any,
-) -> nn.Sequential:
+) -> msnn.SequentialCell:
     """Generate outlooker layers for stage 1.
 
     Args:
@@ -656,7 +663,9 @@             dtype=dtype,
             **kwargs,
         ))
-    blocks = nn.Sequential(*blocks)
+    blocks = msnn.SequentialCell([
+        blocks
+    ])
     return blocks
 
 
@@ -671,7 +680,7 @@         attn_drop: float = 0,
         drop_path_rate: float = 0.,
         **kwargs: Any,
-) -> nn.Sequential:
+) -> msnn.SequentialCell:
     """Generate transformer layers for stage 2.
 
     Args:
@@ -701,11 +710,13 @@             drop_path=block_dpr,
             **kwargs,
         ))
-    blocks = nn.Sequential(*blocks)
+    blocks = msnn.SequentialCell([
+        blocks
+    ])
     return blocks
 
 
-class VOLO(nn.Module):
+class VOLO(msnn.Cell):
     """Vision Outlooker (VOLO) model."""
 
     def __init__(
@@ -790,8 +801,8 @@ 
         # initial positional encoding, we add positional encoding after outlooker blocks
         patch_grid = (img_size[0] // patch_size // pooling_scale, img_size[1] // patch_size // pooling_scale)
-        self.pos_embed = nn.Parameter(torch.zeros(1, patch_grid[0], patch_grid[1], embed_dims[-1], **dd))
-        self.pos_drop = nn.Dropout(p=pos_drop_rate)
+        self.pos_embed = ms.Parameter(mint.zeros(1, patch_grid[0], patch_grid[1], embed_dims[-1], **dd))
+        self.pos_drop = nn.Dropout(p = pos_drop_rate)
 
         # set the main block in network
         self.stage_ends = []
@@ -838,12 +849,12 @@                 r *= 2
                 block_idx += 1
 
-        self.network = nn.ModuleList(network)
+        self.network = msnn.CellList(network)
 
         # set post block, for example, class attention layers
         self.post_network = None
         if post_layers is not None:
-            self.post_network = nn.ModuleList([
+            self.post_network = msnn.CellList([
                 get_block(
                     post_layers[i],
                     dim=embed_dims[-1],
@@ -857,23 +868,24 @@                 )
                 for i in range(len(post_layers))
             ])
-            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dims[-1], **dd))
+            self.cls_token = ms.Parameter(mint.zeros(1, 1, embed_dims[-1], **dd))
             trunc_normal_(self.cls_token, std=.02)
 
         # set output type
         if use_aux_head:
-            self.aux_head = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else nn.Identity()
+            self.aux_head = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else msnn.Identity()
         else:
             self.aux_head = None
         self.norm = norm_layer(self.num_features, **dd)
 
         # Classifier head
         self.head_drop = nn.Dropout(drop_rate)
-        self.head = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else nn.Identity()
+        self.head = nn.Linear(self.num_features, num_classes, **dd) if num_classes > 0 else msnn.Identity()
 
         trunc_normal_(self.pos_embed, std=.02)
         self.apply(self._init_weights)
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def _init_weights(self, m: nn.Module) -> None:
         """Initialize weights for modules.
 
@@ -883,7 +895,7 @@         if isinstance(m, nn.Linear):
             trunc_normal_(m.weight, std=.02)
             if isinstance(m, nn.Linear) and m.bias is not None:
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def no_weight_decay(self) -> set:
@@ -926,6 +938,7 @@         """
         self.grad_checkpointing = enable
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         """Get classifier module.
@@ -948,12 +961,12 @@         device = self.head.weight.device if hasattr(self.head, 'weight') else None
         dtype = self.head.weight.dtype if hasattr(self.head, 'weight') else None
         self.head = nn.Linear(
-            self.num_features, num_classes, device=device, dtype=dtype) if num_classes > 0 else nn.Identity()
+            self.num_features, num_classes, dtype = dtype) if num_classes > 0 else msnn.Identity()  # 'torch.nn.Linear':没有对应的mindspore参数 'device' (position 3);
         if self.aux_head is not None:
             self.aux_head = nn.Linear(
-                self.num_features, num_classes, device=device, dtype=dtype) if num_classes > 0 else nn.Identity()
-
-    def forward_tokens(self, x: torch.Tensor) -> torch.Tensor:
+                self.num_features, num_classes, dtype = dtype) if num_classes > 0 else msnn.Identity()  # 'torch.nn.Linear':没有对应的mindspore参数 'device' (position 3);
+
+    def forward_tokens(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through token processing stages.
 
         Args:
@@ -976,7 +989,7 @@         x = x.reshape(B, -1, C)
         return x
 
-    def forward_cls(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_cls(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through class attention blocks.
 
         Args:
@@ -987,7 +1000,7 @@         """
         B, N, C = x.shape
         cls_tokens = self.cls_token.expand(B, -1, -1)
-        x = torch.cat([cls_tokens, x], dim=1)
+        x = mint.cat([cls_tokens, x], dim = 1)
         for block in self.post_network:
             if self.grad_checkpointing and not torch.jit.is_scripting():
                 x = checkpoint(block, x)
@@ -995,7 +1008,7 @@                 x = block(x)
         return x
 
-    def forward_train(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, Tuple[int, int, int, int]]]:
+    def forward_train(self, x: ms.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, Tuple[int, int, int, int]]]:
         """Forward pass for training with mix token support.
 
         Args:
@@ -1013,7 +1026,7 @@ 
         # mix token, see token labeling for details.
         if self.mix_token and self.training:
-            lam = torch.distributions.Beta(self.beta, self.beta).sample()
+            lam = torch.distributions.Beta(self.beta, self.beta).sample()  # 'torch.distributions.Beta' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.distributions.Beta.sample' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             patch_h, patch_w = x.shape[1] // self.pooling_scale, x.shape[2] // self.pooling_scale
             bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam, scale=self.pooling_scale)
             temp_x = x.clone()
@@ -1058,7 +1071,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
@@ -1143,13 +1156,13 @@         max_index = self.stage_ends[max_index]
         self.network = self.network[:max_index + 1]  # truncate blocks
         if prune_norm:
-            self.norm = nn.Identity()
+            self.norm = msnn.Identity()
         if prune_head:
-            self.post_network = nn.ModuleList()  # prune token blocks with head
+            self.post_network = msnn.CellList()  # prune token blocks with head
             self.reset_classifier(0, '')
         return take_indices
 
-    def forward_features(self, x: torch.Tensor) -> torch.Tensor:
+    def forward_features(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass through feature extraction.
 
         Args:
@@ -1169,7 +1182,7 @@         x = self.norm(x)
         return x
 
-    def forward_head(self, x: torch.Tensor, pre_logits: bool = False) -> torch.Tensor:
+    def forward_head(self, x: ms.Tensor, pre_logits: bool = False) -> ms.Tensor:
         """Forward pass through classification head.
 
         Args:
@@ -1195,7 +1208,7 @@             out = out + 0.5 * aux.max(1)[0]
         return out
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def construct(self, x: ms.Tensor) -> ms.Tensor:
         """Forward pass (simplified, without mix token training).
 
         Args:
