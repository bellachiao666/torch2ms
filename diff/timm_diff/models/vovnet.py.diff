--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ VoVNet (V1 & V2)
 
 Papers:
@@ -13,8 +18,8 @@ 
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ConvNormAct, SeparableConvNormAct, BatchNormAct2d, ClassifierHead, DropPath, \
@@ -27,21 +32,21 @@ __all__ = ['VovNet']  # model_registry will add each entrypoint fn to this
 
 
-class SequentialAppendList(nn.Sequential):
+class SequentialAppendList(msnn.SequentialCell):
     def __init__(self, *args, **kwargs):
         super().__init__(*args)
 
-    def forward(self, x: torch.Tensor, concat_list: List[torch.Tensor]) -> torch.Tensor:
+    def forward(self, x: ms.Tensor, concat_list: List[torch.Tensor]) -> ms.Tensor:
         for i, module in enumerate(self):
             if i == 0:
                 concat_list.append(module(x))
             else:
                 concat_list.append(module(concat_list[-1]))
-        x = torch.cat(concat_list, dim=1)
+        x = mint.cat(concat_list, dim = 1)
         return x
 
 
-class OsaBlock(nn.Module):
+class OsaBlock(msnn.Cell):
 
     def __init__(
             self,
@@ -90,7 +95,7 @@ 
         self.drop_path = drop_path
 
-    def forward(self, x):
+    def construct(self, x):
         output = [x]
         if self.conv_reduction is not None:
             x = self.conv_reduction(x)
@@ -105,7 +110,7 @@         return x
 
 
-class OsaStage(nn.Module):
+class OsaStage(msnn.Cell):
 
     def __init__(
             self,
@@ -129,7 +134,7 @@         self.grad_checkpointing = False
 
         if downsample:
-            self.pool = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)
+            self.pool = nn.MaxPool2d(kernel_size = 3, stride = 2, ceil_mode = True)
         else:
             self.pool = None
 
@@ -154,9 +159,11 @@                 **dd,
             )]
             in_chs = out_chs
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell([
+            blocks
+        ])
+
+    def construct(self, x):
         if self.pool is not None:
             x = self.pool(x)
         if self.grad_checkpointing and not torch.jit.is_scripting():
@@ -166,7 +173,7 @@         return x
 
 
-class VovNet(nn.Module):
+class VovNet(msnn.Cell):
 
     def __init__(
             self,
@@ -214,7 +221,7 @@         # Stem module
         last_stem_stride = stem_stride // 2
         conv_type = SeparableConvNormAct if cfg["depthwise"] else ConvNormAct
-        self.stem = nn.Sequential(*[
+        self.stem = msnn.SequentialCell(*[
             ConvNormAct(in_chans, stem_chs[0], 3, stride=2, **conv_kwargs),
             conv_type(stem_chs[0], stem_chs[1], 3, stride=1, **conv_kwargs),
             conv_type(stem_chs[1], stem_chs[2], 3, stride=last_stem_stride, **conv_kwargs),
@@ -244,16 +251,18 @@             current_stride *= 2 if downsample else 1
             self.feature_info += [dict(num_chs=self.num_features, reduction=current_stride, module=f'stages.{i}')]
 
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell([
+            stages
+        ])
 
         self.head_hidden_size = self.num_features
         self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate, **dd)
 
         for n, m in self.named_modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.Linear):
-                nn.init.zeros_(m.bias)
+                nn.init.zeros_(m.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     @torch.jit.ignore
     def group_matcher(self, coarse=False):
@@ -267,6 +276,7 @@         for s in self.stages:
             s.grad_checkpointing = enable
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.jit.ignore
     def get_classifier(self) -> nn.Module:
         return self.head.fc
@@ -277,7 +287,7 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
@@ -343,7 +353,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
