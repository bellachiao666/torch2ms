--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ VoVNet (V1 & V2)
 
 Papers:
@@ -13,8 +18,8 @@ 
 from typing import List, Optional, Tuple, Union, Type
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD
 from timm.layers import ConvNormAct, SeparableConvNormAct, BatchNormAct2d, ClassifierHead, DropPath, \
@@ -27,21 +32,21 @@ __all__ = ['VovNet']  # model_registry will add each entrypoint fn to this
 
 
-class SequentialAppendList(nn.Sequential):
+class SequentialAppendList(msnn.SequentialCell):
     def __init__(self, *args, **kwargs):
-        super().__init__(*args)
-
-    def forward(self, x: torch.Tensor, concat_list: List[torch.Tensor]) -> torch.Tensor:
+        super().__init__(*args)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def forward(self, x: ms.Tensor, concat_list: List[ms.Tensor]) -> ms.Tensor:
         for i, module in enumerate(self):
             if i == 0:
                 concat_list.append(module(x))
             else:
                 concat_list.append(module(concat_list[-1]))
-        x = torch.cat(concat_list, dim=1)
+        x = mint.cat(concat_list, dim=1)
         return x
 
 
-class OsaBlock(nn.Module):
+class OsaBlock(msnn.Cell):
 
     def __init__(
             self,
@@ -52,9 +57,9 @@             residual: bool = False,
             depthwise: bool = False,
             attn: str = '',
-            norm_layer: Type[nn.Module] = BatchNormAct2d,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            drop_path: Optional[nn.Module] = None,
+            norm_layer: Type[msnn.Cell] = BatchNormAct2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            drop_path: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
@@ -63,34 +68,34 @@ 
         self.residual = residual
         self.depthwise = depthwise
-        conv_kwargs = dict(norm_layer=norm_layer, act_layer=act_layer, **dd)
+        conv_kwargs = dict(norm_layer=norm_layer, act_layer=act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         next_in_chs = in_chs
         if self.depthwise and next_in_chs != mid_chs:
             assert not residual
-            self.conv_reduction = ConvNormAct(next_in_chs, mid_chs, 1, **conv_kwargs)
+            self.conv_reduction = ConvNormAct(next_in_chs, mid_chs, 1, **conv_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.conv_reduction = None
 
         mid_convs = []
         for i in range(layer_per_block):
             if self.depthwise:
-                conv = SeparableConvNormAct(mid_chs, mid_chs, **conv_kwargs)
+                conv = SeparableConvNormAct(mid_chs, mid_chs, **conv_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             else:
-                conv = ConvNormAct(next_in_chs, mid_chs, 3, **conv_kwargs)
+                conv = ConvNormAct(next_in_chs, mid_chs, 3, **conv_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             next_in_chs = mid_chs
             mid_convs.append(conv)
-        self.conv_mid = SequentialAppendList(*mid_convs)
+        self.conv_mid = SequentialAppendList(*mid_convs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # feature aggregation
         next_in_chs = in_chs + layer_per_block * mid_chs
-        self.conv_concat = ConvNormAct(next_in_chs, out_chs, **conv_kwargs)
-
-        self.attn = create_attn(attn, out_chs, **dd) if attn else None
+        self.conv_concat = ConvNormAct(next_in_chs, out_chs, **conv_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.attn = create_attn(attn, out_chs, **dd) if attn else None  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.drop_path = drop_path
 
-    def forward(self, x):
+    def construct(self, x):
         output = [x]
         if self.conv_reduction is not None:
             x = self.conv_reduction(x)
@@ -105,7 +110,7 @@         return x
 
 
-class OsaStage(nn.Module):
+class OsaStage(msnn.Cell):
 
     def __init__(
             self,
@@ -118,8 +123,8 @@             residual: bool = True,
             depthwise: bool = False,
             attn: str = 'ese',
-            norm_layer: Type[nn.Module] = BatchNormAct2d,
-            act_layer: Type[nn.Module] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = BatchNormAct2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
             drop_path_rates: Optional[List[float]] = None,
             device=None,
             dtype=None,
@@ -129,7 +134,7 @@         self.grad_checkpointing = False
 
         if downsample:
-            self.pool = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)
+            self.pool = nn.MaxPool2d(kernel_size = 3, stride = 2, ceil_mode = True)
         else:
             self.pool = None
 
@@ -152,13 +157,14 @@                 act_layer=act_layer,
                 drop_path=drop_path,
                 **dd,
-            )]
+            )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             in_chs = out_chs
-        self.blocks = nn.Sequential(*blocks)
-
-    def forward(self, x):
+        self.blocks = msnn.SequentialCell(*blocks)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         if self.pool is not None:
             x = self.pool(x)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if self.grad_checkpointing and not torch.jit.is_scripting():
             x = checkpoint_seq(self.blocks, x)
         else:
@@ -166,7 +172,7 @@         return x
 
 
-class VovNet(nn.Module):
+class VovNet(msnn.Cell):
 
     def __init__(
             self,
@@ -175,8 +181,8 @@             num_classes: int = 1000,
             global_pool: str = 'avg',
             output_stride: int = 32,
-            norm_layer: Type[nn.Module] = BatchNormAct2d,
-            act_layer: Type[nn.Module] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = BatchNormAct2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
             drop_rate: float = 0.,
             drop_path_rate: float = 0.,
             device=None,
@@ -202,23 +208,23 @@         self.drop_rate = drop_rate
         assert output_stride == 32  # FIXME support dilation
 
-        cfg = dict(cfg, **kwargs)
+        cfg = dict(cfg, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stem_stride = cfg.get("stem_stride", 4)
         stem_chs = cfg["stem_chs"]
         stage_conv_chs = cfg["stage_conv_chs"]
         stage_out_chs = cfg["stage_out_chs"]
         block_per_stage = cfg["block_per_stage"]
         layer_per_block = cfg["layer_per_block"]
-        conv_kwargs = dict(norm_layer=norm_layer, act_layer=act_layer, **dd)
+        conv_kwargs = dict(norm_layer=norm_layer, act_layer=act_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # Stem module
         last_stem_stride = stem_stride // 2
         conv_type = SeparableConvNormAct if cfg["depthwise"] else ConvNormAct
-        self.stem = nn.Sequential(*[
+        self.stem = msnn.SequentialCell(*[
             ConvNormAct(in_chans, stem_chs[0], 3, stride=2, **conv_kwargs),
             conv_type(stem_chs[0], stem_chs[1], 3, stride=1, **conv_kwargs),
             conv_type(stem_chs[1], stem_chs[2], 3, stride=last_stem_stride, **conv_kwargs),
-        ])
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         self.feature_info = [dict(
             num_chs=stem_chs[1], reduction=2, module=f'stem.{1 if stem_stride == 4 else 2}')]
         current_stride = stem_stride
@@ -226,7 +232,7 @@         # OSA stages
         stage_dpr = calculate_drop_path_rates(drop_path_rate, block_per_stage, stagewise=True)
         in_ch_list = stem_chs[-1:] + stage_out_chs[:-1]
-        stage_args = dict(residual=cfg["residual"], depthwise=cfg["depthwise"], attn=cfg["attn"], **conv_kwargs)
+        stage_args = dict(residual=cfg["residual"], depthwise=cfg["depthwise"], attn=cfg["attn"], **conv_kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         stages = []
         for i in range(4):  # num_stages
             downsample = stem_stride == 2 or i > 0  # first stage has no stride/downsample if stem_stride is 4
@@ -239,36 +245,36 @@                 downsample=downsample,
                 drop_path_rates=stage_dpr[i],
                 **stage_args,
-            )]
+            )]  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.num_features = stage_out_chs[i]
             current_stride *= 2 if downsample else 1
             self.feature_info += [dict(num_chs=self.num_features, reduction=current_stride, module=f'stages.{i}')]
 
-        self.stages = nn.Sequential(*stages)
+        self.stages = msnn.SequentialCell(*stages)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         self.head_hidden_size = self.num_features
-        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate, **dd)
+        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=drop_rate, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         for n, m in self.named_modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.Linear):
-                nn.init.zeros_(m.bias)
-
-    @torch.jit.ignore
+                nn.init.zeros_(m.bias)  # 'torch.nn.init.zeros_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',
             blocks=r'^stages\.(\d+)' if coarse else r'^stages\.(\d+).blocks\.(\d+)',
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         for s in self.stages:
             s.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes, global_pool: Optional[str] = None):
@@ -277,13 +283,13 @@ 
     def forward_intermediates(
             self,
-            x: torch.Tensor,
+            x: ms.Tensor,
             indices: Optional[Union[int, List[int]]] = None,
             norm: bool = False,
             stop_early: bool = False,
             output_fmt: str = 'NCHW',
             intermediates_only: bool = False,
-    ) -> Union[List[torch.Tensor], Tuple[torch.Tensor, List[torch.Tensor]]]:
+    ) -> Union[List[ms.Tensor], Tuple[ms.Tensor, List[ms.Tensor]]]:
         """ Forward features that returns intermediates.
 
         Args:
@@ -307,6 +313,7 @@             intermediates.append(x)
 
         x = self.stem[-1](x)
+        # 'torch.jit.is_scripting' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if torch.jit.is_scripting() or not stop_early:  # can't slice blocks in torchscript
             stages = self.stages
         else:
@@ -343,7 +350,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -468,7 +475,7 @@         model_cfg=model_cfgs[variant],
         feature_cfg=dict(flatten_sequential=True),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url='', **kwargs):
@@ -506,47 +513,47 @@ 
 @register_model
 def vovnet39a(pretrained=False, **kwargs) -> VovNet:
-    return _create_vovnet('vovnet39a', pretrained=pretrained, **kwargs)
+    return _create_vovnet('vovnet39a', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def vovnet57a(pretrained=False, **kwargs) -> VovNet:
-    return _create_vovnet('vovnet57a', pretrained=pretrained, **kwargs)
+    return _create_vovnet('vovnet57a', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def ese_vovnet19b_slim_dw(pretrained=False, **kwargs) -> VovNet:
-    return _create_vovnet('ese_vovnet19b_slim_dw', pretrained=pretrained, **kwargs)
+    return _create_vovnet('ese_vovnet19b_slim_dw', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def ese_vovnet19b_dw(pretrained=False, **kwargs) -> VovNet:
-    return _create_vovnet('ese_vovnet19b_dw', pretrained=pretrained, **kwargs)
+    return _create_vovnet('ese_vovnet19b_dw', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def ese_vovnet19b_slim(pretrained=False, **kwargs) -> VovNet:
-    return _create_vovnet('ese_vovnet19b_slim', pretrained=pretrained, **kwargs)
+    return _create_vovnet('ese_vovnet19b_slim', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def ese_vovnet39b(pretrained=False, **kwargs) -> VovNet:
-    return _create_vovnet('ese_vovnet39b', pretrained=pretrained, **kwargs)
+    return _create_vovnet('ese_vovnet39b', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def ese_vovnet57b(pretrained=False, **kwargs) -> VovNet:
-    return _create_vovnet('ese_vovnet57b', pretrained=pretrained, **kwargs)
+    return _create_vovnet('ese_vovnet57b', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def ese_vovnet99b(pretrained=False, **kwargs) -> VovNet:
-    return _create_vovnet('ese_vovnet99b', pretrained=pretrained, **kwargs)
+    return _create_vovnet('ese_vovnet99b', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
 def eca_vovnet39b(pretrained=False, **kwargs) -> VovNet:
-    return _create_vovnet('eca_vovnet39b', pretrained=pretrained, **kwargs)
+    return _create_vovnet('eca_vovnet39b', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 # Experimental Models
@@ -554,5 +561,5 @@ @register_model
 def ese_vovnet39b_evos(pretrained=False, **kwargs) -> VovNet:
     def norm_act_fn(num_features, **nkwargs):
-        return create_norm_act_layer('evonorms0', num_features, jit=False, **nkwargs)
-    return _create_vovnet('ese_vovnet39b_evos', pretrained=pretrained, norm_layer=norm_act_fn, **kwargs)
+        return create_norm_act_layer('evonorms0', num_features, jit=False, **nkwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+    return _create_vovnet('ese_vovnet39b_evos', pretrained=pretrained, norm_layer=norm_act_fn, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
