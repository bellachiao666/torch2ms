--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 Ported to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)
 
@@ -21,9 +26,7 @@ 
 The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299
 """
-import torch.jit
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch.nn as nn
 from typing import Optional
 
 from timm.layers import create_classifier
@@ -33,7 +36,7 @@ __all__ = ['Xception']
 
 
-class SeparableConv2d(nn.Module):
+class SeparableConv2d(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -61,13 +64,13 @@         )
         self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=False, **dd)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.conv1(x)
         x = self.pointwise(x)
         return x
 
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -96,20 +99,22 @@             else:
                 inc = in_channels
                 outc = in_channels if i < (reps - 1) else out_channels
-            rep.append(nn.ReLU(inplace=True))
+            rep.append(nn.ReLU())  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
             rep.append(SeparableConv2d(inc, outc, 3, stride=1, padding=1, **dd))
             rep.append(nn.BatchNorm2d(outc, **dd))
 
         if not start_with_relu:
             rep = rep[1:]
         else:
-            rep[0] = nn.ReLU(inplace=False)
+            rep[0] = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         if strides != 1:
             rep.append(nn.MaxPool2d(3, strides, 1))
-        self.rep = nn.Sequential(*rep)
-
-    def forward(self, inp):
+        self.rep = msnn.SequentialCell([
+            rep
+        ])
+
+    def construct(self, inp):
         x = self.rep(inp)
 
         if self.skip is not None:
@@ -122,7 +127,7 @@         return x
 
 
-class Xception(nn.Module):
+class Xception(msnn.Cell):
     """
     Xception optimized for the ImageNet dataset, as specified in
     https://arxiv.org/pdf/1610.02357.pdf
@@ -150,11 +155,11 @@ 
         self.conv1 = nn.Conv2d(in_chans, 32, 3, 2, 0, bias=False, **dd)
         self.bn1 = nn.BatchNorm2d(32, **dd)
-        self.act1 = nn.ReLU(inplace=True)
+        self.act1 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         self.conv2 = nn.Conv2d(32, 64, 3, bias=False, **dd)
         self.bn2 = nn.BatchNorm2d(64, **dd)
-        self.act2 = nn.ReLU(inplace=True)
+        self.act2 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         self.block1 = Block(64, 128, 2, 2, start_with_relu=False, **dd)
         self.block2 = Block(128, 256, 2, 2, **dd)
@@ -174,11 +179,11 @@ 
         self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1, **dd)
         self.bn3 = nn.BatchNorm2d(1536, **dd)
-        self.act3 = nn.ReLU(inplace=True)
+        self.act3 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         self.conv4 = SeparableConv2d(1536, self.num_features, 3, 1, 1, **dd)
         self.bn4 = nn.BatchNorm2d(self.num_features, **dd)
-        self.act4 = nn.ReLU(inplace=True)
+        self.act4 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.feature_info = [
             dict(num_chs=64, reduction=2, module='act2'),
             dict(num_chs=128, reduction=4, module='block2.rep.0'),
@@ -192,7 +197,7 @@         # #------- init weights --------
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.BatchNorm2d):
                 m.weight.data.fill_(1)
                 m.bias.data.zero_()
@@ -212,7 +217,7 @@         assert not enable, "gradient checkpointing not supported"
 
     @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    def get_classifier(self) -> msnn.Cell:
         return self.fc
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -253,10 +258,10 @@     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         if self.drop_rate:
-            F.dropout(x, self.drop_rate, training=self.training)
+            nn.functional.dropout(x, self.drop_rate, training = self.training)
         return x if pre_logits else self.fc(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
