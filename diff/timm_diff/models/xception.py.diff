--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """
 Ported to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)
 
@@ -21,9 +26,7 @@ 
 The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299
 """
-import torch.jit
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch.nn as nn
 from typing import Optional
 
 from timm.layers import create_classifier
@@ -33,7 +36,7 @@ __all__ = ['Xception']
 
 
-class SeparableConv2d(nn.Module):
+class SeparableConv2d(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -58,16 +61,16 @@             groups=in_channels,
             bias=False,
             **dd,
-        )
-        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=False, **dd)
-
-    def forward(self, x):
+        )  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.conv1(x)
         x = self.pointwise(x)
         return x
 
 
-class Block(nn.Module):
+class Block(msnn.Cell):
     def __init__(
             self,
             in_channels: int,
@@ -83,8 +86,8 @@         super().__init__()
 
         if out_channels != in_channels or strides != 1:
-            self.skip = nn.Conv2d(in_channels, out_channels, 1, stride=strides, bias=False, **dd)
-            self.skipbn = nn.BatchNorm2d(out_channels, **dd)
+            self.skip = nn.Conv2d(in_channels, out_channels, 1, stride=strides, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+            self.skipbn = nn.BatchNorm2d(out_channels, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
         else:
             self.skip = None
 
@@ -96,20 +99,20 @@             else:
                 inc = in_channels
                 outc = in_channels if i < (reps - 1) else out_channels
-            rep.append(nn.ReLU(inplace=True))
-            rep.append(SeparableConv2d(inc, outc, 3, stride=1, padding=1, **dd))
-            rep.append(nn.BatchNorm2d(outc, **dd))
+            rep.append(nn.ReLU())  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+            rep.append(SeparableConv2d(inc, outc, 3, stride=1, padding=1, **dd))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+            rep.append(nn.BatchNorm2d(outc, **dd))  # 存在 *args/**kwargs，需手动确认参数映射;
 
         if not start_with_relu:
             rep = rep[1:]
         else:
-            rep[0] = nn.ReLU(inplace=False)
+            rep[0] = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
 
         if strides != 1:
             rep.append(nn.MaxPool2d(3, strides, 1))
-        self.rep = nn.Sequential(*rep)
-
-    def forward(self, inp):
+        self.rep = msnn.SequentialCell(*rep)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, inp):
         x = self.rep(inp)
 
         if self.skip is not None:
@@ -122,7 +125,7 @@         return x
 
 
-class Xception(nn.Module):
+class Xception(msnn.Cell):
     """
     Xception optimized for the ImageNet dataset, as specified in
     https://arxiv.org/pdf/1610.02357.pdf
@@ -148,37 +151,37 @@         self.num_classes = num_classes
         self.num_features = self.head_hidden_size = 2048
 
-        self.conv1 = nn.Conv2d(in_chans, 32, 3, 2, 0, bias=False, **dd)
-        self.bn1 = nn.BatchNorm2d(32, **dd)
-        self.act1 = nn.ReLU(inplace=True)
-
-        self.conv2 = nn.Conv2d(32, 64, 3, bias=False, **dd)
-        self.bn2 = nn.BatchNorm2d(64, **dd)
-        self.act2 = nn.ReLU(inplace=True)
-
-        self.block1 = Block(64, 128, 2, 2, start_with_relu=False, **dd)
-        self.block2 = Block(128, 256, 2, 2, **dd)
-        self.block3 = Block(256, 728, 2, 2, **dd)
-
-        self.block4 = Block(728, 728, 3, 1, **dd)
-        self.block5 = Block(728, 728, 3, 1, **dd)
-        self.block6 = Block(728, 728, 3, 1, **dd)
-        self.block7 = Block(728, 728, 3, 1, **dd)
-
-        self.block8 = Block(728, 728, 3, 1, **dd)
-        self.block9 = Block(728, 728, 3, 1, **dd)
-        self.block10 = Block(728, 728, 3, 1, **dd)
-        self.block11 = Block(728, 728, 3, 1, **dd)
-
-        self.block12 = Block(728, 1024, 2, 2, grow_first=False, **dd)
-
-        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1, **dd)
-        self.bn3 = nn.BatchNorm2d(1536, **dd)
-        self.act3 = nn.ReLU(inplace=True)
-
-        self.conv4 = SeparableConv2d(1536, self.num_features, 3, 1, 1, **dd)
-        self.bn4 = nn.BatchNorm2d(self.num_features, **dd)
-        self.act4 = nn.ReLU(inplace=True)
+        self.conv1 = nn.Conv2d(in_chans, 32, 3, 2, 0, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn1 = nn.BatchNorm2d(32, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.act1 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+        self.conv2 = nn.Conv2d(32, 64, 3, bias=False, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.bn2 = nn.BatchNorm2d(64, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.act2 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+        self.block1 = Block(64, 128, 2, 2, start_with_relu=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.block2 = Block(128, 256, 2, 2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.block3 = Block(256, 728, 2, 2, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.block4 = Block(728, 728, 3, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.block5 = Block(728, 728, 3, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.block6 = Block(728, 728, 3, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.block7 = Block(728, 728, 3, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.block8 = Block(728, 728, 3, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.block9 = Block(728, 728, 3, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.block10 = Block(728, 728, 3, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.block11 = Block(728, 728, 3, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.block12 = Block(728, 1024, 2, 2, grow_first=False, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.bn3 = nn.BatchNorm2d(1536, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.act3 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
+
+        self.conv4 = SeparableConv2d(1536, self.num_features, 3, 1, 1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.bn4 = nn.BatchNorm2d(self.num_features, **dd)  # 存在 *args/**kwargs，需手动确认参数映射;
+        self.act4 = nn.ReLU()  # 'torch.nn.ReLU':没有对应的mindspore参数 'inplace' (position 0);
         self.feature_info = [
             dict(num_chs=64, reduction=2, module='act2'),
             dict(num_chs=128, reduction=4, module='block2.rep.0'),
@@ -187,17 +190,17 @@             dict(num_chs=2048, reduction=32, module='act4'),
         ]
 
-        self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool, **dd)
+        self.global_pool, self.fc = create_classifier(self.num_features, self.num_classes, pool_type=global_pool, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # #------- init weights --------
         for m in self.modules():
             if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             elif isinstance(m, nn.BatchNorm2d):
                 m.weight.data.fill_(1)
                 m.bias.data.zero_()
 
-    @torch.jit.ignore
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^conv[12]|bn[12]',
@@ -207,12 +210,12 @@             ],
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         assert not enable, "gradient checkpointing not supported"
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.fc
 
     def reset_classifier(self, num_classes: int, global_pool: str = 'avg'):
@@ -253,10 +256,10 @@     def forward_head(self, x, pre_logits: bool = False):
         x = self.global_pool(x)
         if self.drop_rate:
-            F.dropout(x, self.drop_rate, training=self.training)
+            nn.functional.dropout(x, self.drop_rate, training = self.training)
         return x if pre_logits else self.fc(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -266,7 +269,7 @@     return build_model_with_cfg(
         Xception, variant, pretrained,
         feature_cfg=dict(feature_cls='hook'),
-        **kwargs)
+        **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 default_cfgs = generate_default_cfgs({
@@ -289,7 +292,7 @@ 
 @register_model
 def legacy_xception(pretrained=False, **kwargs) -> Xception:
-    return _xception('legacy_xception', pretrained=pretrained, **kwargs)
+    return _xception('legacy_xception', pretrained=pretrained, **kwargs)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 register_model_deprecations(__name__, {
