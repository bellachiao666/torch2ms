--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """Pytorch impl of Aligned Xception 41, 65, 71
 
 This is a correct, from scratch impl of Aligned Xception (Deeplab) models compatible with TF weights at
@@ -8,8 +13,8 @@ from functools import partial
 from typing import List, Dict, Type, Optional
 
-import torch
-import torch.nn as nn
+# import torch
+# import torch.nn as nn
 
 from timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD
 from timm.layers import ClassifierHead, ConvNormAct, DropPath, PadType, create_conv2d, get_norm_act_layer
@@ -21,7 +26,7 @@ __all__ = ['XceptionAligned']
 
 
-class SeparableConv2d(nn.Module):
+class SeparableConv2d(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -30,8 +35,8 @@             stride: int = 1,
             dilation: int = 1,
             padding: PadType = '',
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             device=None,
             dtype=None,
     ):
@@ -50,16 +55,16 @@             dilation=dilation,
             depthwise=True,
             **dd,
-        )
-        self.bn_dw = norm_layer(in_chs, **dd)
-        self.act_dw = act_layer(inplace=True) if act_layer is not None else nn.Identity()
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.bn_dw = norm_layer(in_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.act_dw = act_layer(inplace=True) if act_layer is not None else msnn.Identity()
 
         # pointwise convolution
-        self.conv_pw = create_conv2d(in_chs, out_chs, kernel_size=1, **dd)
-        self.bn_pw = norm_layer(out_chs, **dd)
-        self.act_pw = act_layer(inplace=True) if act_layer is not None else nn.Identity()
-
-    def forward(self, x):
+        self.conv_pw = create_conv2d(in_chs, out_chs, kernel_size=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.bn_pw = norm_layer(out_chs, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.act_pw = act_layer(inplace=True) if act_layer is not None else msnn.Identity()
+
+    def construct(self, x):
         x = self.conv_dw(x)
         x = self.bn_dw(x)
         x = self.act_dw(x)
@@ -69,7 +74,7 @@         return x
 
 
-class PreSeparableConv2d(nn.Module):
+class PreSeparableConv2d(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -78,8 +83,8 @@             stride: int = 1,
             dilation: int = 1,
             padding: PadType = '',
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             first_act: bool = True,
             device=None,
             dtype=None,
@@ -90,7 +95,7 @@         self.kernel_size = kernel_size
         self.dilation = dilation
 
-        self.norm = norm_act_layer(in_chs, inplace=True, **dd) if first_act else nn.Identity()
+        self.norm = norm_act_layer(in_chs, inplace=True, **dd) if first_act else msnn.Identity()  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         # depthwise convolution
         self.conv_dw = create_conv2d(
             in_chs,
@@ -101,19 +106,19 @@             dilation=dilation,
             depthwise=True,
             **dd,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         # pointwise convolution
-        self.conv_pw = create_conv2d(in_chs, out_chs, kernel_size=1, **dd)
-
-    def forward(self, x):
+        self.conv_pw = create_conv2d(in_chs, out_chs, kernel_size=1, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    def construct(self, x):
         x = self.norm(x)
         x = self.conv_dw(x)
         x = self.conv_pw(x)
         return x
 
 
-class XceptionModule(nn.Module):
+class XceptionModule(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -123,9 +128,9 @@             pad_type: PadType = '',
             start_with_relu: bool = True,
             no_skip: bool = False,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Optional[Type[nn.Module]] = None,
-            drop_path: Optional[nn.Module] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
+            drop_path: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
@@ -144,12 +149,12 @@                 norm_layer=norm_layer,
                 apply_act=False,
                 **dd,
-            )
+            )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
             self.shortcut = None
 
         separable_act_layer = None if start_with_relu else act_layer
-        self.stack = nn.Sequential()
+        self.stack = msnn.SequentialCell()
         for i in range(3):
             if start_with_relu:
                 self.stack.add_module(f'act{i + 1}', act_layer(inplace=i > 0))
@@ -163,12 +168,12 @@                 act_layer=separable_act_layer,
                 norm_layer=norm_layer,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             in_chs = out_chs[i]
 
         self.drop_path = drop_path
 
-    def forward(self, x):
+    def construct(self, x):
         skip = x
         x = self.stack(x)
         if self.shortcut is not None:
@@ -180,7 +185,7 @@         return x
 
 
-class PreXceptionModule(nn.Module):
+class PreXceptionModule(msnn.Cell):
     def __init__(
             self,
             in_chs: int,
@@ -189,9 +194,9 @@             dilation: int = 1,
             pad_type: PadType = '',
             no_skip: bool = False,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Optional[Type[nn.Module]] = None,
-            drop_path: Optional[nn.Module] = None,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Optional[Type[msnn.Cell]] = None,
+            drop_path: Optional[msnn.Cell] = None,
             device=None,
             dtype=None,
     ):
@@ -202,12 +207,12 @@         self.out_channels = out_chs[-1]
         self.no_skip = no_skip
         if not no_skip and (self.out_channels != self.in_channels or stride != 1):
-            self.shortcut = create_conv2d(in_chs, self.out_channels, 1, stride=stride, **dd)
+            self.shortcut = create_conv2d(in_chs, self.out_channels, 1, stride=stride, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
         else:
-            self.shortcut = nn.Identity()
-
-        self.norm = get_norm_act_layer(norm_layer, act_layer=act_layer)(in_chs, inplace=True, **dd)
-        self.stack = nn.Sequential()
+            self.shortcut = msnn.Identity()
+
+        self.norm = get_norm_act_layer(norm_layer, act_layer=act_layer)(in_chs, inplace=True, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.stack = msnn.SequentialCell()
         for i in range(3):
             self.stack.add_module(f'conv{i + 1}', PreSeparableConv2d(
                 in_chs,
@@ -220,12 +225,12 @@                 norm_layer=norm_layer,
                 first_act=i > 0,
                 **dd,
-            ))
+            ))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             in_chs = out_chs[i]
 
         self.drop_path = drop_path
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.norm(x)
         skip = x
         x = self.stack(x)
@@ -236,7 +241,7 @@         return x
 
 
-class XceptionAligned(nn.Module):
+class XceptionAligned(msnn.Cell):
     """Modified Aligned Xception
     """
 
@@ -247,8 +252,8 @@             in_chans: int = 3,
             output_stride: int = 32,
             preact: bool = False,
-            act_layer: Type[nn.Module] = nn.ReLU,
-            norm_layer: Type[nn.Module] = nn.BatchNorm2d,
+            act_layer: Type[msnn.Cell] = nn.ReLU,
+            norm_layer: Type[msnn.Cell] = nn.BatchNorm2d,
             drop_rate: float = 0.,
             drop_path_rate: float = 0.,
             global_pool: str = 'avg',
@@ -262,17 +267,17 @@         self.drop_rate = drop_rate
         self.grad_checkpointing = False
 
-        layer_args = dict(act_layer=act_layer, norm_layer=norm_layer, **dd)
-        self.stem = nn.Sequential(*[
+        layer_args = dict(act_layer=act_layer, norm_layer=norm_layer, **dd)  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+        self.stem = msnn.SequentialCell(*[
             ConvNormAct(in_chans, 32, kernel_size=3, stride=2, **layer_args),
             create_conv2d(32, 64, kernel_size=3, stride=1, **dd) if preact else
             ConvNormAct(32, 64, kernel_size=3, stride=1, **layer_args)
-        ])
+        ])  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         curr_dilation = 1
         curr_stride = 2
         self.feature_info = []
-        self.blocks = nn.Sequential()
+        self.blocks = msnn.SequentialCell()
         module_fn = PreXceptionModule if preact else XceptionModule
         net_num_blocks = len(block_cfg)
         net_block_idx = 0
@@ -289,13 +294,13 @@                     b['stride'] = 1
                 else:
                     curr_stride = next_stride
-            self.blocks.add_module(str(i), module_fn(**b, **layer_args))
+            self.blocks.add_module(str(i), module_fn(**b, **layer_args))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
             self.num_features = self.blocks[-1].out_channels
             net_block_idx += 1
 
         self.feature_info += [dict(
             num_chs=self.num_features, reduction=curr_stride, module='blocks.' + str(len(self.blocks) - 1))]
-        self.act = act_layer(inplace=True) if preact else nn.Identity()
+        self.act = act_layer(inplace=True) if preact else msnn.Identity()
         self.head_hidden_size = self.num_features
         self.head = ClassifierHead(
             in_features=self.num_features,
@@ -303,21 +308,21 @@             pool_type=global_pool,
             drop_rate=drop_rate,
             **dd,
-        )
-
-    @torch.jit.ignore
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+    @ms.jit
     def group_matcher(self, coarse=False):
         return dict(
             stem=r'^stem',
             blocks=r'^blocks\.(\d+)',
         )
 
-    @torch.jit.ignore
+    @ms.jit
     def set_grad_checkpointing(self, enable=True):
         self.grad_checkpointing = enable
 
-    @torch.jit.ignore
-    def get_classifier(self) -> nn.Module:
+    @ms.jit
+    def get_classifier(self) -> msnn.Cell:
         return self.head.fc
 
     def reset_classifier(self, num_classes: int, global_pool: Optional[str] = None):
@@ -336,7 +341,7 @@     def forward_head(self, x, pre_logits: bool = False):
         return self.head(x, pre_logits=pre_logits) if pre_logits else self.head(x)
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.forward_features(x)
         x = self.forward_head(x)
         return x
@@ -349,7 +354,7 @@         pretrained,
         feature_cfg=dict(flatten_sequential=True, feature_cls='hook'),
         **kwargs,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def _cfg(url='', **kwargs):
@@ -400,7 +405,7 @@         dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),
     ]
     model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1))
-    return _xception('xception41', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _xception('xception41', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -419,7 +424,7 @@         dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),
     ]
     model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1))
-    return _xception('xception65', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _xception('xception65', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -440,7 +445,7 @@         dict(in_chs=1024, out_chs=(1536, 1536, 2048), stride=1, no_skip=True, start_with_relu=False),
     ]
     model_args = dict(block_cfg=block_cfg, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1))
-    return _xception('xception71', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _xception('xception71', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -459,7 +464,7 @@         dict(in_chs=1024, out_chs=(1536, 1536, 2048), no_skip=True, stride=1),
     ]
     model_args = dict(block_cfg=block_cfg, preact=True, norm_layer=nn.BatchNorm2d)
-    return _xception('xception41p', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _xception('xception41p', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 @register_model
@@ -479,4 +484,4 @@     ]
     model_args = dict(
         block_cfg=block_cfg, preact=True, norm_layer=partial(nn.BatchNorm2d, eps=.001, momentum=.1))
-    return _xception('xception65p', pretrained=pretrained, **dict(model_args, **kwargs))
+    return _xception('xception65p', pretrained=pretrained, **dict(model_args, **kwargs))  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
