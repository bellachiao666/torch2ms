--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Adafactor (Big Vision variant) for PyTorch
 
 Adapted from the implementation in big vision: https://github.com/google-research/big_vision
@@ -12,16 +17,16 @@ """
 from typing import List, Optional, Tuple, Union
 
-import torch
-from torch import Tensor
-from torch.optim import Optimizer
+# import torch
+# from torch import Tensor
+# from torch.optim import Optimizer
 
 from ._types import ParamsT
 
 
 def _get_scalar_dtype():
     """Get the scalar dtype that the optimizer uses for state"""
-    return torch.float64
+    return ms.float64
 
 
 def _factored_dims(
@@ -50,6 +55,7 @@     return int(sorted_dims[-2][1]), int(sorted_dims[-1][1])
 
 
+# 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class AdafactorBigVision(Optimizer):
     """
     PyTorch implementation of BigVision's Adafactor variant with both single and multi tensor implementations.
@@ -57,6 +63,8 @@     Adapted from https://github.com/google-research/big_vision by Ross Wightman
     """
 
+    # 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             params: ParamsT,
@@ -66,7 +74,7 @@             decay_offset: int = 0,
             beta2_cap: float = 0.999,
             momentum: Optional[float] = 0.9,
-            momentum_dtype: Union[str, torch.dtype] = torch.bfloat16,
+            momentum_dtype: Union[str, torch.dtype] = ms.bfloat16,
             eps: Optional[float] = None,
             weight_decay: float = 0.0,
             clipping_threshold: Optional[float] = None,
@@ -78,12 +86,12 @@     ):
         if isinstance(momentum_dtype, str):
             if momentum_dtype == 'float16':
-                momentum_dtype = torch.float16
+                momentum_dtype = ms.float16
             elif momentum_dtype == 'bfloat16':
-                momentum_dtype = torch.bfloat16
+                momentum_dtype = ms.bfloat16
             else:
                 assert momentum_dtype == 'float32', f'{momentum_dtype} dtype not supported'
-                momentum_dtype = torch.float32
+                momentum_dtype = ms.float32
         # FIXME try to check if momentum dtype is appropriate for device? Torch API not great for this.
 
         defaults = dict(
@@ -113,7 +121,7 @@             for p in group['params']:
                 p_state = self.state.get(p, {})
                 if len(p_state) != 0 and not torch.is_tensor(p_state['step']):
-                    p_state['step'] = torch.tensor(float(p_state['step']), dtype=_get_scalar_dtype())
+                    p_state['step'] = ms.Tensor(float(p_state['step']), dtype = _get_scalar_dtype())  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;
 
                 if 'exp_avg' in p_state and torch.is_tensor(p_state['exp_avg']):
                     # FIXME this is a bit of a hack, optimizer.load_state_dict appears to upcast
@@ -121,7 +129,6 @@                     # look into this further. Better to override _process_value_according_to_param_policy?
                     p_state['exp_avg'] = p_state['exp_avg'].to(dtype=self.defaults['momentum_dtype'])
 
-    @torch.no_grad()
     def step(self, closure=None):
         loss = None
         if closure is not None:
@@ -151,7 +158,7 @@ 
                 if len(state) == 0:
                     # NOTE step on CPU, probably need some more though to make capturable
-                    state['step'] = torch.tensor(0.0, dtype=_get_scalar_dtype())
+                    state['step'] = ms.Tensor(0.0, dtype = _get_scalar_dtype())  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;
 
                     shape = p.grad.shape
                     factored_dims = _factored_dims(
@@ -169,10 +176,10 @@                         state['exp_avg_sq_r'] = p.grad.new_zeros(row_shape)
                         state['exp_avg_sq_c'] = p.grad.new_zeros(col_shape)
                     else:
-                        state['exp_avg_sq'] = torch.zeros_like(p.grad, memory_format=torch.preserve_format)
+                        state['exp_avg_sq'] = mint.zeros_like(p.grad)  # 'torch.zeros_like':没有对应的mindspore参数 'memory_format' (position 5);
 
                     if self.defaults['momentum'] is not None:
-                        state['exp_avg'] = torch.zeros_like(p.grad, dtype=self.defaults['momentum_dtype'])
+                        state['exp_avg'] = mint.zeros_like(p.grad, dtype = self.defaults['momentum_dtype'])
 
                 state_steps.append(state['step'])
                 exp_avg_sq_rs.append(state.get('exp_avg_sq_r', None))
@@ -210,14 +217,16 @@         return loss
 
 
+# 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def _single_tensor_adafactor(
-        params: List[Tensor],
-        grads: List[Tensor],
-        exp_avg_sq_rs: List[Optional[Tensor]],
-        exp_avg_sq_cs: List[Optional[Tensor]],
-        exp_avg_sqs: List[Optional[Tensor]],
-        exp_avgs: List[Optional[Tensor]],
-        state_steps: List[Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        exp_avg_sq_rs: List[Optional[ms.Tensor]],
+        exp_avg_sq_cs: List[Optional[ms.Tensor]],
+        exp_avg_sqs: List[Optional[ms.Tensor]],
+        exp_avgs: List[Optional[ms.Tensor]],
+        state_steps: List[ms.Tensor],
         *,
         beta2_decay: float,
         beta2_cap: float,
@@ -241,14 +250,14 @@         step_t = state_steps[i]
         if eps is None:
             # default eps for avoiding div by zero, diff from float type eps
-            eps = 1e-7 if grad.dtype == torch.float16 else 1e-30
+            eps = 1e-7 if grad.dtype == ms.float16 else 1e-30
 
         # Update step
         step_t += 1
         beta2_t = min(beta2_cap, 1.0 - float(step_t) ** (-beta2_decay))
         one_minus_beta2_t = 1 - beta2_t
 
-        grad_sqr = torch.square(grad) + eps
+        grad_sqr = mint.square(grad) + eps
         # NOTE application of eps (epsilon1) mirrors the optax/big vision/t5x approach
         if exp_avg_sq is None:
             # factorized second moment
@@ -312,14 +321,16 @@         param.add_(update, alpha=-1.0)
 
 
+# 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def _multi_tensor_adafactor(
-        params: List[Tensor],
-        grads: List[Tensor],
-        exp_avg_sq_rs: List[Optional[Tensor]],
-        exp_avg_sq_cs: List[Optional[Tensor]],
-        exp_avg_sqs: List[Optional[Tensor]],
-        exp_avgs: List[Optional[Tensor]],
-        state_steps: List[Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        exp_avg_sq_rs: List[Optional[ms.Tensor]],
+        exp_avg_sq_cs: List[Optional[ms.Tensor]],
+        exp_avg_sqs: List[Optional[ms.Tensor]],
+        exp_avgs: List[Optional[ms.Tensor]],
+        state_steps: List[ms.Tensor],
         *,
         beta2_decay: float,
         beta2_cap: float,
