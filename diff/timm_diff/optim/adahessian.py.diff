--- pytorch+++ mindspore@@ -1,11 +1,17 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ AdaHessian Optimizer
 
 Lifted from https://github.com/davda54/ada-hessian/blob/master/ada_hessian.py
 Originally licensed MIT, Copyright 2020, David Samuel
 """
-import torch
+# import torch
 
 
+# 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class Adahessian(torch.optim.Optimizer):
     """
     Implements the AdaHessian algorithm from "ADAHESSIAN: An Adaptive Second OrderOptimizer for Machine Learning"
@@ -52,7 +58,7 @@ 
         # use a separate generator that deterministically generates the same `z`s across all GPUs in case of distributed training
         self.seed = 2147483647
-        self.generator = torch.Generator().manual_seed(self.seed)
+        self.generator = torch.Generator().manual_seed(self.seed)  # 'torch.Generator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.Generator.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         defaults = dict(
             lr=lr,
@@ -87,6 +93,8 @@             if not isinstance(p.hess, float) and self.state[p]["hessian step"] % self.update_each == 0:
                 p.hess.zero_()
 
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 装饰器 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.no_grad()
     def set_hessian(self):
         """
@@ -103,18 +111,20 @@             return
 
         if self.generator.device != params[0].device:  # hackish way of casting the generator to the right device
-            self.generator = torch.Generator(params[0].device).manual_seed(self.seed)
+            self.generator = torch.Generator(params[0].device).manual_seed(self.seed)  # 'torch.Generator' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.Generator.manual_seed' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         grads = [p.grad for p in params]
 
         for i in range(self.n_samples):
             # Rademacher distribution {-1.0, 1.0}
-            zs = [torch.randint(0, 2, p.size(), generator=self.generator, device=p.device) * 2.0 - 1.0 for p in params]
+            zs = [mint.randint(0, 2, p.size(), generator=self.generator, device=p.device) * 2.0 - 1.0 for p in params]
             h_zs = torch.autograd.grad(
-                grads, params, grad_outputs=zs, only_inputs=True, retain_graph=i < self.n_samples - 1)
+                grads, params, grad_outputs=zs, only_inputs=True, retain_graph=i < self.n_samples - 1)  # 'torch.autograd.grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             for h_z, z, p in zip(h_zs, zs, params):
                 p.hess += h_z * z / self.n_samples  # approximate the expected values of z*(H@z)
 
+    # 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 装饰器 'torch.no_grad' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @torch.no_grad()
     def step(self, closure=None):
         """
@@ -136,7 +146,7 @@                     continue
 
                 if self.avg_conv_kernel and p.dim() == 4:
-                    p.hess = torch.abs(p.hess).mean(dim=[2, 3], keepdim=True).expand_as(p.hess).clone()
+                    p.hess = mint.abs(p.hess).mean(dim=[2, 3], keepdim=True).expand_as(p.hess).clone()
 
                 # Perform correct stepweight decay as in AdamW
                 p.mul_(1 - group['lr'] * group['weight_decay'])
@@ -147,9 +157,9 @@                 if len(state) == 1:
                     state['step'] = 0
                     # Exponential moving average of gradient values
-                    state['exp_avg'] = torch.zeros_like(p)
+                    state['exp_avg'] = mint.zeros_like(p)
                     # Exponential moving average of Hessian diagonal square values
-                    state['exp_hessian_diag_sq'] = torch.zeros_like(p)
+                    state['exp_hessian_diag_sq'] = mint.zeros_like(p)
 
                 exp_avg, exp_hessian_diag_sq = state['exp_avg'], state['exp_hessian_diag_sq']
                 beta1, beta2 = group['betas']
