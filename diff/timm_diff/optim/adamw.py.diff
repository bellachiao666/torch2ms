--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ AdamW Optimizer
 Impl copied from PyTorch master
 
@@ -10,13 +15,14 @@ import math
 from typing import List, Optional, Tuple
 
-import torch
-from torch import Tensor
-from torch.optim.optimizer import Optimizer
+# import torch
+# from torch import Tensor
+# from torch.optim.optimizer import Optimizer
 
 from ._types import ParamsT
 
 
+# 'torch.optim.optimizer.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class AdamWLegacy(Optimizer):
     r"""Implements AdamW algorithm.
 
@@ -85,10 +91,10 @@     def __setstate__(self, state):
         super(AdamWLegacy, self).__setstate__(state)
         state_values = list(self.state.values())
-        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(state_values[0]['step'])
+        step_is_tensor = (len(state_values) != 0) and torch.is_tensor(state_values[0]['step'])  # 'torch.is_tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         if not step_is_tensor:
             for s in state_values:
-                s['step'] = torch.tensor(float(s['step']))
+                s['step'] = ms.Tensor(float(s['step']))
         for group in self.param_groups:
             group.setdefault('amsgrad', False)
             group.setdefault('caution', False)
@@ -134,14 +140,14 @@ 
                 # State initialization
                 if len(state) == 0:
-                    state['step'] = torch.tensor(0.)
+                    state['step'] = ms.Tensor(0.)
                     # Exponential moving average of gradient values
-                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                    state['exp_avg'] = mint.zeros_like(p, memory_format=torch.preserve_format)
                     # Exponential moving average of squared gradient values
-                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                    state['exp_avg_sq'] = mint.zeros_like(p, memory_format=torch.preserve_format)
                     if amsgrad:
                         # Maintains max of all exp. moving avg. of sq. grad. values
-                        state['max_exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                        state['max_exp_avg_sq'] = mint.zeros_like(p, memory_format=torch.preserve_format)
 
                 exp_avgs.append(state['exp_avg'])
                 exp_avg_sqs.append(state['exp_avg_sq'])
@@ -173,12 +179,12 @@ 
 
 def adamw(
-        params: List[Tensor],
-        grads: List[Tensor],
-        exp_avgs: List[Tensor],
-        exp_avg_sqs: List[Tensor],
-        max_exp_avg_sqs: List[Tensor],
-        state_steps: List[Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        exp_avgs: List[ms.Tensor],
+        exp_avg_sqs: List[ms.Tensor],
+        max_exp_avg_sqs: List[ms.Tensor],
+        state_steps: List[ms.Tensor],
         foreach: Optional[bool] = None,
         capturable: bool = False,
         *,
@@ -196,7 +202,7 @@       See AdamWLegacy class for details.
     """
 
-    if not all(isinstance(t, torch.Tensor) for t in state_steps):
+    if not all(isinstance(t, ms.Tensor) for t in state_steps):
         raise RuntimeError(
             'API has changed, `state_steps` argument must contain a list of' +
             ' singleton tensors')
@@ -204,7 +210,7 @@     if foreach is None:
         try:
             # cannot do foreach if this overload doesn't exist when caution enabled
-            foreach = not caution or 'Scalar' in torch.ops.aten._foreach_maximum_.overloads()
+            foreach = not caution or 'Scalar' in torch.ops.aten._foreach_maximum_.overloads()  # 'torch.ops.aten._foreach_maximum_.overloads' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         except:
             foreach = False
 
@@ -234,12 +240,12 @@ 
 
 def _single_tensor_adamw(
-        params: List[Tensor],
-        grads: List[Tensor],
-        exp_avgs: List[Tensor],
-        exp_avg_sqs: List[Tensor],
-        max_exp_avg_sqs: List[Tensor],
-        state_steps: List[Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        exp_avgs: List[ms.Tensor],
+        exp_avg_sqs: List[ms.Tensor],
+        max_exp_avg_sqs: List[ms.Tensor],
+        state_steps: List[ms.Tensor],
         *,
         amsgrad: bool,
         beta1: float,
@@ -273,7 +279,7 @@         if amsgrad:
             max_exp_avg_sq = max_exp_avg_sqs[i]
             # Maintains the maximum of all 2nd moment running avg. till now
-            torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)
+            mint.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)
             denom_base = max_exp_avg_sq
         else:
             denom_base = exp_avg_sq
@@ -283,8 +289,8 @@ 
             # 1 - beta1 ** step can't be captured in a CUDA graph, even if step is a CUDA tensor
             # (incurs "RuntimeError: CUDA error: operation not permitted when stream is capturing")
-            bias_correction1 = 1 - torch.pow(beta1, step)
-            bias_correction2 = 1 - torch.pow(beta2, step)
+            bias_correction1 = 1 - mint.pow(beta1, step)
+            bias_correction2 = 1 - mint.pow(beta2, step)
 
             step_size = lr / bias_correction1
             step_size_neg = step_size.neg()
@@ -320,12 +326,12 @@ 
 
 def _multi_tensor_adamw(
-        params: List[Tensor],
-        grads: List[Tensor],
-        exp_avgs: List[Tensor],
-        exp_avg_sqs: List[Tensor],
-        max_exp_avg_sqs: List[Tensor],
-        state_steps: List[Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        exp_avgs: List[ms.Tensor],
+        exp_avg_sqs: List[ms.Tensor],
+        max_exp_avg_sqs: List[ms.Tensor],
+        state_steps: List[ms.Tensor],
         *,
         amsgrad: bool,
         beta1: float,
@@ -347,72 +353,72 @@         ), "If capturable=True, params and state_steps must be CUDA tensors."
 
     if maximize:
-        grads = torch._foreach_neg(tuple(grads))  # type: ignore[assignment]
-
-    grads = [torch.view_as_real(x) if torch.is_complex(x) else x for x in grads]
-    exp_avgs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in exp_avgs]
-    exp_avg_sqs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in exp_avg_sqs]
-    params = [torch.view_as_real(x) if torch.is_complex(x) else x for x in params]
+        grads = torch._foreach_neg(tuple(grads))  # type: ignore[assignment]; 'torch._foreach_neg' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    grads = [torch.view_as_real(x) if torch.is_complex(x) else x for x in grads]  # 'torch.view_as_real' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.is_complex' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    exp_avgs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in exp_avgs]  # 'torch.view_as_real' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.is_complex' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    exp_avg_sqs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in exp_avg_sqs]  # 'torch.view_as_real' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.is_complex' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    params = [torch.view_as_real(x) if torch.is_complex(x) else x for x in params]  # 'torch.view_as_real' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.is_complex' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     # update steps
-    torch._foreach_add_(state_steps, 1)
+    torch._foreach_add_(state_steps, 1)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     # Perform stepweight decay
     wd_scale = lr if max_lr is None else lr ** 2 / max_lr
-    torch._foreach_mul_(params, 1 -  wd_scale * weight_decay)
+    torch._foreach_mul_(params, 1 -  wd_scale * weight_decay)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     # Decay the first and second moment running average coefficient
     #torch._foreach_lerp_(exp_avgs, grads, 1 - beta1)
-    torch._foreach_mul_(exp_avgs, beta1)
-    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)
-
-    torch._foreach_mul_(exp_avg_sqs, beta2)
-    torch._foreach_addcmul_(exp_avg_sqs, grads, grads, 1 - beta2)
+    torch._foreach_mul_(exp_avgs, beta1)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    torch._foreach_mul_(exp_avg_sqs, beta2)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    torch._foreach_addcmul_(exp_avg_sqs, grads, grads, 1 - beta2)  # 'torch._foreach_addcmul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     if capturable:
         # TODO: use foreach_pow if/when foreach_pow is added
-        bias_correction1 = [torch.pow(beta1, step) for step in state_steps]
-        bias_correction2 = [torch.pow(beta2, step) for step in state_steps]
+        bias_correction1 = [mint.pow(beta1, step) for step in state_steps]
+        bias_correction2 = [mint.pow(beta2, step) for step in state_steps]
         # foreach_sub doesn't allow a scalar as the first arg
-        torch._foreach_sub_(bias_correction1, 1)
-        torch._foreach_sub_(bias_correction2, 1)
-        torch._foreach_neg_(bias_correction1)
-        torch._foreach_neg_(bias_correction2)
+        torch._foreach_sub_(bias_correction1, 1)  # 'torch._foreach_sub_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_sub_(bias_correction2, 1)  # 'torch._foreach_sub_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_neg_(bias_correction1)  # 'torch._foreach_neg_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_neg_(bias_correction2)  # 'torch._foreach_neg_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # foreach_div doesn't allow a scalar as the first arg
-        step_size = torch._foreach_div(bias_correction1, lr)
-        torch._foreach_reciprocal_(step_size)
-        torch._foreach_neg_(step_size)
-
-        bias_correction2_sqrt = torch._foreach_sqrt(bias_correction2)
+        step_size = torch._foreach_div(bias_correction1, lr)  # 'torch._foreach_div' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_reciprocal_(step_size)  # 'torch._foreach_reciprocal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_neg_(step_size)  # 'torch._foreach_neg_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+        bias_correction2_sqrt = torch._foreach_sqrt(bias_correction2)  # 'torch._foreach_sqrt' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         if amsgrad:
             # Maintains the maximum of all 2nd moment running avg. till now
-            max_exp_avg_sqs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in max_exp_avg_sqs]
-            torch._foreach_maximum_(max_exp_avg_sqs, exp_avg_sqs)
-            denom_base = torch._foreach_sqrt(max_exp_avg_sqs)
+            max_exp_avg_sqs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in max_exp_avg_sqs]  # 'torch.view_as_real' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.is_complex' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            torch._foreach_maximum_(max_exp_avg_sqs, exp_avg_sqs)  # 'torch._foreach_maximum_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            denom_base = torch._foreach_sqrt(max_exp_avg_sqs)  # 'torch._foreach_sqrt' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
-            denom_base = torch._foreach_sqrt(exp_avg_sqs)
+            denom_base = torch._foreach_sqrt(exp_avg_sqs)  # 'torch._foreach_sqrt' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         torch._foreach_div_(
             denom_base,
             torch._foreach_mul(bias_correction2_sqrt, step_size)
-        )
-        eps_over_step_size = torch._foreach_div(step_size, eps)
-        torch._foreach_reciprocal_(eps_over_step_size)
-        denom = torch._foreach_add(denom_base, eps_over_step_size)
+        )  # 'torch._foreach_mul' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch._foreach_div_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        eps_over_step_size = torch._foreach_div(step_size, eps)  # 'torch._foreach_div' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_reciprocal_(eps_over_step_size)  # 'torch._foreach_reciprocal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        denom = torch._foreach_add(denom_base, eps_over_step_size)  # 'torch._foreach_add' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         if caution:
             # Apply caution as per 'Cautious Optimizers' - https://arxiv.org/abs/2411.16085
-            masks = torch._foreach_mul(exp_avgs, grads)
+            masks = torch._foreach_mul(exp_avgs, grads)  # 'torch._foreach_mul' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             masks = [(m > 0).to(g.dtype) for m, g in zip(masks, grads)]  # capturable?
             mask_scale = [m.mean() for m in masks]
-            torch._foreach_maximum_(mask_scale, 1e-3)
+            torch._foreach_maximum_(mask_scale, 1e-3)  # 'torch._foreach_maximum_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             #torch._foreach_clamp_min_(mask_scale, 1e-3)
-            torch._foreach_div_(masks, mask_scale)
-            exp_avgs = torch._foreach_mul(exp_avgs, masks)
-
-        torch._foreach_addcdiv_(params, exp_avgs, denom)
+            torch._foreach_div_(masks, mask_scale)  # 'torch._foreach_div_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            exp_avgs = torch._foreach_mul(exp_avgs, masks)  # 'torch._foreach_mul' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+        torch._foreach_addcdiv_(params, exp_avgs, denom)  # 'torch._foreach_addcdiv_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     else:
         bias_correction1 = [1 - beta1 ** step.item() for step in state_steps]
         bias_correction2 = [1 - beta2 ** step.item() for step in state_steps]
@@ -423,23 +429,23 @@ 
         if amsgrad:
             # Maintains the maximum of all 2nd moment running avg. till now
-            max_exp_avg_sqs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in max_exp_avg_sqs]
-            torch._foreach_maximum_(max_exp_avg_sqs, exp_avg_sqs)
-            denom = torch._foreach_sqrt(max_exp_avg_sqs)
+            max_exp_avg_sqs = [torch.view_as_real(x) if torch.is_complex(x) else x for x in max_exp_avg_sqs]  # 'torch.view_as_real' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.is_complex' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            torch._foreach_maximum_(max_exp_avg_sqs, exp_avg_sqs)  # 'torch._foreach_maximum_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            denom = torch._foreach_sqrt(max_exp_avg_sqs)  # 'torch._foreach_sqrt' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
-            denom = torch._foreach_sqrt(exp_avg_sqs)
-
-        torch._foreach_div_(denom, bias_correction2_sqrt)
-        torch._foreach_add_(denom, eps)
+            denom = torch._foreach_sqrt(exp_avg_sqs)  # 'torch._foreach_sqrt' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+        torch._foreach_div_(denom, bias_correction2_sqrt)  # 'torch._foreach_div_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_add_(denom, eps)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         if caution:
             # Apply caution as per 'Cautious Optimizers' - https://arxiv.org/abs/2411.16085
-            masks = torch._foreach_mul(exp_avgs, grads)
+            masks = torch._foreach_mul(exp_avgs, grads)  # 'torch._foreach_mul' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             masks = [(m > 0).to(g.dtype) for m, g in zip(masks, grads)]
             mask_scale = [m.mean() for m in masks]
-            torch._foreach_maximum_(mask_scale, 1e-3)
+            torch._foreach_maximum_(mask_scale, 1e-3)  # 'torch._foreach_maximum_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             #torch._foreach_clamp_min_(mask_scale, 1e-3)
-            torch._foreach_div_(masks, mask_scale)
-            exp_avgs = torch._foreach_mul(exp_avgs, masks)
-
-        torch._foreach_addcdiv_(params, exp_avgs, denom, step_size)
+            torch._foreach_div_(masks, mask_scale)  # 'torch._foreach_div_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            exp_avgs = torch._foreach_mul(exp_avgs, masks)  # 'torch._foreach_mul' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+        torch._foreach_addcdiv_(params, exp_avgs, denom, step_size)  # 'torch._foreach_addcdiv_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
