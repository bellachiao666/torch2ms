--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Adan Optimizer
 
 Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models[J]. arXiv preprint arXiv:2208.06677, 2022.
@@ -22,9 +27,9 @@ import math
 from typing import List, Optional, Tuple
 
-import torch
-from torch import Tensor
-from torch.optim.optimizer import Optimizer
+# import torch
+# from torch import Tensor
+# from torch.optim.optimizer import Optimizer
 
 
 class MultiTensorApply(object):
@@ -43,6 +48,7 @@         return op(self.chunk_size, noop_flag_buffer, tensor_lists, *args)
 
 
+# 'torch.optim.optimizer.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class Adan(Optimizer):
     """ Implements a pytorch variant of Adan.
 
@@ -108,11 +114,11 @@                     # State initialization
 
                     # Exponential moving average of gradient values
-                    state['exp_avg'] = torch.zeros_like(p)
+                    state['exp_avg'] = mint.zeros_like(p)
                     # Exponential moving average of squared gradient values
-                    state['exp_avg_sq'] = torch.zeros_like(p)
+                    state['exp_avg_sq'] = mint.zeros_like(p)
                     # Exponential moving average of gradient difference
-                    state['exp_avg_diff'] = torch.zeros_like(p)
+                    state['exp_avg_diff'] = mint.zeros_like(p)
 
     @torch.no_grad()
     def step(self, closure=None):
@@ -123,7 +129,7 @@                 loss = closure()
 
         try:
-            has_scalar_maximum = 'Scalar' in torch.ops.aten._foreach_maximum_.overloads()
+            has_scalar_maximum = 'Scalar' in torch.ops.aten._foreach_maximum_.overloads()  # 'torch.ops.aten._foreach_maximum_.overloads' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         except:
             has_scalar_maximum = False
 
@@ -155,9 +161,9 @@ 
                 state = self.state[p]
                 if len(state) == 0:
-                    state['exp_avg'] = torch.zeros_like(p)
-                    state['exp_avg_sq'] = torch.zeros_like(p)
-                    state['exp_avg_diff'] = torch.zeros_like(p)
+                    state['exp_avg'] = mint.zeros_like(p)
+                    state['exp_avg_sq'] = mint.zeros_like(p)
+                    state['exp_avg_diff'] = mint.zeros_like(p)
 
                 if 'neg_pre_grad' not in state or group['step'] == 1:
                     state['neg_pre_grad'] = -p.grad.clone()
@@ -204,12 +210,12 @@ 
 
 def _single_tensor_adan(
-        params: List[Tensor],
-        grads: List[Tensor],
-        exp_avgs: List[Tensor],
-        exp_avg_sqs: List[Tensor],
-        exp_avg_diffs: List[Tensor],
-        neg_pre_grads: List[Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        exp_avgs: List[ms.Tensor],
+        exp_avg_sqs: List[ms.Tensor],
+        exp_avg_diffs: List[ms.Tensor],
+        neg_pre_grads: List[ms.Tensor],
         *,
         beta1: float,
         beta2: float,
@@ -262,12 +268,12 @@ 
 
 def _multi_tensor_adan(
-        params: List[Tensor],
-        grads: List[Tensor],
-        exp_avgs: List[Tensor],
-        exp_avg_sqs: List[Tensor],
-        exp_avg_diffs: List[Tensor],
-        neg_pre_grads: List[Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        exp_avgs: List[ms.Tensor],
+        exp_avg_sqs: List[ms.Tensor],
+        exp_avg_diffs: List[ms.Tensor],
+        neg_pre_grads: List[ms.Tensor],
         *,
         beta1: float,
         beta2: float,
@@ -285,43 +291,43 @@         return
 
     # for memory saving, we use `neg_pre_grads` to get some temp variable in a inplace way
-    torch._foreach_add_(neg_pre_grads, grads)
-
-    torch._foreach_mul_(exp_avgs, beta1)
-    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  # m_t
-
-    torch._foreach_mul_(exp_avg_diffs, beta2)
-    torch._foreach_add_(exp_avg_diffs, neg_pre_grads, alpha=1 - beta2)  # diff_t
-
-    torch._foreach_mul_(neg_pre_grads, beta2)
-    torch._foreach_add_(neg_pre_grads, grads)
-    torch._foreach_mul_(exp_avg_sqs, beta3)
-    torch._foreach_addcmul_(exp_avg_sqs, neg_pre_grads, neg_pre_grads, value=1 - beta3)  # n_t
-
-    denom = torch._foreach_sqrt(exp_avg_sqs)
-    torch._foreach_div_(denom, bias_correction3_sqrt)
-    torch._foreach_add_(denom, eps)
+    torch._foreach_add_(neg_pre_grads, grads)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    torch._foreach_mul_(exp_avgs, beta1)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    torch._foreach_add_(exp_avgs, grads, alpha=1 - beta1)  # m_t; 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    torch._foreach_mul_(exp_avg_diffs, beta2)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    torch._foreach_add_(exp_avg_diffs, neg_pre_grads, alpha=1 - beta2)  # diff_t; 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    torch._foreach_mul_(neg_pre_grads, beta2)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    torch._foreach_add_(neg_pre_grads, grads)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    torch._foreach_mul_(exp_avg_sqs, beta3)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    torch._foreach_addcmul_(exp_avg_sqs, neg_pre_grads, neg_pre_grads, value=1 - beta3)  # n_t; 'torch._foreach_addcmul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    denom = torch._foreach_sqrt(exp_avg_sqs)  # 'torch._foreach_sqrt' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    torch._foreach_div_(denom, bias_correction3_sqrt)  # 'torch._foreach_div_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    torch._foreach_add_(denom, eps)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     step_size_diff = lr * beta2 / bias_correction2
     step_size = lr / bias_correction1
 
     if caution:
         # Apply caution as per 'Cautious Optimizers' - https://arxiv.org/abs/2411.16085
-        masks = torch._foreach_mul(exp_avgs, grads)
+        masks = torch._foreach_mul(exp_avgs, grads)  # 'torch._foreach_mul' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         masks = [(m > 0).to(g.dtype) for m, g in zip(masks, grads)]
         mask_scale = [m.mean() for m in masks]
-        torch._foreach_maximum_(mask_scale, 1e-3)
-        torch._foreach_div_(masks, mask_scale)
-        exp_avgs = torch._foreach_mul(exp_avgs, masks)
+        torch._foreach_maximum_(mask_scale, 1e-3)  # 'torch._foreach_maximum_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_div_(masks, mask_scale)  # 'torch._foreach_div_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        exp_avgs = torch._foreach_mul(exp_avgs, masks)  # 'torch._foreach_mul' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
     if no_prox:
-        torch._foreach_mul_(params, 1 - lr * weight_decay)
-        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)
-        torch._foreach_addcdiv_(params, exp_avg_diffs, denom, value=-step_size_diff)
+        torch._foreach_mul_(params, 1 - lr * weight_decay)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)  # 'torch._foreach_addcdiv_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_addcdiv_(params, exp_avg_diffs, denom, value=-step_size_diff)  # 'torch._foreach_addcdiv_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     else:
-        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)
-        torch._foreach_addcdiv_(params, exp_avg_diffs, denom, value=-step_size_diff)
-        torch._foreach_div_(params, 1 + lr * weight_decay)
-
-    torch._foreach_zero_(neg_pre_grads)
-    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)
+        torch._foreach_addcdiv_(params, exp_avgs, denom, value=-step_size)  # 'torch._foreach_addcdiv_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_addcdiv_(params, exp_avg_diffs, denom, value=-step_size_diff)  # 'torch._foreach_addcdiv_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        torch._foreach_div_(params, 1 + lr * weight_decay)  # 'torch._foreach_div_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    torch._foreach_zero_(neg_pre_grads)  # 'torch._foreach_zero_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    torch._foreach_add_(neg_pre_grads, grads, alpha=-1.0)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
