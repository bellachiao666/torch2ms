--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ PyTorch impl of LaProp optimizer
 
 Code simplified from https://github.com/Z-T-WANG/LaProp-Optimizer, MIT License
@@ -18,12 +23,13 @@ """
 from typing import Tuple
 
-from torch.optim import Optimizer
-import torch
+# from torch.optim import Optimizer
+# import torch
 
 from ._types import ParamsT
 
 
+# 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class LaProp(Optimizer):
     """ LaProp Optimizer
 
@@ -63,7 +69,6 @@             group.setdefault('caution', False)
             group.setdefault('corrected_weight_decay', False)
 
-    @torch.no_grad()
     def step(self, closure=None):
         """Performs a single optimization step.
 
@@ -90,12 +95,12 @@                 if len(state) == 0:
                     state['step'] = 0
                     # Exponential moving average of gradient values
-                    state['exp_avg'] = torch.zeros_like(p)
+                    state['exp_avg'] = mint.zeros_like(p)
                     # Exponential moving average of learning rates
                     state['exp_avg_lr_1'] = 0.
                     state['exp_avg_lr_2'] = 0.
                     # Exponential moving average of squared gradient values
-                    state['exp_avg_sq'] = torch.zeros_like(p)
+                    state['exp_avg_sq'] = mint.zeros_like(p)
 
                 exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']
                 beta1, beta2 = group['betas']
