--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ PyTorch LARS / LARC Optimizer
 
 An implementation of LARS (SGD) + LARC in PyTorch
@@ -10,10 +15,11 @@ 
 Copyright 2021 Ross Wightman
 """
-import torch
-from torch.optim.optimizer import Optimizer
+# import torch
+# from torch.optim.optimizer import Optimizer
 
 
+# 'torch.optim.optimizer.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class Lars(Optimizer):
     """ LARS for PyTorch
     
@@ -105,13 +111,13 @@                     trust_ratio = trust_coeff * w_norm / (g_norm + w_norm * weight_decay + eps)
                     # FIXME nested where required since logical and/or not working in PT XLA
                     # Set the ratio to 1.0 (no change) if either weight norm or grad norm is zero
-                    trust_ratio = torch.where(
+                    trust_ratio = mint.where(
                         w_norm > 0,
-                        torch.where(g_norm > 0, trust_ratio, 1.0),
+                        mint.where(g_norm > 0, trust_ratio, 1.0),
                         1.0,
                     )
                     if group['trust_clip']:
-                        trust_ratio = torch.clamp(trust_ratio / group['lr'], max=1.0)
+                        trust_ratio = mint.clamp(trust_ratio / group['lr'], max=1.0)
                     grad.add_(p, alpha=weight_decay)
                     grad.mul_(trust_ratio)
 
@@ -119,7 +125,7 @@                 if momentum != 0:
                     param_state = self.state[p]
                     if 'momentum_buffer' not in param_state:
-                        buf = param_state['momentum_buffer'] = torch.clone(grad).detach()
+                        buf = param_state['momentum_buffer'] = mint.clone(grad).detach()
                     else:
                         buf = param_state['momentum_buffer']
                         buf.mul_(momentum).add_(grad, alpha=1. - dampening)
