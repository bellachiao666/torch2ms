--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ PyTorch MARS Optimizer
 
 Code simplified from https://github.com/AGI-Arena/MARS
@@ -16,22 +21,22 @@ import math
 from typing import Optional, Tuple
 
-import torch
-from torch.optim.optimizer import Optimizer
+# import torch
+# from torch.optim.optimizer import Optimizer
 
 from ._types import ParamsT
 
 
 def _mars_single_tensor_step(
-        p: torch.Tensor,
-        grad: torch.Tensor,
-        exp_avg: torch.Tensor,
-        exp_avg_sq: torch.Tensor,
+        p: ms.Tensor,
+        grad: ms.Tensor,
+        exp_avg: ms.Tensor,
+        exp_avg_sq: ms.Tensor,
         lr: float,
         weight_decay: float,
         beta1: float,
         beta2: float,
-        last_grad: torch.Tensor,
+        last_grad: ms.Tensor,
         eps: float,
         step: int,
         gamma: float,
@@ -50,7 +55,7 @@             c_t = grad
         else:
             c_t = (grad - last_grad).mul_(gamma * (beta1 / one_minus_beta1)).add_(grad)
-            c_t_norm = torch.norm(c_t)
+            c_t_norm = mint.norm(c_t)
             if c_t_norm > 1.:
                 c_t = c_t / c_t_norm
         exp_avg.mul_(beta1).add_(c_t, alpha=one_minus_beta1)
@@ -88,6 +93,7 @@     return exp_avg, exp_avg_sq
 
 
+# 'torch.optim.optimizer.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class Mars(Optimizer):
     """ MARS Optimizer
 
@@ -138,7 +144,6 @@         for group in self.param_groups:
             group.setdefault('caution', False)
 
-    @torch.no_grad()
     def step(self, closure=None):
         """Performs a single optimization step.
 
@@ -164,11 +169,11 @@                 if len(state) <= 1:
                     state['step'] = 0
                     # Exponential moving average of gradient values
-                    state['exp_avg'] = torch.zeros_like(p)
+                    state['exp_avg'] = mint.zeros_like(p)
                     # Last Gradient
-                    state['last_grad'] = torch.zeros_like(p)
+                    state['last_grad'] = mint.zeros_like(p)
                     # Exponential moving average of squared gradient values
-                    state['exp_avg_sq'] = torch.zeros_like(p)
+                    state['exp_avg_sq'] = mint.zeros_like(p)
 
                 state['step'] += 1
                 step = state['step']
