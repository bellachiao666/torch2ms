--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Muon Optimizer
 
 Improved Muon optimizer implementation with flexible handling of high-dimensional tensors.
@@ -24,7 +29,7 @@ import numbers
 from typing import List, Mapping, Optional, Sequence, Tuple, Union
 
-import torch
+# import torch
 
 from ._types import ParamsT
 from .adamw import adamw
@@ -104,15 +109,17 @@     return eps * (din / dout) ** 0.5
 
 
+# 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def zeropower_via_newtonschulz(
-        G: torch.Tensor,
+        G: ms.Tensor,
         steps: int,
         coefficients: List[Tuple[float, float, float]],
         eps: float = MUON_EPS,
         safety_factor: float = 1.0,
-        dtype: torch.dtype = torch.bfloat16,
+        dtype: torch.dtype = ms.bfloat16,
         scale_eps: bool = False,
-) -> torch.Tensor:
+) -> ms.Tensor:
     """Newton-Schulz quintic iteration to compute the zeroth power / orthogonalization of gradient.
 
     Supports batched operation over leading dimensions.
@@ -164,9 +171,9 @@ 
     # Pre-allocate
     X = X.contiguous()
-    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
-    B = torch.empty_like(A)
-    C = torch.empty_like(X)
+    A = mint.empty((*X.shape[:-1], X.size(-2)), dtype = X.dtype, device = X.device)
+    B = mint.empty_like(A)
+    C = mint.empty_like(X)
 
     # Perform Newton-Schulz iterations
     for a, b, c in coeff_sequence:
@@ -181,6 +188,8 @@     return X
 
 
+# 'torch.Size' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def get_lr_scale(
         param_shape: torch.Size,
         adjust_lr_fn: str = "match_rms_adamw",
@@ -210,6 +219,8 @@         assert False, f'Invalid scaling function "{adjust_lr_fn}" for Muon'
 
 
+# 'torch.Size' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def get_adamuon_lr_scale(
         param_shape: torch.Size,
         adjust_lr_fn: str = "match_rms_adamw",
@@ -238,7 +249,7 @@ 
 
 def _is_suitable_for_muon(
-        param: torch.Tensor,
+        param: ms.Tensor,
         min_dim_size: int = 4,
         max_aspect_ratio: float = 128.,
         return_reason: bool = False,
@@ -309,10 +320,12 @@     return (True, "ok") if return_reason else True
 
 
+# 'torch.Size' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+# 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def reshape_for_muon(
-        tensor: torch.Tensor,
+        tensor: ms.Tensor,
         mode: str = "flatten",
-) -> Tuple[torch.Tensor, torch.Size]:
+) -> Tuple[ms.Tensor, torch.Size]:
     """Reshape high-dimensional tensor for Muon processing.
 
     Args:
@@ -345,9 +358,9 @@ 
 
 def muon(
-        params: List[torch.Tensor],
-        grads: List[torch.Tensor],
-        momentum_bufs: List[torch.Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        momentum_bufs: List[ms.Tensor],
         *,
         lr: float,
         weight_decay: float,
@@ -383,11 +396,11 @@ 
 
 def adamuon(
-        params: List[torch.Tensor],
-        grads: List[torch.Tensor],
-        momentum_bufs: List[torch.Tensor],
-        exp_avg_sqs: List[torch.Tensor],
-        state_steps: List[torch.Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        momentum_bufs: List[ms.Tensor],
+        exp_avg_sqs: List[ms.Tensor],
+        state_steps: List[ms.Tensor],
         *,
         lr: float,
         weight_decay: float,
@@ -434,9 +447,9 @@ 
 
 def _single_tensor_muon(
-        params: List[torch.Tensor],
-        grads: List[torch.Tensor],
-        momentum_bufs: List[torch.Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        momentum_bufs: List[ms.Tensor],
         *,
         lr: float,
         weight_decay: float,
@@ -503,11 +516,11 @@ 
 
 def _single_tensor_adamuon(
-        params: List[torch.Tensor],
-        grads: List[torch.Tensor],
-        momentum_bufs: List[torch.Tensor],
-        exp_avg_sqs: List[torch.Tensor],
-        state_steps: List[torch.Tensor],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        momentum_bufs: List[ms.Tensor],
+        exp_avg_sqs: List[ms.Tensor],
+        state_steps: List[ms.Tensor],
         *,
         lr: float,
         weight_decay: float,
@@ -618,6 +631,7 @@         param.add_(update_adaptive, alpha=-lr * scale)
 
 
+# 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class Muon(torch.optim.Optimizer):
     """Muon - MomentUm Orthogonalized by Newton-schulz
 
@@ -728,7 +742,6 @@             group.setdefault('algo', 'muon')
             group.setdefault('scale_eps', False)
 
-    @torch.no_grad()
     def step(self, closure=None):
         """Performs a single optimization step."""
         loss = None
@@ -808,14 +821,14 @@ 
                     # State initialization for Muon/AdaMuon
                     if "momentum_buffer" not in state:
-                        state["momentum_buffer"] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                        state["momentum_buffer"] = mint.zeros_like(p)  # 'torch.zeros_like':没有对应的mindspore参数 'memory_format' (position 5);
                     muon_momentum_bufs.append(state["momentum_buffer"])
 
                     # Additional state for adamuon mode
                     if algo == "adamuon":
                         if "step" not in state:
-                            state["step"] = torch.tensor(0.)
-                            state["exp_avg_sq"] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                            state["step"] = ms.Tensor(0.)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;
+                            state["exp_avg_sq"] = mint.zeros_like(p)  # 'torch.zeros_like':没有对应的mindspore参数 'memory_format' (position 5);
                         muon_exp_avg_sqs.append(state["exp_avg_sq"])
                         muon_state_steps.append(state["step"])
                 else:
@@ -826,9 +839,9 @@ 
                     # State initialization for AdamW
                     if "step" not in state:
-                        state["step"] = torch.tensor(0.)
-                        state["exp_avg"] = torch.zeros_like(p, memory_format=torch.preserve_format)
-                        state["exp_avg_sq"] = torch.zeros_like(p, memory_format=torch.preserve_format)
+                        state["step"] = ms.Tensor(0.)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;
+                        state["exp_avg"] = mint.zeros_like(p)  # 'torch.zeros_like':没有对应的mindspore参数 'memory_format' (position 5);
+                        state["exp_avg_sq"] = mint.zeros_like(p)  # 'torch.zeros_like':没有对应的mindspore参数 'memory_format' (position 5);
 
                     adamw_exp_avgs.append(state["exp_avg"])
                     adamw_exp_avg_sqs.append(state["exp_avg_sq"])
