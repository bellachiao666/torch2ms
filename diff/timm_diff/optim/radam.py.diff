--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """RAdam Optimizer.
 Implementation lifted from: https://github.com/LiyuanLucasLiu/RAdam
 Paper: `On the Variance of the Adaptive Learning Rate and Beyond` - https://arxiv.org/abs/1908.03265
@@ -5,10 +10,11 @@ NOTE: This impl has been deprecated in favour of torch.optim.RAdam and remains as a reference
 """
 import math
-import torch
-from torch.optim.optimizer import Optimizer
+# import torch
+# from torch.optim.optimizer import Optimizer
 
 
+# 'torch.optim.optimizer.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class RAdamLegacy(Optimizer):
     """ PyTorch RAdam optimizer
 
@@ -56,8 +62,8 @@ 
                 if len(state) == 0:
                     state['step'] = 0
-                    state['exp_avg'] = torch.zeros_like(p_fp32)
-                    state['exp_avg_sq'] = torch.zeros_like(p_fp32)
+                    state['exp_avg'] = mint.zeros_like(p_fp32)
+                    state['exp_avg_sq'] = mint.zeros_like(p_fp32)
                 else:
                     state['exp_avg'] = state['exp_avg'].type_as(p_fp32)
                     state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_fp32)
