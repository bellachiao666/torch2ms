--- pytorch+++ mindspore@@ -1,3 +1,8 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ SGD with decoupled weight-decay.
 
 References for added functionality:
@@ -8,11 +13,11 @@ """
 from typing import List, Optional
 
-import torch
-from torch import Tensor
-from torch.optim.optimizer import Optimizer
+# import torch
+# from torch import Tensor
+# from torch.optim.optimizer import Optimizer
 try:
-    from torch.optim.optimizer import _use_grad_for_differentiable, _default_to_fused_or_foreach
+    # from torch.optim.optimizer import _default_to_fused_or_foreach
     has_recent_pt = True
 except ImportError:
     has_recent_pt = False
@@ -22,6 +27,7 @@ __all__ = ['SGDW', 'sgdw']
 
 
+# 'torch.optim.optimizer.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 class SGDW(Optimizer):
     def __init__(
             self,
@@ -91,7 +97,6 @@ 
     # FIXME figure out how to make _use_grad_for_differentiable interchangeable with no_grad decorator
     #   without args, for backwards compatibility with old pytorch
-    @torch.no_grad()
     def step(self, closure=None):
         """Performs a single optimization step.
 
@@ -136,9 +141,9 @@ 
 
 def sgdw(
-        params: List[Tensor],
-        grads: List[Tensor],
-        momentum_buffer_list: List[Optional[Tensor]],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        momentum_buffer_list: List[Optional[ms.Tensor]],
         # kwonly args with defaults are not supported by functions compiled with torchscript issue #70627
         # setting this as kwarg for now as functional API is compiled by torch/distributed/optim
         has_sparse_grad: bool = None,
@@ -162,7 +167,7 @@             # why must we be explicit about an if statement for torch.jit.is_scripting here?
             # because JIT can't handle Optionals nor fancy conditionals when scripting
             if not torch.jit.is_scripting():
-                _, foreach = _default_to_fused_or_foreach(params, differentiable=False, use_fused=False)
+                _, foreach = _default_to_fused_or_foreach(params, differentiable=False, use_fused=False)  # 'torch.optim.optimizer._default_to_fused_or_foreach' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
                 foreach = False
 
@@ -193,9 +198,9 @@ 
 
 def _single_tensor_sgdw(
-        params: List[Tensor],
-        grads: List[Tensor],
-        momentum_buffer_list: List[Optional[Tensor]],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        momentum_buffer_list: List[Optional[ms.Tensor]],
         *,
         weight_decay: float,
         momentum: float,
@@ -217,7 +222,7 @@             buf = momentum_buffer_list[i]
 
             if buf is None:
-                buf = torch.clone(grad).detach()
+                buf = mint.clone(grad).detach()
                 momentum_buffer_list[i] = buf
             else:
                 buf.mul_(momentum).add_(grad, alpha=1 - dampening)
@@ -239,9 +244,9 @@ 
 
 def _multi_tensor_sgdw(
-        params: List[Tensor],
-        grads: List[Tensor],
-        momentum_buffer_list: List[Optional[Tensor]],
+        params: List[ms.Tensor],
+        grads: List[ms.Tensor],
+        momentum_buffer_list: List[Optional[ms.Tensor]],
         *,
         weight_decay: float,
         momentum: float,
@@ -257,15 +262,15 @@         return
 
     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(
-        [params, grads, momentum_buffer_list], with_indices=True)
+        [params, grads, momentum_buffer_list], with_indices=True)  # 'torch.optim.optimizer.Optimizer._group_tensors_by_device_and_dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     for ((device_params, device_grads, device_momentum_buffer_list), indices) in grouped_tensors.values():
         device_has_sparse_grad = has_sparse_grad and any(grad.is_sparse for grad in device_grads)
 
         if maximize:
-            device_grads = torch._foreach_neg(device_grads)
+            device_grads = torch._foreach_neg(device_grads)  # 'torch._foreach_neg' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         wd_scale = lr if max_lr is None else lr ** 2 / max_lr
-        torch._foreach_mul_(params, 1. - wd_scale * weight_decay)
+        torch._foreach_mul_(params, 1. - wd_scale * weight_decay)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         if momentum != 0:
             bufs = []
@@ -279,14 +284,14 @@                     bufs.append(device_momentum_buffer_list[i])
 
             if all_states_with_momentum_buffer:
-                torch._foreach_mul_(bufs, momentum)
-                torch._foreach_add_(bufs, device_grads, alpha=1 - dampening)
+                torch._foreach_mul_(bufs, momentum)  # 'torch._foreach_mul_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                torch._foreach_add_(bufs, device_grads, alpha=1 - dampening)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
                 bufs = []
                 for i in range(len(device_momentum_buffer_list)):
                     if device_momentum_buffer_list[i] is None:
                         buf = device_momentum_buffer_list[i] = momentum_buffer_list[indices[i]] = \
-                            torch.clone(device_grads[i]).detach()
+                            mint.clone(device_grads[i]).detach()
                     else:
                         buf = device_momentum_buffer_list[i]
                         buf.mul_(momentum).add_(device_grads[i], alpha=1 - dampening)
@@ -296,22 +301,22 @@             if caution:
                 if nesterov:
                     # Can't do nesterov in-place if we want to compare against orig grad for caution
-                    bufs = torch._foreach_add(device_grads, bufs, alpha=momentum)
+                    bufs = torch._foreach_add(device_grads, bufs, alpha=momentum)  # 'torch._foreach_add' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 # Apply caution as per 'Cautious Optimizers' - https://arxiv.org/abs/2411.16085
-                masks = torch._foreach_mul(bufs, device_grads)
+                masks = torch._foreach_mul(bufs, device_grads)  # 'torch._foreach_mul' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 masks = [(m > 0).to(g.dtype) for m, g in zip(masks, device_grads)]
                 mask_scale = [m.mean() for m in masks]
-                torch._foreach_maximum_(mask_scale, 1e-3)
-                torch._foreach_div_(masks, mask_scale)
-                device_grads = torch._foreach_mul(bufs, masks)
+                torch._foreach_maximum_(mask_scale, 1e-3)  # 'torch._foreach_maximum_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                torch._foreach_div_(masks, mask_scale)  # 'torch._foreach_div_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+                device_grads = torch._foreach_mul(bufs, masks)  # 'torch._foreach_mul' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             else:
                 if nesterov:
-                    torch._foreach_add_(device_grads, bufs, alpha=momentum)
+                    torch._foreach_add_(device_grads, bufs, alpha=momentum)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
                 else:
                     device_grads = bufs
 
         if not device_has_sparse_grad:
-            torch._foreach_add_(device_params, device_grads, alpha=-lr)
+            torch._foreach_add_(device_params, device_grads, alpha=-lr)  # 'torch._foreach_add_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             # foreach APIs don't support sparse
             for i in range(len(device_params)):
