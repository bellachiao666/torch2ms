--- pytorch+++ mindspore@@ -1,9 +1,14 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Scheduler Factory
 Hacked together by / Copyright 2021 Ross Wightman
 """
 from typing import List, Optional, Union
 
-from torch.optim import Optimizer
+# from torch.optim import Optimizer
 
 from .cosine_lr import CosineLRScheduler
 from .multistep_lr import MultiStepLRScheduler
@@ -57,7 +62,7 @@         optimizer=optimizer,
         **scheduler_kwargs(args),
         updates_per_epoch=updates_per_epoch,
-    )
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
 
 def create_scheduler_v2(
@@ -140,7 +145,7 @@             **warmup_args,
             **noise_args,
             k_decay=k_decay,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif sched == 'tanh':
         lr_scheduler = TanhLRScheduler(
             optimizer,
@@ -150,7 +155,7 @@             **cycle_args,
             **warmup_args,
             **noise_args,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif sched == 'step':
         lr_scheduler = StepLRScheduler(
             optimizer,
@@ -159,7 +164,7 @@             t_in_epochs=step_on_epochs,
             **warmup_args,
             **noise_args,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif sched == 'multistep':
         lr_scheduler = MultiStepLRScheduler(
             optimizer,
@@ -168,7 +173,7 @@             t_in_epochs=step_on_epochs,
             **warmup_args,
             **noise_args,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif sched == 'plateau':
         assert step_on_epochs, 'Plateau LR only supports step per epoch.'
         warmup_args.pop('warmup_prefix', False)
@@ -181,7 +186,7 @@             lr_min=min_lr,
             mode=plateau_mode,
             **noise_args,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif sched == 'poly':
         lr_scheduler = PolyLRScheduler(
             optimizer,
@@ -193,7 +198,7 @@             **cycle_args,
             **warmup_args,
             **noise_args,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     if hasattr(lr_scheduler, 'get_cycle_length'):
         # For cycle based schedulers (cosine, tanh, poly) recalculate total epochs w/ cycles & cooldown
