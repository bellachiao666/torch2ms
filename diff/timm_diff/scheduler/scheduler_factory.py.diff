--- pytorch+++ mindspore@@ -1,9 +1,14 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """ Scheduler Factory
 Hacked together by / Copyright 2021 Ross Wightman
 """
 from typing import List, Optional, Union
 
-from torch.optim import Optimizer
+# from torch.optim import Optimizer
 
 from .cosine_lr import CosineLRScheduler
 from .multistep_lr import MultiStepLRScheduler
@@ -48,6 +53,7 @@     return kwargs
 
 
+# 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def create_scheduler(
         args,
         optimizer: Optimizer,
@@ -57,9 +63,10 @@         optimizer=optimizer,
         **scheduler_kwargs(args),
         updates_per_epoch=updates_per_epoch,
-    )
-
-
+    )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
+
+
+# 'torch.optim.Optimizer' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 def create_scheduler_v2(
         optimizer: Optimizer,
         sched: str = 'cosine',
@@ -140,7 +147,7 @@             **warmup_args,
             **noise_args,
             k_decay=k_decay,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif sched == 'tanh':
         lr_scheduler = TanhLRScheduler(
             optimizer,
@@ -150,7 +157,7 @@             **cycle_args,
             **warmup_args,
             **noise_args,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif sched == 'step':
         lr_scheduler = StepLRScheduler(
             optimizer,
@@ -159,7 +166,7 @@             t_in_epochs=step_on_epochs,
             **warmup_args,
             **noise_args,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif sched == 'multistep':
         lr_scheduler = MultiStepLRScheduler(
             optimizer,
@@ -168,7 +175,7 @@             t_in_epochs=step_on_epochs,
             **warmup_args,
             **noise_args,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif sched == 'plateau':
         assert step_on_epochs, 'Plateau LR only supports step per epoch.'
         warmup_args.pop('warmup_prefix', False)
@@ -181,7 +188,7 @@             lr_min=min_lr,
             mode=plateau_mode,
             **noise_args,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
     elif sched == 'poly':
         lr_scheduler = PolyLRScheduler(
             optimizer,
@@ -193,7 +200,7 @@             **cycle_args,
             **warmup_args,
             **noise_args,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
     if hasattr(lr_scheduler, 'get_cycle_length'):
         # For cycle based schedulers (cosine, tanh, poly) recalculate total epochs w/ cycles & cooldown
