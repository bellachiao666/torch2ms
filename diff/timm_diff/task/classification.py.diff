--- pytorch+++ mindspore@@ -1,9 +1,13 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """Classification training task."""
 import logging
 from typing import Callable, Dict, Optional, Union
 
-import torch
-import torch.nn as nn
+# import torch
 
 from .task import TrainingTask
 
@@ -29,10 +33,13 @@         >>> result['loss'].backward()
     """
 
+    # 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
-            model: nn.Module,
-            criterion: Union[nn.Module, Callable],
+            model: msnn.Cell,
+            criterion: Union[msnn.Cell, Callable],
             device: Optional[torch.device] = None,
             dtype: Optional[torch.dtype] = None,
             verbose: bool = True,
@@ -61,15 +68,15 @@         Returns:
             self (for method chaining)
         """
-        from torch.nn.parallel import DistributedDataParallel as DDP
-        self.model = DDP(self.model, device_ids=device_ids, **ddp_kwargs)
+        # from torch.nn.parallel import DistributedDataParallel as DDP
+        self.model = DDP(self.model, device_ids=device_ids, **ddp_kwargs)  # 'torch.nn.parallel.DistributedDataParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return self
 
     def forward(
             self,
-            input: torch.Tensor,
-            target: torch.Tensor,
-    ) -> Dict[str, torch.Tensor]:
+            input: ms.Tensor,
+            target: ms.Tensor,
+    ) -> Dict[str, ms.Tensor]:
         """Forward pass through model and compute classification loss.
 
         Args:
