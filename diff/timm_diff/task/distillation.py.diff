--- pytorch+++ mindspore@@ -1,10 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """Knowledge distillation training tasks and components."""
 import logging
 from typing import Dict, Optional, Tuple
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from timm.models import create_model
 from timm.utils import unwrap_model
@@ -15,7 +20,7 @@ _logger = logging.getLogger(__name__)
 
 
-class DistillationTeacher(nn.Module):
+class DistillationTeacher(msnn.Cell):
     """Wrapper for a teacher model used in knowledge distillation.
 
     Creates and manages a pre-trained teacher model for knowledge distillation,
@@ -58,23 +63,23 @@             device=device,
             dtype=dtype,
             **pretrained_kwargs,
-        )
+        )  # 存在 *args/**kwargs，未转换，需手动确认参数映射;
 
         model_kd.eval()
         self.model = model_kd
 
         # Register normalization values as non-persistent buffers
         # Shape: [1, 3, 1, 1] for proper broadcasting over BCHW images
-        mean_kd = torch.tensor(model_kd.pretrained_cfg['mean'], device=device, dtype=dtype).view(1, -1, 1, 1)
-        std_kd = torch.tensor(model_kd.pretrained_cfg['std'], device=device, dtype=dtype).view(1, -1, 1, 1)
+        mean_kd = ms.Tensor(model_kd.pretrained_cfg['mean'], device=device, dtype=dtype).view(1, -1, 1, 1)
+        std_kd = ms.Tensor(model_kd.pretrained_cfg['std'], device=device, dtype=dtype).view(1, -1, 1, 1)
         self.register_buffer('mean_kd', mean_kd, persistent=False)
         self.register_buffer('std_kd', std_kd, persistent=False)
 
-    def forward(
-            self,
-            input: torch.Tensor,
+    def construct(
+            self,
+            input: ms.Tensor,
             return_features: bool = False,
-    ) -> torch.Tensor:
+    ) -> ms.Tensor:
         """Forward pass through teacher model.
 
         Args:
@@ -98,10 +103,10 @@ 
     def normalize_input(
             self,
-            input: torch.Tensor,
-            student_mean: Optional[torch.Tensor] = None,
-            student_std: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
+            input: ms.Tensor,
+            student_mean: Optional[ms.Tensor] = None,
+            student_std: Optional[ms.Tensor] = None,
+    ) -> ms.Tensor:
         """Normalize input to match teacher's expected normalization.
 
         Handles different normalization between teacher and student models by
@@ -120,7 +125,7 @@             return input
 
         # Check if renormalization is actually needed
-        if torch.equal(student_mean, self.mean_kd) and torch.equal(student_std, self.std_kd):
+        if mint.equal(student_mean, self.mean_kd) and mint.equal(student_std, self.std_kd):
             return input
 
         # De-normalize (Student) -> Re-normalize (Teacher)
@@ -168,9 +173,9 @@ 
     def __init__(
             self,
-            student_model: nn.Module,
+            student_model: msnn.Cell,
             teacher: DistillationTeacher,
-            criterion: nn.Module,
+            criterion: msnn.Cell,
             loss_type: str = 'kl',
             distill_loss_weight: Optional[float] = None,
             task_loss_weight: Optional[float] = None,
@@ -192,12 +197,12 @@         # Register student normalization values as non-persistent buffers
         # Shape: [1, 3, 1, 1] for proper broadcasting over BCHW images
         student_unwrapped = unwrap_model(student_model)
-        student_mean = torch.tensor(
+        student_mean = ms.Tensor(
             student_unwrapped.pretrained_cfg['mean'],
             device=self.device,
             dtype=self.dtype,
         ).view(1, -1, 1, 1)
-        student_std = torch.tensor(
+        student_std = ms.Tensor(
             student_unwrapped.pretrained_cfg['std'],
             device=self.device,
             dtype=self.dtype,
@@ -256,21 +261,21 @@         Returns:
             self (for method chaining)
         """
-        from torch.nn.parallel import DistributedDataParallel as DDP
+        # from torch.nn.parallel import DistributedDataParallel as DDP
 
         # Ensure teacher parameters are frozen
         for param in self.teacher.parameters():
             param.requires_grad = False
 
         # Wrap only student in DDP
-        self.student = DDP(self.student, device_ids=device_ids, **ddp_kwargs)
+        self.student = DDP(self.student, device_ids=device_ids, **ddp_kwargs)  # 'torch.nn.parallel.DistributedDataParallel.DDP' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         return self
 
     def forward(
             self,
-            input: torch.Tensor,
-            target: torch.Tensor,
-    ) -> Dict[str, torch.Tensor]:
+            input: ms.Tensor,
+            target: ms.Tensor,
+    ) -> Dict[str, ms.Tensor]:
         """Forward pass with logit distillation.
 
         Args:
@@ -296,9 +301,9 @@             teacher_logits = self.teacher(input_kd.detach(), return_features=False)
 
         # Compute distillation loss (KL divergence with temperature scaling)
-        prob_s = F.log_softmax(student_logits / self.temperature, dim=-1)
-        prob_t = F.log_softmax(teacher_logits / self.temperature, dim=-1)
-        kd_loss = F.kl_div(prob_s, prob_t, reduction='batchmean', log_target=True) * (self.temperature ** 2)
+        prob_s = mint.special.log_softmax(student_logits / self.temperature, dim=-1)
+        prob_t = mint.special.log_softmax(teacher_logits / self.temperature, dim=-1)
+        kd_loss = F.kl_div(prob_s, prob_t, reduction='batchmean', log_target=True) * (self.temperature ** 2)  # 'torch.nn.functional.kl_div' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # Combine losses with weights
         total_loss = self.task_loss_weight * task_loss + self.distill_loss_weight * kd_loss
@@ -311,7 +316,7 @@         }
 
 
-class FeatureDistillationTrainableModule(nn.Module):
+class FeatureDistillationTrainableModule(msnn.Cell):
     """Trainable module for feature distillation.
 
     Wraps student model and projection layer into a single module where all
@@ -328,14 +333,14 @@ 
     def __init__(
             self,
-            student_model: nn.Module,
-            projection: Optional[nn.Module] = None,
+            student_model: msnn.Cell,
+            projection: Optional[msnn.Cell] = None,
     ):
         super().__init__()
         self.student = student_model
         self.projection = projection
 
-    def forward(self, input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
+    def construct(self, input: ms.Tensor) -> Tuple[ms.Tensor, ms.Tensor]:
         """Forward pass through student and projection.
 
         Args:
@@ -397,9 +402,9 @@ 
     def __init__(
             self,
-            student_model: nn.Module,
+            student_model: msnn.Cell,
             teacher: DistillationTeacher,
-            criterion: nn.Module,
+            criterion: msnn.Cell,
             distill_loss_weight: Optional[float] = None,
             task_loss_weight: Optional[float] = None,
             student_feature_dim: Optional[int] = None,
@@ -454,7 +459,7 @@                 _logger.info(
                     f"Creating projection layer: {student_feature_dim} -> {teacher_feature_dim}"
                 )
-            projection = nn.Linear(student_feature_dim, teacher_feature_dim, device=self.device, dtype=self.dtype)
+            projection = nn.Linear(student_feature_dim, teacher_feature_dim, dtype = self.dtype)  # 'torch.nn.Linear':没有对应的mindspore参数 'device' (position 3);
         else:
             if self.verbose:
                 _logger.info("Feature dimensions match, no projection needed")
@@ -465,12 +470,12 @@         # Register student normalization values as non-persistent buffers
         # Shape: [1, 3, 1, 1] for proper broadcasting over BCHW images
         student_unwrapped = unwrap_model(student_model)
-        student_mean = torch.tensor(
+        student_mean = ms.Tensor(
             student_unwrapped.pretrained_cfg['mean'],
             device=self.device,
             dtype=self.dtype,
         ).view(1, -1, 1, 1)
-        student_std = torch.tensor(
+        student_std = ms.Tensor(
             student_unwrapped.pretrained_cfg['std'],
             device=self.device,
             dtype=self.dtype,
@@ -485,7 +490,7 @@             )
 
     @staticmethod
-    def _detect_feature_dim(model: nn.Module) -> int:
+    def _detect_feature_dim(model: msnn.Cell) -> int:
         """Auto-detect feature dimension from model.
 
         Tries head_hidden_size first (pre-logits dimension), then num_features.
@@ -521,21 +526,21 @@         Returns:
             self (for method chaining)
         """
-        from torch.nn.parallel import DistributedDataParallel as DDP
+        # from torch.nn.parallel import DistributedDataParallel as DDP
 
         # Ensure teacher parameters are frozen
         for param in self.teacher.parameters():
             param.requires_grad = False
 
         # Wrap trainable module (student + projection) in DDP
-        self.trainable_module = DDP(self.trainable_module, device_ids=device_ids, **ddp_kwargs)
+        self.trainable_module = DDP(self.trainable_module, device_ids=device_ids, **ddp_kwargs)  # 'torch.nn.parallel.DistributedDataParallel.DDP' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 存在 *args/**kwargs，未转换，需手动确认参数映射;
         return self
 
     def forward(
             self,
-            input: torch.Tensor,
-            target: torch.Tensor,
-    ) -> Dict[str, torch.Tensor]:
+            input: ms.Tensor,
+            target: ms.Tensor,
+    ) -> Dict[str, ms.Tensor]:
         """Forward pass with feature distillation.
 
         Args:
@@ -561,7 +566,7 @@             teacher_features = self.teacher(input_kd.detach(), return_features=True)
 
         # Compute feature distillation loss (MSE)
-        kd_loss = F.mse_loss(student_features, teacher_features)
+        kd_loss = nn.functional.mse_loss(student_features, teacher_features)
 
         # Combine losses with weights
         total_loss = self.task_loss_weight * task_loss + self.distill_loss_weight * kd_loss
