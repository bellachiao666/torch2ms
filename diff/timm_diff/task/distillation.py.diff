--- pytorch+++ mindspore@@ -1,10 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 """Knowledge distillation training tasks and components."""
 import logging
 from typing import Dict, Optional, Tuple
 
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
+# import torch
+# import torch.nn as nn
+# import torch.nn.functional as F
 
 from timm.models import create_model
 from timm.utils import unwrap_model
@@ -15,7 +20,7 @@ _logger = logging.getLogger(__name__)
 
 
-class DistillationTeacher(nn.Module):
+class DistillationTeacher(msnn.Cell):
     """Wrapper for a teacher model used in knowledge distillation.
 
     Creates and manages a pre-trained teacher model for knowledge distillation,
@@ -30,6 +35,8 @@         dtype: Model dtype (uses float32 if None)
     """
 
+    # 类型标注 'torch.device' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    # 类型标注 'torch.dtype' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             model_name: str,
@@ -65,16 +72,16 @@ 
         # Register normalization values as non-persistent buffers
         # Shape: [1, 3, 1, 1] for proper broadcasting over BCHW images
-        mean_kd = torch.tensor(model_kd.pretrained_cfg['mean'], device=device, dtype=dtype).view(1, -1, 1, 1)
-        std_kd = torch.tensor(model_kd.pretrained_cfg['std'], device=device, dtype=dtype).view(1, -1, 1, 1)
+        mean_kd = ms.Tensor(model_kd.pretrained_cfg['mean'], dtype = dtype).view(1, -1, 1, 1)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;; 'torch.tensor':没有对应的mindspore参数 'device' (position 2);
+        std_kd = ms.Tensor(model_kd.pretrained_cfg['std'], dtype = dtype).view(1, -1, 1, 1)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;; 'torch.tensor':没有对应的mindspore参数 'device' (position 2);
         self.register_buffer('mean_kd', mean_kd, persistent=False)
         self.register_buffer('std_kd', std_kd, persistent=False)
 
-    def forward(
-            self,
-            input: torch.Tensor,
+    def construct(
+            self,
+            input: ms.Tensor,
             return_features: bool = False,
-    ) -> torch.Tensor:
+    ) -> ms.Tensor:
         """Forward pass through teacher model.
 
         Args:
@@ -98,10 +105,10 @@ 
     def normalize_input(
             self,
-            input: torch.Tensor,
+            input: ms.Tensor,
             student_mean: Optional[torch.Tensor] = None,
             student_std: Optional[torch.Tensor] = None,
-    ) -> torch.Tensor:
+    ) -> ms.Tensor:
         """Normalize input to match teacher's expected normalization.
 
         Handles different normalization between teacher and student models by
@@ -120,7 +127,7 @@             return input
 
         # Check if renormalization is actually needed
-        if torch.equal(student_mean, self.mean_kd) and torch.equal(student_std, self.std_kd):
+        if mint.equal(student_mean, self.mean_kd) and mint.equal(student_std, self.std_kd):
             return input
 
         # De-normalize (Student) -> Re-normalize (Teacher)
@@ -166,6 +173,7 @@         ... )
     """
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             student_model: nn.Module,
@@ -192,16 +200,10 @@         # Register student normalization values as non-persistent buffers
         # Shape: [1, 3, 1, 1] for proper broadcasting over BCHW images
         student_unwrapped = unwrap_model(student_model)
-        student_mean = torch.tensor(
-            student_unwrapped.pretrained_cfg['mean'],
-            device=self.device,
-            dtype=self.dtype,
-        ).view(1, -1, 1, 1)
-        student_std = torch.tensor(
-            student_unwrapped.pretrained_cfg['std'],
-            device=self.device,
-            dtype=self.dtype,
-        ).view(1, -1, 1, 1)
+        student_mean = ms.Tensor(
+            student_unwrapped.pretrained_cfg['mean'], dtype = self.dtype).view(1, -1, 1, 1)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;; 'torch.tensor':没有对应的mindspore参数 'device' (position 2);
+        student_std = ms.Tensor(
+            student_unwrapped.pretrained_cfg['std'], dtype = self.dtype).view(1, -1, 1, 1)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;; 'torch.tensor':没有对应的mindspore参数 'device' (position 2);
         self.register_buffer('student_mean', student_mean, persistent=False)
         self.register_buffer('student_std', student_std, persistent=False)
 
@@ -256,20 +258,20 @@         Returns:
             self (for method chaining)
         """
-        from torch.nn.parallel import DistributedDataParallel as DDP
+        # from torch.nn.parallel import DistributedDataParallel as DDP
 
         # Ensure teacher parameters are frozen
         for param in self.teacher.parameters():
             param.requires_grad = False
 
         # Wrap only student in DDP
-        self.student = DDP(self.student, device_ids=device_ids, **ddp_kwargs)
+        self.student = DDP(self.student, device_ids=device_ids, **ddp_kwargs)  # 'torch.nn.parallel.DistributedDataParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return self
 
     def forward(
             self,
-            input: torch.Tensor,
-            target: torch.Tensor,
+            input: ms.Tensor,
+            target: ms.Tensor,
     ) -> Dict[str, torch.Tensor]:
         """Forward pass with logit distillation.
 
@@ -296,9 +298,9 @@             teacher_logits = self.teacher(input_kd.detach(), return_features=False)
 
         # Compute distillation loss (KL divergence with temperature scaling)
-        prob_s = F.log_softmax(student_logits / self.temperature, dim=-1)
-        prob_t = F.log_softmax(teacher_logits / self.temperature, dim=-1)
-        kd_loss = F.kl_div(prob_s, prob_t, reduction='batchmean', log_target=True) * (self.temperature ** 2)
+        prob_s = mint.special.log_softmax(student_logits / self.temperature, dim = -1)
+        prob_t = mint.special.log_softmax(teacher_logits / self.temperature, dim = -1)
+        kd_loss = F.kl_div(prob_s, prob_t, reduction='batchmean', log_target=True) * (self.temperature ** 2)  # 'torch.nn.functional.kl_div' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # Combine losses with weights
         total_loss = self.task_loss_weight * task_loss + self.distill_loss_weight * kd_loss
@@ -311,7 +313,7 @@         }
 
 
-class FeatureDistillationTrainableModule(nn.Module):
+class FeatureDistillationTrainableModule(msnn.Cell):
     """Trainable module for feature distillation.
 
     Wraps student model and projection layer into a single module where all
@@ -326,6 +328,7 @@         Tuple of (logits, projected_features)
     """
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             student_model: nn.Module,
@@ -335,7 +338,7 @@         self.student = student_model
         self.projection = projection
 
-    def forward(self, input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
+    def construct(self, input: ms.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
         """Forward pass through student and projection.
 
         Args:
@@ -395,6 +398,7 @@         ... )
     """
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     def __init__(
             self,
             student_model: nn.Module,
@@ -454,7 +458,7 @@                 _logger.info(
                     f"Creating projection layer: {student_feature_dim} -> {teacher_feature_dim}"
                 )
-            projection = nn.Linear(student_feature_dim, teacher_feature_dim, device=self.device, dtype=self.dtype)
+            projection = nn.Linear(student_feature_dim, teacher_feature_dim, dtype = self.dtype)  # 'torch.nn.Linear':没有对应的mindspore参数 'device' (position 3);
         else:
             if self.verbose:
                 _logger.info("Feature dimensions match, no projection needed")
@@ -465,16 +469,10 @@         # Register student normalization values as non-persistent buffers
         # Shape: [1, 3, 1, 1] for proper broadcasting over BCHW images
         student_unwrapped = unwrap_model(student_model)
-        student_mean = torch.tensor(
-            student_unwrapped.pretrained_cfg['mean'],
-            device=self.device,
-            dtype=self.dtype,
-        ).view(1, -1, 1, 1)
-        student_std = torch.tensor(
-            student_unwrapped.pretrained_cfg['std'],
-            device=self.device,
-            dtype=self.dtype,
-        ).view(1, -1, 1, 1)
+        student_mean = ms.Tensor(
+            student_unwrapped.pretrained_cfg['mean'], dtype = self.dtype).view(1, -1, 1, 1)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;; 'torch.tensor':没有对应的mindspore参数 'device' (position 2);
+        student_std = ms.Tensor(
+            student_unwrapped.pretrained_cfg['std'], dtype = self.dtype).view(1, -1, 1, 1)  # 'torch.tensor':默认参数名不一致(position 0): PyTorch=data, MindSpore=input_data;; 'torch.tensor':没有对应的mindspore参数 'device' (position 2);
         self.register_buffer('student_mean', student_mean, persistent=False)
         self.register_buffer('student_std', student_std, persistent=False)
 
@@ -484,6 +482,7 @@                 f"student_dim={student_feature_dim}, teacher_dim={teacher_feature_dim}"
             )
 
+    # 类型标注 'torch.nn.Module' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
     @staticmethod
     def _detect_feature_dim(model: nn.Module) -> int:
         """Auto-detect feature dimension from model.
@@ -521,20 +520,20 @@         Returns:
             self (for method chaining)
         """
-        from torch.nn.parallel import DistributedDataParallel as DDP
+        # from torch.nn.parallel import DistributedDataParallel as DDP
 
         # Ensure teacher parameters are frozen
         for param in self.teacher.parameters():
             param.requires_grad = False
 
         # Wrap trainable module (student + projection) in DDP
-        self.trainable_module = DDP(self.trainable_module, device_ids=device_ids, **ddp_kwargs)
+        self.trainable_module = DDP(self.trainable_module, device_ids=device_ids, **ddp_kwargs)  # 'torch.nn.parallel.DistributedDataParallel' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return self
 
     def forward(
             self,
-            input: torch.Tensor,
-            target: torch.Tensor,
+            input: ms.Tensor,
+            target: ms.Tensor,
     ) -> Dict[str, torch.Tensor]:
         """Forward pass with feature distillation.
 
@@ -561,7 +560,7 @@             teacher_features = self.teacher(input_kd.detach(), return_features=True)
 
         # Compute feature distillation loss (MSE)
-        kd_loss = F.mse_loss(student_features, teacher_features)
+        kd_loss = nn.functional.mse_loss(student_features, teacher_features)
 
         # Combine losses with weights
         total_loss = self.task_loss_weight * task_loss + self.distill_loss_weight * kd_loss
