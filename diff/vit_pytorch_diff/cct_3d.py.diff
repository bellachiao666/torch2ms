--- pytorch+++ mindspore@@ -1,6 +1,11 @@-import torch
-from torch import nn, einsum
-import torch.nn.functional as F
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# import torch
+# from torch import nn, einsum
+# import torch.nn.functional as F
 
 from einops import rearrange, repeat
 
@@ -74,26 +79,26 @@ 
 def sinusoidal_embedding(n_channels, dim):
     pe = torch.FloatTensor([[p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]
-                            for p in range(n_channels)])
-    pe[:, 0::2] = torch.sin(pe[:, 0::2])
-    pe[:, 1::2] = torch.cos(pe[:, 1::2])
+                            for p in range(n_channels)])  # 'torch.FloatTensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+    pe[:, 0::2] = mint.sin(pe[:, 0::2])
+    pe[:, 1::2] = mint.cos(pe[:, 1::2])
     return rearrange(pe, '... -> 1 ...')
 
 # modules
 
-class Attention(nn.Module):
+class Attention(msnn.Cell):
     def __init__(self, dim, num_heads=8, attention_dropout=0.1, projection_dropout=0.1):
         super().__init__()
         self.heads = num_heads
         head_dim = dim // self.heads
         self.scale = head_dim ** -0.5
 
-        self.qkv = nn.Linear(dim, dim * 3, bias=False)
+        self.qkv = nn.Linear(dim, dim * 3, bias = False)
         self.attn_drop = nn.Dropout(attention_dropout)
         self.proj = nn.Linear(dim, dim)
         self.proj_drop = nn.Dropout(projection_dropout)
 
-    def forward(self, x):
+    def construct(self, x):
         B, N, C = x.shape
 
         qkv = self.qkv(x).chunk(3, dim = -1)
@@ -101,17 +106,17 @@ 
         q = q * self.scale
 
-        attn = einsum('b h i d, b h j d -> b h i j', q, k)
+        attn = mint.einsum('b h i d, b h j d -> b h i j', q, k)
         attn = attn.softmax(dim=-1)
         attn = self.attn_drop(attn)
 
-        x = einsum('b h i j, b h j d -> b h i d', attn, v)
+        x = mint.einsum('b h i j, b h j d -> b h i d', attn, v)
         x = rearrange(x, 'b h n d -> b n (h d)')
 
         return self.proj_drop(self.proj(x))
 
 
-class TransformerEncoderLayer(nn.Module):
+class TransformerEncoderLayer(msnn.Cell):
     """
     Inspired by torch.nn.TransformerEncoderLayer and
     rwightman's timm package.
@@ -134,19 +139,19 @@ 
         self.activation = F.gelu
 
-    def forward(self, src, *args, **kwargs):
+    def construct(self, src, *args, **kwargs):
         src = src + self.drop_path(self.self_attn(self.pre_norm(src)))
         src = self.norm1(src)
         src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))
         src = src + self.drop_path(self.dropout2(src2))
         return src
 
-class DropPath(nn.Module):
+class DropPath(msnn.Cell):
     def __init__(self, drop_prob=None):
         super().__init__()
         self.drop_prob = float(drop_prob)
 
-    def forward(self, x):
+    def construct(self, x):
         batch, drop_prob, device, dtype = x.shape[0], self.drop_prob, x.device, x.dtype
 
         if drop_prob <= 0. or not self.training:
@@ -155,11 +160,11 @@         keep_prob = 1 - self.drop_prob
         shape = (batch, *((1,) * (x.ndim - 1)))
 
-        keep_mask = torch.zeros(shape, device = device).float().uniform_(0, 1) < keep_prob
+        keep_mask = mint.zeros(shape).float().uniform_(0, 1) < keep_prob  # 'torch.zeros':没有对应的mindspore参数 'device' (position 4);
         output = x.div(keep_prob) * keep_mask.float()
         return output
 
-class Tokenizer(nn.Module):
+class Tokenizer(msnn.Cell):
     def __init__(
         self,
         frame_kernel_size,
@@ -196,36 +201,36 @@         if frame_pooling_padding is None:
             frame_pooling_padding = frame_pooling_kernel_size // 2
 
-        self.conv_layers = nn.Sequential(
-            *[nn.Sequential(
-                nn.Conv3d(chan_in, chan_out,
-                          kernel_size=(frame_kernel_size, kernel_size, kernel_size),
-                          stride=(frame_stride, stride, stride),
-                          padding=(frame_padding, padding, padding), bias=conv_bias),
-                nn.Identity() if not exists(activation) else activation(),
-                nn.MaxPool3d(kernel_size=(frame_pooling_kernel_size, pooling_kernel_size, pooling_kernel_size),
+        self.conv_layers = msnn.SequentialCell(
+            [
+            [msnn.SequentialCell(
+                [
+            nn.Conv3d(chan_in, chan_out, kernel_size = (frame_kernel_size, kernel_size, kernel_size), stride = (frame_stride, stride, stride), padding = (frame_padding, padding, padding), bias = conv_bias),
+            msnn.Identity() if not exists(activation) else activation(),
+            nn.MaxPool3d(kernel_size=(frame_pooling_kernel_size, pooling_kernel_size, pooling_kernel_size),
                              stride=(frame_pooling_stride, pooling_stride, pooling_stride),
-                             padding=(frame_pooling_padding, pooling_padding, pooling_padding)) if max_pool else nn.Identity()
-            )
+                             padding=(frame_pooling_padding, pooling_padding, pooling_padding)) if max_pool else msnn.Identity()
+        ])
                 for chan_in, chan_out in n_filter_list_pairs
-            ])
+            ]
+        ])  # 'torch.nn.MaxPool3d' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         self.apply(self.init_weight)
 
     def sequence_length(self, n_channels=3, frames=8, height=224, width=224):
-        return self.forward(torch.zeros((1, n_channels, frames, height, width))).shape[1]
-
-    def forward(self, x):
+        return self.forward(mint.zeros((1, n_channels, frames, height, width))).shape[1]
+
+    def construct(self, x):
         x = self.conv_layers(x)
         return rearrange(x, 'b c f h w -> b (f h w) c')
 
     @staticmethod
     def init_weight(m):
         if isinstance(m, nn.Conv3d):
-            nn.init.kaiming_normal_(m.weight)
-
-
-class TransformerClassifier(nn.Module):
+            nn.init.kaiming_normal_(m.weight)  # 'torch.nn.init.kaiming_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+
+class TransformerClassifier(msnn.Cell):
     def __init__(
         self,
         seq_pool=True,
@@ -255,23 +260,23 @@ 
         if not seq_pool:
             sequence_length += 1
-            self.class_emb = nn.Parameter(torch.zeros(1, 1, self.embedding_dim))
+            self.class_emb = ms.Parameter(mint.zeros(size = (1, 1, self.embedding_dim)))
         else:
             self.attention_pool = nn.Linear(self.embedding_dim, 1)
 
         if positional_embedding == 'none':
             self.positional_emb = None
         elif positional_embedding == 'learnable':
-            self.positional_emb = nn.Parameter(torch.zeros(1, sequence_length, embedding_dim))
-            nn.init.trunc_normal_(self.positional_emb, std = 0.2)
+            self.positional_emb = ms.Parameter(mint.zeros(size = (1, sequence_length, embedding_dim)))
+            nn.init.trunc_normal_(self.positional_emb, std = 0.2)  # 'torch.nn.init.trunc_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         else:
             self.register_buffer('positional_emb', sinusoidal_embedding(sequence_length, embedding_dim))
 
-        self.dropout = nn.Dropout(p=dropout_rate)
-
-        dpr = [x.item() for x in torch.linspace(0, stochastic_depth_rate, num_layers)]
-
-        self.blocks = nn.ModuleList([
+        self.dropout = nn.Dropout(p = dropout_rate)
+
+        dpr = [x.item() for x in mint.linspace(0, stochastic_depth_rate, num_layers)]
+
+        self.blocks = msnn.CellList([
             TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,
                                     dim_feedforward=dim_feedforward, dropout=dropout_rate,
                                     attention_dropout=attention_dropout, drop_path_rate=layer_dpr)
@@ -285,22 +290,22 @@     @staticmethod
     def init_weight(m):
         if isinstance(m, nn.Linear):
-            nn.init.trunc_normal_(m.weight, std=.02)
+            nn.init.trunc_normal_(m.weight, std=.02)  # 'torch.nn.init.trunc_normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
             if isinstance(m, nn.Linear) and exists(m.bias):
-                nn.init.constant_(m.bias, 0)
+                nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         elif isinstance(m, nn.LayerNorm):
-            nn.init.constant_(m.bias, 0)
-            nn.init.constant_(m.weight, 1.0)
-
-    def forward(self, x):
+            nn.init.constant_(m.bias, 0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+            nn.init.constant_(m.weight, 1.0)  # 'torch.nn.init.constant_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x):
         b = x.shape[0]
 
         if not exists(self.positional_emb) and x.size(1) < self.sequence_length:
-            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)
+            x = nn.functional.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode = 'constant', value = 0)
 
         if not self.seq_pool:
             cls_token = repeat(self.class_emb, '1 1 d -> b 1 d', b = b)
-            x = torch.cat((cls_token, x), dim=1)
+            x = mint.cat((cls_token, x), dim = 1)
 
         if exists(self.positional_emb):
             x += self.positional_emb
@@ -314,7 +319,7 @@ 
         if self.seq_pool:
             attn_weights = rearrange(self.attention_pool(x), 'b n 1 -> b n')
-            x = einsum('b n, b n d -> b d', attn_weights.softmax(dim = 1), x)
+            x = mint.einsum('b n, b n d -> b d', attn_weights.softmax(dim = 1), x)
         else:
             x = x[:, 0]
 
@@ -322,7 +327,7 @@ 
 # CCT Main model
 
-class CCT(nn.Module):
+class CCT(msnn.Cell):
     def __init__(
         self,
         img_size=224,
@@ -383,6 +388,6 @@             *args, **kwargs
         )
 
-    def forward(self, x):
+    def construct(self, x):
         x = self.tokenizer(x)
         return self.classifier(x)
