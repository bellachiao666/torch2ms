--- pytorch+++ mindspore@@ -1,12 +1,16 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 import copy
 import random
 from functools import wraps, partial
 
-import torch
-from torch import nn
-import torch.nn.functional as F
-
-from torchvision import transforms as T
+# import torch
+# from torch import nn
+
+# from torchvision import transforms as T
 
 # helper functions
 
@@ -50,17 +54,17 @@     teacher_logits = teacher_logits.detach()
     student_probs = (student_logits / student_temp).softmax(dim = -1)
     teacher_probs = ((teacher_logits - centers) / teacher_temp).softmax(dim = -1)
-    return - (teacher_probs * torch.log(student_probs + eps)).sum(dim = -1).mean()
+    return - (teacher_probs * mint.log(student_probs + eps)).sum(dim = -1).mean()
 
 # augmentation utils
 
-class RandomApply(nn.Module):
+class RandomApply(msnn.Cell):
     def __init__(self, fn, p):
         super().__init__()
         self.fn = fn
         self.p = p
 
-    def forward(self, x):
+    def construct(self, x):
         if random.random() > self.p:
             return x
         return self.fn(x)
@@ -84,12 +88,12 @@ 
 # MLP class for projector and predictor
 
-class L2Norm(nn.Module):
-    def forward(self, x, eps = 1e-6):
+class L2Norm(msnn.Cell):
+    def construct(self, x, eps = 1e-6):
         norm = x.norm(dim = 1, keepdim = True).clamp(min = eps)
         return x / norm
 
-class MLP(nn.Module):
+class MLP(msnn.Cell):
     def __init__(self, dim, dim_out, num_layers, hidden_size = 256):
         super().__init__()
 
@@ -101,23 +105,24 @@ 
             layers.extend([
                 nn.Linear(layer_dim_in, layer_dim_out),
-                nn.GELU() if not is_last else nn.Identity()
+                nn.GELU() if not is_last else msnn.Identity()
             ])
 
-        self.net = nn.Sequential(
-            *layers,
+        self.net = msnn.SequentialCell(
+            [
+            layers,
             L2Norm(),
             nn.Linear(hidden_size, dim_out)
-        )
-
-    def forward(self, x):
+        ])
+
+    def construct(self, x):
         return self.net(x)
 
 # a wrapper class for the base neural network
 # will manage the interception of the hidden layer output
 # and pipe it into the projecter and predictor nets
 
-class NetWrapper(nn.Module):
+class NetWrapper(msnn.Cell):
     def __init__(self, net, output_dim, projection_hidden_size, projection_num_layers, layer = -2):
         super().__init__()
         self.net = net
@@ -171,7 +176,7 @@         assert hidden is not None, f'hidden layer {self.layer} never emitted an output'
         return hidden
 
-    def forward(self, x, return_projection = True):
+    def construct(self, x, return_projection = True):
         embed = self.get_embedding(x)
         if not return_projection:
             return embed
@@ -181,7 +186,7 @@ 
 # main class
 
-class Dino(nn.Module):
+class Dino(msnn.Cell):
     def __init__(
         self,
         net,
@@ -204,37 +209,38 @@ 
         # default BYOL augmentation
 
-        DEFAULT_AUG = torch.nn.Sequential(
+        DEFAULT_AUG = msnn.SequentialCell(
+            [
             RandomApply(
-                T.ColorJitter(0.8, 0.8, 0.8, 0.2),
+                ms.dataset.vision.RandomColorAdjust(0.8, 0.8, 0.8, 0.2),
                 p = 0.3
             ),
             T.RandomGrayscale(p=0.2),
             T.RandomHorizontalFlip(),
             RandomApply(
-                T.GaussianBlur((3, 3), (1.0, 2.0)),
+                ms.dataset.vision.GaussianBlur((3, 3), (1.0, 2.0)),
                 p = 0.2
             ),
             T.Normalize(
                 mean=torch.tensor([0.485, 0.456, 0.406]),
-                std=torch.tensor([0.229, 0.224, 0.225])),
-        )
+                std=torch.tensor([0.229, 0.224, 0.225]))
+        ])  # 'torchvision.transforms.RandomGrayscale' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torchvision.transforms.RandomHorizontalFlip' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torchvision.transforms.Normalize' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         self.augment1 = default(augment_fn, DEFAULT_AUG)
         self.augment2 = default(augment_fn2, DEFAULT_AUG)
 
         # local and global crops
 
-        self.local_crop = T.RandomResizedCrop((image_size, image_size), scale = (0.05, local_upper_crop_scale))
-        self.global_crop = T.RandomResizedCrop((image_size, image_size), scale = (global_lower_crop_scale, 1.))
+        self.local_crop = T.RandomResizedCrop((image_size, image_size), scale = (0.05, local_upper_crop_scale))  # 'torchvision.transforms.RandomResizedCrop' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        self.global_crop = T.RandomResizedCrop((image_size, image_size), scale = (global_lower_crop_scale, 1.))  # 'torchvision.transforms.RandomResizedCrop' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         self.student_encoder = NetWrapper(net, num_classes_K, projection_hidden_size, projection_layers, layer = hidden_layer)
 
         self.teacher_encoder = None
         self.teacher_ema_updater = EMA(moving_average_decay)
 
-        self.register_buffer('teacher_centers', torch.zeros(1, num_classes_K))
-        self.register_buffer('last_teacher_centers',  torch.zeros(1, num_classes_K))
+        self.register_buffer('teacher_centers', mint.zeros(size = (1, num_classes_K)))
+        self.register_buffer('last_teacher_centers',  mint.zeros(size = (1, num_classes_K)))
 
         self.teacher_centering_ema_updater = EMA(center_moving_average_decay)
 
@@ -246,7 +252,7 @@         self.to(device)
 
         # send a mock image tensor to instantiate singleton parameters
-        self.forward(torch.randn(2, 3, image_size, image_size, device=device))
+        self.forward(mint.randn(size = (2, 3, image_size, image_size)))  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
 
     @singleton('teacher_encoder')
     def _get_teacher_encoder(self):
@@ -265,7 +271,7 @@         new_teacher_centers = self.teacher_centering_ema_updater.update_average(self.teacher_centers, self.last_teacher_centers)
         self.teacher_centers.copy_(new_teacher_centers)
 
-    def forward(
+    def construct(
         self,
         x,
         return_embedding = False,
@@ -296,7 +302,7 @@             centers = self.teacher_centers
         )
 
-        teacher_logits_avg = torch.cat((teacher_proj_one, teacher_proj_two)).mean(dim = 0)
+        teacher_logits_avg = mint.cat((teacher_proj_one, teacher_proj_two)).mean(dim = 0)
         self.last_teacher_centers.copy_(teacher_logits_avg)
 
         loss = (loss_fn_(teacher_proj_one, student_proj_two) + loss_fn_(teacher_proj_two, student_proj_one)) / 2
