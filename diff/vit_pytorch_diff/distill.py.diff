--- pytorch+++ mindspore@@ -1,11 +1,15 @@-import torch
-from torch import nn
-from torch.nn import Module
-import torch.nn.functional as F
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# import torch
+# from torch import nn
+# import torch.nn.functional as F
 
-from vit_pytorch.vit import ViT
-from vit_pytorch.t2t import T2TViT
-from vit_pytorch.efficient import ViT as EfficientViT
+# from vit_pytorch.vit import ViT
+# from vit_pytorch.t2t import T2TViT
+# from vit_pytorch.efficient import ViT as EfficientViT
 
 from einops import rearrange, repeat
 
@@ -26,12 +30,12 @@         b, n, _ = x.shape
 
         cls_tokens = repeat(self.cls_token, '1 n d -> b n d', b = b)
-        x = torch.cat((cls_tokens, x), dim = 1)
+        x = mint.cat((cls_tokens, x), dim = 1)
         x += self.pos_embedding[:, :(n + 1)]
 
         if distilling:
             distill_tokens = repeat(distill_token, '1 n d -> b n d', b = b)
-            x = torch.cat((x, distill_tokens), dim = 1)
+            x = mint.cat((x, distill_tokens), dim = 1)
 
         x = self._attend(x)
 
@@ -102,7 +106,7 @@ 
 # knowledge distillation wrapper
 
-class DistillWrapper(Module):
+class DistillWrapper(msnn.Cell):
     def __init__(
         self,
         *,
@@ -125,14 +129,15 @@         self.alpha = alpha
         self.hard = hard
 
-        self.distillation_token = nn.Parameter(torch.randn(1, 1, dim))
+        self.distillation_token = ms.Parameter(mint.randn(size = (1, 1, dim)))
 
-        self.distill_mlp = nn.Sequential(
-            nn.LayerNorm(dim) if mlp_layernorm else nn.Identity(),
+        self.distill_mlp = msnn.SequentialCell(
+            [
+            nn.LayerNorm(dim) if mlp_layernorm else msnn.Identity(),
             nn.Linear(dim, num_classes)
-        )
+        ])
 
-    def forward(self, img, labels, temperature = None, alpha = None, **kwargs):
+    def construct(self, img, labels, temperature = None, alpha = None, **kwargs):
 
         alpha = default(alpha, self.alpha)
         T = default(temperature, self.temperature)
@@ -147,8 +152,8 @@ 
         if not self.hard:
             distill_loss = F.kl_div(
-                F.log_softmax(distill_logits / T, dim = -1),
-                F.softmax(teacher_logits / T, dim = -1).detach(),
+                mint.special.log_softmax(distill_logits / T, dim = -1),
+                nn.functional.softmax(teacher_logits / T, dim = -1).detach(),
             reduction = 'batchmean')
             distill_loss *= T ** 2
 
