--- pytorch+++ mindspore@@ -1,10 +1,15 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from functools import partial
 
-import torch
-from torch import nn, einsum
+# import torch
+# from torch import nn, einsum
 
 from einops import rearrange, repeat
-from einops.layers.torch import Rearrange, Reduce
+# from einops.layers.torch import Rearrange, Reduce
 
 # helpers
 
@@ -19,72 +24,74 @@ 
 # helper classes
 
-class Residual(nn.Module):
+class Residual(msnn.Cell):
     def __init__(self, dim, fn):
         super().__init__()
         self.fn = fn
 
-    def forward(self, x):
+    def construct(self, x):
         return self.fn(x) + x
 
-class FeedForward(nn.Module):
+class FeedForward(msnn.Cell):
     def __init__(self, dim, mult = 4, dropout = 0.):
         super().__init__()
         inner_dim = int(dim * mult)
-        self.net = nn.Sequential(
+        self.net = msnn.SequentialCell(
+            [
             nn.LayerNorm(dim),
             nn.Linear(dim, inner_dim),
             nn.GELU(),
             nn.Dropout(dropout),
             nn.Linear(inner_dim, dim),
             nn.Dropout(dropout)
-        )
-    def forward(self, x):
+        ])
+    def construct(self, x):
         return self.net(x)
 
 # MBConv
 
-class SqueezeExcitation(nn.Module):
+class SqueezeExcitation(msnn.Cell):
     def __init__(self, dim, shrinkage_rate = 0.25):
         super().__init__()
         hidden_dim = int(dim * shrinkage_rate)
 
-        self.gate = nn.Sequential(
+        self.gate = msnn.SequentialCell(
+            [
             Reduce('b c h w -> b c', 'mean'),
             nn.Linear(dim, hidden_dim, bias = False),
             nn.SiLU(),
             nn.Linear(hidden_dim, dim, bias = False),
             nn.Sigmoid(),
             Rearrange('b c -> b c 1 1')
-        )
-
-    def forward(self, x):
+        ])  # 'torch.nn.Sigmoid' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+
+    def construct(self, x):
         return x * self.gate(x)
 
 
-class MBConvResidual(nn.Module):
+class MBConvResidual(msnn.Cell):
     def __init__(self, fn, dropout = 0.):
         super().__init__()
         self.fn = fn
         self.dropsample = Dropsample(dropout)
 
-    def forward(self, x):
+    def construct(self, x):
         out = self.fn(x)
         out = self.dropsample(out)
         return out + x
 
-class Dropsample(nn.Module):
+class Dropsample(msnn.Cell):
     def __init__(self, prob = 0):
         super().__init__()
         self.prob = prob
   
-    def forward(self, x):
+    def construct(self, x):
         device = x.device
 
         if self.prob == 0. or (not self.training):
             return x
 
-        keep_mask = torch.FloatTensor((x.shape[0], 1, 1, 1), device = device).uniform_() > self.prob
+        keep_mask = torch.FloatTensor((x.shape[0], 1, 1, 1), device = device).uniform_() > self.prob  # 'torch.FloatTensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;; 'torch.FloatTensor.uniform_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         return x * keep_mask / (1 - self.prob)
 
 def MBConv(
@@ -99,7 +106,8 @@     hidden_dim = int(expansion_rate * dim_out)
     stride = 2 if downsample else 1
 
-    net = nn.Sequential(
+    net = msnn.SequentialCell(
+        [
         nn.Conv2d(dim_in, hidden_dim, 1),
         nn.BatchNorm2d(hidden_dim),
         nn.GELU(),
@@ -109,7 +117,7 @@         SqueezeExcitation(hidden_dim, shrinkage_rate = shrinkage_rate),
         nn.Conv2d(hidden_dim, dim_out, 1),
         nn.BatchNorm2d(dim_out)
-    )
+    ])
 
     if dim_in == dim_out and not downsample:
         net = MBConvResidual(net, dropout = dropout)
@@ -118,7 +126,7 @@ 
 # attention related classes
 
-class Attention(nn.Module):
+class Attention(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -135,30 +143,32 @@         self.norm = nn.LayerNorm(dim)
         self.to_qkv = nn.Linear(dim, dim * 3, bias = False)
 
-        self.attend = nn.Sequential(
+        self.attend = msnn.SequentialCell(
+            [
             nn.Softmax(dim = -1),
             nn.Dropout(dropout)
-        )
-
-        self.to_out = nn.Sequential(
+        ])
+
+        self.to_out = msnn.SequentialCell(
+            [
             nn.Linear(dim, dim, bias = False),
             nn.Dropout(dropout)
-        )
+        ])
 
         # relative positional bias
 
         self.rel_pos_bias = nn.Embedding((2 * window_size - 1) ** 2, self.heads)
 
-        pos = torch.arange(window_size)
-        grid = torch.stack(torch.meshgrid(pos, pos, indexing = 'ij'))
+        pos = mint.arange(window_size)
+        grid = mint.stack(mint.meshgrid(pos, pos, indexing = 'ij'))
         grid = rearrange(grid, 'c i j -> (i j) c')
         rel_pos = rearrange(grid, 'i ... -> i 1 ...') - rearrange(grid, 'j ... -> 1 j ...')
         rel_pos += window_size - 1
-        rel_pos_indices = (rel_pos * torch.tensor([2 * window_size - 1, 1])).sum(dim = -1)
+        rel_pos_indices = (rel_pos * torch.tensor([2 * window_size - 1, 1])).sum(dim = -1)  # 'torch.tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         self.register_buffer('rel_pos_indices', rel_pos_indices, persistent = False)
 
-    def forward(self, x):
+    def construct(self, x):
         batch, height, width, window_height, window_width, _, device, h = *x.shape, x.device, self.heads
 
         x = self.norm(x)
@@ -181,7 +191,7 @@ 
         # sim
 
-        sim = einsum('b h i d, b h j d -> b h i j', q, k)
+        sim = mint.einsum('b h i d, b h j d -> b h i j', q, k)
 
         # add positional bias
 
@@ -194,7 +204,7 @@ 
         # aggregate
 
-        out = einsum('b h i j, b h j d -> b h i d', attn, v)
+        out = mint.einsum('b h i j, b h j d -> b h i d', attn, v)
 
         # merge heads
 
@@ -205,7 +215,7 @@         out = self.to_out(out)
         return rearrange(out, '(b x y) ... -> b x y ...', x = height, y = width)
 
-class MaxViT(nn.Module):
+class MaxViT(msnn.Cell):
     def __init__(
         self,
         *,
@@ -227,10 +237,11 @@ 
         dim_conv_stem = default(dim_conv_stem, dim)
 
-        self.conv_stem = nn.Sequential(
+        self.conv_stem = msnn.SequentialCell(
+            [
             nn.Conv2d(channels, dim_conv_stem, 3, stride = 2, padding = 1),
             nn.Conv2d(dim_conv_stem, dim_conv_stem, 3, padding = 1)
-        )
+        ])
 
         # variables
 
@@ -240,7 +251,7 @@         dims = (dim_conv_stem, *dims)
         dim_pairs = tuple(zip(dims[:-1], dims[1:]))
 
-        self.layers = nn.ModuleList([])
+        self.layers = msnn.CellList([])
 
         # shorthand for window size for efficient block - grid like attention
 
@@ -253,7 +264,8 @@                 is_first = stage_ind == 0
                 stage_dim_in = layer_dim_in if is_first else layer_dim
 
-                block = nn.Sequential(
+                block = msnn.SequentialCell(
+                    [
                     MBConv(
                         stage_dim_in,
                         layer_dim,
@@ -261,28 +273,28 @@                         expansion_rate = mbconv_expansion_rate,
                         shrinkage_rate = mbconv_shrinkage_rate
                     ),
-                    Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1 = w, w2 = w),  # block-like attention
+                    Rearrange('b d (x w1) (y w2) -> b x y w1 w2 d', w1 = w, w2 = w),
                     Residual(layer_dim, Attention(dim = layer_dim, dim_head = dim_head, dropout = dropout, window_size = w)),
                     Residual(layer_dim, FeedForward(dim = layer_dim, dropout = dropout)),
                     Rearrange('b x y w1 w2 d -> b d (x w1) (y w2)'),
-
-                    Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1 = w, w2 = w),  # grid-like attention
+                    Rearrange('b d (w1 x) (w2 y) -> b x y w1 w2 d', w1 = w, w2 = w),
                     Residual(layer_dim, Attention(dim = layer_dim, dim_head = dim_head, dropout = dropout, window_size = w)),
                     Residual(layer_dim, FeedForward(dim = layer_dim, dropout = dropout)),
-                    Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)'),
-                )
+                    Rearrange('b x y w1 w2 d -> b d (w1 x) (w2 y)')
+                ])
 
                 self.layers.append(block)
 
         # mlp head out
 
-        self.mlp_head = nn.Sequential(
+        self.mlp_head = msnn.SequentialCell(
+            [
             Reduce('b d h w -> b d', 'mean'),
             nn.LayerNorm(dims[-1]),
             nn.Linear(dims[-1], num_classes)
-        )
-
-    def forward(self, x):
+        ])
+
+    def construct(self, x):
         x = self.conv_stem(x)
 
         for stage in self.layers:
