--- pytorch+++ mindspore@@ -1,12 +1,17 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from __future__ import annotations
 
 from functools import partial, lru_cache
 from typing import List
 
-import torch
-import torch.nn.functional as F
-from torch import nn, Tensor
-from torch.nn.utils.rnn import pad_sequence as orig_pad_sequence
+# import torch
+# import torch.nn.functional as F
+# from torch import nn, Tensor
+# from torch.nn.utils.rnn import pad_sequence as orig_pad_sequence
 
 from einops import rearrange, repeat
 
@@ -29,9 +34,9 @@ 
 @lru_cache(maxsize=128)
 def posemb_grid(ph, pw, device):
-    h_idx = torch.arange(ph, device=device).repeat_interleave(pw)
-    w_idx = torch.arange(pw, device=device).repeat(ph)
-    return torch.stack([h_idx, w_idx], dim=-1)
+    h_idx = mint.arange(ph).repeat_interleave(pw)  # 'torch.arange':没有对应的mindspore参数 'device' (position 6);
+    w_idx = mint.arange(pw).repeat(ph)  # 'torch.arange':没有对应的mindspore参数 'device' (position 6);
+    return mint.stack([h_idx, w_idx], dim = -1)
 
 # auto grouping images
 
@@ -79,40 +84,41 @@ # normalization
 # they use layernorm without bias, something that pytorch does not offer
 
-class LayerNorm(nn.Module):
+class LayerNorm(msnn.Cell):
     def __init__(self, dim):
         super().__init__()
-        self.gamma = nn.Parameter(torch.ones(dim))
-        self.register_buffer('beta', torch.zeros(dim))
-
-    def forward(self, x):
-        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)
+        self.gamma = ms.Parameter(mint.ones(dim))
+        self.register_buffer('beta', mint.zeros(dim))
+
+    def construct(self, x):
+        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)  # 'torch.nn.functional.layer_norm' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
 # they use a query-key normalization that is equivalent to rms norm (no mean-centering, learned gamma), from vit 22B paper
 
-class RMSNorm(nn.Module):
+class RMSNorm(msnn.Cell):
     def __init__(self, heads, dim):
         super().__init__()
         self.scale = dim ** 0.5
-        self.gamma = nn.Parameter(torch.ones(heads, 1, dim))
-
-    def forward(self, x):
-        normed = F.normalize(x, dim = -1)
+        self.gamma = ms.Parameter(mint.ones(size = (heads, 1, dim)))
+
+    def construct(self, x):
+        normed = nn.functional.normalize(x, dim = -1)
         return normed * self.scale * self.gamma
 
 # feedforward
 
 def FeedForward(dim, hidden_dim, dropout = 0.):
-    return nn.Sequential(
+    return msnn.SequentialCell(
+        [
         LayerNorm(dim),
         nn.Linear(dim, hidden_dim),
         nn.GELU(),
         nn.Dropout(dropout),
         nn.Linear(hidden_dim, dim),
         nn.Dropout(dropout)
-    )
-
-class Attention(nn.Module):
+    ])
+
+class Attention(msnn.Cell):
     def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
         super().__init__()
         inner_dim = dim_head *  heads
@@ -127,12 +133,13 @@         self.to_q = nn.Linear(dim, inner_dim, bias = False)
         self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)
 
-        self.to_out = nn.Sequential(
+        self.to_out = msnn.SequentialCell(
+            [
             nn.Linear(inner_dim, dim, bias = False),
             nn.Dropout(dropout)
-        )
-
-    def forward(
+        ])
+
+    def construct(
         self,
         x,
         context = None,
@@ -163,24 +170,24 @@             attn_mask = attn_mask,
             dropout_p = self.dropout_p if self.training else 0.,
             scale = 1.  # RMSNorm already includes sqrt(dim) scaling
-        )
+        )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         out = rearrange(out, 'b h n d -> b n (h d)')
         return self.to_out(out)
 
-class Transformer(nn.Module):
+class Transformer(msnn.Cell):
     def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
         super().__init__()
-        self.layers = nn.ModuleList([])
+        self.layers = msnn.CellList([])
         for _ in range(depth):
-            self.layers.append(nn.ModuleList([
+            self.layers.append(msnn.CellList([
                 Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),
                 FeedForward(dim, mlp_dim, dropout = dropout)
             ]))
 
         self.norm = LayerNorm(dim)
 
-    def forward(
+    def construct(
         self,
         x,
         mask = None,
@@ -192,7 +199,7 @@ 
         return self.norm(x)
 
-class NaViT(nn.Module):
+class NaViT(msnn.Cell):
     def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0., token_dropout_prob = None):
         super().__init__()
         image_height, image_width = pair(image_size)
@@ -221,14 +228,15 @@         self.channels = channels
         self.patch_size = patch_size
 
-        self.to_patch_embedding = nn.Sequential(
+        self.to_patch_embedding = msnn.SequentialCell(
+            [
             LayerNorm(patch_dim),
             nn.Linear(patch_dim, dim),
-            LayerNorm(dim),
-        )
-
-        self.pos_embed_height = nn.Parameter(torch.randn(patch_height_dim, dim))
-        self.pos_embed_width = nn.Parameter(torch.randn(patch_width_dim, dim))
+            LayerNorm(dim)
+        ])
+
+        self.pos_embed_height = ms.Parameter(mint.randn(size = (patch_height_dim, dim)))
+        self.pos_embed_width = ms.Parameter(mint.randn(size = (patch_width_dim, dim)))
 
         self.dropout = nn.Dropout(emb_dropout)
 
@@ -236,23 +244,24 @@ 
         # final attention pooling queries
 
-        self.attn_pool_queries = nn.Parameter(torch.randn(dim))
+        self.attn_pool_queries = ms.Parameter(mint.randn(dim))
         self.attn_pool = Attention(dim = dim, dim_head = dim_head, heads = heads)
 
         # output to logits
 
-        self.to_latent = nn.Identity()
-
-        self.mlp_head = nn.Sequential(
+        self.to_latent = msnn.Identity()
+
+        self.mlp_head = msnn.SequentialCell(
+            [
             LayerNorm(dim),
             nn.Linear(dim, num_classes, bias = False)
-        )
+        ])
 
     @property
     def device(self):
         return next(self.parameters()).device
 
-    def forward(
+    def construct(
         self,
         batched_images: List[Tensor] | List[List[Tensor]], # assume different resolution images already grouped correctly
         group_images = False,
@@ -309,24 +318,22 @@                     token_dropout = self.calc_token_dropout(*image_dims)
                     seq_len = seq.shape[0]
                     num_keep = max(1, int(seq_len * (1 - token_dropout)))
-                    keep_indices = torch.randn((seq_len,), device=device).topk(num_keep, dim=-1).indices
+                    keep_indices = mint.randn((seq_len,)).topk(num_keep, dim=-1).indices  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
                     sequences[i] = seq[keep_indices]
                     positions[i] = pos[keep_indices]
 
             # build image_ids efficiently using repeat_interleave
             patch_counts = [seq.shape[0] for seq in sequences]
-            image_ids = torch.repeat_interleave(
-                arange(len(images)),
-                torch.tensor(patch_counts, device=device)
-            )
+            image_ids = mint.repeat_interleave(
+                arange(len(images)), torch.tensor(patch_counts, device=device))  # 'torch.tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
             batched_image_ids.append(image_ids)
-            batched_sequences.append(torch.cat(sequences, dim=0))
-            batched_positions.append(torch.cat(positions, dim=0))
+            batched_sequences.append(mint.cat(sequences, dim = 0))
+            batched_positions.append(mint.cat(positions, dim = 0))
 
         # derive key padding mask
 
-        lengths = torch.tensor([seq.shape[-2] for seq in batched_sequences], device = device, dtype = torch.long)
+        lengths = torch.tensor([seq.shape[-2] for seq in batched_sequences], device = device, dtype = torch.long)  # 'torch.tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
         seq_arange = arange(lengths.amax().item())
         key_pad_mask = rearrange(seq_arange, 'n -> 1 n') < rearrange(lengths, 'b -> b 1')
 
@@ -343,7 +350,7 @@ 
         # need to know how many images for final attention pooling
 
-        num_images = torch.tensor(num_images, device = device, dtype = torch.long)        
+        num_images = torch.tensor(num_images, device = device, dtype = torch.long)  # 'torch.tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # to patches
 
