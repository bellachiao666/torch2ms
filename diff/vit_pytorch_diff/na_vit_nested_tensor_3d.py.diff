--- pytorch+++ mindspore@@ -1,18 +1,22 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from __future__ import annotations
 
 from typing import List
 from functools import partial
 
-import torch
+# import torch
 import packaging.version as pkg_version
 
-from torch import nn, Tensor
-import torch.nn.functional as F
-from torch.nn import Module, ModuleList
-from torch.nested import nested_tensor
+# from torch import nn, Tensor
+# import torch.nn.functional as F
+# from torch.nested import nested_tensor
 
 from einops import rearrange
-from einops.layers.torch import Rearrange
+# from einops.layers.torch import Rearrange
 
 # helpers
 
@@ -31,16 +35,17 @@ # feedforward
 
 def FeedForward(dim, hidden_dim, dropout = 0.):
-    return nn.Sequential(
+    return msnn.SequentialCell(
+        [
         nn.LayerNorm(dim, bias = False),
         nn.Linear(dim, hidden_dim),
         nn.GELU(),
         nn.Dropout(dropout),
         nn.Linear(hidden_dim, dim),
         nn.Dropout(dropout)
-    )
-
-class Attention(Module):
+    ])
+
+class Attention(msnn.Cell):
     def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., qk_norm = True):
         super().__init__()
         self.norm = nn.LayerNorm(dim, bias = False)
@@ -56,14 +61,14 @@         # in the paper, they employ qk rmsnorm, a way to stabilize attention
         # will use layernorm in place of rmsnorm, which has been shown to work in certain papers. requires l2norm on non-ragged dimension to be supported in nested tensors
 
-        self.query_norm = nn.LayerNorm(dim_head, bias = False) if qk_norm else nn.Identity()
-        self.key_norm = nn.LayerNorm(dim_head, bias = False) if qk_norm else nn.Identity()
+        self.query_norm = nn.LayerNorm(dim_head, bias = False) if qk_norm else msnn.Identity()
+        self.key_norm = nn.LayerNorm(dim_head, bias = False) if qk_norm else msnn.Identity()
 
         self.dropout = dropout
 
         self.to_out = nn.Linear(dim_inner, dim, bias = False)
 
-    def forward(
+    def construct(
         self, 
         x,
         context: Tensor | None = None
@@ -103,7 +108,7 @@         out = F.scaled_dot_product_attention(
             query, key, value,
             dropout_p = self.dropout if self.training else 0.
-        )
+        )  # 'torch.nn.functional.scaled_dot_product_attention' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # merge heads
 
@@ -111,20 +116,20 @@ 
         return self.to_out(out)
 
-class Transformer(Module):
+class Transformer(msnn.Cell):
     def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0., qk_norm = True):
         super().__init__()
-        self.layers = ModuleList([])
+        self.layers = msnn.CellList([])
 
         for _ in range(depth):
-            self.layers.append(ModuleList([
+            self.layers.append(msnn.CellList([
                 Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout, qk_norm = qk_norm),
                 FeedForward(dim, mlp_dim, dropout = dropout)
             ]))
 
         self.norm = nn.LayerNorm(dim, bias = False)
 
-    def forward(self, x):
+    def construct(self, x):
 
         for attn, ff in self.layers:
             x = attn(x) + x
@@ -132,7 +137,7 @@ 
         return self.norm(x)
 
-class NaViT(Module):
+class NaViT(msnn.Cell):
     def __init__(
         self,
         *,
@@ -178,24 +183,25 @@         self.patch_size = patch_size
         self.to_patches = Rearrange('c (f pf) (h p1) (w p2) -> f h w (c pf p1 p2)', p1 = patch_size, p2 = patch_size, pf = frame_patch_size)
 
-        self.to_patch_embedding = nn.Sequential(
+        self.to_patch_embedding = msnn.SequentialCell(
+            [
             nn.LayerNorm(patch_dim),
             nn.Linear(patch_dim, dim),
-            nn.LayerNorm(dim),
-        )
-
-        self.pos_embed_frame = nn.Parameter(torch.zeros(patch_frame_dim, dim))
-        self.pos_embed_height = nn.Parameter(torch.zeros(patch_height_dim, dim))
-        self.pos_embed_width = nn.Parameter(torch.zeros(patch_width_dim, dim))
+            nn.LayerNorm(dim)
+        ])
+
+        self.pos_embed_frame = ms.Parameter(mint.zeros(size = (patch_frame_dim, dim)))
+        self.pos_embed_height = ms.Parameter(mint.zeros(size = (patch_height_dim, dim)))
+        self.pos_embed_width = ms.Parameter(mint.zeros(size = (patch_width_dim, dim)))
 
         # register tokens
 
-        self.register_tokens = nn.Parameter(torch.zeros(num_registers, dim))
-
-        nn.init.normal_(self.pos_embed_frame, std = 0.02)
-        nn.init.normal_(self.pos_embed_height, std = 0.02)
-        nn.init.normal_(self.pos_embed_width, std = 0.02)
-        nn.init.normal_(self.register_tokens, std = 0.02)
+        self.register_tokens = ms.Parameter(mint.zeros(size = (num_registers, dim)))
+
+        nn.init.normal_(self.pos_embed_frame, std = 0.02)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.normal_(self.pos_embed_height, std = 0.02)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.normal_(self.pos_embed_width, std = 0.02)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
+        nn.init.normal_(self.register_tokens, std = 0.02)  # 'torch.nn.init.normal_' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         self.dropout = nn.Dropout(emb_dropout)
 
@@ -203,23 +209,24 @@ 
         # final attention pooling queries
 
-        self.attn_pool_queries = nn.Parameter(torch.randn(dim))
+        self.attn_pool_queries = ms.Parameter(mint.randn(dim))
         self.attn_pool = Attention(dim = dim, dim_head = dim_head, heads = heads)
 
         # output to logits
 
-        self.to_latent = nn.Identity()
-
-        self.mlp_head = nn.Sequential(
+        self.to_latent = msnn.Identity()
+
+        self.mlp_head = msnn.SequentialCell(
+            [
             nn.LayerNorm(dim, bias = False),
             nn.Linear(dim, num_classes, bias = False)
-        )
+        ])
 
     @property
     def device(self):
         return next(self.parameters()).device
 
-    def forward(
+    def construct(
         self,
         volumes: List[Tensor], # different resolution images / CT scans
     ):
@@ -236,7 +243,7 @@ 
         for patches in all_patches:
             patch_frame, patch_height, patch_width = patches.shape[:3]
-            fhw_indices = torch.stack(torch.meshgrid((arange(patch_frame), arange(patch_height), arange(patch_width)), indexing = 'ij'), dim = -1)
+            fhw_indices = mint.stack(mint.meshgrid((arange(patch_frame), arange(patch_height), arange(patch_width)), indexing = 'ij'), dim = -1)
             fhw_indices = rearrange(fhw_indices, 'f h w c -> (f h w) c')
 
             positions.append(fhw_indices)
@@ -247,7 +254,7 @@ 
         # handle token dropout
 
-        seq_lens = torch.tensor([i.shape[0] for i in tokens], device = device)
+        seq_lens = torch.tensor([i.shape[0] for i in tokens], device = device)  # 'torch.tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         if self.training and self.token_dropout_prob > 0:
 
@@ -257,7 +264,7 @@             kept_positions = []
 
             for one_image_tokens, one_image_positions, seq_len, num_keep in zip(tokens, positions, seq_lens, keep_seq_lens):
-                keep_indices = torch.randn((seq_len,), device = device).topk(num_keep, dim = -1).indices
+                keep_indices = mint.randn((seq_len,)).topk(num_keep, dim = -1).indices  # 'torch.randn':没有对应的mindspore参数 'device' (position 5);
 
                 one_image_kept_tokens = one_image_tokens[keep_indices]
                 one_image_kept_positions = one_image_positions[keep_indices]
@@ -270,12 +277,12 @@         # add all height and width factorized positions
 
 
-        frame_indices, height_indices, width_indices = torch.cat(positions).unbind(dim = -1)
+        frame_indices, height_indices, width_indices = mint.cat(positions).unbind(dim = -1)
         frame_embed, height_embed, width_embed = self.pos_embed_frame[frame_indices], self.pos_embed_height[height_indices], self.pos_embed_width[width_indices]
 
         pos_embed = frame_embed + height_embed + width_embed
 
-        tokens = torch.cat(tokens)
+        tokens = mint.cat(tokens)
 
         # linear projection to patch embeddings
 
@@ -289,11 +296,11 @@ 
         tokens = tokens.split(seq_lens.tolist())
 
-        tokens = [torch.cat((self.register_tokens, one_tokens)) for one_tokens in tokens]
+        tokens = [mint.cat((self.register_tokens, one_tokens)) for one_tokens in tokens]
 
         # use nested tensor for transformers and save on padding computation
 
-        tokens = nested_tensor(tokens, layout = torch.jagged, device = device)
+        tokens = nested_tensor(tokens, layout = torch.jagged, device = device)  # 'torch.nested.nested_tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         # embedding dropout
 
@@ -308,13 +315,13 @@ 
         attn_pool_queries = [rearrange(self.attn_pool_queries, '... -> 1 ...')] * batch
 
-        attn_pool_queries = nested_tensor(attn_pool_queries, layout = torch.jagged)
+        attn_pool_queries = nested_tensor(attn_pool_queries, layout = torch.jagged)  # 'torch.nested.nested_tensor' 未在映射表(api_mapping_out_excel.json)中找到，需手动确认;
 
         pooled = self.attn_pool(attn_pool_queries, tokens)
 
         # back to unjagged
 
-        logits = torch.stack(pooled.unbind())
+        logits = mint.stack(pooled.unbind())
 
         logits = rearrange(logits, 'b 1 d -> b d')
 
@@ -346,9 +353,9 @@     # 5 volumetric data (videos or CT scans) of different resolutions - List[Tensor]
 
     volumes = [
-        torch.randn(3, 2, 256, 256), torch.randn(3, 8, 128, 128),
-        torch.randn(3, 4, 128, 256), torch.randn(3, 2, 256, 128),
-        torch.randn(3, 4, 64, 256)
+        mint.randn(size = (3, 2, 256, 256)), mint.randn(size = (3, 8, 128, 128)),
+        mint.randn(size = (3, 4, 128, 256)), mint.randn(size = (3, 2, 256, 128)),
+        mint.randn(size = (3, 4, 64, 256))
     ]
 
     assert v(volumes).shape == (5, 1000)
