--- pytorch+++ mindspore@@ -1,11 +1,15 @@-import torch
-from torch import nn
-from torch.nn import Module, ModuleList
-import torch.nn.functional as F
-import torch.nn.utils.parametrize as parametrize
+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
+# import torch
+# from torch import nn
+# import torch.nn.functional as F
+# import torch.nn.utils.parametrize as parametrize
 
 from einops import rearrange, reduce
-from einops.layers.torch import Rearrange
+# from einops.layers.torch import Rearrange
 
 # functions
 
@@ -22,19 +26,19 @@     return (numer % denom) == 0
 
 def l2norm(t, dim = -1):
-    return F.normalize(t, dim = dim, p = 2)
+    return nn.functional.normalize(t, p = 2, dim = dim)
 
 # for use with parametrize
 
-class L2Norm(Module):
+class L2Norm(msnn.Cell):
     def __init__(self, dim = -1):
         super().__init__()
         self.dim = dim
 
-    def forward(self, t):
+    def construct(self, t):
         return l2norm(t, dim = self.dim)
 
-class NormLinear(Module):
+class NormLinear(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -54,12 +58,12 @@     def weight(self):
         return self.linear.weight
 
-    def forward(self, x):
+    def construct(self, x):
         return self.linear(x)
 
 # attention and feedforward
 
-class Attention(Module):
+class Attention(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -76,15 +80,15 @@ 
         self.dropout = dropout
 
-        self.q_scale = nn.Parameter(torch.ones(heads, 1, dim_head) * (dim_head ** 0.25))
-        self.k_scale = nn.Parameter(torch.ones(heads, 1, dim_head) * (dim_head ** 0.25))
+        self.q_scale = ms.Parameter(mint.ones(size = (heads, 1, dim_head)) * (dim_head ** 0.25))
+        self.k_scale = ms.Parameter(mint.ones(size = (heads, 1, dim_head)) * (dim_head ** 0.25))
 
         self.split_heads = Rearrange('b n (h d) -> b h n d', h = heads)
         self.merge_heads = Rearrange('b h n d -> b n (h d)')
 
         self.to_out = NormLinear(dim_inner, dim, norm_dim_in = False)
 
-    def forward(
+    def construct(
         self,
         x
     ):
@@ -110,7 +114,7 @@         out = self.merge_heads(out)
         return self.to_out(out)
 
-class FeedForward(Module):
+class FeedForward(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -127,25 +131,25 @@         self.to_hidden = NormLinear(dim, dim_inner)
         self.to_gate = NormLinear(dim, dim_inner)
 
-        self.hidden_scale = nn.Parameter(torch.ones(dim_inner))
-        self.gate_scale = nn.Parameter(torch.ones(dim_inner))
+        self.hidden_scale = ms.Parameter(mint.ones(dim_inner))
+        self.gate_scale = ms.Parameter(mint.ones(dim_inner))
 
         self.to_out = NormLinear(dim_inner, dim, norm_dim_in = False)
 
-    def forward(self, x):
+    def construct(self, x):
         hidden, gate = self.to_hidden(x), self.to_gate(x)
 
         hidden = hidden * self.hidden_scale
         gate = gate * self.gate_scale * (self.dim ** 0.5)
 
-        hidden = F.silu(gate) * hidden
+        hidden = nn.functional.silu(gate) * hidden
 
         hidden = self.dropout(hidden)
         return self.to_out(hidden)
 
 # classes
 
-class nViT(Module):
+class nViT(msnn.Cell):
     """ https://arxiv.org/abs/2410.01131 """
 
     def __init__(
@@ -177,10 +181,8 @@         self.channels = channels
         self.patch_size = patch_size
 
-        self.to_patch_embedding = nn.Sequential(
-            Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)', p1 = patch_size, p2 = patch_size),
-            NormLinear(patch_dim, dim, norm_dim_in = False),
-        )
+        self.to_patch_embedding = msnn.SequentialCell(
+            [Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)', p1 = patch_size, p2 = patch_size), NormLinear(patch_dim, dim, norm_dim_in = False)])
 
         self.abs_pos_emb = NormLinear(dim, num_patches)
 
@@ -191,21 +193,21 @@         self.dim = dim
         self.scale = dim ** 0.5
 
-        self.layers = ModuleList([])
+        self.layers = msnn.CellList([])
         self.residual_lerp_scales = nn.ParameterList([])
 
         for _ in range(depth):
-            self.layers.append(ModuleList([
+            self.layers.append(msnn.CellList([
                 Attention(dim, dim_head = dim_head, heads = heads, dropout = dropout),
                 FeedForward(dim, dim_inner = mlp_dim, dropout = dropout),
             ]))
 
             self.residual_lerp_scales.append(nn.ParameterList([
-                nn.Parameter(torch.ones(dim) * residual_lerp_scale_init / self.scale),
-                nn.Parameter(torch.ones(dim) * residual_lerp_scale_init / self.scale),
+                ms.Parameter(mint.ones(dim) * residual_lerp_scale_init / self.scale),
+                ms.Parameter(mint.ones(dim) * residual_lerp_scale_init / self.scale),
             ]))
 
-        self.logit_scale = nn.Parameter(torch.ones(num_classes))
+        self.logit_scale = ms.Parameter(mint.ones(num_classes))
 
         self.to_pred = NormLinear(dim, num_classes)
 
@@ -220,13 +222,13 @@ 
             original.copy_(normed)
 
-    def forward(self, images):
+    def construct(self, images):
         device = images.device
 
         tokens = self.to_patch_embedding(images)
 
         seq_len = tokens.shape[-2]
-        pos_emb = self.abs_pos_emb.weight[torch.arange(seq_len, device = device)]
+        pos_emb = self.abs_pos_emb.weight[mint.arange(seq_len)]  # 'torch.arange':没有对应的mindspore参数 'device' (position 6);
 
         tokens = l2norm(tokens + pos_emb)
 
@@ -259,6 +261,6 @@         mlp_dim = 2048,
     )
 
-    img = torch.randn(4, 3, 256, 256)
+    img = mint.randn(size = (4, 3, 256, 256))
     logits = v(img) # (4, 1000)
     assert logits.shape == (4, 1000)
