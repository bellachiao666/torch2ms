--- pytorch+++ mindspore@@ -1,13 +1,16 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from functools import wraps
-import torch
-from torch import nn
 
-from vit_pytorch.vit import Attention
+# from vit_pytorch.vit import Attention
 
 def find_modules(nn_module, type):
     return [module for module in nn_module.modules() if isinstance(module, type)]
 
-class Recorder(nn.Module):
+class Recorder(msnn.Cell):
     def __init__(self, vit, device = None):
         super().__init__()
         self.vit = vit
@@ -43,7 +46,7 @@         recording = attn.clone().detach()
         self.recordings.append(recording)
 
-    def forward(self, img):
+    def construct(self, img):
         assert not self.ejected, 'recorder has been ejected, cannot be used anymore'
         self.clear()
         if not self.hook_registered:
@@ -55,5 +58,5 @@         target_device = self.device if self.device is not None else img.device
         recordings = tuple(map(lambda t: t.to(target_device), self.recordings))
 
-        attns = torch.stack(recordings, dim = 1) if len(recordings) > 0 else None
+        attns = mint.stack(recordings, dim = 1) if len(recordings) > 0 else None
         return pred, attns
