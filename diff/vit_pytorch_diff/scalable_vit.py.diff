--- pytorch+++ mindspore@@ -1,9 +1,13 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from functools import partial
-import torch
-from torch import nn
+# from torch import nn
 
 from einops import rearrange, repeat
-from einops.layers.torch import Rearrange, Reduce
+# from einops.layers.torch import Rearrange, Reduce
 
 # helpers
 
@@ -21,54 +25,55 @@ 
 # helper classes
 
-class ChanLayerNorm(nn.Module):
+class ChanLayerNorm(msnn.Cell):
     def __init__(self, dim, eps = 1e-5):
         super().__init__()
         self.eps = eps
-        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))
-        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))
-
-    def forward(self, x):
-        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)
-        mean = torch.mean(x, dim = 1, keepdim = True)
+        self.g = ms.Parameter(mint.ones(size = (1, dim, 1, 1)))
+        self.b = ms.Parameter(mint.zeros(size = (1, dim, 1, 1)))
+
+    def construct(self, x):
+        var = mint.var(x, dim = 1, keepdim = True)
+        mean = mint.mean(x, dim = 1, keepdim = True)
         return (x - mean) / (var + self.eps).sqrt() * self.g + self.b
 
-class Downsample(nn.Module):
+class Downsample(msnn.Cell):
     def __init__(self, dim_in, dim_out):
         super().__init__()
         self.conv = nn.Conv2d(dim_in, dim_out, 3, stride = 2, padding = 1)
 
-    def forward(self, x):
+    def construct(self, x):
         return self.conv(x)
 
-class PEG(nn.Module):
+class PEG(msnn.Cell):
     def __init__(self, dim, kernel_size = 3):
         super().__init__()
-        self.proj = nn.Conv2d(dim, dim, kernel_size = kernel_size, padding = kernel_size // 2, groups = dim, stride = 1)
-
-    def forward(self, x):
+        self.proj = nn.Conv2d(dim, dim, kernel_size = kernel_size, stride = 1, padding = kernel_size // 2, groups = dim)
+
+    def construct(self, x):
         return self.proj(x) + x
 
 # feedforward
 
-class FeedForward(nn.Module):
+class FeedForward(msnn.Cell):
     def __init__(self, dim, expansion_factor = 4, dropout = 0.):
         super().__init__()
         inner_dim = dim * expansion_factor
-        self.net = nn.Sequential(
+        self.net = msnn.SequentialCell(
+            [
             ChanLayerNorm(dim),
             nn.Conv2d(dim, inner_dim, 1),
             nn.GELU(),
             nn.Dropout(dropout),
             nn.Conv2d(inner_dim, dim, 1),
             nn.Dropout(dropout)
-        )
-    def forward(self, x):
+        ])
+    def construct(self, x):
         return self.net(x)
 
 # attention
 
-class ScalableSelfAttention(nn.Module):
+class ScalableSelfAttention(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -89,12 +94,13 @@         self.to_k = nn.Conv2d(dim, dim_key * heads, reduction_factor, stride = reduction_factor, bias = False)
         self.to_v = nn.Conv2d(dim, dim_value * heads, reduction_factor, stride = reduction_factor, bias = False)
 
-        self.to_out = nn.Sequential(
+        self.to_out = msnn.SequentialCell(
+            [
             nn.Conv2d(dim_value * heads, dim, 1),
             nn.Dropout(dropout)
-        )
-
-    def forward(self, x):
+        ])
+
+    def construct(self, x):
         height, width, heads = *x.shape[-2:], self.heads
 
         x = self.norm(x)
@@ -107,7 +113,7 @@ 
         # similarity
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = mint.matmul(q, k.transpose(-1, -2)) * self.scale
 
         # attention
 
@@ -116,14 +122,14 @@ 
         # aggregate values
 
-        out = torch.matmul(attn, v)
+        out = mint.matmul(attn, v)
 
         # merge back heads
 
         out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = height, y = width)
         return self.to_out(out)
 
-class InteractiveWindowedSelfAttention(nn.Module):
+class InteractiveWindowedSelfAttention(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -147,12 +153,13 @@         self.to_k = nn.Conv2d(dim, dim_key * heads, 1, bias = False)
         self.to_v = nn.Conv2d(dim, dim_value * heads, 1, bias = False)
 
-        self.to_out = nn.Sequential(
+        self.to_out = msnn.SequentialCell(
+            [
             nn.Conv2d(dim_value * heads, dim, 1),
             nn.Dropout(dropout)
-        )
-
-    def forward(self, x):
+        ])
+
+    def construct(self, x):
         height, width, heads, wsz = *x.shape[-2:], self.heads, self.window_size
 
         x = self.norm(x)
@@ -172,7 +179,7 @@ 
         # similarity
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = mint.matmul(q, k.transpose(-1, -2)) * self.scale
 
         # attention
 
@@ -181,7 +188,7 @@ 
         # aggregate values
 
-        out = torch.matmul(attn, v)
+        out = mint.matmul(attn, v)
 
         # reshape the windows back to full feature map (and merge heads)
 
@@ -193,7 +200,7 @@ 
         return self.to_out(out)
 
-class Transformer(nn.Module):
+class Transformer(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -210,11 +217,11 @@         norm_output = True
     ):
         super().__init__()
-        self.layers = nn.ModuleList([])
+        self.layers = msnn.CellList([])
         for ind in range(depth):
             is_first = ind == 0
 
-            self.layers.append(nn.ModuleList([
+            self.layers.append(msnn.CellList([
                 ScalableSelfAttention(dim, heads = heads, dim_key = ssa_dim_key, dim_value = ssa_dim_value, reduction_factor = ssa_reduction_factor, dropout = dropout),
                 FeedForward(dim, expansion_factor = ff_expansion_factor, dropout = dropout),
                 PEG(dim) if is_first else None,
@@ -222,9 +229,9 @@                 InteractiveWindowedSelfAttention(dim, heads = heads, dim_key = iwsa_dim_key, dim_value = iwsa_dim_value, window_size = iwsa_window_size, dropout = dropout)
             ]))
 
-        self.norm = ChanLayerNorm(dim) if norm_output else nn.Identity()
-
-    def forward(self, x):
+        self.norm = ChanLayerNorm(dim) if norm_output else msnn.Identity()
+
+    def construct(self, x):
         for ssa, ff1, peg, iwsa, ff2 in self.layers:
             x = ssa(x) + x
             x = ff1(x) + x
@@ -237,7 +244,7 @@ 
         return self.norm(x)
 
-class ScalableViT(nn.Module):
+class ScalableViT(msnn.Cell):
     def __init__(
         self,
         *,
@@ -276,23 +283,24 @@         hyperparams_per_stage = list(map(partial(cast_tuple, length = num_stages), hyperparams_per_stage))
         assert all(tuple(map(lambda arr: len(arr) == num_stages, hyperparams_per_stage)))
 
-        self.layers = nn.ModuleList([])
+        self.layers = msnn.CellList([])
 
         for ind, (layer_dim, layer_depth, layer_heads, layer_ssa_dim_key, layer_ssa_dim_value, layer_ssa_reduction_factor, layer_iwsa_dim_key, layer_iwsa_dim_value, layer_window_size) in enumerate(zip(dims, depth, *hyperparams_per_stage)):
             is_last = ind == (num_stages - 1)
 
-            self.layers.append(nn.ModuleList([
+            self.layers.append(msnn.CellList([
                 Transformer(dim = layer_dim, depth = layer_depth, heads = layer_heads, ff_expansion_factor = ff_expansion_factor, dropout = dropout, ssa_dim_key = layer_ssa_dim_key, ssa_dim_value = layer_ssa_dim_value, ssa_reduction_factor = layer_ssa_reduction_factor, iwsa_dim_key = layer_iwsa_dim_key, iwsa_dim_value = layer_iwsa_dim_value, iwsa_window_size = layer_window_size, norm_output = not is_last),
                 Downsample(layer_dim, layer_dim * 2) if not is_last else None
             ]))
 
-        self.mlp_head = nn.Sequential(
+        self.mlp_head = msnn.SequentialCell(
+            [
             Reduce('b d h w -> b d', 'mean'),
             nn.LayerNorm(dims[-1]),
             nn.Linear(dims[-1], num_classes)
-        )
-
-    def forward(self, img):
+        ])
+
+    def construct(self, img):
         x = self.to_patches(img)
 
         for transformer, downsample in self.layers:
