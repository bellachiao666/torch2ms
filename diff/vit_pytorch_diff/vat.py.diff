--- pytorch+++ mindspore@@ -1,13 +1,16 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from __future__ import annotations
 from contextlib import nullcontext
 
-import torch
-import torch.nn.functional as F
-from torch import nn, cat, stack, tensor
-from torch.nn import Module, ModuleList
+# import torch
+# from torch import nn, cat, stack, tensor
 
 from einops import rearrange, repeat, pack, unpack
-from einops.layers.torch import Rearrange
+# from einops.layers.torch import Rearrange
 
 # helpers
 
@@ -22,7 +25,7 @@ 
 # classes
 
-class FiLM(Module):
+class FiLM(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -30,20 +33,21 @@         super().__init__()
         proj = nn.Linear(dim, dim * 2)
 
-        self.to_gamma_beta = nn.Sequential(
+        self.to_gamma_beta = msnn.SequentialCell(
+            [
             proj,
             Rearrange('b (two d) -> two b 1 d', two = 2)
-        )
+        ])
 
         nn.init.zeros_(proj.weight)
         nn.init.zeros_(proj.bias)
 
-    def forward(self, tokens, cond):
+    def construct(self, tokens, cond):
         gamma, beta = self.to_gamma_beta(cond)
 
         return tokens * gamma + beta
 
-class FeedForward(Module):
+class FeedForward(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -51,19 +55,20 @@         dropout = 0.
     ):
         super().__init__()
-        self.net = nn.Sequential(
+        self.net = msnn.SequentialCell(
+            [
             nn.LayerNorm(dim),
             nn.Linear(dim, hidden_dim),
             nn.GELU(),
             nn.Dropout(dropout),
             nn.Linear(hidden_dim, dim),
             nn.Dropout(dropout)
-        )
-
-    def forward(self, x):
+        ])
+
+    def construct(self, x):
         return self.net(x)
 
-class Attention(Module):
+class Attention(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -92,12 +97,13 @@         self.to_q = nn.Linear(dim, inner_dim, bias = False)
         self.to_kv = nn.Linear(dim_context, inner_dim * 2, bias = False)
 
-        self.to_out = nn.Sequential(
+        self.to_out = msnn.SequentialCell(
+            [
             nn.Linear(inner_dim, dim),
             nn.Dropout(dropout)
-        ) if project_out else nn.Identity()
-
-    def forward(self, x, context = None):
+        ]) if project_out else msnn.Identity()
+
+    def construct(self, x, context = None):
 
         assert not (self.cross_attend ^ exists(context)), 'context must be passed in if cross attending, or vice versa'
 
@@ -116,16 +122,16 @@         qkv = (self.to_q(x), *self.to_kv(kv_input).chunk(2, dim = -1))
         q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)
 
-        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
+        dots = mint.matmul(q, k.transpose(-1, -2)) * self.scale
 
         attn = self.attend(dots)
         attn = self.dropout(attn)
 
-        out = torch.matmul(attn, v)
+        out = mint.matmul(attn, v)
         out = rearrange(out, 'b h n d -> b n (h d)')
         return self.to_out(out)
 
-class Transformer(Module):
+class Transformer(msnn.Cell):
     def __init__(
         self,
         dim,
@@ -137,15 +143,15 @@     ):
         super().__init__()
         self.norm = nn.LayerNorm(dim)
-        self.layers = ModuleList([])
+        self.layers = msnn.CellList([])
 
         for _ in range(depth):
-            self.layers.append(ModuleList([
+            self.layers.append(msnn.CellList([
                 Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),
                 FeedForward(dim, mlp_dim, dropout = dropout)
             ]))
 
-    def forward(
+    def construct(
         self,
         x,
         return_hiddens = False
@@ -166,7 +172,7 @@ 
         return x, hiddens
 
-class ViT(Module):
+class ViT(msnn.Cell):
     def __init__(
         self,
         *,
@@ -197,27 +203,28 @@         patch_dim = channels * patch_height * patch_width
         assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
 
-        self.to_patch_embedding = nn.Sequential(
+        self.to_patch_embedding = msnn.SequentialCell(
+            [
             Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
             nn.LayerNorm(patch_dim),
             nn.Linear(patch_dim, dim),
-            nn.LayerNorm(dim),
-        )
-
-        self.pos_embedding = nn.Parameter(torch.randn(num_patches, dim))
-        self.cls_token = nn.Parameter(torch.randn(dim))
+            nn.LayerNorm(dim)
+        ])
+
+        self.pos_embedding = ms.Parameter(mint.randn(size = (num_patches, dim)))
+        self.cls_token = ms.Parameter(mint.randn(dim))
         self.dropout = nn.Dropout(emb_dropout)
 
         self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)
 
         self.pool = pool
-        self.to_latent = nn.Identity()
+        self.to_latent = msnn.Identity()
 
         self.mlp_head = nn.Linear(dim, num_classes)
 
-        self.register_tokens = nn.Parameter(torch.randn(num_register_tokens, dim) * 1e-2)
-
-    def forward(self, img, return_hiddens = False):
+        self.register_tokens = ms.Parameter(mint.randn(size = (num_register_tokens, dim)) * 1e-2)
+
+    def construct(self, img, return_hiddens = False):
         x = self.to_patch_embedding(img)
         b, n, _ = x.shape
 
@@ -235,7 +242,7 @@         # return the representation trajectory
 
         if return_hiddens:
-            return x, stack(hiddens)
+            return x, mint.stack(hiddens)
 
         register_tokens, cls_tokens, x = unpack(x, packed_shape, 'b * d')
 
@@ -249,7 +256,7 @@ # https://openreview.net/forum?id=TalHOvvLZu
 # simple way to get SOTA on Libero dataset (beating fine-tuned pi-zero)
 
-class VAT(Module):
+class VAT(msnn.Cell):
     def __init__(
         self,
         vit: ViT | dict,
@@ -295,34 +302,34 @@ 
         self.is_video = is_video
         self.time_seq_len = time_seq_len
-        self.time_pos_emb = nn.Parameter(torch.randn(time_seq_len, vit_dim) * 1e-2) if is_video else None
+        self.time_pos_emb = ms.Parameter(mint.randn(size = (time_seq_len, vit_dim)) * 1e-2) if is_video else None
 
         # maybe view embeddings
 
-        self.view_emb = nn.Parameter(torch.randn(num_views, vit_dim) * 1e-2) if exists(num_views) and num_views > 1 else None
+        self.view_emb = ms.Parameter(mint.randn(size = (num_views, vit_dim)) * 1e-2) if exists(num_views) and num_views > 1 else None
 
         # handle maybe task conditioning
 
         self.has_tasks = exists(num_tasks)
 
         if self.has_tasks:
-            self.task_emb = nn.Parameter(torch.randn(num_tasks, dim) * 1e-2)
+            self.task_emb = ms.Parameter(mint.randn(size = (num_tasks, dim)) * 1e-2)
 
         # register tokens from Darcet et al.
 
-        self.register_tokens = nn.Parameter(torch.randn(num_register_tokens, dim) * 1e-2)
+        self.register_tokens = ms.Parameter(mint.randn(size = (num_register_tokens, dim)) * 1e-2)
 
         # to action tokens
 
-        self.action_pos_emb = nn.Parameter(torch.randn(action_chunk_len, dim) * 1e-2)
-
-        self.layers = ModuleList([])
+        self.action_pos_emb = ms.Parameter(mint.randn(size = (action_chunk_len, dim)) * 1e-2)
+
+        self.layers = msnn.CellList([])
 
         for _ in range(depth):
             maybe_film = FiLM(dim = dim) if self.has_tasks else None
             maybe_self_attn = Attention(dim = dim, heads = self_attn_heads, dim_head = self_attn_dim_head, dropout = dropout) if add_self_attn else None
 
-            self.layers.append(ModuleList([
+            self.layers.append(msnn.CellList([
                 maybe_film,
                 maybe_self_attn,
                 Attention(dim = dim, dim_context = vit_dim, heads = heads, dim_head = dim_head, dropout = dropout, cross_attend = True),
@@ -339,7 +346,7 @@         if exists(dim_extra_token):
             self.to_extra_token = nn.Linear(dim_extra_token, dim)
 
-    def forward(
+    def construct(
         self,
         video_or_image,   # (b v? c t? h w) - batch, views [wrist + third person or more], channels, maybe time, height, width
         *,
@@ -380,7 +387,7 @@         with vit_forward_context():
             embed, hiddens = self.vit(images, return_hiddens = True)
 
-        hiddens = cat((hiddens, embed[None, ...]))
+        hiddens = mint.cat((hiddens, embed[None, ...]))
 
         # extract the hiddens needed for the action cross attention
 
@@ -471,13 +478,13 @@             if not return_hiddens:
                 return pred_action
 
-            return pred_action, stack(hiddens)
+            return pred_action, mint.stack(hiddens)
 
         assert pred_action.shape[1] == actions.shape[1]
 
         # they found l1 loss suffices
 
-        return F.l1_loss(pred_action, actions)
+        return nn.functional.l1_loss(pred_action, actions)
 
 # quick test
 
@@ -512,11 +519,11 @@         )
     )
 
-    images = torch.randn(2, 2, 3, 4, 256, 256) # (2 views with 4 frames)
-    tasks = torch.randint(0, 4, (2,))
-    extra = torch.randn(2, 33)                 # extra internal state
-
-    actions = torch.randn(2, 7, 20)         # actions for learning
+    images = mint.randn(size = (2, 2, 3, 4, 256, 256)) # (2 views with 4 frames)
+    tasks = mint.randint(0, 4, (2,))
+    extra = mint.randn(size = (2, 33))                 # extra internal state
+
+    actions = mint.randn(size = (2, 7, 20))         # actions for learning
 
     loss = vat(images, actions = actions, tasks = tasks, extra = extra, freeze_vit = True)
     loss.backward()
