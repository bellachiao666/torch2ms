--- pytorch+++ mindspore@@ -1,12 +1,13 @@+import mindspore as ms
+import mindspore.nn as msnn
+import mindspore.ops as msops
+import mindspore.mint as mint
+from mindspore.mint import nn, ops
 from random import randrange
-
-import torch
-from torch import nn, einsum
-from torch.nn import Module, ModuleList
-import torch.nn.functional as F
+# from torch import nn, einsum
 
 from einops import rearrange, repeat, pack, unpack
-from einops.layers.torch import Rearrange
+# from einops.layers.torch import Rearrange
 
 # helpers
 
@@ -20,14 +21,14 @@     return unpack(t, ps, pattern)[0]
 
 def l2norm(t):
-    return F.normalize(t, dim = -1, p = 2)
+    return nn.functional.normalize(t, p = 2, dim = -1)
 
 def dropout_layers(layers, dropout):
     if dropout == 0:
         return layers
 
     num_layers = len(layers)
-    to_drop = torch.zeros(num_layers).uniform_(0., 1.) < dropout
+    to_drop = mint.zeros(num_layers).uniform_(0., 1.) < dropout
 
     # make sure at least one layer makes it
     if all(to_drop):
@@ -39,7 +40,7 @@ 
 # classes
 
-class LayerScale(Module):
+class LayerScale(msnn.Cell):
     def __init__(self, dim, fn, depth):
         super().__init__()
         if depth <= 18:
@@ -50,26 +51,27 @@             init_eps = 1e-6
 
         self.fn = fn
-        self.scale = nn.Parameter(torch.full((dim,), init_eps))
-
-    def forward(self, x, **kwargs):
+        self.scale = ms.Parameter(mint.full(size = ((dim,), init_eps)))
+
+    def construct(self, x, **kwargs):
         return self.fn(x, **kwargs) * self.scale
 
-class FeedForward(Module):
+class FeedForward(msnn.Cell):
     def __init__(self, dim, hidden_dim, dropout = 0.):
         super().__init__()
-        self.net = nn.Sequential(
+        self.net = msnn.SequentialCell(
+            [
             nn.LayerNorm(dim),
             nn.Linear(dim, hidden_dim),
             nn.GELU(),
             nn.Dropout(dropout),
             nn.Linear(hidden_dim, dim),
             nn.Dropout(dropout)
-        )
-    def forward(self, x):
+        ])
+    def construct(self, x):
         return self.net(x)
 
-class Attention(Module):
+class Attention(msnn.Cell):
     def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
         super().__init__()
         inner_dim = dim_head * heads
@@ -83,30 +85,31 @@         self.attend = nn.Softmax(dim = -1)
         self.dropout = nn.Dropout(dropout)
 
-        self.to_out = nn.Sequential(
+        self.to_out = msnn.SequentialCell(
+            [
             nn.Linear(inner_dim, dim),
             nn.Dropout(dropout)
-        )
-
-    def forward(self, x, context = None):
+        ])
+
+    def construct(self, x, context = None):
         h = self.heads
 
         x = self.norm(x)
-        context = x if not exists(context) else torch.cat((x, context), dim = 1)
+        context = x if not exists(context) else mint.cat((x, context), dim = 1)
 
         qkv = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))
         q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)
 
-        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale
+        sim = mint.einsum('b h i d, b h j d -> b h i j', q, k) * self.scale
 
         attn = self.attend(sim)
         attn = self.dropout(attn)
 
-        out = einsum('b h i j, b h j d -> b h i d', attn, v)
+        out = mint.einsum('b h i j, b h j d -> b h i d', attn, v)
         out = rearrange(out, 'b h n d -> b n (h d)')
         return self.to_out(out)
 
-class XCAttention(Module):
+class XCAttention(msnn.Cell):
     def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
         super().__init__()
         inner_dim = dim_head * heads
@@ -115,17 +118,18 @@ 
         self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)
 
-        self.temperature = nn.Parameter(torch.ones(heads, 1, 1))
+        self.temperature = ms.Parameter(mint.ones(size = (heads, 1, 1)))
 
         self.attend = nn.Softmax(dim = -1)
         self.dropout = nn.Dropout(dropout)
 
-        self.to_out = nn.Sequential(
+        self.to_out = msnn.SequentialCell(
+            [
             nn.Linear(inner_dim, dim),
             nn.Dropout(dropout)
-        )
-
-    def forward(self, x):
+        ])
+
+    def construct(self, x):
         h = self.heads
         x, ps = pack_one(x, 'b * d')
 
@@ -136,50 +140,51 @@ 
         q, k = map(l2norm, (q, k))
 
-        sim = einsum('b h i n, b h j n -> b h i j', q, k) * self.temperature.exp()
+        sim = mint.einsum('b h i n, b h j n -> b h i j', q, k) * self.temperature.exp()
 
         attn = self.attend(sim)
         attn = self.dropout(attn)
 
-        out = einsum('b h i j, b h j n -> b h i n', attn, v)
+        out = mint.einsum('b h i j, b h j n -> b h i n', attn, v)
         out = rearrange(out, 'b h d n -> b n (h d)')
 
         out = unpack_one(out, ps, 'b * d')
         return self.to_out(out)
 
-class LocalPatchInteraction(Module):
+class LocalPatchInteraction(msnn.Cell):
     def __init__(self, dim, kernel_size = 3):
         super().__init__()
         assert (kernel_size % 2) == 1
         padding = kernel_size // 2
 
-        self.net = nn.Sequential(
+        self.net = msnn.SequentialCell(
+            [
             nn.LayerNorm(dim),
             Rearrange('b h w c -> b c h w'),
             nn.Conv2d(dim, dim, kernel_size, padding = padding, groups = dim),
             nn.BatchNorm2d(dim),
             nn.GELU(),
             nn.Conv2d(dim, dim, kernel_size, padding = padding, groups = dim),
-            Rearrange('b c h w -> b h w c'),
-        )
-
-    def forward(self, x):
+            Rearrange('b c h w -> b h w c')
+        ])
+
+    def construct(self, x):
         return self.net(x)
 
-class Transformer(Module):
+class Transformer(msnn.Cell):
     def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0., layer_dropout = 0.):
         super().__init__()
-        self.layers = ModuleList([])
+        self.layers = msnn.CellList([])
         self.layer_dropout = layer_dropout
 
         for ind in range(depth):
             layer = ind + 1
-            self.layers.append(ModuleList([
+            self.layers.append(msnn.CellList([
                 LayerScale(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout), depth = layer),
                 LayerScale(dim, FeedForward(dim, mlp_dim, dropout = dropout), depth = layer)
             ]))
 
-    def forward(self, x, context = None):
+    def construct(self, x, context = None):
         layers = dropout_layers(self.layers, dropout = self.layer_dropout)
 
         for attn, ff in layers:
@@ -188,21 +193,21 @@ 
         return x
 
-class XCATransformer(Module):
+class XCATransformer(msnn.Cell):
     def __init__(self, dim, depth, heads, dim_head, mlp_dim, local_patch_kernel_size = 3, dropout = 0., layer_dropout = 0.):
         super().__init__()
-        self.layers = ModuleList([])
+        self.layers = msnn.CellList([])
         self.layer_dropout = layer_dropout
 
         for ind in range(depth):
             layer = ind + 1
-            self.layers.append(ModuleList([
+            self.layers.append(msnn.CellList([
                 LayerScale(dim, XCAttention(dim, heads = heads, dim_head = dim_head, dropout = dropout), depth = layer),
                 LayerScale(dim, LocalPatchInteraction(dim, local_patch_kernel_size), depth = layer),
                 LayerScale(dim, FeedForward(dim, mlp_dim, dropout = dropout), depth = layer)
             ]))
 
-    def forward(self, x):
+    def construct(self, x):
         layers = dropout_layers(self.layers, dropout = self.layer_dropout)
 
         for cross_covariance_attn, local_patch_interaction, ff in layers:
@@ -212,7 +217,7 @@ 
         return x
 
-class XCiT(Module):
+class XCiT(msnn.Cell):
     def __init__(
         self,
         *,
@@ -236,15 +241,16 @@         num_patches = (image_size // patch_size) ** 2
         patch_dim = 3 * patch_size ** 2
 
-        self.to_patch_embedding = nn.Sequential(
+        self.to_patch_embedding = msnn.SequentialCell(
+            [
             Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1 = patch_size, p2 = patch_size),
             nn.LayerNorm(patch_dim),
             nn.Linear(patch_dim, dim),
             nn.LayerNorm(dim)
-        )
-
-        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))
-        self.cls_token = nn.Parameter(torch.randn(dim))
+        ])
+
+        self.pos_embedding = ms.Parameter(mint.randn(size = (1, num_patches, dim)))
+        self.cls_token = ms.Parameter(mint.randn(dim))
 
         self.dropout = nn.Dropout(emb_dropout)
 
@@ -254,12 +260,13 @@ 
         self.cls_transformer = Transformer(dim, cls_depth, heads, dim_head, mlp_dim, dropout, layer_dropout)
 
-        self.mlp_head = nn.Sequential(
+        self.mlp_head = msnn.SequentialCell(
+            [
             nn.LayerNorm(dim),
             nn.Linear(dim, num_classes)
-        )
-
-    def forward(self, img):
+        ])
+
+    def construct(self, img):
         x = self.to_patch_embedding(img)
 
         x, ps = pack_one(x, 'b * d')
