--- pytorch+++ mindspore@@ -17,9 +17,9 @@ 
         # 基础卷积
         self.stem = nn.Sequential(
-            nn.Conv2d(in_channels, 32, kernel_size=3, padding=1, bias=False),
-            nn.BatchNorm2d(32),
-            nn.ReLU(inplace=True),
+            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1, bias=False),
+            nn.BatchNorm2d(num_features=32),
+            nn.ReLU()  # 没有对应的mindspore参数 'inplace',
         )
 
         # 测试 mint.nn 前缀，你的转换器需要把这些都转换掉
@@ -31,11 +31,11 @@                 kernel_size=3,
                 stride=1,
                 padding=1,
-                bias=nn.Linear(1, 1)  # 参数中嵌套 API
+                bias=nn.Linear(in_features=1, out_features=1)  # 参数中嵌套 API
             ),
 
             nn.Sequential(
-                nn.ReLU(inplace=True),
+                nn.ReLU()  # 没有对应的mindspore参数 'inplace',
                 nn.Conv2d(
                     in_channels=64,
                     out_channels=64,
@@ -57,9 +57,9 @@ 
         # 全连接部分
         self.classifier = nn.Sequential(
-            nn.Linear(128 * 7 * 7, num_classes),
-            nn.ReLU(inplace=True),
-            nn.Linear(num_classes, num_classes)
+            nn.Linear(in_features=128 * 7 * 7, out_features=num_classes),
+            nn.ReLU()  # 没有对应的mindspore参数 'inplace',
+            nn.Linear(in_features=num_classes, out_features=num_classes)
         )
 
 
@@ -82,6 +82,6 @@         x = self.classifier(x)
 
         # 测试 F.xxx 不应误伤
-        x = F.softmax(x, dim=1)
+        x = nn.softmax(x=x, axis=1)  # 默认参数名不一致: axis (PyTorch=dim, MindSpore=axis)
 
         return x
